{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76bffd5b",
      "metadata": {},
      "source": [
        "<small>\n",
        "\n",
        "**Key differences from JAX implementation:**  \n",
        "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
        "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
        "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
        "\n",
        "</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_datasets as tfds\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "558de4e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST from TensorFlow Datasets\n",
        "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0fb40841",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalise(x, x_max=255.0):\n",
        "    return x / x_max\n",
        "\n",
        "def convert_to_jax(data_np, data_type):\n",
        "    if data_type == \"image\":\n",
        "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
        "    elif data_type == \"label\":\n",
        "        data_jax = jnp.array(data_np)\n",
        "    else:\n",
        "        raise ValueError(\"not image or label\")\n",
        "    return data_jax\n",
        "\n",
        "def flatten_image_for_mlp(data_jax):\n",
        "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
        "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
        "    data_flattened = data_jax.reshape(n_batch, -1)\n",
        "    return data_flattened\n",
        "\n",
        "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
        "    data_jax = {}\n",
        "    for data_type, data_tf in data_dict.items():\n",
        "        data_numpy = data_tf.numpy()\n",
        "        data = convert_to_jax(data_numpy, data_type)\n",
        "        if data_type == \"image\":\n",
        "            data = flatten_image_for_mlp(data)\n",
        "        if subsample_size > 0:\n",
        "            data = data[:subsample_size]\n",
        "        data_jax[data_type] = data\n",
        "\n",
        "    return data_jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "244a19b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
        "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
        "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
        "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
        "    return cross_entropy_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fa6224c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_training_step(training_state, images, labels):\n",
        "    \"\"\"\n",
        "    Single training step \n",
        "    The model and optimiser are passed in the training state\n",
        "    returns a training state\n",
        "    \"\"\"\n",
        "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
        "        training_state.apply_fn,\n",
        "        images,\n",
        "        labels,\n",
        "    )\n",
        "    return training_state.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batches(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "27fcb22b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(layer_sizes, optimizer):\n",
        "    layer_part = \"mlp_\" + \"-\".join(str(s) for s in layer_sizes)\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"{layer_part}_{opt_name}\"\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"mlp\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "72c149a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    images,\n",
        "    labels,\n",
        "    n_steps,\n",
        "    layer_sizes,\n",
        "    optimizer,\n",
        "    checkpoint_manager,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    training_state,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    ): \n",
        "    \"\"\"\n",
        "    The training state ('state') is an instance of TrainState that holds:\n",
        "    - apply_fn: the model's apply function, used for forward passes\n",
        "    - params: the parameters of the neural network\n",
        "    - tx: the optimizers (Optax transformation) for parameter updates\n",
        "    - opt_state: the state of the optimizer\n",
        "    \"\"\"\n",
        "    if training_state is None:\n",
        "        training_state = create_training_state(\n",
        "            layer_sizes,\n",
        "            optimizer,\n",
        "            key,\n",
        "            use_lowrank=use_lowrank,\n",
        "            rank=rank,\n",
        "        )\n",
        "\n",
        "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
        "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
        "        step = training_state.step\n",
        "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
        "        print(f\"step {step}: loss={loss}\")\n",
        "        if step == 1 or step % steps_per_save == 0:\n",
        "            step_dir = step\n",
        "            checkpoint_manager.save(\n",
        "                step_dir,\n",
        "                args=ocp.args.StandardSave(training_state)\n",
        "                )\n",
        "\n",
        "    return training_state.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "852ef6e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    n_steps=10**3,\n",
        "    steps_per_save=100,\n",
        "    training_state=None,\n",
        "    key=jax.random.key(0),\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=(784, 128, 10),\n",
        "):\n",
        "    layer_sizes = list(layer_sizes)\n",
        "    experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        experiment_name = experiment_name + f\"_lowrank-r{rank}\"\n",
        "\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    final_params = run_training(\n",
        "        images=train_data[\"image\"],\n",
        "        labels=train_data[\"label\"],\n",
        "        n_steps=n_steps,\n",
        "        layer_sizes=layer_sizes,\n",
        "        optimizer=optimizer,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        training_state=training_state,\n",
        "        use_lowrank=use_lowrank,\n",
        "        rank=rank,\n",
        "    )\n",
        "    return final_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "dca913e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_layer_sizes(params):\n",
        "    layer_sizes = []\n",
        "    for layer, layer_params in enumerate(params.values()):\n",
        "        if layer == 0:\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
        "        else:\n",
        "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
        "    return layer_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1b7d5d50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mlp(\n",
        "    test_data,\n",
        "    params,\n",
        "    n_examples=10,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=None,\n",
        "):\n",
        "    images = test_data[\"image\"]\n",
        "    labels = test_data[\"label\"]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if layer_sizes is None:\n",
        "            raise ValueError(\"layer_sizes must be provided when use_lowrank=True\")\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=layer_sizes[1:], rank=rank)\n",
        "    else:\n",
        "        layer_sizes = extract_layer_sizes(params)\n",
        "        model = MLP(layer_sizes=layer_sizes[1:])\n",
        "\n",
        "    apply_fn = model.apply\n",
        "\n",
        "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
        "    example_images = images[:n_examples]\n",
        "    example_labels = labels[:n_examples]\n",
        "    logits = apply_fn({\"params\": params}, example_images)\n",
        "    example_predictions = jnp.argmax(logits, axis=1)\n",
        "\n",
        "    prefix = \"[low-rank] \" if use_lowrank else \"\"\n",
        "    print(prefix + \"Mean loss       \", mean_loss)\n",
        "    print(prefix + \"True labels:    \", example_labels)\n",
        "    print(prefix + \"Predictions:    \", example_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b876ad27",
      "metadata": {},
      "source": [
        "1. Learning rate decay\n",
        "2. Weight decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
        "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b492873c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=2.2421865463256836\n",
            "step 2: loss=1.4549815654754639\n",
            "step 3: loss=3.0767364501953125\n",
            "step 4: loss=1.6018767356872559\n",
            "step 5: loss=1.9223350286483765\n",
            "step 6: loss=1.551820993423462\n",
            "step 7: loss=2.2047572135925293\n",
            "step 8: loss=2.283871650695801\n",
            "step 9: loss=1.3409205675125122\n",
            "step 10: loss=2.344090700149536\n",
            "step 11: loss=2.6371212005615234\n",
            "step 12: loss=3.4621238708496094\n",
            "step 13: loss=1.713283658027649\n",
            "step 14: loss=1.9781702756881714\n",
            "step 15: loss=3.341600179672241\n",
            "step 16: loss=2.9397130012512207\n",
            "step 17: loss=1.0316028594970703\n",
            "step 18: loss=2.92775821685791\n",
            "step 19: loss=2.7445106506347656\n",
            "step 20: loss=1.4158217906951904\n",
            "step 21: loss=1.9610297679901123\n",
            "step 22: loss=1.998666524887085\n",
            "step 23: loss=1.672989845275879\n",
            "step 24: loss=2.457414150238037\n",
            "step 25: loss=1.5557224750518799\n",
            "step 26: loss=2.3051018714904785\n",
            "step 27: loss=2.603789806365967\n",
            "step 28: loss=1.988590955734253\n",
            "step 29: loss=2.3711462020874023\n",
            "step 30: loss=1.6227277517318726\n",
            "step 31: loss=1.8622857332229614\n",
            "step 32: loss=2.1689186096191406\n",
            "step 33: loss=1.7543294429779053\n",
            "step 34: loss=1.3137750625610352\n",
            "step 35: loss=1.7799021005630493\n",
            "step 36: loss=1.6797170639038086\n",
            "step 37: loss=2.9055235385894775\n",
            "step 38: loss=1.509118914604187\n",
            "step 39: loss=2.635629892349243\n",
            "step 40: loss=1.9035999774932861\n",
            "step 41: loss=0.833787202835083\n",
            "step 42: loss=2.479337692260742\n",
            "step 43: loss=2.0102224349975586\n",
            "step 44: loss=1.8198356628417969\n",
            "step 45: loss=2.361449956893921\n",
            "step 46: loss=1.0358593463897705\n",
            "step 47: loss=2.0157792568206787\n",
            "step 48: loss=1.8103774785995483\n",
            "step 49: loss=3.01554536819458\n",
            "step 50: loss=1.4894686937332153\n",
            "step 51: loss=1.1730493307113647\n",
            "step 52: loss=1.2065201997756958\n",
            "step 53: loss=3.2560641765594482\n",
            "step 54: loss=1.3365161418914795\n",
            "step 55: loss=1.2758264541625977\n",
            "step 56: loss=2.6482057571411133\n",
            "step 57: loss=2.5407874584198\n",
            "step 58: loss=1.3128734827041626\n",
            "step 59: loss=2.506495714187622\n",
            "step 60: loss=1.0716588497161865\n",
            "step 61: loss=0.8002647161483765\n",
            "step 62: loss=1.2150105237960815\n",
            "step 63: loss=3.913295269012451\n",
            "step 64: loss=2.5136899948120117\n",
            "step 65: loss=2.394390821456909\n",
            "step 66: loss=0.41791653633117676\n",
            "step 67: loss=0.9733763933181763\n",
            "step 68: loss=0.6354427337646484\n",
            "step 69: loss=3.0767982006073\n",
            "step 70: loss=1.309507131576538\n",
            "step 71: loss=3.29673171043396\n",
            "step 72: loss=3.5359926223754883\n",
            "step 73: loss=0.5057766437530518\n",
            "step 74: loss=1.0847032070159912\n",
            "step 75: loss=0.8231463432312012\n",
            "step 76: loss=0.12777495384216309\n",
            "step 77: loss=2.5281646251678467\n",
            "step 78: loss=2.692049980163574\n",
            "step 79: loss=1.4572802782058716\n",
            "step 80: loss=0.3264431953430176\n",
            "step 81: loss=0.3224921226501465\n",
            "step 82: loss=0.8804373741149902\n",
            "step 83: loss=1.687135100364685\n",
            "step 84: loss=0.7689778804779053\n",
            "step 85: loss=2.236729621887207\n",
            "step 86: loss=0.39814281463623047\n",
            "step 87: loss=2.505405902862549\n",
            "step 88: loss=1.0165311098098755\n",
            "step 89: loss=1.1767767667770386\n",
            "step 90: loss=1.611859679222107\n",
            "step 91: loss=1.302021861076355\n",
            "step 92: loss=1.0679254531860352\n",
            "step 93: loss=0.22047114372253418\n",
            "step 94: loss=1.8968678712844849\n",
            "step 95: loss=0.7754878997802734\n",
            "step 96: loss=2.2688071727752686\n",
            "step 97: loss=1.6609829664230347\n",
            "step 98: loss=1.7149423360824585\n",
            "step 99: loss=0.8860684633255005\n",
            "step 100: loss=1.0432376861572266\n",
            "step 101: loss=1.6429299116134644\n",
            "step 102: loss=1.3760747909545898\n",
            "step 103: loss=2.3424901962280273\n",
            "step 104: loss=0.7394160032272339\n",
            "step 105: loss=0.8303948640823364\n",
            "step 106: loss=1.688915491104126\n",
            "step 107: loss=1.8497143983840942\n",
            "step 108: loss=0.8009697198867798\n",
            "step 109: loss=1.921038269996643\n",
            "step 110: loss=1.5583200454711914\n",
            "step 111: loss=0.6788965463638306\n",
            "step 112: loss=0.4137570858001709\n",
            "step 113: loss=1.6254088878631592\n",
            "step 114: loss=1.488877773284912\n",
            "step 115: loss=0.21036005020141602\n",
            "step 116: loss=0.943779706954956\n",
            "step 117: loss=1.3162198066711426\n",
            "step 118: loss=0.7798060178756714\n",
            "step 119: loss=0.9784128665924072\n",
            "step 120: loss=1.2214596271514893\n",
            "step 121: loss=3.1054036617279053\n",
            "step 122: loss=1.6958218812942505\n",
            "step 123: loss=2.7126429080963135\n",
            "step 124: loss=0.5880529880523682\n",
            "step 125: loss=0.2147374153137207\n",
            "step 126: loss=1.36958909034729\n",
            "step 127: loss=1.7903406620025635\n",
            "step 128: loss=0.8010847568511963\n",
            "step 129: loss=0.47148561477661133\n",
            "step 130: loss=0.42186450958251953\n",
            "step 131: loss=0.9197474718093872\n",
            "step 132: loss=2.370388984680176\n",
            "step 133: loss=0.7040364742279053\n",
            "step 134: loss=0.6195163726806641\n",
            "step 135: loss=0.8623491525650024\n",
            "step 136: loss=0.2626211643218994\n",
            "step 137: loss=0.47997212409973145\n",
            "step 138: loss=0.6141397953033447\n",
            "step 139: loss=1.0177690982818604\n",
            "step 140: loss=1.6761689186096191\n",
            "step 141: loss=0.37014293670654297\n",
            "step 142: loss=2.50685977935791\n",
            "step 143: loss=0.47397661209106445\n",
            "step 144: loss=1.822238802909851\n",
            "step 145: loss=0.10611200332641602\n",
            "step 146: loss=1.3444865942001343\n",
            "step 147: loss=1.8902567625045776\n",
            "step 148: loss=2.019211769104004\n",
            "step 149: loss=0.7089798450469971\n",
            "step 150: loss=0.3448455333709717\n",
            "step 151: loss=1.4451549053192139\n",
            "step 152: loss=0.8879592418670654\n",
            "step 153: loss=1.6465026140213013\n",
            "step 154: loss=1.2734512090682983\n",
            "step 155: loss=0.8892343044281006\n",
            "step 156: loss=0.8326754570007324\n",
            "step 157: loss=1.032266616821289\n",
            "step 158: loss=1.7533609867095947\n",
            "step 159: loss=0.9787423610687256\n",
            "step 160: loss=1.5738199949264526\n",
            "step 161: loss=0.48970139026641846\n",
            "step 162: loss=0.15803861618041992\n",
            "step 163: loss=0.7275892496109009\n",
            "step 164: loss=0.5339553356170654\n",
            "step 165: loss=1.5419628620147705\n",
            "step 166: loss=0.4611384868621826\n",
            "step 167: loss=2.06067156791687\n",
            "step 168: loss=2.2312281131744385\n",
            "step 169: loss=0.656808614730835\n",
            "step 170: loss=0.4211156368255615\n",
            "step 171: loss=0.1600961685180664\n",
            "step 172: loss=0.6779334545135498\n",
            "step 173: loss=0.1400456428527832\n",
            "step 174: loss=0.704398512840271\n",
            "step 175: loss=2.044097423553467\n",
            "step 176: loss=0.5685373544692993\n",
            "step 177: loss=0.7488731145858765\n",
            "step 178: loss=0.2943863868713379\n",
            "step 179: loss=0.2867746353149414\n",
            "step 180: loss=2.0111539363861084\n",
            "step 181: loss=0.5551998615264893\n",
            "step 182: loss=0.42223334312438965\n",
            "step 183: loss=0.4737052917480469\n",
            "step 184: loss=2.3058724403381348\n",
            "step 185: loss=0.40650391578674316\n",
            "step 186: loss=0.4796104431152344\n",
            "step 187: loss=0.6871424913406372\n",
            "step 188: loss=2.680981159210205\n",
            "step 189: loss=0.7373931407928467\n",
            "step 190: loss=0.6501474380493164\n",
            "step 191: loss=0.44893407821655273\n",
            "step 192: loss=0.12216806411743164\n",
            "step 193: loss=0.12809205055236816\n",
            "step 194: loss=0.356212854385376\n",
            "step 195: loss=0.060632944107055664\n",
            "step 196: loss=1.4033048152923584\n",
            "step 197: loss=1.2201972007751465\n",
            "step 198: loss=0.2856616973876953\n",
            "step 199: loss=2.5699079036712646\n",
            "step 200: loss=1.3816466331481934\n",
            "step 201: loss=0.4719282388687134\n",
            "step 202: loss=2.0478813648223877\n",
            "step 203: loss=0.4015340805053711\n",
            "step 204: loss=0.14312219619750977\n",
            "step 205: loss=0.49718475341796875\n",
            "step 206: loss=0.7516833543777466\n",
            "step 207: loss=0.17197656631469727\n",
            "step 208: loss=0.5553795099258423\n",
            "step 209: loss=0.37429213523864746\n",
            "step 210: loss=0.7562178373336792\n",
            "step 211: loss=0.15678930282592773\n",
            "step 212: loss=1.6474082469940186\n",
            "step 213: loss=1.2462327480316162\n",
            "step 214: loss=1.5997607707977295\n",
            "step 215: loss=0.15593624114990234\n",
            "step 216: loss=0.29805707931518555\n",
            "step 217: loss=0.29372286796569824\n",
            "step 218: loss=0.15705633163452148\n",
            "step 219: loss=0.22482776641845703\n",
            "step 220: loss=0.9453974962234497\n",
            "step 221: loss=1.1685105562210083\n",
            "step 222: loss=2.3041563034057617\n",
            "step 223: loss=0.9989071488380432\n",
            "step 224: loss=0.7327048778533936\n",
            "step 225: loss=0.37357234954833984\n",
            "step 226: loss=0.692794919013977\n",
            "step 227: loss=0.42007970809936523\n",
            "step 228: loss=0.535027265548706\n",
            "step 229: loss=0.6040942668914795\n",
            "step 230: loss=0.569710373878479\n",
            "step 231: loss=0.8951532244682312\n",
            "step 232: loss=0.281496524810791\n",
            "step 233: loss=0.0047473907470703125\n",
            "step 234: loss=2.366014003753662\n",
            "step 235: loss=0.27408552169799805\n",
            "step 236: loss=1.1047797203063965\n",
            "step 237: loss=0.040659427642822266\n",
            "step 238: loss=0.22864508628845215\n",
            "step 239: loss=0.3796803951263428\n",
            "step 240: loss=0.34667134284973145\n",
            "step 241: loss=0.4054981470108032\n",
            "step 242: loss=0.1246330738067627\n",
            "step 243: loss=0.5560510158538818\n",
            "step 244: loss=0.44894957542419434\n",
            "step 245: loss=0.4151647090911865\n",
            "step 246: loss=0.9324599504470825\n",
            "step 247: loss=0.44788575172424316\n",
            "step 248: loss=3.0575428009033203\n",
            "step 249: loss=0.05451393127441406\n",
            "step 250: loss=0.06193351745605469\n",
            "step 251: loss=2.078672170639038\n",
            "step 252: loss=0.5296134948730469\n",
            "step 253: loss=0.9901062250137329\n",
            "step 254: loss=0.09525799751281738\n",
            "step 255: loss=0.07671642303466797\n",
            "step 256: loss=0.489521861076355\n",
            "step 257: loss=2.7890677452087402\n",
            "step 258: loss=0.3984379768371582\n",
            "step 259: loss=0.8330891132354736\n",
            "step 260: loss=0.47151803970336914\n",
            "step 261: loss=0.9831005334854126\n",
            "step 262: loss=2.733567953109741\n",
            "step 263: loss=0.32002758979797363\n",
            "step 264: loss=0.1883378028869629\n",
            "step 265: loss=0.4457814693450928\n",
            "step 266: loss=0.6294000148773193\n",
            "step 267: loss=0.4692997932434082\n",
            "step 268: loss=0.7847534418106079\n",
            "step 269: loss=0.032675743103027344\n",
            "step 270: loss=0.6592975854873657\n",
            "step 271: loss=0.03919816017150879\n",
            "step 272: loss=0.7018623352050781\n",
            "step 273: loss=0.5264933109283447\n",
            "step 274: loss=0.12643933296203613\n",
            "step 275: loss=1.0928401947021484\n",
            "step 276: loss=0.03591275215148926\n",
            "step 277: loss=0.29474592208862305\n",
            "step 278: loss=0.43141698837280273\n",
            "step 279: loss=0.4353444576263428\n",
            "step 280: loss=0.35237348079681396\n",
            "step 281: loss=0.08173036575317383\n",
            "step 282: loss=0.05996131896972656\n",
            "step 283: loss=1.4846302270889282\n",
            "step 284: loss=1.8887834548950195\n",
            "step 285: loss=0.4659996032714844\n",
            "step 286: loss=0.2334282398223877\n",
            "step 287: loss=0.15755367279052734\n",
            "step 288: loss=0.07255125045776367\n",
            "step 289: loss=2.1834230422973633\n",
            "step 290: loss=0.18325090408325195\n",
            "step 291: loss=0.37158799171447754\n",
            "step 292: loss=1.1927608251571655\n",
            "step 293: loss=0.044721126556396484\n",
            "step 294: loss=0.009671211242675781\n",
            "step 295: loss=0.8239341974258423\n",
            "step 296: loss=0.2812347412109375\n",
            "step 297: loss=0.29264330863952637\n",
            "step 298: loss=0.07653141021728516\n",
            "step 299: loss=0.012074947357177734\n",
            "step 300: loss=0.05053567886352539\n",
            "step 301: loss=0.006805896759033203\n",
            "step 302: loss=2.7376484870910645\n",
            "step 303: loss=0.9433908462524414\n",
            "step 304: loss=0.3315999507904053\n",
            "step 305: loss=0.45537054538726807\n",
            "step 306: loss=0.324662446975708\n",
            "step 307: loss=0.9534531831741333\n",
            "step 308: loss=0.044396400451660156\n",
            "step 309: loss=0.4811129570007324\n",
            "step 310: loss=0.48668503761291504\n",
            "step 311: loss=2.132352828979492\n",
            "step 312: loss=0.17094135284423828\n",
            "step 313: loss=0.12753868103027344\n",
            "step 314: loss=0.5518331527709961\n",
            "step 315: loss=0.07557868957519531\n",
            "step 316: loss=0.30028343200683594\n",
            "step 317: loss=0.334165096282959\n",
            "step 318: loss=0.33794426918029785\n",
            "step 319: loss=0.21451210975646973\n",
            "step 320: loss=0.35225796699523926\n",
            "step 321: loss=0.8380550146102905\n",
            "step 322: loss=0.17711257934570312\n",
            "step 323: loss=0.09087562561035156\n",
            "step 324: loss=0.07382750511169434\n",
            "step 325: loss=0.23125982284545898\n",
            "step 326: loss=1.4439148902893066\n",
            "step 327: loss=0.47332286834716797\n",
            "step 328: loss=2.746385097503662\n",
            "step 329: loss=0.005428791046142578\n",
            "step 330: loss=0.507765531539917\n",
            "step 331: loss=0.06275653839111328\n",
            "step 332: loss=0.3127431869506836\n",
            "step 333: loss=0.04568958282470703\n",
            "step 334: loss=0.0166778564453125\n",
            "step 335: loss=0.9150269031524658\n",
            "step 336: loss=0.34367990493774414\n",
            "step 337: loss=1.1579874753952026\n",
            "step 338: loss=0.33712172508239746\n",
            "step 339: loss=0.47450244426727295\n",
            "step 340: loss=0.0789024829864502\n",
            "step 341: loss=1.6881625652313232\n",
            "step 342: loss=0.3500049114227295\n",
            "step 343: loss=0.08574986457824707\n",
            "step 344: loss=1.0642505884170532\n",
            "step 345: loss=0.7578302621841431\n",
            "step 346: loss=0.07385015487670898\n",
            "step 347: loss=0.2304244041442871\n",
            "step 348: loss=0.23037433624267578\n",
            "step 349: loss=0.35393309593200684\n",
            "step 350: loss=0.01847219467163086\n",
            "step 351: loss=0.36508679389953613\n",
            "step 352: loss=1.012511134147644\n",
            "step 353: loss=2.5447537899017334\n",
            "step 354: loss=0.7907414436340332\n",
            "step 355: loss=0.7476092576980591\n",
            "step 356: loss=0.012488365173339844\n",
            "step 357: loss=0.37247979640960693\n",
            "step 358: loss=1.1695146560668945\n",
            "step 359: loss=0.17859268188476562\n",
            "step 360: loss=0.08150196075439453\n",
            "step 361: loss=0.5774655342102051\n",
            "step 362: loss=0.12587308883666992\n",
            "step 363: loss=3.19923734664917\n",
            "step 364: loss=0.12036800384521484\n",
            "step 365: loss=1.0232315063476562\n",
            "step 366: loss=0.520953893661499\n",
            "step 367: loss=0.4752535820007324\n",
            "step 368: loss=0.01161336898803711\n",
            "step 369: loss=0.19076156616210938\n",
            "step 370: loss=2.1197047233581543\n",
            "step 371: loss=0.198927640914917\n",
            "step 372: loss=0.015585899353027344\n",
            "step 373: loss=0.014299869537353516\n",
            "step 374: loss=0.08033037185668945\n",
            "step 375: loss=1.9676306247711182\n",
            "step 376: loss=1.014322280883789\n",
            "step 377: loss=0.5524725914001465\n",
            "step 378: loss=0.6297664642333984\n",
            "step 379: loss=3.3354456424713135\n",
            "step 380: loss=0.0397944450378418\n",
            "step 381: loss=0.008146286010742188\n",
            "step 382: loss=0.39034605026245117\n",
            "step 383: loss=0.3461267948150635\n",
            "step 384: loss=0.0007467269897460938\n",
            "step 385: loss=0.06762409210205078\n",
            "step 386: loss=0.09310150146484375\n",
            "step 387: loss=0.11734533309936523\n",
            "step 388: loss=0.42160630226135254\n",
            "step 389: loss=0.04375648498535156\n",
            "step 390: loss=0.6377226114273071\n",
            "step 391: loss=0.13796305656433105\n",
            "step 392: loss=0.9367662668228149\n",
            "step 393: loss=1.7518149614334106\n",
            "step 394: loss=3.922168731689453\n",
            "step 395: loss=0.05047798156738281\n",
            "step 396: loss=0.25234508514404297\n",
            "step 397: loss=0.1284027099609375\n",
            "step 398: loss=0.9674408435821533\n",
            "step 399: loss=0.41379332542419434\n",
            "step 400: loss=0.02946615219116211\n",
            "step 401: loss=0.5387717485427856\n",
            "step 402: loss=0.44220614433288574\n",
            "step 403: loss=0.004082202911376953\n",
            "step 404: loss=0.01567983627319336\n",
            "step 405: loss=0.015554428100585938\n",
            "step 406: loss=0.6025826930999756\n",
            "step 407: loss=0.08917903900146484\n",
            "step 408: loss=0.018439769744873047\n",
            "step 409: loss=3.631786823272705\n",
            "step 410: loss=0.054201602935791016\n",
            "step 411: loss=0.5691432952880859\n",
            "step 412: loss=0.01025390625\n",
            "step 413: loss=0.16602587699890137\n",
            "step 414: loss=0.2280123233795166\n",
            "step 415: loss=0.033725738525390625\n",
            "step 416: loss=2.893092155456543\n",
            "step 417: loss=0.4680638313293457\n",
            "step 418: loss=0.11901974678039551\n",
            "step 419: loss=1.3566269874572754\n",
            "step 420: loss=0.05841684341430664\n",
            "step 421: loss=0.28403472900390625\n",
            "step 422: loss=0.8525373935699463\n",
            "step 423: loss=0.2811431884765625\n",
            "step 424: loss=0.020607948303222656\n",
            "step 425: loss=0.8618515729904175\n",
            "step 426: loss=1.1827752590179443\n",
            "step 427: loss=0.879900336265564\n",
            "step 428: loss=0.19707989692687988\n",
            "step 429: loss=0.2692441940307617\n",
            "step 430: loss=0.0019240379333496094\n",
            "step 431: loss=0.5260112285614014\n",
            "step 432: loss=0.2273728847503662\n",
            "step 433: loss=0.3134946823120117\n",
            "step 434: loss=1.1230807304382324\n",
            "step 435: loss=0.05976676940917969\n",
            "step 436: loss=0.18072009086608887\n",
            "step 437: loss=0.002167224884033203\n",
            "step 438: loss=0.01470041275024414\n",
            "step 439: loss=0.9843231439590454\n",
            "step 440: loss=0.011989116668701172\n",
            "step 441: loss=0.14016485214233398\n",
            "step 442: loss=0.7073944807052612\n",
            "step 443: loss=1.4848887920379639\n",
            "step 444: loss=1.0383212566375732\n",
            "step 445: loss=0.4178335666656494\n",
            "step 446: loss=0.7536647319793701\n",
            "step 447: loss=0.07221364974975586\n",
            "step 448: loss=0.05558323860168457\n",
            "step 449: loss=0.8591423034667969\n",
            "step 450: loss=3.5648746490478516\n",
            "step 451: loss=0.006774425506591797\n",
            "step 452: loss=1.6691206693649292\n",
            "step 453: loss=0.12167143821716309\n",
            "step 454: loss=3.2515273094177246\n",
            "step 455: loss=1.7813029289245605\n",
            "step 456: loss=1.3433033227920532\n",
            "step 457: loss=0.18211889266967773\n",
            "step 458: loss=0.7063823938369751\n",
            "step 459: loss=0.08410120010375977\n",
            "step 460: loss=0.03919649124145508\n",
            "step 461: loss=2.979452133178711\n",
            "step 462: loss=0.0026497840881347656\n",
            "step 463: loss=0.4368898868560791\n",
            "step 464: loss=0.07270312309265137\n",
            "step 465: loss=1.103981852531433\n",
            "step 466: loss=0.03798484802246094\n",
            "step 467: loss=1.9705098867416382\n",
            "step 468: loss=0.11281657218933105\n",
            "step 469: loss=0.6874208450317383\n",
            "step 470: loss=0.13335704803466797\n",
            "step 471: loss=2.6948349475860596\n",
            "step 472: loss=1.5483874082565308\n",
            "step 473: loss=0.11220884323120117\n",
            "step 474: loss=0.44478654861450195\n",
            "step 475: loss=0.013322830200195312\n",
            "step 476: loss=0.38378405570983887\n",
            "step 477: loss=0.04301595687866211\n",
            "step 478: loss=0.026033878326416016\n",
            "step 479: loss=0.1391465663909912\n",
            "step 480: loss=0.3230702877044678\n",
            "step 481: loss=0.24133872985839844\n",
            "step 482: loss=0.29029393196105957\n",
            "step 483: loss=0.4514620304107666\n",
            "step 484: loss=0.44598937034606934\n",
            "step 485: loss=1.3443803787231445\n",
            "step 486: loss=0.010129451751708984\n",
            "step 487: loss=0.008464813232421875\n",
            "step 488: loss=0.10316658020019531\n",
            "step 489: loss=0.14998936653137207\n",
            "step 490: loss=0.24096250534057617\n",
            "step 491: loss=0.0014219284057617188\n",
            "step 492: loss=0.05595660209655762\n",
            "step 493: loss=0.04845285415649414\n",
            "step 494: loss=0.3449218273162842\n",
            "step 495: loss=0.24515211582183838\n",
            "step 496: loss=1.8468499183654785\n",
            "step 497: loss=0.025726318359375\n",
            "step 498: loss=0.10625171661376953\n",
            "step 499: loss=3.827440023422241\n",
            "step 500: loss=0.8061552047729492\n",
            "step 501: loss=0.02237081527709961\n",
            "step 502: loss=0.3995702266693115\n",
            "step 503: loss=0.20708966255187988\n",
            "step 504: loss=0.045786142349243164\n",
            "step 505: loss=0.032854557037353516\n",
            "step 506: loss=0.20965003967285156\n",
            "step 507: loss=0.9962095022201538\n",
            "step 508: loss=0.16164588928222656\n",
            "step 509: loss=0.03909635543823242\n",
            "step 510: loss=0.10247039794921875\n",
            "step 511: loss=0.33330416679382324\n",
            "step 512: loss=0.12527990341186523\n",
            "step 513: loss=0.21143198013305664\n",
            "step 514: loss=0.3745748996734619\n",
            "step 515: loss=0.027648448944091797\n",
            "step 516: loss=0.08393144607543945\n",
            "step 517: loss=0.5669989585876465\n",
            "step 518: loss=0.9578934907913208\n",
            "step 519: loss=0.02363729476928711\n",
            "step 520: loss=1.4831033945083618\n",
            "step 521: loss=0.13979196548461914\n",
            "step 522: loss=0.042284488677978516\n",
            "step 523: loss=0.062416791915893555\n",
            "step 524: loss=0.03997659683227539\n",
            "step 525: loss=0.14003419876098633\n",
            "step 526: loss=3.1203136444091797\n",
            "step 527: loss=0.0059146881103515625\n",
            "step 528: loss=0.00048542022705078125\n",
            "step 529: loss=0.29979002475738525\n",
            "step 530: loss=0.05205678939819336\n",
            "step 531: loss=0.16924405097961426\n",
            "step 532: loss=0.0011043548583984375\n",
            "step 533: loss=0.22463726997375488\n",
            "step 534: loss=0.05318403244018555\n",
            "step 535: loss=0.08916521072387695\n",
            "step 536: loss=0.07006692886352539\n",
            "step 537: loss=0.07175874710083008\n",
            "step 538: loss=1.6056580543518066\n",
            "step 539: loss=0.857620358467102\n",
            "step 540: loss=3.2982356548309326\n",
            "step 541: loss=0.0028433799743652344\n",
            "step 542: loss=0.19463729858398438\n",
            "step 543: loss=0.8926622271537781\n",
            "step 544: loss=0.12177920341491699\n",
            "step 545: loss=0.7661072015762329\n",
            "step 546: loss=0.6626362800598145\n",
            "step 547: loss=0.07155418395996094\n",
            "step 548: loss=0.01363372802734375\n",
            "step 549: loss=0.2269117832183838\n",
            "step 550: loss=0.19848084449768066\n",
            "step 551: loss=0.9459027051925659\n",
            "step 552: loss=0.6102805137634277\n",
            "step 553: loss=0.054195404052734375\n",
            "step 554: loss=2.051149606704712\n",
            "step 555: loss=0.6343874931335449\n",
            "step 556: loss=1.5816800594329834\n",
            "step 557: loss=0.35944628715515137\n",
            "step 558: loss=0.005198001861572266\n",
            "step 559: loss=0.0024509429931640625\n",
            "step 560: loss=0.09252738952636719\n",
            "step 561: loss=0.15797710418701172\n",
            "step 562: loss=0.5221741199493408\n",
            "step 563: loss=0.19676423072814941\n",
            "step 564: loss=0.16375017166137695\n",
            "step 565: loss=0.019205570220947266\n",
            "step 566: loss=2.894334554672241\n",
            "step 567: loss=0.06458353996276855\n",
            "step 568: loss=0.23741066455841064\n",
            "step 569: loss=0.3637881278991699\n",
            "step 570: loss=0.05800652503967285\n",
            "step 571: loss=0.04111528396606445\n",
            "step 572: loss=0.05983901023864746\n",
            "step 573: loss=0.3058300018310547\n",
            "step 574: loss=1.2378265857696533\n",
            "step 575: loss=0.2852437496185303\n",
            "step 576: loss=0.21417760848999023\n",
            "step 577: loss=0.4051244258880615\n",
            "step 578: loss=0.02850055694580078\n",
            "step 579: loss=0.01888132095336914\n",
            "step 580: loss=0.13847827911376953\n",
            "step 581: loss=0.016454219818115234\n",
            "step 582: loss=1.3980793952941895\n",
            "step 583: loss=0.5642735958099365\n",
            "step 584: loss=0.09487390518188477\n",
            "step 585: loss=0.09597134590148926\n",
            "step 586: loss=1.049293041229248\n",
            "step 587: loss=0.47356534004211426\n",
            "step 588: loss=0.8383779525756836\n",
            "step 589: loss=3.1775450706481934\n",
            "step 590: loss=0.4497389793395996\n",
            "step 591: loss=0.0792701244354248\n",
            "step 592: loss=1.3574609756469727\n",
            "step 593: loss=0.2543761730194092\n",
            "step 594: loss=0.07349824905395508\n",
            "step 595: loss=2.837045907974243\n",
            "step 596: loss=0.7364481091499329\n",
            "step 597: loss=0.723122239112854\n",
            "step 598: loss=1.3741117715835571\n",
            "step 599: loss=0.4321751594543457\n",
            "step 600: loss=0.17777228355407715\n",
            "step 601: loss=0.07130718231201172\n",
            "step 602: loss=0.2371366024017334\n",
            "step 603: loss=0.30576348304748535\n",
            "step 604: loss=2.6684279441833496\n",
            "step 605: loss=1.1140676736831665\n",
            "step 606: loss=0.5520974397659302\n",
            "step 607: loss=0.8259553909301758\n",
            "step 608: loss=0.4451253414154053\n",
            "step 609: loss=0.7199971675872803\n",
            "step 610: loss=0.3776886463165283\n",
            "step 611: loss=2.0839900970458984\n",
            "step 612: loss=1.128218173980713\n",
            "step 613: loss=0.2851424217224121\n",
            "step 614: loss=0.04206252098083496\n",
            "step 615: loss=1.4286003112792969\n",
            "step 616: loss=0.5027272701263428\n",
            "step 617: loss=0.012112617492675781\n",
            "step 618: loss=0.7415883541107178\n",
            "step 619: loss=0.06085538864135742\n",
            "step 620: loss=0.5705982446670532\n",
            "step 621: loss=1.8605389595031738\n",
            "step 622: loss=0.23227763175964355\n",
            "step 623: loss=0.04270195960998535\n",
            "step 624: loss=2.6792232990264893\n",
            "step 625: loss=0.2715728282928467\n",
            "step 626: loss=0.929381251335144\n",
            "step 627: loss=0.16028666496276855\n",
            "step 628: loss=0.010308265686035156\n",
            "step 629: loss=1.2906098365783691\n",
            "step 630: loss=0.08072900772094727\n",
            "step 631: loss=0.27584075927734375\n",
            "step 632: loss=0.2107229232788086\n",
            "step 633: loss=0.5600342750549316\n",
            "step 634: loss=0.011282920837402344\n",
            "step 635: loss=3.116433620452881\n",
            "step 636: loss=0.023334503173828125\n",
            "step 637: loss=0.49912142753601074\n",
            "step 638: loss=0.21662282943725586\n",
            "step 639: loss=0.0618896484375\n",
            "step 640: loss=1.0932860374450684\n",
            "step 641: loss=0.25585639476776123\n",
            "step 642: loss=0.010252952575683594\n",
            "step 643: loss=1.6186257600784302\n",
            "step 644: loss=0.011042594909667969\n",
            "step 645: loss=0.30058908462524414\n",
            "step 646: loss=0.09044694900512695\n",
            "step 647: loss=1.9067976474761963\n",
            "step 648: loss=0.12402224540710449\n",
            "step 649: loss=0.05713081359863281\n",
            "step 650: loss=0.10149407386779785\n",
            "step 651: loss=0.2526705265045166\n",
            "step 652: loss=0.6070406436920166\n",
            "step 653: loss=0.004278659820556641\n",
            "step 654: loss=0.7120635509490967\n",
            "step 655: loss=0.6677548885345459\n",
            "step 656: loss=0.45967626571655273\n",
            "step 657: loss=0.05305337905883789\n",
            "step 658: loss=0.636506199836731\n",
            "step 659: loss=0.12805700302124023\n",
            "step 660: loss=0.6878719329833984\n",
            "step 661: loss=0.004768848419189453\n",
            "step 662: loss=0.17098379135131836\n",
            "step 663: loss=1.0485875606536865\n",
            "step 664: loss=0.2143077850341797\n",
            "step 665: loss=0.040230751037597656\n",
            "step 666: loss=0.016267776489257812\n",
            "step 667: loss=0.011294364929199219\n",
            "step 668: loss=1.369489073753357\n",
            "step 669: loss=2.1131978034973145\n",
            "step 670: loss=0.31995701789855957\n",
            "step 671: loss=1.4489622116088867\n",
            "step 672: loss=0.7029209136962891\n",
            "step 673: loss=1.1986883878707886\n",
            "step 674: loss=0.05759167671203613\n",
            "step 675: loss=0.8192205429077148\n",
            "step 676: loss=0.053372859954833984\n",
            "step 677: loss=0.10202956199645996\n",
            "step 678: loss=0.4015166759490967\n",
            "step 679: loss=0.11560511589050293\n",
            "step 680: loss=0.0026750564575195312\n",
            "step 681: loss=1.3169841766357422\n",
            "step 682: loss=0.007153034210205078\n",
            "step 683: loss=0.5806121826171875\n",
            "step 684: loss=1.1635653972625732\n",
            "step 685: loss=0.16467785835266113\n",
            "step 686: loss=0.029392719268798828\n",
            "step 687: loss=0.089141845703125\n",
            "step 688: loss=0.034459590911865234\n",
            "step 689: loss=1.5467557907104492\n",
            "step 690: loss=0.760047197341919\n",
            "step 691: loss=0.3672492504119873\n",
            "step 692: loss=1.7930371761322021\n",
            "step 693: loss=0.04040336608886719\n",
            "step 694: loss=0.09868192672729492\n",
            "step 695: loss=0.09852075576782227\n",
            "step 696: loss=0.05927014350891113\n",
            "step 697: loss=1.263828992843628\n",
            "step 698: loss=0.5462386608123779\n",
            "step 699: loss=0.7163467407226562\n",
            "step 700: loss=0.0858759880065918\n",
            "step 701: loss=0.03256988525390625\n",
            "step 702: loss=0.02334737777709961\n",
            "step 703: loss=0.1107792854309082\n",
            "step 704: loss=0.030237197875976562\n",
            "step 705: loss=0.1586596965789795\n",
            "step 706: loss=0.14440298080444336\n",
            "step 707: loss=0.7704958915710449\n",
            "step 708: loss=0.2733790874481201\n",
            "step 709: loss=0.12483501434326172\n",
            "step 710: loss=0.6951861381530762\n",
            "step 711: loss=1.7305409908294678\n",
            "step 712: loss=0.6946096420288086\n",
            "step 713: loss=0.1503129005432129\n",
            "step 714: loss=0.09028935432434082\n",
            "step 715: loss=0.06392288208007812\n",
            "step 716: loss=0.03419208526611328\n",
            "step 717: loss=0.28113746643066406\n",
            "step 718: loss=0.3789249658584595\n",
            "step 719: loss=0.010700225830078125\n",
            "step 720: loss=0.020720958709716797\n",
            "step 721: loss=0.061791419982910156\n",
            "step 722: loss=0.13160371780395508\n",
            "step 723: loss=0.022855758666992188\n",
            "step 724: loss=0.16715717315673828\n",
            "step 725: loss=4.0715651512146\n",
            "step 726: loss=0.4545590877532959\n",
            "step 727: loss=0.00247955322265625\n",
            "step 728: loss=0.424182653427124\n",
            "step 729: loss=1.0255892276763916\n",
            "step 730: loss=0.11156582832336426\n",
            "step 731: loss=0.061654090881347656\n",
            "step 732: loss=1.2832696437835693\n",
            "step 733: loss=0.6139693856239319\n",
            "step 734: loss=0.032423973083496094\n",
            "step 735: loss=0.8118247985839844\n",
            "step 736: loss=0.009581565856933594\n",
            "step 737: loss=0.28511977195739746\n",
            "step 738: loss=0.1188817024230957\n",
            "step 739: loss=0.0021009445190429688\n",
            "step 740: loss=0.11481595039367676\n",
            "step 741: loss=0.9182496070861816\n",
            "step 742: loss=5.225607872009277\n",
            "step 743: loss=1.7042115926742554\n",
            "step 744: loss=0.04639291763305664\n",
            "step 745: loss=0.577684760093689\n",
            "step 746: loss=0.01789712905883789\n",
            "step 747: loss=1.5026147365570068\n",
            "step 748: loss=0.03278923034667969\n",
            "step 749: loss=0.01644754409790039\n",
            "step 750: loss=0.8263120651245117\n",
            "step 751: loss=0.04602527618408203\n",
            "step 752: loss=0.21524906158447266\n",
            "step 753: loss=0.061154842376708984\n",
            "step 754: loss=0.3718116283416748\n",
            "step 755: loss=0.28887081146240234\n",
            "step 756: loss=1.556701898574829\n",
            "step 757: loss=0.028926372528076172\n",
            "step 758: loss=0.18595385551452637\n",
            "step 759: loss=0.07905268669128418\n",
            "step 760: loss=0.4115283489227295\n",
            "step 761: loss=0.008421897888183594\n",
            "step 762: loss=0.4036262035369873\n",
            "step 763: loss=0.9590024948120117\n",
            "step 764: loss=0.7133638858795166\n",
            "step 765: loss=0.48048675060272217\n",
            "step 766: loss=0.3986121416091919\n",
            "step 767: loss=0.13569283485412598\n",
            "step 768: loss=0.4674081802368164\n",
            "step 769: loss=0.00102996826171875\n",
            "step 770: loss=0.16486167907714844\n",
            "step 771: loss=0.43318843841552734\n",
            "step 772: loss=1.4921140670776367\n",
            "step 773: loss=0.08220386505126953\n",
            "step 774: loss=0.038150787353515625\n",
            "step 775: loss=0.013050556182861328\n",
            "step 776: loss=0.2003769874572754\n",
            "step 777: loss=0.05414152145385742\n",
            "step 778: loss=0.011188030242919922\n",
            "step 779: loss=0.014467239379882812\n",
            "step 780: loss=0.02270984649658203\n",
            "step 781: loss=0.0022478103637695312\n",
            "step 782: loss=0.07770586013793945\n",
            "step 783: loss=0.027918338775634766\n",
            "step 784: loss=0.04948854446411133\n",
            "step 785: loss=0.35353732109069824\n",
            "step 786: loss=0.013762474060058594\n",
            "step 787: loss=0.11862373352050781\n",
            "step 788: loss=1.435457706451416\n",
            "step 789: loss=0.17925167083740234\n",
            "step 790: loss=0.4444911479949951\n",
            "step 791: loss=0.15286684036254883\n",
            "step 792: loss=0.09328126907348633\n",
            "step 793: loss=0.32610607147216797\n",
            "step 794: loss=0.9303807020187378\n",
            "step 795: loss=0.011697769165039062\n",
            "step 796: loss=0.029285907745361328\n",
            "step 797: loss=0.231231689453125\n",
            "step 798: loss=2.6338624954223633\n",
            "step 799: loss=0.5665868520736694\n",
            "step 800: loss=1.24492347240448\n",
            "step 801: loss=0.013893604278564453\n",
            "step 802: loss=0.01511383056640625\n",
            "step 803: loss=0.12432432174682617\n",
            "step 804: loss=0.4973618984222412\n",
            "step 805: loss=0.0024280548095703125\n",
            "step 806: loss=0.002131938934326172\n",
            "step 807: loss=0.018868446350097656\n",
            "step 808: loss=0.002613067626953125\n",
            "step 809: loss=0.7255866527557373\n",
            "step 810: loss=0.0009622573852539062\n",
            "step 811: loss=0.9114749431610107\n",
            "step 812: loss=0.3157074451446533\n",
            "step 813: loss=0.07942962646484375\n",
            "step 814: loss=0.46149277687072754\n",
            "step 815: loss=0.03197526931762695\n",
            "step 816: loss=0.2038557529449463\n",
            "step 817: loss=0.5515196323394775\n",
            "step 818: loss=0.2785801887512207\n",
            "step 819: loss=0.005415439605712891\n",
            "step 820: loss=0.17008399963378906\n",
            "step 821: loss=0.03214550018310547\n",
            "step 822: loss=0.037631988525390625\n",
            "step 823: loss=0.08344030380249023\n",
            "step 824: loss=0.22780489921569824\n",
            "step 825: loss=2.4254870414733887\n",
            "step 826: loss=0.014660835266113281\n",
            "step 827: loss=2.551929473876953\n",
            "step 828: loss=0.01477813720703125\n",
            "step 829: loss=0.25369691848754883\n",
            "step 830: loss=0.11380338668823242\n",
            "step 831: loss=0.19821643829345703\n",
            "step 832: loss=0.005218505859375\n",
            "step 833: loss=0.15487146377563477\n",
            "step 834: loss=1.653292179107666\n",
            "step 835: loss=0.43225574493408203\n",
            "step 836: loss=0.01710987091064453\n",
            "step 837: loss=0.015863895416259766\n",
            "step 838: loss=0.010437488555908203\n",
            "step 839: loss=0.038317203521728516\n",
            "step 840: loss=0.12788152694702148\n",
            "step 841: loss=0.005878925323486328\n",
            "step 842: loss=0.046411991119384766\n",
            "step 843: loss=0.01076507568359375\n",
            "step 844: loss=0.25150275230407715\n",
            "step 845: loss=0.0012178421020507812\n",
            "step 846: loss=0.2321014404296875\n",
            "step 847: loss=0.31379008293151855\n",
            "step 848: loss=0.06747245788574219\n",
            "step 849: loss=0.0008087158203125\n",
            "step 850: loss=0.07829809188842773\n",
            "step 851: loss=0.28147411346435547\n",
            "step 852: loss=0.38886141777038574\n",
            "step 853: loss=0.4496626853942871\n",
            "step 854: loss=0.09763097763061523\n",
            "step 855: loss=1.7243692874908447\n",
            "step 856: loss=0.1643972396850586\n",
            "step 857: loss=0.11453390121459961\n",
            "step 858: loss=0.3992927074432373\n",
            "step 859: loss=0.16950535774230957\n",
            "step 860: loss=0.6861878633499146\n",
            "step 861: loss=0.02004098892211914\n",
            "step 862: loss=0.003284931182861328\n",
            "step 863: loss=0.10630512237548828\n",
            "step 864: loss=0.12004256248474121\n",
            "step 865: loss=0.8591485023498535\n",
            "step 866: loss=0.05156230926513672\n",
            "step 867: loss=0.7235798835754395\n",
            "step 868: loss=0.044901371002197266\n",
            "step 869: loss=2.3743135929107666\n",
            "step 870: loss=0.0057239532470703125\n",
            "step 871: loss=0.21019411087036133\n",
            "step 872: loss=0.1750349998474121\n",
            "step 873: loss=0.03566455841064453\n",
            "step 874: loss=1.4096548557281494\n",
            "step 875: loss=0.3176445960998535\n",
            "step 876: loss=1.5774853229522705\n",
            "step 877: loss=0.1490786075592041\n",
            "step 878: loss=0.14512395858764648\n",
            "step 879: loss=0.014568805694580078\n",
            "step 880: loss=0.1957552433013916\n",
            "step 881: loss=0.025074481964111328\n",
            "step 882: loss=0.11042547225952148\n",
            "step 883: loss=0.10820245742797852\n",
            "step 884: loss=0.013692378997802734\n",
            "step 885: loss=0.025472640991210938\n",
            "step 886: loss=1.4292900562286377\n",
            "step 887: loss=0.3023533821105957\n",
            "step 888: loss=0.16981196403503418\n",
            "step 889: loss=0.5759787559509277\n",
            "step 890: loss=0.39397740364074707\n",
            "step 891: loss=0.01878833770751953\n",
            "step 892: loss=0.0006103515625\n",
            "step 893: loss=0.14224481582641602\n",
            "step 894: loss=0.16683459281921387\n",
            "step 895: loss=0.02306652069091797\n",
            "step 896: loss=0.005665779113769531\n",
            "step 897: loss=0.26515936851501465\n",
            "step 898: loss=0.5112065076828003\n",
            "step 899: loss=0.01642131805419922\n",
            "step 900: loss=0.04950976371765137\n",
            "step 901: loss=0.10270166397094727\n",
            "step 902: loss=0.008306503295898438\n",
            "step 903: loss=1.204566240310669\n",
            "step 904: loss=0.02616596221923828\n",
            "step 905: loss=0.5364210605621338\n",
            "step 906: loss=0.4150726795196533\n",
            "step 907: loss=0.6363639831542969\n",
            "step 908: loss=0.002647876739501953\n",
            "step 909: loss=0.06528759002685547\n",
            "step 910: loss=0.5764923095703125\n",
            "step 911: loss=0.29312705993652344\n",
            "step 912: loss=0.005534172058105469\n",
            "step 913: loss=0.04647350311279297\n",
            "step 914: loss=3.039876937866211\n",
            "step 915: loss=0.04770612716674805\n",
            "step 916: loss=0.23157477378845215\n",
            "step 917: loss=0.34737443923950195\n",
            "step 918: loss=0.5393335819244385\n",
            "step 919: loss=0.3387320041656494\n",
            "step 920: loss=0.10210371017456055\n",
            "step 921: loss=0.2000105381011963\n",
            "step 922: loss=0.46993470191955566\n",
            "step 923: loss=0.056276798248291016\n",
            "step 924: loss=0.006319999694824219\n",
            "step 925: loss=0.01444244384765625\n",
            "step 926: loss=0.02815389633178711\n",
            "step 927: loss=0.13592910766601562\n",
            "step 928: loss=2.047471046447754\n",
            "step 929: loss=0.08091592788696289\n",
            "step 930: loss=0.23891806602478027\n",
            "step 931: loss=0.1295325756072998\n",
            "step 932: loss=0.03899955749511719\n",
            "step 933: loss=0.02909708023071289\n",
            "step 934: loss=0.022089481353759766\n",
            "step 935: loss=0.010469913482666016\n",
            "step 936: loss=1.1239633560180664\n",
            "step 937: loss=0.004912853240966797\n",
            "step 938: loss=0.24308133125305176\n",
            "step 939: loss=0.5980684757232666\n",
            "step 940: loss=0.12193965911865234\n",
            "step 941: loss=0.02315378189086914\n",
            "step 942: loss=0.37062692642211914\n",
            "step 943: loss=0.009302139282226562\n",
            "step 944: loss=0.011314868927001953\n",
            "step 945: loss=3.5264339447021484\n",
            "step 946: loss=0.0910792350769043\n",
            "step 947: loss=0.15913820266723633\n",
            "step 948: loss=0.01747608184814453\n",
            "step 949: loss=0.11133003234863281\n",
            "step 950: loss=0.09948968887329102\n",
            "step 951: loss=0.052641868591308594\n",
            "step 952: loss=0.00896453857421875\n",
            "step 953: loss=0.01663351058959961\n",
            "step 954: loss=0.05906391143798828\n",
            "step 955: loss=0.002895355224609375\n",
            "step 956: loss=0.07044506072998047\n",
            "step 957: loss=0.007376194000244141\n",
            "step 958: loss=0.2260732650756836\n",
            "step 959: loss=0.16261935234069824\n",
            "step 960: loss=0.6109170913696289\n",
            "step 961: loss=0.5834783315658569\n",
            "step 962: loss=0.055864810943603516\n",
            "step 963: loss=0.004221916198730469\n",
            "step 964: loss=0.033403873443603516\n",
            "step 965: loss=0.7937377691268921\n",
            "step 966: loss=0.37858080863952637\n",
            "step 967: loss=0.09524416923522949\n",
            "step 968: loss=0.6360640525817871\n",
            "step 969: loss=0.03384065628051758\n",
            "step 970: loss=0.6383435726165771\n",
            "step 971: loss=0.03069305419921875\n",
            "step 972: loss=0.14579248428344727\n",
            "step 973: loss=0.07487893104553223\n",
            "step 974: loss=1.3001095056533813\n",
            "step 975: loss=2.297577142715454\n",
            "step 976: loss=0.01396322250366211\n",
            "step 977: loss=0.15135955810546875\n",
            "step 978: loss=0.8422961235046387\n",
            "step 979: loss=1.9849613904953003\n",
            "step 980: loss=0.5528028011322021\n",
            "step 981: loss=0.9595497846603394\n",
            "step 982: loss=0.15296173095703125\n",
            "step 983: loss=0.6124076843261719\n",
            "step 984: loss=0.04457712173461914\n",
            "step 985: loss=0.19276797771453857\n",
            "step 986: loss=0.14833617210388184\n",
            "step 987: loss=0.07393813133239746\n",
            "step 988: loss=0.07053279876708984\n",
            "step 989: loss=0.001033782958984375\n",
            "step 990: loss=0.31380701065063477\n",
            "step 991: loss=0.14849281311035156\n",
            "step 992: loss=0.2512540817260742\n",
            "step 993: loss=0.010190486907958984\n",
            "step 994: loss=0.709064245223999\n",
            "step 995: loss=0.2817842960357666\n",
            "step 996: loss=0.0027513504028320312\n",
            "step 997: loss=0.0024824142456054688\n",
            "step 998: loss=0.18323850631713867\n",
            "step 999: loss=0.29120373725891113\n",
            "step 1000: loss=0.002166748046875\n",
            "Mean loss        0.48416764\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f61693",
      "metadata": {},
      "source": [
        "GANs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "15a25657",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss_single_label(logits, label):\n",
        "    targets = jnp.full_like(logits, label)\n",
        "    return optax.sigmoid_binary_cross_entropy(logits, targets).mean()\n",
        "\n",
        "def generator_loss(pfake_logits_given_fake):\n",
        "    \"\"\"Label=0 because the generator is trying to make fake images look real (low p(fake)).\"\"\"\n",
        "    return cross_entropy_loss_single_label(logits=pfake_logits_given_fake, label=0)\n",
        "\n",
        "def discriminator_loss(pfake_logits_given_real, pfake_logits_given_fake):\n",
        "    logits_given_real = cross_entropy_loss_single_label(logits=pfake_logits_given_real, label=0)\n",
        "    logits_given_fake = cross_entropy_loss_single_label(logits=pfake_logits_given_fake, label=1)\n",
        "    return (logits_given_real + logits_given_fake) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b22555e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_generator_loss(generator_params, discriminator_params, generator_apply_fn, discriminator_apply_fn, z_vector):\n",
        "    fake_images = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    pfake_logits_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    return generator_loss(pfake_logits_given_fake)\n",
        "\n",
        "\n",
        "def calculate_discriminator_loss(discriminator_params, generator_params, generator_apply_fn, discriminator_apply_fn, z_vector, real_images):\n",
        "    fake_images = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    pfake_logits_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    pfake_logits_given_real = discriminator_apply_fn({\"params\": discriminator_params}, real_images)\n",
        "    return discriminator_loss(pfake_logits_given_real, pfake_logits_given_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "efbab3ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_generator_step(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_discriminator_step(generator, discriminator, z_vector, real_images):\n",
        "    grads_by_params_fn = jax.grad(calculate_discriminator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        discriminator.params,\n",
        "        generator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        real_images,\n",
        "    )\n",
        "    return discriminator.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e79102c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training_gan(\n",
        "    train_data,\n",
        "    n_steps,\n",
        "    generator_training_state,\n",
        "    discriminator_training_state,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    checkpoint_manager,\n",
        "):\n",
        "    real_images = train_data[\"image\"]\n",
        "\n",
        "    for step in range(1, n_steps + 1):\n",
        "        key, key_z_current_step = jax.random.split(key)\n",
        "        z_vectors = jax.random.normal(key_z_current_step, real_images.shape)\n",
        "\n",
        "        discriminator_training_state = take_discriminator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "            real_images,\n",
        "        )\n",
        "        generator_training_state = take_generator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "        )\n",
        "\n",
        "        generator_loss_value = calculate_generator_loss(\n",
        "            generator_training_state.params,\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "        )\n",
        "        discriminator_loss_value = calculate_discriminator_loss(\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images,\n",
        "        )\n",
        "        print(f\"step {step}: generator_loss={generator_loss_value}, discriminator_loss={discriminator_loss_value}\")\n",
        "\n",
        "        if step == 1 or step % steps_per_save == 0:\n",
        "            checkpoint_manager.save(\n",
        "                step,\n",
        "                args=ocp.args.StandardSave(\n",
        "                    {\n",
        "                        \"generator\": generator_training_state,\n",
        "                        \"discriminator\": discriminator_training_state,\n",
        "                    }\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "4b0c85d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_gan_training(optimizer, key=jax.random.key(0)):\n",
        "    N_PIXELS = 784\n",
        "    N_HIDDEN_LAYER = 128\n",
        "    N_BINARY_CATEGORIES = 1\n",
        "\n",
        "    layer_sizes_generator = [N_PIXELS, N_HIDDEN_LAYER, N_PIXELS]\n",
        "    layer_sizes_discriminator = [N_PIXELS, N_HIDDEN_LAYER, N_BINARY_CATEGORIES]\n",
        "\n",
        "    g_key, d_key = jax.random.split(key)\n",
        "    generator_training_state = create_training_state(layer_sizes_generator, optimizer, g_key)\n",
        "    discriminator_training_state = create_training_state(layer_sizes_discriminator, optimizer, d_key)\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "42aaa206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_gan(\n",
        "    train_data, \n",
        "    optimizer, \n",
        "    n_steps=10**3, \n",
        "    steps_per_save=100, \n",
        "    key=jax.random.key(0),\n",
        "    ):\n",
        "    experiment_name = f\"gan_{optimizer.__class__.__name__}\"\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=key,\n",
        "    )\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = run_training_gan(\n",
        "        train_data=train_data,\n",
        "        n_steps=n_steps,\n",
        "        generator_training_state=generator_training_state,\n",
        "        discriminator_training_state=discriminator_training_state,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "    )\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "6e7f6d97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: generator_loss=2.6123626232147217, discriminator_loss=0.8546755909919739\n",
            "step 2: generator_loss=1.774949312210083, discriminator_loss=0.9896562695503235\n",
            "step 3: generator_loss=1.076843500137329, discriminator_loss=1.2804691791534424\n",
            "step 4: generator_loss=0.7178224325180054, discriminator_loss=1.5891304016113281\n",
            "step 5: generator_loss=0.4751269221305847, discriminator_loss=1.9167433977127075\n",
            "step 6: generator_loss=0.3422739803791046, discriminator_loss=2.1171340942382812\n",
            "step 7: generator_loss=0.31401827931404114, discriminator_loss=2.3400914669036865\n",
            "step 8: generator_loss=0.3016282916069031, discriminator_loss=2.330327033996582\n",
            "step 9: generator_loss=0.30341944098472595, discriminator_loss=2.3457579612731934\n",
            "step 10: generator_loss=0.33698737621307373, discriminator_loss=2.246582508087158\n",
            "step 11: generator_loss=0.5274619460105896, discriminator_loss=1.9459904432296753\n",
            "step 12: generator_loss=0.6122791767120361, discriminator_loss=1.828513264656067\n",
            "step 13: generator_loss=0.7559558749198914, discriminator_loss=1.5753055810928345\n",
            "step 14: generator_loss=1.0923621654510498, discriminator_loss=1.4109818935394287\n",
            "step 15: generator_loss=1.549296259880066, discriminator_loss=1.0850541591644287\n",
            "step 16: generator_loss=1.998237133026123, discriminator_loss=0.9470704793930054\n",
            "step 17: generator_loss=2.4083213806152344, discriminator_loss=0.8120062947273254\n",
            "step 18: generator_loss=2.7288379669189453, discriminator_loss=0.7138072848320007\n",
            "step 19: generator_loss=2.745952844619751, discriminator_loss=0.6770913004875183\n",
            "step 20: generator_loss=2.7768614292144775, discriminator_loss=0.6741413474082947\n",
            "step 21: generator_loss=3.079439163208008, discriminator_loss=0.6560496091842651\n",
            "step 22: generator_loss=2.6820716857910156, discriminator_loss=0.7873899340629578\n",
            "step 23: generator_loss=2.513256549835205, discriminator_loss=0.8039920926094055\n",
            "step 24: generator_loss=2.4684557914733887, discriminator_loss=0.8517767190933228\n",
            "step 25: generator_loss=1.8783173561096191, discriminator_loss=1.000004768371582\n",
            "step 26: generator_loss=1.9426933526992798, discriminator_loss=1.0148721933364868\n",
            "step 27: generator_loss=1.8357596397399902, discriminator_loss=1.1263632774353027\n",
            "step 28: generator_loss=1.6324468851089478, discriminator_loss=1.1468180418014526\n",
            "step 29: generator_loss=1.7407805919647217, discriminator_loss=1.163883924484253\n",
            "step 30: generator_loss=1.7225898504257202, discriminator_loss=1.106878638267517\n",
            "step 31: generator_loss=2.022047996520996, discriminator_loss=1.0328269004821777\n",
            "step 32: generator_loss=2.2013015747070312, discriminator_loss=0.9135149121284485\n",
            "step 33: generator_loss=2.4666731357574463, discriminator_loss=0.813370406627655\n",
            "step 34: generator_loss=2.8611299991607666, discriminator_loss=0.7349002361297607\n",
            "step 35: generator_loss=3.2060604095458984, discriminator_loss=0.6490703225135803\n",
            "step 36: generator_loss=3.303349018096924, discriminator_loss=0.6066697239875793\n",
            "step 37: generator_loss=3.5498931407928467, discriminator_loss=0.5384259223937988\n",
            "step 38: generator_loss=3.6104063987731934, discriminator_loss=0.524329662322998\n",
            "step 39: generator_loss=3.6802897453308105, discriminator_loss=0.5324055552482605\n",
            "step 40: generator_loss=3.5517828464508057, discriminator_loss=0.5511666536331177\n",
            "step 41: generator_loss=3.4578137397766113, discriminator_loss=0.5523561835289001\n",
            "step 42: generator_loss=3.5466716289520264, discriminator_loss=0.5121429562568665\n",
            "step 43: generator_loss=3.5684783458709717, discriminator_loss=0.5054914355278015\n",
            "step 44: generator_loss=3.858177661895752, discriminator_loss=0.43311119079589844\n",
            "step 45: generator_loss=3.7815680503845215, discriminator_loss=0.4548843502998352\n",
            "step 46: generator_loss=4.014816761016846, discriminator_loss=0.4127631187438965\n",
            "step 47: generator_loss=4.360837459564209, discriminator_loss=0.32795166969299316\n",
            "step 48: generator_loss=4.4939961433410645, discriminator_loss=0.33119338750839233\n",
            "step 49: generator_loss=4.568976879119873, discriminator_loss=0.32035863399505615\n",
            "step 50: generator_loss=4.4761962890625, discriminator_loss=0.3042735457420349\n",
            "step 51: generator_loss=4.812506198883057, discriminator_loss=0.26291829347610474\n",
            "step 52: generator_loss=4.843411445617676, discriminator_loss=0.2751258611679077\n",
            "step 53: generator_loss=4.804684638977051, discriminator_loss=0.2583944499492645\n",
            "step 54: generator_loss=4.961971759796143, discriminator_loss=0.262748658657074\n",
            "step 55: generator_loss=5.0198283195495605, discriminator_loss=0.23396360874176025\n",
            "step 56: generator_loss=5.113055229187012, discriminator_loss=0.21282750368118286\n",
            "step 57: generator_loss=5.011951446533203, discriminator_loss=0.1934790462255478\n",
            "step 58: generator_loss=5.071321487426758, discriminator_loss=0.21014299988746643\n",
            "step 59: generator_loss=5.117437839508057, discriminator_loss=0.20102918148040771\n",
            "step 60: generator_loss=5.073759078979492, discriminator_loss=0.18704178929328918\n",
            "step 61: generator_loss=5.1745285987854, discriminator_loss=0.18167394399642944\n",
            "step 62: generator_loss=5.370163440704346, discriminator_loss=0.15129587054252625\n",
            "step 63: generator_loss=5.3514580726623535, discriminator_loss=0.1498185396194458\n",
            "step 64: generator_loss=5.482137680053711, discriminator_loss=0.1342916488647461\n",
            "step 65: generator_loss=5.741271495819092, discriminator_loss=0.11659033596515656\n",
            "step 66: generator_loss=5.57038688659668, discriminator_loss=0.12675324082374573\n",
            "step 67: generator_loss=5.536861896514893, discriminator_loss=0.1333010345697403\n",
            "step 68: generator_loss=5.644913196563721, discriminator_loss=0.1274433732032776\n",
            "step 69: generator_loss=5.635421276092529, discriminator_loss=0.1232718825340271\n",
            "step 70: generator_loss=5.614825248718262, discriminator_loss=0.11505909264087677\n",
            "step 71: generator_loss=5.618678569793701, discriminator_loss=0.10808806121349335\n",
            "step 72: generator_loss=5.719946384429932, discriminator_loss=0.11010328680276871\n",
            "step 73: generator_loss=5.795516014099121, discriminator_loss=0.12029613554477692\n",
            "step 74: generator_loss=6.005905628204346, discriminator_loss=0.11173060536384583\n",
            "step 75: generator_loss=6.175571441650391, discriminator_loss=0.10559152066707611\n",
            "step 76: generator_loss=6.275784969329834, discriminator_loss=0.08060608804225922\n",
            "step 77: generator_loss=6.25020170211792, discriminator_loss=0.08778542280197144\n",
            "step 78: generator_loss=6.049323558807373, discriminator_loss=0.09566935896873474\n",
            "step 79: generator_loss=6.242954254150391, discriminator_loss=0.09324470162391663\n",
            "step 80: generator_loss=6.119951248168945, discriminator_loss=0.10132424533367157\n",
            "step 81: generator_loss=6.0500288009643555, discriminator_loss=0.09573114663362503\n",
            "step 82: generator_loss=6.187067031860352, discriminator_loss=0.09369783103466034\n",
            "step 83: generator_loss=6.4831390380859375, discriminator_loss=0.08779069781303406\n",
            "step 84: generator_loss=6.4739670753479, discriminator_loss=0.07720523327589035\n",
            "step 85: generator_loss=6.287788391113281, discriminator_loss=0.08227728307247162\n",
            "step 86: generator_loss=6.547408580780029, discriminator_loss=0.08666952699422836\n",
            "step 87: generator_loss=6.774918079376221, discriminator_loss=0.06934462487697601\n",
            "step 88: generator_loss=6.794936180114746, discriminator_loss=0.06715936213731766\n",
            "step 89: generator_loss=7.165131568908691, discriminator_loss=0.06495946645736694\n",
            "step 90: generator_loss=7.147000312805176, discriminator_loss=0.061501968652009964\n",
            "step 91: generator_loss=7.345617771148682, discriminator_loss=0.06422128528356552\n",
            "step 92: generator_loss=7.553991794586182, discriminator_loss=0.05350327864289284\n",
            "step 93: generator_loss=7.498496055603027, discriminator_loss=0.05875016748905182\n",
            "step 94: generator_loss=7.7244768142700195, discriminator_loss=0.06063324958086014\n",
            "step 95: generator_loss=7.675307750701904, discriminator_loss=0.05777844414114952\n",
            "step 96: generator_loss=7.8395185470581055, discriminator_loss=0.05580029636621475\n",
            "step 97: generator_loss=8.114436149597168, discriminator_loss=0.04925251752138138\n",
            "step 98: generator_loss=7.984457969665527, discriminator_loss=0.052219536155462265\n",
            "step 99: generator_loss=8.139069557189941, discriminator_loss=0.06114058941602707\n",
            "step 100: generator_loss=8.447978973388672, discriminator_loss=0.06278470158576965\n",
            "step 101: generator_loss=8.664045333862305, discriminator_loss=0.05935782194137573\n",
            "step 102: generator_loss=9.368789672851562, discriminator_loss=0.058625876903533936\n",
            "step 103: generator_loss=9.580767631530762, discriminator_loss=0.05375345051288605\n",
            "step 104: generator_loss=10.0199613571167, discriminator_loss=0.045831598341464996\n",
            "step 105: generator_loss=10.654000282287598, discriminator_loss=0.04832719266414642\n",
            "step 106: generator_loss=10.642539978027344, discriminator_loss=0.05818086862564087\n",
            "step 107: generator_loss=10.46323013305664, discriminator_loss=0.05313564091920853\n",
            "step 108: generator_loss=10.419535636901855, discriminator_loss=0.051822446286678314\n",
            "step 109: generator_loss=10.87695598602295, discriminator_loss=0.059160806238651276\n",
            "step 110: generator_loss=11.095948219299316, discriminator_loss=0.05870405584573746\n",
            "step 111: generator_loss=11.383293151855469, discriminator_loss=0.061482615768909454\n",
            "step 112: generator_loss=11.53302001953125, discriminator_loss=0.049902983009815216\n",
            "step 113: generator_loss=11.510110855102539, discriminator_loss=0.04417646303772926\n",
            "step 114: generator_loss=11.594192504882812, discriminator_loss=0.05197328329086304\n",
            "step 115: generator_loss=11.579785346984863, discriminator_loss=0.04148158058524132\n",
            "step 116: generator_loss=11.963799476623535, discriminator_loss=0.058006614446640015\n",
            "step 117: generator_loss=12.314762115478516, discriminator_loss=0.0397370308637619\n",
            "step 118: generator_loss=12.449837684631348, discriminator_loss=0.040633879601955414\n",
            "step 119: generator_loss=12.125323295593262, discriminator_loss=0.04181632399559021\n",
            "step 120: generator_loss=12.147551536560059, discriminator_loss=0.0406113937497139\n",
            "step 121: generator_loss=12.588483810424805, discriminator_loss=0.04097980260848999\n",
            "step 122: generator_loss=12.93055534362793, discriminator_loss=0.03586580604314804\n",
            "step 123: generator_loss=13.078180313110352, discriminator_loss=0.03447636216878891\n",
            "step 124: generator_loss=13.165353775024414, discriminator_loss=0.03815075010061264\n",
            "step 125: generator_loss=12.666500091552734, discriminator_loss=0.03752606734633446\n",
            "step 126: generator_loss=11.940611839294434, discriminator_loss=0.04180406779050827\n",
            "step 127: generator_loss=12.050836563110352, discriminator_loss=0.04979540407657623\n",
            "step 128: generator_loss=11.811837196350098, discriminator_loss=0.04440934583544731\n",
            "step 129: generator_loss=11.453309059143066, discriminator_loss=0.04246048629283905\n",
            "step 130: generator_loss=11.905488967895508, discriminator_loss=0.04411675035953522\n",
            "step 131: generator_loss=12.479143142700195, discriminator_loss=0.03777669370174408\n",
            "step 132: generator_loss=12.324803352355957, discriminator_loss=0.03849928826093674\n",
            "step 133: generator_loss=12.687362670898438, discriminator_loss=0.041521310806274414\n",
            "step 134: generator_loss=13.05743408203125, discriminator_loss=0.03747638687491417\n",
            "step 135: generator_loss=13.0858736038208, discriminator_loss=0.04059789329767227\n",
            "step 136: generator_loss=12.993168830871582, discriminator_loss=0.040306393057107925\n",
            "step 137: generator_loss=12.689491271972656, discriminator_loss=0.043279979377985\n",
            "step 138: generator_loss=12.137042045593262, discriminator_loss=0.048517532646656036\n",
            "step 139: generator_loss=12.129386901855469, discriminator_loss=0.045155033469200134\n",
            "step 140: generator_loss=12.66614818572998, discriminator_loss=0.04564244672656059\n",
            "step 141: generator_loss=12.386338233947754, discriminator_loss=0.048367805778980255\n",
            "step 142: generator_loss=12.649500846862793, discriminator_loss=0.044990211725234985\n",
            "step 143: generator_loss=12.9307861328125, discriminator_loss=0.04749973863363266\n",
            "step 144: generator_loss=13.513962745666504, discriminator_loss=0.0435282364487648\n",
            "step 145: generator_loss=13.363715171813965, discriminator_loss=0.04273863509297371\n",
            "step 146: generator_loss=13.147809028625488, discriminator_loss=0.05079788714647293\n",
            "step 147: generator_loss=13.07861614227295, discriminator_loss=0.050372414290905\n",
            "step 148: generator_loss=12.414031028747559, discriminator_loss=0.05199210345745087\n",
            "step 149: generator_loss=12.64826774597168, discriminator_loss=0.05096912384033203\n",
            "step 150: generator_loss=11.79549789428711, discriminator_loss=0.05241900682449341\n",
            "step 151: generator_loss=12.05044174194336, discriminator_loss=0.052895594388246536\n",
            "step 152: generator_loss=12.526823043823242, discriminator_loss=0.06160787492990494\n",
            "step 153: generator_loss=13.413020133972168, discriminator_loss=0.05034884065389633\n",
            "step 154: generator_loss=14.004878044128418, discriminator_loss=0.05222395434975624\n",
            "step 155: generator_loss=14.029685974121094, discriminator_loss=0.055344682186841965\n",
            "step 156: generator_loss=13.956598281860352, discriminator_loss=0.054654017090797424\n",
            "step 157: generator_loss=13.615530014038086, discriminator_loss=0.05349186062812805\n",
            "step 158: generator_loss=13.353240966796875, discriminator_loss=0.056813161820173264\n",
            "step 159: generator_loss=12.792699813842773, discriminator_loss=0.060839273035526276\n",
            "step 160: generator_loss=12.381352424621582, discriminator_loss=0.060191523283720016\n",
            "step 161: generator_loss=12.485169410705566, discriminator_loss=0.060205575078725815\n",
            "step 162: generator_loss=12.706250190734863, discriminator_loss=0.05517887696623802\n",
            "step 163: generator_loss=12.765206336975098, discriminator_loss=0.05429453402757645\n",
            "step 164: generator_loss=12.336697578430176, discriminator_loss=0.059852540493011475\n",
            "step 165: generator_loss=12.645994186401367, discriminator_loss=0.059213753789663315\n",
            "step 166: generator_loss=12.348432540893555, discriminator_loss=0.06004980951547623\n",
            "step 167: generator_loss=12.36260986328125, discriminator_loss=0.055954743176698685\n",
            "step 168: generator_loss=12.36005687713623, discriminator_loss=0.05917847156524658\n",
            "step 169: generator_loss=12.128523826599121, discriminator_loss=0.05252579227089882\n",
            "step 170: generator_loss=11.79549789428711, discriminator_loss=0.05513453111052513\n",
            "step 171: generator_loss=11.467062950134277, discriminator_loss=0.057124149054288864\n",
            "step 172: generator_loss=11.10094165802002, discriminator_loss=0.057596251368522644\n",
            "step 173: generator_loss=11.275639533996582, discriminator_loss=0.05384252220392227\n",
            "step 174: generator_loss=11.648467063903809, discriminator_loss=0.05381893366575241\n",
            "step 175: generator_loss=11.53493595123291, discriminator_loss=0.05063207820057869\n",
            "step 176: generator_loss=11.779623985290527, discriminator_loss=0.050635240972042084\n",
            "step 177: generator_loss=11.780366897583008, discriminator_loss=0.050962258130311966\n",
            "step 178: generator_loss=11.156975746154785, discriminator_loss=0.04703952744603157\n",
            "step 179: generator_loss=11.070119857788086, discriminator_loss=0.050781942903995514\n",
            "step 180: generator_loss=10.69389820098877, discriminator_loss=0.04841921478509903\n",
            "step 181: generator_loss=10.799015998840332, discriminator_loss=0.05231168866157532\n",
            "step 182: generator_loss=11.065421104431152, discriminator_loss=0.046006910502910614\n",
            "step 183: generator_loss=10.769379615783691, discriminator_loss=0.043981485068798065\n",
            "step 184: generator_loss=10.837384223937988, discriminator_loss=0.05129434913396835\n",
            "step 185: generator_loss=10.898669242858887, discriminator_loss=0.04829706624150276\n",
            "step 186: generator_loss=11.473621368408203, discriminator_loss=0.051479388028383255\n",
            "step 187: generator_loss=11.644495964050293, discriminator_loss=0.04559825360774994\n",
            "step 188: generator_loss=11.927143096923828, discriminator_loss=0.04812699183821678\n",
            "step 189: generator_loss=12.404595375061035, discriminator_loss=0.04868664592504501\n",
            "step 190: generator_loss=12.78646183013916, discriminator_loss=0.04374462366104126\n",
            "step 191: generator_loss=12.574935913085938, discriminator_loss=0.043721482157707214\n",
            "step 192: generator_loss=12.080217361450195, discriminator_loss=0.04656708613038063\n",
            "step 193: generator_loss=11.943059921264648, discriminator_loss=0.04752124473452568\n",
            "step 194: generator_loss=11.908044815063477, discriminator_loss=0.045696575194597244\n",
            "step 195: generator_loss=12.17562198638916, discriminator_loss=0.048738084733486176\n",
            "step 196: generator_loss=11.805377960205078, discriminator_loss=0.04397791624069214\n",
            "step 197: generator_loss=11.438774108886719, discriminator_loss=0.04521852359175682\n",
            "step 198: generator_loss=11.686036109924316, discriminator_loss=0.04763055220246315\n",
            "step 199: generator_loss=12.040596008300781, discriminator_loss=0.04426499828696251\n",
            "step 200: generator_loss=11.655854225158691, discriminator_loss=0.045050229877233505\n",
            "step 201: generator_loss=11.792832374572754, discriminator_loss=0.04324633628129959\n",
            "step 202: generator_loss=11.34382438659668, discriminator_loss=0.043488964438438416\n",
            "step 203: generator_loss=11.646628379821777, discriminator_loss=0.04324166104197502\n",
            "step 204: generator_loss=11.330154418945312, discriminator_loss=0.04153714329004288\n",
            "step 205: generator_loss=11.076590538024902, discriminator_loss=0.043083321303129196\n",
            "step 206: generator_loss=11.139357566833496, discriminator_loss=0.0397525280714035\n",
            "step 207: generator_loss=10.662981033325195, discriminator_loss=0.041165485978126526\n",
            "step 208: generator_loss=10.002626419067383, discriminator_loss=0.04387703910470009\n",
            "step 209: generator_loss=9.966928482055664, discriminator_loss=0.0417088158428669\n",
            "step 210: generator_loss=9.74033260345459, discriminator_loss=0.043225325644016266\n",
            "step 211: generator_loss=10.000482559204102, discriminator_loss=0.04131651669740677\n",
            "step 212: generator_loss=10.15046501159668, discriminator_loss=0.04147767648100853\n",
            "step 213: generator_loss=10.083281517028809, discriminator_loss=0.03767586126923561\n",
            "step 214: generator_loss=10.08691120147705, discriminator_loss=0.037923865020275116\n",
            "step 215: generator_loss=9.908629417419434, discriminator_loss=0.03689224272966385\n",
            "step 216: generator_loss=9.894851684570312, discriminator_loss=0.036485541611909866\n",
            "step 217: generator_loss=9.48481273651123, discriminator_loss=0.0363851822912693\n",
            "step 218: generator_loss=8.906074523925781, discriminator_loss=0.0344952754676342\n",
            "step 219: generator_loss=8.643170356750488, discriminator_loss=0.0347101204097271\n",
            "step 220: generator_loss=8.13879680633545, discriminator_loss=0.0353841707110405\n",
            "step 221: generator_loss=8.070772171020508, discriminator_loss=0.03296497464179993\n",
            "step 222: generator_loss=7.567434787750244, discriminator_loss=0.03604879975318909\n",
            "step 223: generator_loss=7.515453338623047, discriminator_loss=0.033393532037734985\n",
            "step 224: generator_loss=7.590554237365723, discriminator_loss=0.032918818295001984\n",
            "step 225: generator_loss=7.751674175262451, discriminator_loss=0.03090459294617176\n",
            "step 226: generator_loss=7.948326110839844, discriminator_loss=0.02939528599381447\n",
            "step 227: generator_loss=7.856945514678955, discriminator_loss=0.028217146173119545\n",
            "step 228: generator_loss=7.875897407531738, discriminator_loss=0.02735752984881401\n",
            "step 229: generator_loss=7.647400856018066, discriminator_loss=0.02746644616127014\n",
            "step 230: generator_loss=7.739124298095703, discriminator_loss=0.027960019186139107\n",
            "step 231: generator_loss=7.40372371673584, discriminator_loss=0.027804691344499588\n",
            "step 232: generator_loss=7.380863666534424, discriminator_loss=0.02889629825949669\n",
            "step 233: generator_loss=7.277824878692627, discriminator_loss=0.028269432485103607\n",
            "step 234: generator_loss=7.105244159698486, discriminator_loss=0.027686070650815964\n",
            "step 235: generator_loss=7.2634172439575195, discriminator_loss=0.026426319032907486\n",
            "step 236: generator_loss=7.2555365562438965, discriminator_loss=0.026272300630807877\n",
            "step 237: generator_loss=7.280795574188232, discriminator_loss=0.025959642603993416\n",
            "step 238: generator_loss=7.15653657913208, discriminator_loss=0.025995874777436256\n",
            "step 239: generator_loss=7.298652648925781, discriminator_loss=0.024843713268637657\n",
            "step 240: generator_loss=7.323746204376221, discriminator_loss=0.024038083851337433\n",
            "step 241: generator_loss=7.349942207336426, discriminator_loss=0.025113653391599655\n",
            "step 242: generator_loss=7.196470260620117, discriminator_loss=0.024793075397610664\n",
            "step 243: generator_loss=7.316102027893066, discriminator_loss=0.024945827201008797\n",
            "step 244: generator_loss=7.404191017150879, discriminator_loss=0.022626882418990135\n",
            "step 245: generator_loss=7.189554214477539, discriminator_loss=0.024390846490859985\n",
            "step 246: generator_loss=7.0850090980529785, discriminator_loss=0.024195803329348564\n",
            "step 247: generator_loss=7.009917259216309, discriminator_loss=0.02433708868920803\n",
            "step 248: generator_loss=7.034090042114258, discriminator_loss=0.024058159440755844\n",
            "step 249: generator_loss=6.946345806121826, discriminator_loss=0.023375961929559708\n",
            "step 250: generator_loss=7.10773229598999, discriminator_loss=0.023205002769827843\n",
            "step 251: generator_loss=7.046420574188232, discriminator_loss=0.023607995361089706\n",
            "step 252: generator_loss=6.913980007171631, discriminator_loss=0.024883244186639786\n",
            "step 253: generator_loss=7.15311861038208, discriminator_loss=0.02356811985373497\n",
            "step 254: generator_loss=7.072432994842529, discriminator_loss=0.022164100781083107\n",
            "step 255: generator_loss=7.211752414703369, discriminator_loss=0.022292226552963257\n",
            "step 256: generator_loss=7.186977863311768, discriminator_loss=0.021478617563843727\n",
            "step 257: generator_loss=7.250208377838135, discriminator_loss=0.02139415219426155\n",
            "step 258: generator_loss=7.242965221405029, discriminator_loss=0.02241654507815838\n",
            "step 259: generator_loss=7.145642280578613, discriminator_loss=0.020974349230527878\n",
            "step 260: generator_loss=7.159382343292236, discriminator_loss=0.02083851583302021\n",
            "step 261: generator_loss=7.207153797149658, discriminator_loss=0.020170733332633972\n",
            "step 262: generator_loss=7.087294101715088, discriminator_loss=0.02064180001616478\n",
            "step 263: generator_loss=7.081168174743652, discriminator_loss=0.019541257992386818\n",
            "step 264: generator_loss=7.1390533447265625, discriminator_loss=0.02062050625681877\n",
            "step 265: generator_loss=6.88482666015625, discriminator_loss=0.022942407056689262\n",
            "step 266: generator_loss=6.786438941955566, discriminator_loss=0.02058553695678711\n",
            "step 267: generator_loss=7.0771565437316895, discriminator_loss=0.017876431345939636\n",
            "step 268: generator_loss=6.816531658172607, discriminator_loss=0.019613491371273994\n",
            "step 269: generator_loss=6.857675552368164, discriminator_loss=0.01944589428603649\n",
            "step 270: generator_loss=6.875261306762695, discriminator_loss=0.021479256451129913\n",
            "step 271: generator_loss=6.850047588348389, discriminator_loss=0.01986965723335743\n",
            "step 272: generator_loss=6.936288833618164, discriminator_loss=0.019615039229393005\n",
            "step 273: generator_loss=7.0094780921936035, discriminator_loss=0.018553128466010094\n",
            "step 274: generator_loss=7.022727012634277, discriminator_loss=0.017783427610993385\n",
            "step 275: generator_loss=7.129738807678223, discriminator_loss=0.017237184569239616\n",
            "step 276: generator_loss=6.854759216308594, discriminator_loss=0.017363520339131355\n",
            "step 277: generator_loss=6.807745456695557, discriminator_loss=0.01751832850277424\n",
            "step 278: generator_loss=6.8535284996032715, discriminator_loss=0.01846366748213768\n",
            "step 279: generator_loss=6.732686996459961, discriminator_loss=0.020006023347377777\n",
            "step 280: generator_loss=6.71099853515625, discriminator_loss=0.019702086225152016\n",
            "step 281: generator_loss=6.898499488830566, discriminator_loss=0.022379327565431595\n",
            "step 282: generator_loss=6.7839531898498535, discriminator_loss=0.01938619092106819\n",
            "step 283: generator_loss=6.941007137298584, discriminator_loss=0.016730129718780518\n",
            "step 284: generator_loss=6.9442033767700195, discriminator_loss=0.017973266541957855\n",
            "step 285: generator_loss=6.8107500076293945, discriminator_loss=0.01796768233180046\n",
            "step 286: generator_loss=6.966135501861572, discriminator_loss=0.017399432137608528\n",
            "step 287: generator_loss=6.740983486175537, discriminator_loss=0.016407104209065437\n",
            "step 288: generator_loss=6.850815296173096, discriminator_loss=0.015829283744096756\n",
            "step 289: generator_loss=6.753127574920654, discriminator_loss=0.017844583839178085\n",
            "step 290: generator_loss=6.660054683685303, discriminator_loss=0.01749112270772457\n",
            "step 291: generator_loss=6.586442470550537, discriminator_loss=0.018456611782312393\n",
            "step 292: generator_loss=6.642635822296143, discriminator_loss=0.016924317926168442\n",
            "step 293: generator_loss=6.470057010650635, discriminator_loss=0.016734810546040535\n",
            "step 294: generator_loss=6.445556640625, discriminator_loss=0.017153998836874962\n",
            "step 295: generator_loss=6.302323341369629, discriminator_loss=0.017413783818483353\n",
            "step 296: generator_loss=6.290978908538818, discriminator_loss=0.019725002348423004\n",
            "step 297: generator_loss=6.209100246429443, discriminator_loss=0.01774320751428604\n",
            "step 298: generator_loss=6.341518402099609, discriminator_loss=0.0168850626796484\n",
            "step 299: generator_loss=6.226273536682129, discriminator_loss=0.017155075445771217\n",
            "step 300: generator_loss=6.153433799743652, discriminator_loss=0.017542120069265366\n",
            "step 301: generator_loss=6.160391807556152, discriminator_loss=0.019939443096518517\n",
            "step 302: generator_loss=6.298000812530518, discriminator_loss=0.017106330022215843\n",
            "step 303: generator_loss=6.136068344116211, discriminator_loss=0.01782483607530594\n",
            "step 304: generator_loss=6.274207592010498, discriminator_loss=0.017262686043977737\n",
            "step 305: generator_loss=6.202847003936768, discriminator_loss=0.01677505299448967\n",
            "step 306: generator_loss=6.253763198852539, discriminator_loss=0.018529267981648445\n",
            "step 307: generator_loss=6.235805511474609, discriminator_loss=0.01633235067129135\n",
            "step 308: generator_loss=6.2725324630737305, discriminator_loss=0.016074908897280693\n",
            "step 309: generator_loss=6.171889781951904, discriminator_loss=0.016822626814246178\n",
            "step 310: generator_loss=6.164940357208252, discriminator_loss=0.017676690593361855\n",
            "step 311: generator_loss=6.166038513183594, discriminator_loss=0.0164480097591877\n",
            "step 312: generator_loss=6.103418827056885, discriminator_loss=0.01730889081954956\n",
            "step 313: generator_loss=6.1875834465026855, discriminator_loss=0.017540866509079933\n",
            "step 314: generator_loss=6.178131103515625, discriminator_loss=0.01622856967151165\n",
            "step 315: generator_loss=6.136803150177002, discriminator_loss=0.01627255044877529\n",
            "step 316: generator_loss=6.021705150604248, discriminator_loss=0.01616406999528408\n",
            "step 317: generator_loss=6.005953788757324, discriminator_loss=0.016761580482125282\n",
            "step 318: generator_loss=6.072920322418213, discriminator_loss=0.017297154292464256\n",
            "step 319: generator_loss=5.839330673217773, discriminator_loss=0.01948648877441883\n",
            "step 320: generator_loss=5.8657426834106445, discriminator_loss=0.016866322606801987\n",
            "step 321: generator_loss=5.9111528396606445, discriminator_loss=0.017865624278783798\n",
            "step 322: generator_loss=5.852369785308838, discriminator_loss=0.017294850200414658\n",
            "step 323: generator_loss=5.848097324371338, discriminator_loss=0.017913732677698135\n",
            "step 324: generator_loss=5.94047737121582, discriminator_loss=0.017232302576303482\n",
            "step 325: generator_loss=5.759531497955322, discriminator_loss=0.017391573637723923\n",
            "step 326: generator_loss=5.830861568450928, discriminator_loss=0.017978504300117493\n",
            "step 327: generator_loss=5.846219539642334, discriminator_loss=0.017925208434462547\n",
            "step 328: generator_loss=5.905989646911621, discriminator_loss=0.017355648800730705\n",
            "step 329: generator_loss=5.83978271484375, discriminator_loss=0.017700890079140663\n",
            "step 330: generator_loss=5.877753257751465, discriminator_loss=0.01715071126818657\n",
            "step 331: generator_loss=5.843164443969727, discriminator_loss=0.016979653388261795\n",
            "step 332: generator_loss=5.885195255279541, discriminator_loss=0.016945913434028625\n",
            "step 333: generator_loss=5.745428562164307, discriminator_loss=0.01731356978416443\n",
            "step 334: generator_loss=5.727367401123047, discriminator_loss=0.017179956659674644\n",
            "step 335: generator_loss=5.856420993804932, discriminator_loss=0.01703992858529091\n",
            "step 336: generator_loss=5.680755138397217, discriminator_loss=0.01915581151843071\n",
            "step 337: generator_loss=5.641961097717285, discriminator_loss=0.017060764133930206\n",
            "step 338: generator_loss=5.660099983215332, discriminator_loss=0.017109984531998634\n",
            "step 339: generator_loss=5.590421199798584, discriminator_loss=0.017977846786379814\n",
            "step 340: generator_loss=5.557816982269287, discriminator_loss=0.017198417335748672\n",
            "step 341: generator_loss=5.421024322509766, discriminator_loss=0.017786305397748947\n",
            "step 342: generator_loss=5.464900493621826, discriminator_loss=0.017679445445537567\n",
            "step 343: generator_loss=5.482944965362549, discriminator_loss=0.017861876636743546\n",
            "step 344: generator_loss=5.410203456878662, discriminator_loss=0.019193707033991814\n",
            "step 345: generator_loss=5.422821998596191, discriminator_loss=0.01720278710126877\n",
            "step 346: generator_loss=5.367817401885986, discriminator_loss=0.019008252769708633\n",
            "step 347: generator_loss=5.368399143218994, discriminator_loss=0.018288813531398773\n",
            "step 348: generator_loss=5.4189629554748535, discriminator_loss=0.01760845258831978\n",
            "step 349: generator_loss=5.354533672332764, discriminator_loss=0.020009655505418777\n",
            "step 350: generator_loss=5.370744705200195, discriminator_loss=0.018543317914009094\n",
            "step 351: generator_loss=5.388192176818848, discriminator_loss=0.018326804041862488\n",
            "step 352: generator_loss=5.397832870483398, discriminator_loss=0.018651727586984634\n",
            "step 353: generator_loss=5.376162052154541, discriminator_loss=0.01849639043211937\n",
            "step 354: generator_loss=5.510198593139648, discriminator_loss=0.01763877645134926\n",
            "step 355: generator_loss=5.4682087898254395, discriminator_loss=0.01799945719540119\n",
            "step 356: generator_loss=5.419661045074463, discriminator_loss=0.018916312605142593\n",
            "step 357: generator_loss=5.519305229187012, discriminator_loss=0.01785856857895851\n",
            "step 358: generator_loss=5.589649677276611, discriminator_loss=0.018504412844777107\n",
            "step 359: generator_loss=5.548460483551025, discriminator_loss=0.019311286509037018\n",
            "step 360: generator_loss=5.544114589691162, discriminator_loss=0.01820347085595131\n",
            "step 361: generator_loss=5.465296268463135, discriminator_loss=0.017695352435112\n",
            "step 362: generator_loss=5.462815761566162, discriminator_loss=0.018729396164417267\n",
            "step 363: generator_loss=5.434205055236816, discriminator_loss=0.01845862716436386\n",
            "step 364: generator_loss=5.445152759552002, discriminator_loss=0.01940067857503891\n",
            "step 365: generator_loss=5.340597152709961, discriminator_loss=0.018746159970760345\n",
            "step 366: generator_loss=5.423258304595947, discriminator_loss=0.017781775444746017\n",
            "step 367: generator_loss=5.389705181121826, discriminator_loss=0.020477022975683212\n",
            "step 368: generator_loss=5.329588890075684, discriminator_loss=0.023366164416074753\n",
            "step 369: generator_loss=5.347349643707275, discriminator_loss=0.021213125437498093\n",
            "step 370: generator_loss=5.33997106552124, discriminator_loss=0.022978702560067177\n",
            "step 371: generator_loss=5.384779453277588, discriminator_loss=0.019673265516757965\n",
            "step 372: generator_loss=5.335809707641602, discriminator_loss=0.022628173232078552\n",
            "step 373: generator_loss=5.468684673309326, discriminator_loss=0.020318690687417984\n",
            "step 374: generator_loss=5.387876987457275, discriminator_loss=0.022295719012618065\n",
            "step 375: generator_loss=5.584802627563477, discriminator_loss=0.02093552239239216\n",
            "step 376: generator_loss=5.533779621124268, discriminator_loss=0.020248953253030777\n",
            "step 377: generator_loss=5.5260419845581055, discriminator_loss=0.018917636945843697\n",
            "step 378: generator_loss=5.464617729187012, discriminator_loss=0.021129902452230453\n",
            "step 379: generator_loss=5.5310187339782715, discriminator_loss=0.020541461184620857\n",
            "step 380: generator_loss=5.484554767608643, discriminator_loss=0.01968340203166008\n",
            "step 381: generator_loss=5.386096954345703, discriminator_loss=0.02215576171875\n",
            "step 382: generator_loss=5.304933071136475, discriminator_loss=0.02282274141907692\n",
            "step 383: generator_loss=5.289249420166016, discriminator_loss=0.023435693234205246\n",
            "step 384: generator_loss=5.3931565284729, discriminator_loss=0.02151571586728096\n",
            "step 385: generator_loss=5.317617893218994, discriminator_loss=0.024426616728305817\n",
            "step 386: generator_loss=5.306138038635254, discriminator_loss=0.025180872529745102\n",
            "step 387: generator_loss=5.242270469665527, discriminator_loss=0.025512905791401863\n",
            "step 388: generator_loss=5.2969207763671875, discriminator_loss=0.025633912533521652\n",
            "step 389: generator_loss=5.30958366394043, discriminator_loss=0.0250210203230381\n",
            "step 390: generator_loss=5.265603065490723, discriminator_loss=0.02675880864262581\n",
            "step 391: generator_loss=5.368155479431152, discriminator_loss=0.02492167428135872\n",
            "step 392: generator_loss=5.232083320617676, discriminator_loss=0.02537696808576584\n",
            "step 393: generator_loss=5.385451316833496, discriminator_loss=0.02648177184164524\n",
            "step 394: generator_loss=5.436140060424805, discriminator_loss=0.023632628843188286\n",
            "step 395: generator_loss=5.421279430389404, discriminator_loss=0.02520064078271389\n",
            "step 396: generator_loss=5.37000036239624, discriminator_loss=0.024212755262851715\n",
            "step 397: generator_loss=5.398438930511475, discriminator_loss=0.022129997611045837\n",
            "step 398: generator_loss=5.329987525939941, discriminator_loss=0.025256041437387466\n",
            "step 399: generator_loss=5.269957542419434, discriminator_loss=0.027151258662343025\n",
            "step 400: generator_loss=5.297041893005371, discriminator_loss=0.02742900885641575\n",
            "step 401: generator_loss=5.144198417663574, discriminator_loss=0.03005168028175831\n",
            "step 402: generator_loss=5.213944435119629, discriminator_loss=0.027176622301340103\n",
            "step 403: generator_loss=5.131820201873779, discriminator_loss=0.02980279177427292\n",
            "step 404: generator_loss=5.17779016494751, discriminator_loss=0.029050609096884727\n",
            "step 405: generator_loss=5.136307716369629, discriminator_loss=0.026922423392534256\n",
            "step 406: generator_loss=5.105323314666748, discriminator_loss=0.02777465432882309\n",
            "step 407: generator_loss=5.059233665466309, discriminator_loss=0.03159615024924278\n",
            "step 408: generator_loss=5.020543098449707, discriminator_loss=0.03161945566534996\n",
            "step 409: generator_loss=5.006378650665283, discriminator_loss=0.030523762106895447\n",
            "step 410: generator_loss=5.119451999664307, discriminator_loss=0.02917424961924553\n",
            "step 411: generator_loss=4.915095806121826, discriminator_loss=0.0356941893696785\n",
            "step 412: generator_loss=4.984206199645996, discriminator_loss=0.030836567282676697\n",
            "step 413: generator_loss=5.0031514167785645, discriminator_loss=0.02977008745074272\n",
            "step 414: generator_loss=5.097065448760986, discriminator_loss=0.02912023477256298\n",
            "step 415: generator_loss=4.944108486175537, discriminator_loss=0.029627609997987747\n",
            "step 416: generator_loss=4.989686965942383, discriminator_loss=0.031097155064344406\n",
            "step 417: generator_loss=4.948925495147705, discriminator_loss=0.03202834725379944\n",
            "step 418: generator_loss=4.92100715637207, discriminator_loss=0.031698327511548996\n",
            "step 419: generator_loss=4.880023002624512, discriminator_loss=0.03388996794819832\n",
            "step 420: generator_loss=4.9286651611328125, discriminator_loss=0.03168996796011925\n",
            "step 421: generator_loss=4.842127323150635, discriminator_loss=0.03299909085035324\n",
            "step 422: generator_loss=4.720981121063232, discriminator_loss=0.03547270596027374\n",
            "step 423: generator_loss=4.799603462219238, discriminator_loss=0.03392103314399719\n",
            "step 424: generator_loss=4.7448272705078125, discriminator_loss=0.03592139855027199\n",
            "step 425: generator_loss=4.70885705947876, discriminator_loss=0.034941524267196655\n",
            "step 426: generator_loss=4.752480506896973, discriminator_loss=0.038236893713474274\n",
            "step 427: generator_loss=4.774478435516357, discriminator_loss=0.03629044443368912\n",
            "step 428: generator_loss=4.603238582611084, discriminator_loss=0.039418235421180725\n",
            "step 429: generator_loss=4.678058624267578, discriminator_loss=0.03682681545615196\n",
            "step 430: generator_loss=4.7660627365112305, discriminator_loss=0.03816460072994232\n",
            "step 431: generator_loss=4.669813632965088, discriminator_loss=0.038620419800281525\n",
            "step 432: generator_loss=4.611315727233887, discriminator_loss=0.03927110508084297\n",
            "step 433: generator_loss=4.4739580154418945, discriminator_loss=0.03977452591061592\n",
            "step 434: generator_loss=4.671830654144287, discriminator_loss=0.03758622705936432\n",
            "step 435: generator_loss=4.582763195037842, discriminator_loss=0.03784094378352165\n",
            "step 436: generator_loss=4.559175968170166, discriminator_loss=0.039563633501529694\n",
            "step 437: generator_loss=4.405930519104004, discriminator_loss=0.04477255046367645\n",
            "step 438: generator_loss=4.544806957244873, discriminator_loss=0.041864246129989624\n",
            "step 439: generator_loss=4.4511637687683105, discriminator_loss=0.04188702255487442\n",
            "step 440: generator_loss=4.337157249450684, discriminator_loss=0.04448465257883072\n",
            "step 441: generator_loss=4.410231113433838, discriminator_loss=0.0434022955596447\n",
            "step 442: generator_loss=4.412683486938477, discriminator_loss=0.04363218694925308\n",
            "step 443: generator_loss=4.373985767364502, discriminator_loss=0.041037723422050476\n",
            "step 444: generator_loss=4.377878665924072, discriminator_loss=0.04415718466043472\n",
            "step 445: generator_loss=4.244742393493652, discriminator_loss=0.04481074959039688\n",
            "step 446: generator_loss=4.186912536621094, discriminator_loss=0.049509353935718536\n",
            "step 447: generator_loss=4.108360767364502, discriminator_loss=0.04827866703271866\n",
            "step 448: generator_loss=4.235889434814453, discriminator_loss=0.04528453201055527\n",
            "step 449: generator_loss=4.212643623352051, discriminator_loss=0.04827645421028137\n",
            "step 450: generator_loss=4.013858318328857, discriminator_loss=0.051170919090509415\n",
            "step 451: generator_loss=3.910707712173462, discriminator_loss=0.052289023995399475\n",
            "step 452: generator_loss=4.015157699584961, discriminator_loss=0.051804207265377045\n",
            "step 453: generator_loss=3.9478445053100586, discriminator_loss=0.05237036198377609\n",
            "step 454: generator_loss=4.005002498626709, discriminator_loss=0.052051473408937454\n",
            "step 455: generator_loss=3.886154890060425, discriminator_loss=0.054115474224090576\n",
            "step 456: generator_loss=3.959123134613037, discriminator_loss=0.05241633206605911\n",
            "step 457: generator_loss=3.7865006923675537, discriminator_loss=0.054362550377845764\n",
            "step 458: generator_loss=3.7511470317840576, discriminator_loss=0.058642640709877014\n",
            "step 459: generator_loss=3.7962915897369385, discriminator_loss=0.05627550929784775\n",
            "step 460: generator_loss=3.7512764930725098, discriminator_loss=0.057908788323402405\n",
            "step 461: generator_loss=3.769946575164795, discriminator_loss=0.05758800357580185\n",
            "step 462: generator_loss=3.7906370162963867, discriminator_loss=0.057157181203365326\n",
            "step 463: generator_loss=3.72114896774292, discriminator_loss=0.0591849684715271\n",
            "step 464: generator_loss=3.615804433822632, discriminator_loss=0.06201602891087532\n",
            "step 465: generator_loss=3.6275954246520996, discriminator_loss=0.06104276329278946\n",
            "step 466: generator_loss=3.5931758880615234, discriminator_loss=0.06267282366752625\n",
            "step 467: generator_loss=3.593816041946411, discriminator_loss=0.06348887085914612\n",
            "step 468: generator_loss=3.4821064472198486, discriminator_loss=0.06492714583873749\n",
            "step 469: generator_loss=3.567669630050659, discriminator_loss=0.06477134674787521\n",
            "step 470: generator_loss=3.466411828994751, discriminator_loss=0.06742566078901291\n",
            "step 471: generator_loss=3.477522611618042, discriminator_loss=0.06574554741382599\n",
            "step 472: generator_loss=3.5194146633148193, discriminator_loss=0.06752260774374008\n",
            "step 473: generator_loss=3.5532419681549072, discriminator_loss=0.06465435028076172\n",
            "step 474: generator_loss=3.362687110900879, discriminator_loss=0.0708162784576416\n",
            "step 475: generator_loss=3.4111294746398926, discriminator_loss=0.0697115883231163\n",
            "step 476: generator_loss=3.3909709453582764, discriminator_loss=0.07024030387401581\n",
            "step 477: generator_loss=3.3420016765594482, discriminator_loss=0.07284043729305267\n",
            "step 478: generator_loss=3.3051276206970215, discriminator_loss=0.07410948723554611\n",
            "step 479: generator_loss=3.2461440563201904, discriminator_loss=0.07700933516025543\n",
            "step 480: generator_loss=3.2868385314941406, discriminator_loss=0.0743698924779892\n",
            "step 481: generator_loss=3.163097381591797, discriminator_loss=0.07610474526882172\n",
            "step 482: generator_loss=3.1481289863586426, discriminator_loss=0.07788063585758209\n",
            "step 483: generator_loss=3.1851367950439453, discriminator_loss=0.07622068375349045\n",
            "step 484: generator_loss=3.0957999229431152, discriminator_loss=0.08199947327375412\n",
            "step 485: generator_loss=3.091545343399048, discriminator_loss=0.08207426965236664\n",
            "step 486: generator_loss=2.9934701919555664, discriminator_loss=0.08513151109218597\n",
            "step 487: generator_loss=2.958711624145508, discriminator_loss=0.08781828731298447\n",
            "step 488: generator_loss=3.0020651817321777, discriminator_loss=0.08648989349603653\n",
            "step 489: generator_loss=2.9273457527160645, discriminator_loss=0.08621668815612793\n",
            "step 490: generator_loss=2.8688714504241943, discriminator_loss=0.08882521092891693\n",
            "step 491: generator_loss=2.7795474529266357, discriminator_loss=0.09225844591856003\n",
            "step 492: generator_loss=2.7327189445495605, discriminator_loss=0.09637579321861267\n",
            "step 493: generator_loss=2.8198599815368652, discriminator_loss=0.09243813157081604\n",
            "step 494: generator_loss=2.661550998687744, discriminator_loss=0.09942486137151718\n",
            "step 495: generator_loss=2.656636953353882, discriminator_loss=0.10110247135162354\n",
            "step 496: generator_loss=2.701115369796753, discriminator_loss=0.09593971073627472\n",
            "step 497: generator_loss=2.6151368618011475, discriminator_loss=0.10236506164073944\n",
            "step 498: generator_loss=2.5879065990448, discriminator_loss=0.10357193648815155\n",
            "step 499: generator_loss=2.5370190143585205, discriminator_loss=0.10516991466283798\n",
            "step 500: generator_loss=2.565629482269287, discriminator_loss=0.10302281379699707\n",
            "step 501: generator_loss=2.451319932937622, discriminator_loss=0.10902729630470276\n",
            "step 502: generator_loss=2.506618022918701, discriminator_loss=0.10567566752433777\n",
            "step 503: generator_loss=2.4500982761383057, discriminator_loss=0.10647214949131012\n",
            "step 504: generator_loss=2.382838487625122, discriminator_loss=0.10977733135223389\n",
            "step 505: generator_loss=2.435918092727661, discriminator_loss=0.10774242877960205\n",
            "step 506: generator_loss=2.277801275253296, discriminator_loss=0.11289872974157333\n",
            "step 507: generator_loss=2.293196201324463, discriminator_loss=0.11229656636714935\n",
            "step 508: generator_loss=2.236398458480835, discriminator_loss=0.11578076332807541\n",
            "step 509: generator_loss=2.2152822017669678, discriminator_loss=0.11628952622413635\n",
            "step 510: generator_loss=2.170290231704712, discriminator_loss=0.11627686768770218\n",
            "step 511: generator_loss=2.0954768657684326, discriminator_loss=0.12233027070760727\n",
            "step 512: generator_loss=2.1211435794830322, discriminator_loss=0.11872311681509018\n",
            "step 513: generator_loss=2.0526785850524902, discriminator_loss=0.1230088323354721\n",
            "step 514: generator_loss=2.1274776458740234, discriminator_loss=0.1198483407497406\n",
            "step 515: generator_loss=2.02187442779541, discriminator_loss=0.12762680649757385\n",
            "step 516: generator_loss=2.002258539199829, discriminator_loss=0.12729865312576294\n",
            "step 517: generator_loss=1.982901692390442, discriminator_loss=0.12766006588935852\n",
            "step 518: generator_loss=1.9873957633972168, discriminator_loss=0.12761744856834412\n",
            "step 519: generator_loss=1.9451091289520264, discriminator_loss=0.1321364790201187\n",
            "step 520: generator_loss=1.9623372554779053, discriminator_loss=0.12902823090553284\n",
            "step 521: generator_loss=1.9643621444702148, discriminator_loss=0.13051722943782806\n",
            "step 522: generator_loss=1.9561389684677124, discriminator_loss=0.13587304949760437\n",
            "step 523: generator_loss=1.9418914318084717, discriminator_loss=0.1356251984834671\n",
            "step 524: generator_loss=1.9528460502624512, discriminator_loss=0.13464601337909698\n",
            "step 525: generator_loss=1.9474997520446777, discriminator_loss=0.13953489065170288\n",
            "step 526: generator_loss=1.9587714672088623, discriminator_loss=0.13877896964550018\n",
            "step 527: generator_loss=1.948459506034851, discriminator_loss=0.13876867294311523\n",
            "step 528: generator_loss=1.9070857763290405, discriminator_loss=0.14652007818222046\n",
            "step 529: generator_loss=1.9307461977005005, discriminator_loss=0.14655694365501404\n",
            "step 530: generator_loss=1.8830317258834839, discriminator_loss=0.1547781080007553\n",
            "step 531: generator_loss=1.8821433782577515, discriminator_loss=0.15843160450458527\n",
            "step 532: generator_loss=1.854817509651184, discriminator_loss=0.16139918565750122\n",
            "step 533: generator_loss=1.8151721954345703, discriminator_loss=0.17104247212409973\n",
            "step 534: generator_loss=1.7913460731506348, discriminator_loss=0.17676858603954315\n",
            "step 535: generator_loss=1.8074116706848145, discriminator_loss=0.18190012872219086\n",
            "step 536: generator_loss=1.7487674951553345, discriminator_loss=0.1893022060394287\n",
            "step 537: generator_loss=1.769504189491272, discriminator_loss=0.1844114363193512\n",
            "step 538: generator_loss=1.7297288179397583, discriminator_loss=0.19736823439598083\n",
            "step 539: generator_loss=1.7262085676193237, discriminator_loss=0.20034164190292358\n",
            "step 540: generator_loss=1.7487308979034424, discriminator_loss=0.19782090187072754\n",
            "step 541: generator_loss=1.7263784408569336, discriminator_loss=0.2066602259874344\n",
            "step 542: generator_loss=1.7010979652404785, discriminator_loss=0.2142936885356903\n",
            "step 543: generator_loss=1.7373675107955933, discriminator_loss=0.2132464051246643\n",
            "step 544: generator_loss=1.778645396232605, discriminator_loss=0.21125952899456024\n",
            "step 545: generator_loss=1.745303988456726, discriminator_loss=0.22082588076591492\n",
            "step 546: generator_loss=1.805545687675476, discriminator_loss=0.22020401060581207\n",
            "step 547: generator_loss=1.832908272743225, discriminator_loss=0.21961213648319244\n",
            "step 548: generator_loss=1.8402396440505981, discriminator_loss=0.22103223204612732\n",
            "step 549: generator_loss=1.8294588327407837, discriminator_loss=0.23201653361320496\n",
            "step 550: generator_loss=1.8440136909484863, discriminator_loss=0.23020285367965698\n",
            "step 551: generator_loss=1.8785072565078735, discriminator_loss=0.2400253266096115\n",
            "step 552: generator_loss=1.8036677837371826, discriminator_loss=0.24965035915374756\n",
            "step 553: generator_loss=1.8026373386383057, discriminator_loss=0.2662200331687927\n",
            "step 554: generator_loss=1.8386094570159912, discriminator_loss=0.26882851123809814\n",
            "step 555: generator_loss=1.8757153749465942, discriminator_loss=0.2710179090499878\n",
            "step 556: generator_loss=1.932316541671753, discriminator_loss=0.2826288342475891\n",
            "step 557: generator_loss=1.9093269109725952, discriminator_loss=0.2938173711299896\n",
            "step 558: generator_loss=1.9678446054458618, discriminator_loss=0.2887967824935913\n",
            "step 559: generator_loss=2.0035955905914307, discriminator_loss=0.28542396426200867\n",
            "step 560: generator_loss=2.005401611328125, discriminator_loss=0.29437580704689026\n",
            "step 561: generator_loss=1.93464994430542, discriminator_loss=0.30928361415863037\n",
            "step 562: generator_loss=1.879477620124817, discriminator_loss=0.32934466004371643\n",
            "step 563: generator_loss=1.917860984802246, discriminator_loss=0.34008467197418213\n",
            "step 564: generator_loss=2.0056114196777344, discriminator_loss=0.30867457389831543\n",
            "step 565: generator_loss=1.8881065845489502, discriminator_loss=0.32290688157081604\n",
            "step 566: generator_loss=1.906260371208191, discriminator_loss=0.3153609037399292\n",
            "step 567: generator_loss=1.947774052619934, discriminator_loss=0.32445716857910156\n",
            "step 568: generator_loss=1.9269046783447266, discriminator_loss=0.32389989495277405\n",
            "step 569: generator_loss=1.935725450515747, discriminator_loss=0.3155028223991394\n",
            "step 570: generator_loss=1.9547487497329712, discriminator_loss=0.29713448882102966\n",
            "step 571: generator_loss=1.9622175693511963, discriminator_loss=0.29720523953437805\n",
            "step 572: generator_loss=2.044239044189453, discriminator_loss=0.27217692136764526\n",
            "step 573: generator_loss=2.0650577545166016, discriminator_loss=0.28050029277801514\n",
            "step 574: generator_loss=2.0473556518554688, discriminator_loss=0.27961570024490356\n",
            "step 575: generator_loss=2.0642242431640625, discriminator_loss=0.2741708755493164\n",
            "step 576: generator_loss=2.029085874557495, discriminator_loss=0.2891994118690491\n",
            "step 577: generator_loss=2.056501626968384, discriminator_loss=0.28923845291137695\n",
            "step 578: generator_loss=2.0540618896484375, discriminator_loss=0.28692877292633057\n",
            "step 579: generator_loss=2.0725464820861816, discriminator_loss=0.28021562099456787\n",
            "step 580: generator_loss=2.032101631164551, discriminator_loss=0.2920118272304535\n",
            "step 581: generator_loss=2.0416762828826904, discriminator_loss=0.2989351153373718\n",
            "step 582: generator_loss=2.081342935562134, discriminator_loss=0.2790892720222473\n",
            "step 583: generator_loss=2.0625391006469727, discriminator_loss=0.28345155715942383\n",
            "step 584: generator_loss=1.9983501434326172, discriminator_loss=0.3050782084465027\n",
            "step 585: generator_loss=2.0570240020751953, discriminator_loss=0.29484328627586365\n",
            "step 586: generator_loss=2.010594606399536, discriminator_loss=0.29528942704200745\n",
            "step 587: generator_loss=2.008880853652954, discriminator_loss=0.2885785400867462\n",
            "step 588: generator_loss=1.9783999919891357, discriminator_loss=0.3062037229537964\n",
            "step 589: generator_loss=1.9168058633804321, discriminator_loss=0.3119317889213562\n",
            "step 590: generator_loss=1.952762246131897, discriminator_loss=0.30429816246032715\n",
            "step 591: generator_loss=1.9570082426071167, discriminator_loss=0.30445557832717896\n",
            "step 592: generator_loss=1.9373934268951416, discriminator_loss=0.2997778654098511\n",
            "step 593: generator_loss=1.9093440771102905, discriminator_loss=0.3031345009803772\n",
            "step 594: generator_loss=1.9079866409301758, discriminator_loss=0.2969607710838318\n",
            "step 595: generator_loss=1.822989821434021, discriminator_loss=0.2998860478401184\n",
            "step 596: generator_loss=1.7568358182907104, discriminator_loss=0.3042067885398865\n",
            "step 597: generator_loss=1.7526570558547974, discriminator_loss=0.3039262294769287\n",
            "step 598: generator_loss=1.7875339984893799, discriminator_loss=0.3056814670562744\n",
            "step 599: generator_loss=1.8046563863754272, discriminator_loss=0.3013630211353302\n",
            "step 600: generator_loss=1.8358854055404663, discriminator_loss=0.2974125146865845\n",
            "step 601: generator_loss=1.9894219636917114, discriminator_loss=0.2842404842376709\n",
            "step 602: generator_loss=2.020846366882324, discriminator_loss=0.28077811002731323\n",
            "step 603: generator_loss=2.0248143672943115, discriminator_loss=0.28162065148353577\n",
            "step 604: generator_loss=2.210738182067871, discriminator_loss=0.26081568002700806\n",
            "step 605: generator_loss=2.3193819522857666, discriminator_loss=0.2482764720916748\n",
            "step 606: generator_loss=2.34289813041687, discriminator_loss=0.2487422525882721\n",
            "step 607: generator_loss=2.4971725940704346, discriminator_loss=0.22470179200172424\n",
            "step 608: generator_loss=2.4548985958099365, discriminator_loss=0.22536402940750122\n",
            "step 609: generator_loss=2.527743339538574, discriminator_loss=0.2092762589454651\n",
            "step 610: generator_loss=2.627575635910034, discriminator_loss=0.19240373373031616\n",
            "step 611: generator_loss=2.580390453338623, discriminator_loss=0.1936640441417694\n",
            "step 612: generator_loss=2.577104091644287, discriminator_loss=0.18770092725753784\n",
            "step 613: generator_loss=2.599501848220825, discriminator_loss=0.17725138366222382\n",
            "step 614: generator_loss=2.628450393676758, discriminator_loss=0.17131811380386353\n",
            "step 615: generator_loss=2.6408660411834717, discriminator_loss=0.16349482536315918\n",
            "step 616: generator_loss=2.6130993366241455, discriminator_loss=0.1603478193283081\n",
            "step 617: generator_loss=2.545431137084961, discriminator_loss=0.15976162254810333\n",
            "step 618: generator_loss=2.5536255836486816, discriminator_loss=0.1536397933959961\n",
            "step 619: generator_loss=2.4683990478515625, discriminator_loss=0.15573802590370178\n",
            "step 620: generator_loss=2.419402837753296, discriminator_loss=0.1573721170425415\n",
            "step 621: generator_loss=2.320909023284912, discriminator_loss=0.15845876932144165\n",
            "step 622: generator_loss=2.247377395629883, discriminator_loss=0.16253052651882172\n",
            "step 623: generator_loss=2.179053783416748, discriminator_loss=0.16975879669189453\n",
            "step 624: generator_loss=2.0924038887023926, discriminator_loss=0.18095900118350983\n",
            "step 625: generator_loss=1.9975744485855103, discriminator_loss=0.19280901551246643\n",
            "step 626: generator_loss=1.9555420875549316, discriminator_loss=0.20273810625076294\n",
            "step 627: generator_loss=1.867869257926941, discriminator_loss=0.22359883785247803\n",
            "step 628: generator_loss=1.7703077793121338, discriminator_loss=0.2551014721393585\n",
            "step 629: generator_loss=1.7214080095291138, discriminator_loss=0.28599458932876587\n",
            "step 630: generator_loss=1.711972713470459, discriminator_loss=0.31295859813690186\n",
            "step 631: generator_loss=1.6275228261947632, discriminator_loss=0.37376219034194946\n",
            "step 632: generator_loss=1.6249605417251587, discriminator_loss=0.4045414924621582\n",
            "step 633: generator_loss=1.6294256448745728, discriminator_loss=0.4441260099411011\n",
            "step 634: generator_loss=1.6295244693756104, discriminator_loss=0.5025085806846619\n",
            "step 635: generator_loss=1.547663927078247, discriminator_loss=0.5817499756813049\n",
            "step 636: generator_loss=1.616810917854309, discriminator_loss=0.5946162939071655\n",
            "step 637: generator_loss=1.5656622648239136, discriminator_loss=0.6348267793655396\n",
            "step 638: generator_loss=1.4771710634231567, discriminator_loss=0.692372739315033\n",
            "step 639: generator_loss=1.4252737760543823, discriminator_loss=0.6969685554504395\n",
            "step 640: generator_loss=1.3355178833007812, discriminator_loss=0.7150020599365234\n",
            "step 641: generator_loss=1.4512711763381958, discriminator_loss=0.6895154714584351\n",
            "step 642: generator_loss=1.507920265197754, discriminator_loss=0.6721084713935852\n",
            "step 643: generator_loss=1.6998125314712524, discriminator_loss=0.6157911419868469\n",
            "step 644: generator_loss=1.8848514556884766, discriminator_loss=0.5953382849693298\n",
            "step 645: generator_loss=2.120077133178711, discriminator_loss=0.5537100434303284\n",
            "step 646: generator_loss=2.395003318786621, discriminator_loss=0.515400767326355\n",
            "step 647: generator_loss=2.70511531829834, discriminator_loss=0.47333312034606934\n",
            "step 648: generator_loss=3.0550150871276855, discriminator_loss=0.4211871027946472\n",
            "step 649: generator_loss=3.3683698177337646, discriminator_loss=0.38128864765167236\n",
            "step 650: generator_loss=3.777674913406372, discriminator_loss=0.3426102101802826\n",
            "step 651: generator_loss=3.8723104000091553, discriminator_loss=0.2865927219390869\n",
            "step 652: generator_loss=3.9849023818969727, discriminator_loss=0.2829999029636383\n",
            "step 653: generator_loss=4.133060932159424, discriminator_loss=0.24894928932189941\n",
            "step 654: generator_loss=4.132483959197998, discriminator_loss=0.22920402884483337\n",
            "step 655: generator_loss=4.08785343170166, discriminator_loss=0.21299123764038086\n",
            "step 656: generator_loss=4.020091533660889, discriminator_loss=0.19447189569473267\n",
            "step 657: generator_loss=3.8997092247009277, discriminator_loss=0.191749706864357\n",
            "step 658: generator_loss=3.8714096546173096, discriminator_loss=0.19005921483039856\n",
            "step 659: generator_loss=3.660362958908081, discriminator_loss=0.17889918386936188\n",
            "step 660: generator_loss=3.446775436401367, discriminator_loss=0.19296421110630035\n",
            "step 661: generator_loss=3.2302258014678955, discriminator_loss=0.18593594431877136\n",
            "step 662: generator_loss=3.085972309112549, discriminator_loss=0.18581873178482056\n",
            "step 663: generator_loss=2.7869861125946045, discriminator_loss=0.1927819550037384\n",
            "step 664: generator_loss=2.682614326477051, discriminator_loss=0.19415220618247986\n",
            "step 665: generator_loss=2.454073905944824, discriminator_loss=0.20320984721183777\n",
            "step 666: generator_loss=2.3629820346832275, discriminator_loss=0.20927968621253967\n",
            "step 667: generator_loss=2.2255027294158936, discriminator_loss=0.21388478577136993\n",
            "step 668: generator_loss=2.085433006286621, discriminator_loss=0.22179262340068817\n",
            "step 669: generator_loss=1.929270625114441, discriminator_loss=0.2339850664138794\n",
            "step 670: generator_loss=1.8938385248184204, discriminator_loss=0.23590008914470673\n",
            "step 671: generator_loss=1.7800254821777344, discriminator_loss=0.24659861624240875\n",
            "step 672: generator_loss=1.7234630584716797, discriminator_loss=0.25497597455978394\n",
            "step 673: generator_loss=1.711050033569336, discriminator_loss=0.25789952278137207\n",
            "step 674: generator_loss=1.6673985719680786, discriminator_loss=0.26229849457740784\n",
            "step 675: generator_loss=1.6325711011886597, discriminator_loss=0.26662981510162354\n",
            "step 676: generator_loss=1.6562894582748413, discriminator_loss=0.2679484784603119\n",
            "step 677: generator_loss=1.619650959968567, discriminator_loss=0.2706880569458008\n",
            "step 678: generator_loss=1.6291636228561401, discriminator_loss=0.2708740830421448\n",
            "step 679: generator_loss=1.5833181142807007, discriminator_loss=0.27387914061546326\n",
            "step 680: generator_loss=1.5927720069885254, discriminator_loss=0.27468809485435486\n",
            "step 681: generator_loss=1.5633866786956787, discriminator_loss=0.27497509121894836\n",
            "step 682: generator_loss=1.5981333255767822, discriminator_loss=0.2672300338745117\n",
            "step 683: generator_loss=1.5915182828903198, discriminator_loss=0.264985591173172\n",
            "step 684: generator_loss=1.6153262853622437, discriminator_loss=0.25844651460647583\n",
            "step 685: generator_loss=1.5817443132400513, discriminator_loss=0.25960850715637207\n",
            "step 686: generator_loss=1.6068474054336548, discriminator_loss=0.25126907229423523\n",
            "step 687: generator_loss=1.622552752494812, discriminator_loss=0.2478744089603424\n",
            "step 688: generator_loss=1.5962127447128296, discriminator_loss=0.24804654717445374\n",
            "step 689: generator_loss=1.6316035985946655, discriminator_loss=0.2424091398715973\n",
            "step 690: generator_loss=1.6325099468231201, discriminator_loss=0.24073272943496704\n",
            "step 691: generator_loss=1.6594538688659668, discriminator_loss=0.23698005080223083\n",
            "step 692: generator_loss=1.6740872859954834, discriminator_loss=0.23462530970573425\n",
            "step 693: generator_loss=1.6802232265472412, discriminator_loss=0.2343275547027588\n",
            "step 694: generator_loss=1.7043691873550415, discriminator_loss=0.2305302619934082\n",
            "step 695: generator_loss=1.696391224861145, discriminator_loss=0.23220473527908325\n",
            "step 696: generator_loss=1.7057886123657227, discriminator_loss=0.23142442107200623\n",
            "step 697: generator_loss=1.700142741203308, discriminator_loss=0.23145945370197296\n",
            "step 698: generator_loss=1.7227998971939087, discriminator_loss=0.23016457259655\n",
            "step 699: generator_loss=1.7178953886032104, discriminator_loss=0.23144319653511047\n",
            "step 700: generator_loss=1.6927756071090698, discriminator_loss=0.23555326461791992\n",
            "step 701: generator_loss=1.713566541671753, discriminator_loss=0.23642536997795105\n",
            "step 702: generator_loss=1.730384111404419, discriminator_loss=0.2374516725540161\n",
            "step 703: generator_loss=1.7512760162353516, discriminator_loss=0.23928508162498474\n",
            "step 704: generator_loss=1.7800418138504028, discriminator_loss=0.2388472706079483\n",
            "step 705: generator_loss=1.7815250158309937, discriminator_loss=0.24286562204360962\n",
            "step 706: generator_loss=1.814793348312378, discriminator_loss=0.24238570034503937\n",
            "step 707: generator_loss=1.790108561515808, discriminator_loss=0.25032904744148254\n",
            "step 708: generator_loss=1.8074493408203125, discriminator_loss=0.250855416059494\n",
            "step 709: generator_loss=1.7875703573226929, discriminator_loss=0.25813257694244385\n",
            "step 710: generator_loss=1.8106306791305542, discriminator_loss=0.2610185742378235\n",
            "step 711: generator_loss=1.8141809701919556, discriminator_loss=0.2636643350124359\n",
            "step 712: generator_loss=1.7690660953521729, discriminator_loss=0.27688050270080566\n",
            "step 713: generator_loss=1.7847870588302612, discriminator_loss=0.27960267663002014\n",
            "step 714: generator_loss=1.7778339385986328, discriminator_loss=0.2930198907852173\n",
            "step 715: generator_loss=1.7890733480453491, discriminator_loss=0.2978798747062683\n",
            "step 716: generator_loss=1.7707929611206055, discriminator_loss=0.307300329208374\n",
            "step 717: generator_loss=1.811213493347168, discriminator_loss=0.31402355432510376\n",
            "step 718: generator_loss=1.764719843864441, discriminator_loss=0.32675641775131226\n",
            "step 719: generator_loss=1.7420012950897217, discriminator_loss=0.334922730922699\n",
            "step 720: generator_loss=1.7727481126785278, discriminator_loss=0.3366098403930664\n",
            "step 721: generator_loss=1.7697008848190308, discriminator_loss=0.3540831506252289\n",
            "step 722: generator_loss=1.799439787864685, discriminator_loss=0.33906054496765137\n",
            "step 723: generator_loss=1.7973030805587769, discriminator_loss=0.3486098349094391\n",
            "step 724: generator_loss=1.8417370319366455, discriminator_loss=0.33417394757270813\n",
            "step 725: generator_loss=1.855047583580017, discriminator_loss=0.3284592032432556\n",
            "step 726: generator_loss=1.8684722185134888, discriminator_loss=0.3132011890411377\n",
            "step 727: generator_loss=1.8782075643539429, discriminator_loss=0.304095059633255\n",
            "step 728: generator_loss=1.9211666584014893, discriminator_loss=0.2905214726924896\n",
            "step 729: generator_loss=1.912693977355957, discriminator_loss=0.2849094271659851\n",
            "step 730: generator_loss=1.9805073738098145, discriminator_loss=0.2709960341453552\n",
            "step 731: generator_loss=1.9661314487457275, discriminator_loss=0.269692987203598\n",
            "step 732: generator_loss=1.959617018699646, discriminator_loss=0.26649677753448486\n",
            "step 733: generator_loss=1.91480553150177, discriminator_loss=0.26783865690231323\n",
            "step 734: generator_loss=1.8459036350250244, discriminator_loss=0.2783792316913605\n",
            "step 735: generator_loss=1.8115040063858032, discriminator_loss=0.2849779725074768\n",
            "step 736: generator_loss=1.7864859104156494, discriminator_loss=0.29825496673583984\n",
            "step 737: generator_loss=1.799967885017395, discriminator_loss=0.3095625638961792\n",
            "step 738: generator_loss=1.8138985633850098, discriminator_loss=0.3235277831554413\n",
            "step 739: generator_loss=1.847424864768982, discriminator_loss=0.34007829427719116\n",
            "step 740: generator_loss=1.8312816619873047, discriminator_loss=0.35542646050453186\n",
            "step 741: generator_loss=1.7874505519866943, discriminator_loss=0.3758118152618408\n",
            "step 742: generator_loss=1.757983922958374, discriminator_loss=0.4018968939781189\n",
            "step 743: generator_loss=1.7487515211105347, discriminator_loss=0.4158405661582947\n",
            "step 744: generator_loss=1.7217541933059692, discriminator_loss=0.4395616054534912\n",
            "step 745: generator_loss=1.7423877716064453, discriminator_loss=0.4539644718170166\n",
            "step 746: generator_loss=1.7371312379837036, discriminator_loss=0.45725756883621216\n",
            "step 747: generator_loss=1.7526931762695312, discriminator_loss=0.46146899461746216\n",
            "step 748: generator_loss=1.74880850315094, discriminator_loss=0.46650952100753784\n",
            "step 749: generator_loss=1.8645360469818115, discriminator_loss=0.45112907886505127\n",
            "step 750: generator_loss=1.9403223991394043, discriminator_loss=0.4449494183063507\n",
            "step 751: generator_loss=2.0201785564422607, discriminator_loss=0.4238237142562866\n",
            "step 752: generator_loss=2.1083927154541016, discriminator_loss=0.4035775065422058\n",
            "step 753: generator_loss=2.196584463119507, discriminator_loss=0.3853253126144409\n",
            "step 754: generator_loss=2.255737781524658, discriminator_loss=0.36274784803390503\n",
            "step 755: generator_loss=2.3209798336029053, discriminator_loss=0.34322094917297363\n",
            "step 756: generator_loss=2.4079079627990723, discriminator_loss=0.32558026909828186\n",
            "step 757: generator_loss=2.434295415878296, discriminator_loss=0.2998732328414917\n",
            "step 758: generator_loss=2.4072108268737793, discriminator_loss=0.2920333445072174\n",
            "step 759: generator_loss=2.536848306655884, discriminator_loss=0.27225375175476074\n",
            "step 760: generator_loss=2.480220317840576, discriminator_loss=0.26236027479171753\n",
            "step 761: generator_loss=2.407503604888916, discriminator_loss=0.26408857107162476\n",
            "step 762: generator_loss=2.42679500579834, discriminator_loss=0.2573688328266144\n",
            "step 763: generator_loss=2.3841042518615723, discriminator_loss=0.25067564845085144\n",
            "step 764: generator_loss=2.3484699726104736, discriminator_loss=0.257112979888916\n",
            "step 765: generator_loss=2.2590322494506836, discriminator_loss=0.266567587852478\n",
            "step 766: generator_loss=2.123910903930664, discriminator_loss=0.2840215861797333\n",
            "step 767: generator_loss=2.150197744369507, discriminator_loss=0.2919621169567108\n",
            "step 768: generator_loss=2.092487335205078, discriminator_loss=0.2953452467918396\n",
            "step 769: generator_loss=2.04647159576416, discriminator_loss=0.2967297434806824\n",
            "step 770: generator_loss=1.928572654724121, discriminator_loss=0.3028374910354614\n",
            "step 771: generator_loss=1.846116542816162, discriminator_loss=0.3197745680809021\n",
            "step 772: generator_loss=1.8275576829910278, discriminator_loss=0.313182532787323\n",
            "step 773: generator_loss=1.7880311012268066, discriminator_loss=0.3153036832809448\n",
            "step 774: generator_loss=1.8831270933151245, discriminator_loss=0.2994100749492645\n",
            "step 775: generator_loss=1.916316270828247, discriminator_loss=0.2957024872303009\n",
            "step 776: generator_loss=1.9844696521759033, discriminator_loss=0.287986695766449\n",
            "step 777: generator_loss=1.9966551065444946, discriminator_loss=0.2888549566268921\n",
            "step 778: generator_loss=1.9712672233581543, discriminator_loss=0.2803562879562378\n",
            "step 779: generator_loss=1.9612544775009155, discriminator_loss=0.28344154357910156\n",
            "step 780: generator_loss=1.8771615028381348, discriminator_loss=0.28502142429351807\n",
            "step 781: generator_loss=1.7937674522399902, discriminator_loss=0.29062002897262573\n",
            "step 782: generator_loss=1.803855061531067, discriminator_loss=0.28899747133255005\n",
            "step 783: generator_loss=1.7329212427139282, discriminator_loss=0.29122304916381836\n",
            "step 784: generator_loss=1.7200428247451782, discriminator_loss=0.29565465450286865\n",
            "step 785: generator_loss=1.6897209882736206, discriminator_loss=0.3031492233276367\n",
            "step 786: generator_loss=1.656766653060913, discriminator_loss=0.30882835388183594\n",
            "step 787: generator_loss=1.6285629272460938, discriminator_loss=0.3117733597755432\n",
            "step 788: generator_loss=1.6043411493301392, discriminator_loss=0.31812584400177\n",
            "step 789: generator_loss=1.6160132884979248, discriminator_loss=0.3177095949649811\n",
            "step 790: generator_loss=1.637738585472107, discriminator_loss=0.3166564106941223\n",
            "step 791: generator_loss=1.6536197662353516, discriminator_loss=0.3150104880332947\n",
            "step 792: generator_loss=1.6750767230987549, discriminator_loss=0.31055641174316406\n",
            "step 793: generator_loss=1.74173104763031, discriminator_loss=0.3005923926830292\n",
            "step 794: generator_loss=1.762001633644104, discriminator_loss=0.29857999086380005\n",
            "step 795: generator_loss=1.8139557838439941, discriminator_loss=0.2853074073791504\n",
            "step 796: generator_loss=1.8179007768630981, discriminator_loss=0.2849299907684326\n",
            "step 797: generator_loss=1.7955964803695679, discriminator_loss=0.2777303159236908\n",
            "step 798: generator_loss=1.8061773777008057, discriminator_loss=0.27438294887542725\n",
            "step 799: generator_loss=1.8401035070419312, discriminator_loss=0.2661387324333191\n",
            "step 800: generator_loss=1.8211112022399902, discriminator_loss=0.2639259099960327\n",
            "step 801: generator_loss=1.8444910049438477, discriminator_loss=0.25729045271873474\n",
            "step 802: generator_loss=1.8200041055679321, discriminator_loss=0.2569860816001892\n",
            "step 803: generator_loss=1.822196125984192, discriminator_loss=0.2562101483345032\n",
            "step 804: generator_loss=1.8071461915969849, discriminator_loss=0.25929826498031616\n",
            "step 805: generator_loss=1.7596004009246826, discriminator_loss=0.26990827918052673\n",
            "step 806: generator_loss=1.756219744682312, discriminator_loss=0.27414757013320923\n",
            "step 807: generator_loss=1.7544044256210327, discriminator_loss=0.283708781003952\n",
            "step 808: generator_loss=1.694919228553772, discriminator_loss=0.2989189624786377\n",
            "step 809: generator_loss=1.6389195919036865, discriminator_loss=0.31830114126205444\n",
            "step 810: generator_loss=1.5722154378890991, discriminator_loss=0.34595873951911926\n",
            "step 811: generator_loss=1.5706393718719482, discriminator_loss=0.3574911952018738\n",
            "step 812: generator_loss=1.5078967809677124, discriminator_loss=0.3834175765514374\n",
            "step 813: generator_loss=1.4529157876968384, discriminator_loss=0.4021366238594055\n",
            "step 814: generator_loss=1.3969488143920898, discriminator_loss=0.426200270652771\n",
            "step 815: generator_loss=1.3364319801330566, discriminator_loss=0.44537830352783203\n",
            "step 816: generator_loss=1.3131719827651978, discriminator_loss=0.45917701721191406\n",
            "step 817: generator_loss=1.328540563583374, discriminator_loss=0.4644399881362915\n",
            "step 818: generator_loss=1.3456846475601196, discriminator_loss=0.46013039350509644\n",
            "step 819: generator_loss=1.3480305671691895, discriminator_loss=0.452455997467041\n",
            "step 820: generator_loss=1.3784703016281128, discriminator_loss=0.44164755940437317\n",
            "step 821: generator_loss=1.412064790725708, discriminator_loss=0.4238138794898987\n",
            "step 822: generator_loss=1.3811912536621094, discriminator_loss=0.4207804203033447\n",
            "step 823: generator_loss=1.4358060359954834, discriminator_loss=0.40035492181777954\n",
            "step 824: generator_loss=1.4339641332626343, discriminator_loss=0.3974173069000244\n",
            "step 825: generator_loss=1.4579612016677856, discriminator_loss=0.38595855236053467\n",
            "step 826: generator_loss=1.513038158416748, discriminator_loss=0.37517911195755005\n",
            "step 827: generator_loss=1.568015456199646, discriminator_loss=0.3629489243030548\n",
            "step 828: generator_loss=1.6181213855743408, discriminator_loss=0.35109394788742065\n",
            "step 829: generator_loss=1.6402047872543335, discriminator_loss=0.34248608350753784\n",
            "step 830: generator_loss=1.6622107028961182, discriminator_loss=0.3350997567176819\n",
            "step 831: generator_loss=1.6835376024246216, discriminator_loss=0.3277890682220459\n",
            "step 832: generator_loss=1.7101460695266724, discriminator_loss=0.3203037977218628\n",
            "step 833: generator_loss=1.6291298866271973, discriminator_loss=0.32981735467910767\n",
            "step 834: generator_loss=1.634914517402649, discriminator_loss=0.3332441449165344\n",
            "step 835: generator_loss=1.599534273147583, discriminator_loss=0.3400251865386963\n",
            "step 836: generator_loss=1.4979487657546997, discriminator_loss=0.354679137468338\n",
            "step 837: generator_loss=1.4753222465515137, discriminator_loss=0.3674488365650177\n",
            "step 838: generator_loss=1.5223091840744019, discriminator_loss=0.37160420417785645\n",
            "step 839: generator_loss=1.5189249515533447, discriminator_loss=0.37935328483581543\n",
            "step 840: generator_loss=1.511890172958374, discriminator_loss=0.3868064880371094\n",
            "step 841: generator_loss=1.4771007299423218, discriminator_loss=0.4024813175201416\n",
            "step 842: generator_loss=1.4807324409484863, discriminator_loss=0.40303516387939453\n",
            "step 843: generator_loss=1.464187502861023, discriminator_loss=0.41130417585372925\n",
            "step 844: generator_loss=1.4772591590881348, discriminator_loss=0.41030365228652954\n",
            "step 845: generator_loss=1.3951592445373535, discriminator_loss=0.4169066250324249\n",
            "step 846: generator_loss=1.4202169179916382, discriminator_loss=0.4098527729511261\n",
            "step 847: generator_loss=1.3919036388397217, discriminator_loss=0.41735121607780457\n",
            "step 848: generator_loss=1.4626179933547974, discriminator_loss=0.40317869186401367\n",
            "step 849: generator_loss=1.499936580657959, discriminator_loss=0.39657413959503174\n",
            "step 850: generator_loss=1.5254865884780884, discriminator_loss=0.3878211975097656\n",
            "step 851: generator_loss=1.5263500213623047, discriminator_loss=0.3869713544845581\n",
            "step 852: generator_loss=1.527833342552185, discriminator_loss=0.38503140211105347\n",
            "step 853: generator_loss=1.5514048337936401, discriminator_loss=0.3863118886947632\n",
            "step 854: generator_loss=1.5131356716156006, discriminator_loss=0.39258265495300293\n",
            "step 855: generator_loss=1.4912079572677612, discriminator_loss=0.3932327628135681\n",
            "step 856: generator_loss=1.4627395868301392, discriminator_loss=0.4076118469238281\n",
            "step 857: generator_loss=1.4157345294952393, discriminator_loss=0.4166174829006195\n",
            "step 858: generator_loss=1.3782497644424438, discriminator_loss=0.41665250062942505\n",
            "step 859: generator_loss=1.2930186986923218, discriminator_loss=0.44351696968078613\n",
            "step 860: generator_loss=1.308106541633606, discriminator_loss=0.4472455084323883\n",
            "step 861: generator_loss=1.2966399192810059, discriminator_loss=0.45952850580215454\n",
            "step 862: generator_loss=1.2907700538635254, discriminator_loss=0.4629341959953308\n",
            "step 863: generator_loss=1.3125793933868408, discriminator_loss=0.4630346894264221\n",
            "step 864: generator_loss=1.2633345127105713, discriminator_loss=0.47511035203933716\n",
            "step 865: generator_loss=1.2963340282440186, discriminator_loss=0.46008890867233276\n",
            "step 866: generator_loss=1.2447656393051147, discriminator_loss=0.4681199789047241\n",
            "step 867: generator_loss=1.2526060342788696, discriminator_loss=0.4553722143173218\n",
            "step 868: generator_loss=1.2791897058486938, discriminator_loss=0.4423117935657501\n",
            "step 869: generator_loss=1.2821687459945679, discriminator_loss=0.4368495047092438\n",
            "step 870: generator_loss=1.3286250829696655, discriminator_loss=0.4182531535625458\n",
            "step 871: generator_loss=1.370436668395996, discriminator_loss=0.4012126326560974\n",
            "step 872: generator_loss=1.4020402431488037, discriminator_loss=0.3889768719673157\n",
            "step 873: generator_loss=1.400915265083313, discriminator_loss=0.376468300819397\n",
            "step 874: generator_loss=1.342782974243164, discriminator_loss=0.37557685375213623\n",
            "step 875: generator_loss=1.36441969871521, discriminator_loss=0.3630315959453583\n",
            "step 876: generator_loss=1.3270255327224731, discriminator_loss=0.369321346282959\n",
            "step 877: generator_loss=1.343705177307129, discriminator_loss=0.36325711011886597\n",
            "step 878: generator_loss=1.4075047969818115, discriminator_loss=0.3565303385257721\n",
            "step 879: generator_loss=1.442762851715088, discriminator_loss=0.3567979633808136\n",
            "step 880: generator_loss=1.486642837524414, discriminator_loss=0.3629862368106842\n",
            "step 881: generator_loss=1.498032808303833, discriminator_loss=0.37060314416885376\n",
            "step 882: generator_loss=1.5853012800216675, discriminator_loss=0.37059512734413147\n",
            "step 883: generator_loss=1.6145938634872437, discriminator_loss=0.3736096918582916\n",
            "step 884: generator_loss=1.6077523231506348, discriminator_loss=0.3868481516838074\n",
            "step 885: generator_loss=1.6836293935775757, discriminator_loss=0.386846125125885\n",
            "step 886: generator_loss=1.7023614645004272, discriminator_loss=0.3742346465587616\n",
            "step 887: generator_loss=1.6687337160110474, discriminator_loss=0.3832629919052124\n",
            "step 888: generator_loss=1.6577950716018677, discriminator_loss=0.38003820180892944\n",
            "step 889: generator_loss=1.687187910079956, discriminator_loss=0.3846708834171295\n",
            "step 890: generator_loss=1.603071689605713, discriminator_loss=0.3809661269187927\n",
            "step 891: generator_loss=1.669568419456482, discriminator_loss=0.3890121579170227\n",
            "step 892: generator_loss=1.7355403900146484, discriminator_loss=0.37245500087738037\n",
            "step 893: generator_loss=1.687015175819397, discriminator_loss=0.3893125653266907\n",
            "step 894: generator_loss=1.6731990575790405, discriminator_loss=0.394561231136322\n",
            "step 895: generator_loss=1.614777684211731, discriminator_loss=0.40895360708236694\n",
            "step 896: generator_loss=1.5852584838867188, discriminator_loss=0.4216979742050171\n",
            "step 897: generator_loss=1.5549583435058594, discriminator_loss=0.4288138151168823\n",
            "step 898: generator_loss=1.5454636812210083, discriminator_loss=0.43714120984077454\n",
            "step 899: generator_loss=1.4849154949188232, discriminator_loss=0.4540613889694214\n",
            "step 900: generator_loss=1.4895577430725098, discriminator_loss=0.46408897638320923\n",
            "step 901: generator_loss=1.4984642267227173, discriminator_loss=0.4713912010192871\n",
            "step 902: generator_loss=1.5488743782043457, discriminator_loss=0.47368356585502625\n",
            "step 903: generator_loss=1.5934771299362183, discriminator_loss=0.4766777753829956\n",
            "step 904: generator_loss=1.640528917312622, discriminator_loss=0.47657978534698486\n",
            "step 905: generator_loss=1.6621160507202148, discriminator_loss=0.4689095616340637\n",
            "step 906: generator_loss=1.7331585884094238, discriminator_loss=0.4646967053413391\n",
            "step 907: generator_loss=1.6716599464416504, discriminator_loss=0.4599653482437134\n",
            "step 908: generator_loss=1.6171107292175293, discriminator_loss=0.45155078172683716\n",
            "step 909: generator_loss=1.5999575853347778, discriminator_loss=0.4422956109046936\n",
            "step 910: generator_loss=1.602841854095459, discriminator_loss=0.4358256161212921\n",
            "step 911: generator_loss=1.6405704021453857, discriminator_loss=0.42596161365509033\n",
            "step 912: generator_loss=1.6424463987350464, discriminator_loss=0.4188445210456848\n",
            "step 913: generator_loss=1.6817684173583984, discriminator_loss=0.422730952501297\n",
            "step 914: generator_loss=1.7398096323013306, discriminator_loss=0.41437438130378723\n",
            "step 915: generator_loss=1.6137058734893799, discriminator_loss=0.4238991141319275\n",
            "step 916: generator_loss=1.5175191164016724, discriminator_loss=0.4269041419029236\n",
            "step 917: generator_loss=1.4741729497909546, discriminator_loss=0.43004873394966125\n",
            "step 918: generator_loss=1.4775294065475464, discriminator_loss=0.41248267889022827\n",
            "step 919: generator_loss=1.4371132850646973, discriminator_loss=0.4091089963912964\n",
            "step 920: generator_loss=1.5102139711380005, discriminator_loss=0.3931220769882202\n",
            "step 921: generator_loss=1.6066436767578125, discriminator_loss=0.366707444190979\n",
            "step 922: generator_loss=1.6607308387756348, discriminator_loss=0.34231486916542053\n",
            "step 923: generator_loss=1.737048625946045, discriminator_loss=0.3226039409637451\n",
            "step 924: generator_loss=1.7435081005096436, discriminator_loss=0.3099586069583893\n",
            "step 925: generator_loss=1.824877142906189, discriminator_loss=0.2899019718170166\n",
            "step 926: generator_loss=1.8364382982254028, discriminator_loss=0.2796287536621094\n",
            "step 927: generator_loss=1.9273126125335693, discriminator_loss=0.26428285241127014\n",
            "step 928: generator_loss=1.9287359714508057, discriminator_loss=0.25888338685035706\n",
            "step 929: generator_loss=1.9702876806259155, discriminator_loss=0.2503207325935364\n",
            "step 930: generator_loss=1.899137258529663, discriminator_loss=0.2538968622684479\n",
            "step 931: generator_loss=1.9167717695236206, discriminator_loss=0.25071221590042114\n",
            "step 932: generator_loss=1.887614369392395, discriminator_loss=0.25238466262817383\n",
            "step 933: generator_loss=1.8835846185684204, discriminator_loss=0.2557932734489441\n",
            "step 934: generator_loss=1.8158411979675293, discriminator_loss=0.26251405477523804\n",
            "step 935: generator_loss=1.7295176982879639, discriminator_loss=0.27446746826171875\n",
            "step 936: generator_loss=1.6716971397399902, discriminator_loss=0.2922173738479614\n",
            "step 937: generator_loss=1.6739217042922974, discriminator_loss=0.2964744567871094\n",
            "step 938: generator_loss=1.6421642303466797, discriminator_loss=0.31008267402648926\n",
            "step 939: generator_loss=1.604119062423706, discriminator_loss=0.3315649628639221\n",
            "step 940: generator_loss=1.6295242309570312, discriminator_loss=0.3429546356201172\n",
            "step 941: generator_loss=1.6274304389953613, discriminator_loss=0.35411909222602844\n",
            "step 942: generator_loss=1.6252468824386597, discriminator_loss=0.35734081268310547\n",
            "step 943: generator_loss=1.5872336626052856, discriminator_loss=0.3684161305427551\n",
            "step 944: generator_loss=1.5830022096633911, discriminator_loss=0.3680844008922577\n",
            "step 945: generator_loss=1.5252561569213867, discriminator_loss=0.37760859727859497\n",
            "step 946: generator_loss=1.5028482675552368, discriminator_loss=0.38402318954467773\n",
            "step 947: generator_loss=1.448607325553894, discriminator_loss=0.38989633321762085\n",
            "step 948: generator_loss=1.4374442100524902, discriminator_loss=0.39198845624923706\n",
            "step 949: generator_loss=1.5070699453353882, discriminator_loss=0.38297051191329956\n",
            "step 950: generator_loss=1.5136152505874634, discriminator_loss=0.3832739591598511\n",
            "step 951: generator_loss=1.502945065498352, discriminator_loss=0.39251452684402466\n",
            "step 952: generator_loss=1.535514235496521, discriminator_loss=0.38399171829223633\n",
            "step 953: generator_loss=1.656351923942566, discriminator_loss=0.37327706813812256\n",
            "step 954: generator_loss=1.6332027912139893, discriminator_loss=0.36854150891304016\n",
            "step 955: generator_loss=1.583329200744629, discriminator_loss=0.3743460476398468\n",
            "step 956: generator_loss=1.558021903038025, discriminator_loss=0.3636819124221802\n",
            "step 957: generator_loss=1.6558738946914673, discriminator_loss=0.3564065098762512\n",
            "step 958: generator_loss=1.6378711462020874, discriminator_loss=0.35450825095176697\n",
            "step 959: generator_loss=1.7129902839660645, discriminator_loss=0.33968061208724976\n",
            "step 960: generator_loss=1.6734880208969116, discriminator_loss=0.3498933017253876\n",
            "step 961: generator_loss=1.7121689319610596, discriminator_loss=0.3490922451019287\n",
            "step 962: generator_loss=1.7244476079940796, discriminator_loss=0.3492010235786438\n",
            "step 963: generator_loss=1.7485501766204834, discriminator_loss=0.3494434654712677\n",
            "step 964: generator_loss=1.7667263746261597, discriminator_loss=0.35498201847076416\n",
            "step 965: generator_loss=1.689217209815979, discriminator_loss=0.36368870735168457\n",
            "step 966: generator_loss=1.6606407165527344, discriminator_loss=0.3689975142478943\n",
            "step 967: generator_loss=1.5805097818374634, discriminator_loss=0.3773046135902405\n",
            "step 968: generator_loss=1.5926074981689453, discriminator_loss=0.3841916024684906\n",
            "step 969: generator_loss=1.588939905166626, discriminator_loss=0.38227003812789917\n",
            "step 970: generator_loss=1.5656753778457642, discriminator_loss=0.38642895221710205\n",
            "step 971: generator_loss=1.5812512636184692, discriminator_loss=0.3964846134185791\n",
            "step 972: generator_loss=1.583120584487915, discriminator_loss=0.3911493122577667\n",
            "step 973: generator_loss=1.586702585220337, discriminator_loss=0.3973029851913452\n",
            "step 974: generator_loss=1.6003515720367432, discriminator_loss=0.3994213342666626\n",
            "step 975: generator_loss=1.5979377031326294, discriminator_loss=0.40496352314949036\n",
            "step 976: generator_loss=1.4750196933746338, discriminator_loss=0.42378169298171997\n",
            "step 977: generator_loss=1.4479420185089111, discriminator_loss=0.43031901121139526\n",
            "step 978: generator_loss=1.4296146631240845, discriminator_loss=0.43750178813934326\n",
            "step 979: generator_loss=1.4006240367889404, discriminator_loss=0.44344085454940796\n",
            "step 980: generator_loss=1.3596023321151733, discriminator_loss=0.4554045796394348\n",
            "step 981: generator_loss=1.259667158126831, discriminator_loss=0.47408148646354675\n",
            "step 982: generator_loss=1.321526288986206, discriminator_loss=0.46852725744247437\n",
            "step 983: generator_loss=1.3031470775604248, discriminator_loss=0.46310707926750183\n",
            "step 984: generator_loss=1.265113353729248, discriminator_loss=0.4715052843093872\n",
            "step 985: generator_loss=1.2510634660720825, discriminator_loss=0.466891348361969\n",
            "step 986: generator_loss=1.2983442544937134, discriminator_loss=0.46317365765571594\n",
            "step 987: generator_loss=1.202025055885315, discriminator_loss=0.47030067443847656\n",
            "step 988: generator_loss=1.2223999500274658, discriminator_loss=0.4677006006240845\n",
            "step 989: generator_loss=1.2561269998550415, discriminator_loss=0.46634846925735474\n",
            "step 990: generator_loss=1.2215757369995117, discriminator_loss=0.47182464599609375\n",
            "step 991: generator_loss=1.2214093208312988, discriminator_loss=0.4687468707561493\n",
            "step 992: generator_loss=1.181839942932129, discriminator_loss=0.48088762164115906\n",
            "step 993: generator_loss=1.229413390159607, discriminator_loss=0.47061628103256226\n",
            "step 994: generator_loss=1.2633135318756104, discriminator_loss=0.46376848220825195\n",
            "step 995: generator_loss=1.2491953372955322, discriminator_loss=0.4624062180519104\n",
            "step 996: generator_loss=1.1874295473098755, discriminator_loss=0.4640936851501465\n",
            "step 997: generator_loss=1.1998469829559326, discriminator_loss=0.45431947708129883\n",
            "step 998: generator_loss=1.1919282674789429, discriminator_loss=0.44218534231185913\n",
            "step 999: generator_loss=1.2347168922424316, discriminator_loss=0.4236457943916321\n",
            "step 1000: generator_loss=1.2553682327270508, discriminator_loss=0.4109814167022705\n"
          ]
        }
      ],
      "source": [
        "generator_training_state, discriminator_training_state, key = train_gan(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    n_steps=1000,\n",
        "    key=jax.random.key(0),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "84de1050",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample latent vector and a real image index\n",
        "key, z_key, real_key = jax.random.split(key, 3)\n",
        "z_vector = jax.random.normal(z_key, (1, train_data[\"image\"].shape[1]))\n",
        "real_idx = jax.random.randint(real_key, shape=(), minval=0, maxval=train_data[\"image\"].shape[0])\n",
        "\n",
        "# Generate fake image\n",
        "fake_image_flat = generator_training_state.apply_fn(\n",
        "    {\"params\": generator_training_state.params},\n",
        "    z_vector,\n",
        ")\n",
        "fake_image = fake_image_flat[0].reshape(28, 28)\n",
        "\n",
        "# Select corresponding real image\n",
        "real_image_flat = train_data[\"image\"][real_idx]\n",
        "real_image = real_image_flat.reshape(28, 28)\n",
        "\n",
        "# Plot real vs fake side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
        "axes[0].imshow(real_image, cmap=\"gray\")\n",
        "axes[0].set_title(\"Real\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(fake_image, cmap=\"gray\")\n",
        "axes[1].set_title(\"Fake\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
