{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76bffd5b",
      "metadata": {},
      "source": [
        "<small>\n",
        "\n",
        "**Key differences from JAX implementation:**  \n",
        "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
        "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
        "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
        "\n",
        "</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_datasets as tfds\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "558de4e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765323872.906212    1569 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1765323872.921617    1569 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST from TensorFlow Datasets\n",
        "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0fb40841",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalise(x, x_max=255.0):\n",
        "    return x / x_max\n",
        "\n",
        "def convert_to_jax(data_np, data_type):\n",
        "    if data_type == \"image\":\n",
        "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
        "    elif data_type == \"label\":\n",
        "        data_jax = jnp.array(data_np)\n",
        "    else:\n",
        "        raise ValueError(\"not image or label\")\n",
        "    return data_jax\n",
        "\n",
        "def flatten_image_for_mlp(data_jax):\n",
        "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
        "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
        "    data_flattened = data_jax.reshape(n_batch, -1)\n",
        "    return data_flattened\n",
        "\n",
        "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
        "    data_jax = {}\n",
        "    for data_type, data_tf in data_dict.items():\n",
        "        data_numpy = data_tf.numpy()\n",
        "        data = convert_to_jax(data_numpy, data_type)\n",
        "        if data_type == \"image\":\n",
        "            data = flatten_image_for_mlp(data)\n",
        "        if subsample_size > 0:\n",
        "            data = data[:subsample_size]\n",
        "        data_jax[data_type] = data\n",
        "\n",
        "    return data_jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "244a19b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
        "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
        "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
        "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
        "    return cross_entropy_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fa6224c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_training_step(training_state, images, labels):\n",
        "    \"\"\"\n",
        "    Single training step \n",
        "    The model and optimiser are passed in the training state\n",
        "    returns a training state\n",
        "    \"\"\"\n",
        "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
        "        training_state.apply_fn,\n",
        "        images,\n",
        "        labels,\n",
        "    )\n",
        "    return training_state.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batches(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27fcb22b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(layer_sizes, optimizer):\n",
        "    layer_part = \"mlp_\" + \"-\".join(str(s) for s in layer_sizes)\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"{layer_part}_{opt_name}\"\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"mlp\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "72c149a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    images,\n",
        "    labels,\n",
        "    n_steps,\n",
        "    layer_sizes,\n",
        "    optimizer,\n",
        "    checkpoint_manager,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    training_state,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    ): \n",
        "    \"\"\"\n",
        "    The training state ('state') is an instance of TrainState that holds:\n",
        "    - apply_fn: the model's apply function, used for forward passes\n",
        "    - params: the parameters of the neural network\n",
        "    - tx: the optimizers (Optax transformation) for parameter updates\n",
        "    - opt_state: the state of the optimizer\n",
        "    \"\"\"\n",
        "    if training_state is None:\n",
        "        training_state = create_training_state(\n",
        "            layer_sizes,\n",
        "            optimizer,\n",
        "            key,\n",
        "            use_lowrank=use_lowrank,\n",
        "            rank=rank,\n",
        "        )\n",
        "\n",
        "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
        "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
        "        step = training_state.step\n",
        "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
        "        print(f\"step {step}: loss={loss}\")\n",
        "        if step == 1 or step % steps_per_save == 0:\n",
        "            step_dir = step\n",
        "            checkpoint_manager.save(\n",
        "                step_dir,\n",
        "                args=ocp.args.StandardSave(training_state)\n",
        "                )\n",
        "\n",
        "    return training_state.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "852ef6e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:2025-12-09 23:44:34,989:jax._src.xla_bridge:854: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
            "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
          ]
        }
      ],
      "source": [
        "def train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    n_steps=10**3,\n",
        "    steps_per_save=100,\n",
        "    training_state=None,\n",
        "    key=jax.random.key(0),\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=(784, 128, 10),\n",
        "):\n",
        "    layer_sizes = list(layer_sizes)\n",
        "    experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        experiment_name = experiment_name + f\"_lowrank-r{rank}\"\n",
        "\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    final_params = run_training(\n",
        "        images=train_data[\"image\"],\n",
        "        labels=train_data[\"label\"],\n",
        "        n_steps=n_steps,\n",
        "        layer_sizes=layer_sizes,\n",
        "        optimizer=optimizer,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        training_state=training_state,\n",
        "        use_lowrank=use_lowrank,\n",
        "        rank=rank,\n",
        "    )\n",
        "    return final_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dca913e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_layer_sizes(params):\n",
        "    layer_sizes = []\n",
        "    for layer, layer_params in enumerate(params.values()):\n",
        "        if layer == 0:\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
        "        else:\n",
        "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
        "    return layer_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1b7d5d50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mlp(\n",
        "    test_data,\n",
        "    params,\n",
        "    n_examples=10,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=None,\n",
        "):\n",
        "    images = test_data[\"image\"]\n",
        "    labels = test_data[\"label\"]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if layer_sizes is None:\n",
        "            raise ValueError(\"layer_sizes must be provided when use_lowrank=True\")\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=layer_sizes[1:], rank=rank)\n",
        "    else:\n",
        "        layer_sizes = extract_layer_sizes(params)\n",
        "        model = MLP(layer_sizes=layer_sizes[1:])\n",
        "\n",
        "    apply_fn = model.apply\n",
        "\n",
        "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
        "    example_images = images[:n_examples]\n",
        "    example_labels = labels[:n_examples]\n",
        "    logits = apply_fn({\"params\": params}, example_images)\n",
        "    example_predictions = jnp.argmax(logits, axis=1)\n",
        "\n",
        "    prefix = \"[low-rank] \" if use_lowrank else \"\"\n",
        "    print(prefix + \"Mean loss       \", mean_loss)\n",
        "    print(prefix + \"True labels:    \", example_labels)\n",
        "    print(prefix + \"Predictions:    \", example_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b876ad27",
      "metadata": {},
      "source": [
        "1. Learning rate decay\n",
        "2. Weight decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
        "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b492873c",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c4d036d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_from_step = 1000  # e.g. resume from checkpoint at step 1000\n",
        "layer_sizes = [784, 128, 10]\n",
        "\n",
        "experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "template_state = create_training_state(layer_sizes, optimizer, jax.random.key(0))\n",
        "restored_state = checkpoint_manager.restore(\n",
        "    resume_from_step,\n",
        "    args=ocp.args.StandardRestore(template_state),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6352f95d",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "rank = 32\n",
        "layer_sizes = (784, 128, 10)\n",
        "\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "params_lowrank = train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    use_lowrank=True,\n",
        "    rank=rank,\n",
        "    layer_sizes=layer_sizes,\n",
        ")\n",
        "evaluate_mlp(\n",
        "    test_data,\n",
        "    params_lowrank,\n",
        "    use_lowrank=True,\n",
        "    rank=rank,\n",
        "    layer_sizes=layer_sizes,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f61693",
      "metadata": {},
      "source": [
        "GANs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc99c571",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gan_discriminator_loss(d_real_logits, d_fake_logits):\n",
        "    real_targets = jnp.ones_like(d_real_logits)\n",
        "    fake_targets = jnp.zeros_like(d_fake_logits)\n",
        "    real_loss = optax.sigmoid_binary_cross_entropy(d_real_logits, real_targets)\n",
        "    fake_loss = optax.sigmoid_binary_cross_entropy(d_fake_logits, fake_targets)\n",
        "    return (real_loss.mean() + fake_loss.mean())\n",
        "\n",
        "\n",
        "def gan_generator_loss(d_fake_logits):\n",
        "    targets = jnp.ones_like(d_fake_logits)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(d_fake_logits, targets)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "98852b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_discriminator_loss_batch(real_logits, fake_logits):\n",
        "    \"\"\"\n",
        "    Discriminator loss.\n",
        "\n",
        "    D sees real images and fake images.\n",
        "    It should output 1 for real and 0 for fake, so we use\n",
        "    two binary-cross-entropy terms: one for real, one for fake.\n",
        "    \"\"\"\n",
        "    real_labels = jnp.ones_like(real_logits)\n",
        "    fake_labels = jnp.zeros_like(fake_logits)\n",
        "\n",
        "    real_loss = optax.sigmoid_binary_cross_entropy(logits=real_logits, labels=real_labels).mean()\n",
        "    fake_loss = optax.sigmoid_binary_cross_entropy(logits=fake_logits, labels=fake_labels).mean()\n",
        "\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "\n",
        "def calculate_generator_loss_batch(fake_logits):\n",
        "    \"\"\"\n",
        "    Generator loss.\n",
        "\n",
        "    G only affects fake images, so its loss only depends on D(fake).\n",
        "    We DON'T use the exact mirror of the discriminator's loss,\n",
        "    because that version gives almost no gradient once D gets good.\n",
        "    Instead we use the 'non-saturating' version: make D(fake) look real (label=1).\n",
        "    \"\"\"\n",
        "    fool_labels = jnp.ones_like(fake_logits)  # generator wants D(fake)=1\n",
        "    return optax.sigmoid_binary_cross_entropy(logits=fake_logits, labels=fool_labels).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a3523c4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D loss: 1.386295\n",
            "G loss: 0.6931475\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "real_logits = jnp.zeros((batch_size, 1), dtype=jnp.float32)  # predicts 0.5 for real\n",
        "fake_logits = jnp.zeros((batch_size, 1), dtype=jnp.float32)  # predicts 0.5 for fake\n",
        "\n",
        "print(\"D loss:\", calculate_discriminator_loss_batch(real_logits, fake_logits))  # ~ 1.386\n",
        "print(\"G loss:\", calculate_generator_loss_batch(fake_logits))                  # ~ 0.693"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
