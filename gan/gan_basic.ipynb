{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76bffd5b",
      "metadata": {},
      "source": [
        "<small>\n",
        "\n",
        "**Key differences from JAX implementation:**  \n",
        "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
        "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
        "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
        "\n",
        "</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_datasets as tfds\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "558de4e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST from TensorFlow Datasets\n",
        "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0fb40841",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalise(x, x_max=255.0):\n",
        "    return x / x_max\n",
        "\n",
        "def convert_to_jax(data_np, data_type):\n",
        "    if data_type == \"image\":\n",
        "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
        "    elif data_type == \"label\":\n",
        "        data_jax = jnp.array(data_np)\n",
        "    else:\n",
        "        raise ValueError(\"not image or label\")\n",
        "    return data_jax\n",
        "\n",
        "def flatten_image_for_mlp(data_jax):\n",
        "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
        "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
        "    data_flattened = data_jax.reshape(n_batch, -1)\n",
        "    return data_flattened\n",
        "\n",
        "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
        "    data_jax = {}\n",
        "    for data_type, data_tf in data_dict.items():\n",
        "        data_numpy = data_tf.numpy()\n",
        "        data = convert_to_jax(data_numpy, data_type)\n",
        "        if data_type == \"image\":\n",
        "            data = flatten_image_for_mlp(data)\n",
        "        if subsample_size > 0:\n",
        "            data = data[:subsample_size]\n",
        "        data_jax[data_type] = data\n",
        "\n",
        "    return data_jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "244a19b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
        "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
        "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
        "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
        "    return cross_entropy_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fa6224c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_training_step(training_state, images, labels):\n",
        "    \"\"\"\n",
        "    Single training step \n",
        "    The model and optimiser are passed in the training state\n",
        "    returns a training state\n",
        "    \"\"\"\n",
        "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
        "        training_state.apply_fn,\n",
        "        images,\n",
        "        labels,\n",
        "    )\n",
        "    return training_state.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batches(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "27fcb22b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(layer_sizes, optimizer):\n",
        "    layer_part = \"mlp_\" + \"-\".join(str(s) for s in layer_sizes)\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"{layer_part}_{opt_name}\"\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"mlp\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "72c149a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    images,\n",
        "    labels,\n",
        "    n_steps,\n",
        "    layer_sizes,\n",
        "    optimizer,\n",
        "    checkpoint_manager,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    training_state,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    ): \n",
        "    \"\"\"\n",
        "    The training state ('state') is an instance of TrainState that holds:\n",
        "    - apply_fn: the model's apply function, used for forward passes\n",
        "    - params: the parameters of the neural network\n",
        "    - tx: the optimizers (Optax transformation) for parameter updates\n",
        "    - opt_state: the state of the optimizer\n",
        "    \"\"\"\n",
        "    if training_state is None:\n",
        "        training_state = create_training_state(\n",
        "            layer_sizes,\n",
        "            optimizer,\n",
        "            key,\n",
        "            use_lowrank=use_lowrank,\n",
        "            rank=rank,\n",
        "        )\n",
        "\n",
        "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
        "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
        "        step = training_state.step\n",
        "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
        "        print(f\"step {step}: loss={loss}\")\n",
        "        if step == 1 or step % steps_per_save == 0:\n",
        "            step_dir = step\n",
        "            checkpoint_manager.save(\n",
        "                step_dir,\n",
        "                args=ocp.args.StandardSave(training_state)\n",
        "                )\n",
        "\n",
        "    return training_state.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "852ef6e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    n_steps=10**3,\n",
        "    steps_per_save=100,\n",
        "    training_state=None,\n",
        "    key=jax.random.key(0),\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=(784, 128, 10),\n",
        "):\n",
        "    layer_sizes = list(layer_sizes)\n",
        "    experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        experiment_name = experiment_name + f\"_lowrank-r{rank}\"\n",
        "\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    final_params = run_training(\n",
        "        images=train_data[\"image\"],\n",
        "        labels=train_data[\"label\"],\n",
        "        n_steps=n_steps,\n",
        "        layer_sizes=layer_sizes,\n",
        "        optimizer=optimizer,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        training_state=training_state,\n",
        "        use_lowrank=use_lowrank,\n",
        "        rank=rank,\n",
        "    )\n",
        "    return final_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "dca913e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_layer_sizes(params):\n",
        "    layer_sizes = []\n",
        "    for layer, layer_params in enumerate(params.values()):\n",
        "        if layer == 0:\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
        "        else:\n",
        "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
        "    return layer_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1b7d5d50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mlp(\n",
        "    test_data,\n",
        "    params,\n",
        "    n_examples=10,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=None,\n",
        "):\n",
        "    images = test_data[\"image\"]\n",
        "    labels = test_data[\"label\"]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if layer_sizes is None:\n",
        "            raise ValueError(\"layer_sizes must be provided when use_lowrank=True\")\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=layer_sizes[1:], rank=rank)\n",
        "    else:\n",
        "        layer_sizes = extract_layer_sizes(params)\n",
        "        model = MLP(layer_sizes=layer_sizes[1:])\n",
        "\n",
        "    apply_fn = model.apply\n",
        "\n",
        "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
        "    example_images = images[:n_examples]\n",
        "    example_labels = labels[:n_examples]\n",
        "    logits = apply_fn({\"params\": params}, example_images)\n",
        "    example_predictions = jnp.argmax(logits, axis=1)\n",
        "\n",
        "    prefix = \"[low-rank] \" if use_lowrank else \"\"\n",
        "    print(prefix + \"Mean loss       \", mean_loss)\n",
        "    print(prefix + \"True labels:    \", example_labels)\n",
        "    print(prefix + \"Predictions:    \", example_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b876ad27",
      "metadata": {},
      "source": [
        "1. Learning rate decay\n",
        "2. Weight decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
        "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b492873c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=2.2421865463256836\n",
            "step 2: loss=1.4549814462661743\n",
            "step 3: loss=3.0767364501953125\n",
            "step 4: loss=1.6018766164779663\n",
            "step 5: loss=1.922335147857666\n",
            "step 6: loss=1.5518207550048828\n",
            "step 7: loss=2.2047572135925293\n",
            "step 8: loss=2.28387188911438\n",
            "step 9: loss=1.3409204483032227\n",
            "step 10: loss=2.344090700149536\n",
            "step 11: loss=2.6371214389801025\n",
            "step 12: loss=3.4621236324310303\n",
            "step 13: loss=1.713283658027649\n",
            "step 14: loss=1.9781702756881714\n",
            "step 15: loss=3.3416006565093994\n",
            "step 16: loss=2.9397127628326416\n",
            "step 17: loss=1.0316027402877808\n",
            "step 18: loss=2.92775821685791\n",
            "step 19: loss=2.7445106506347656\n",
            "step 20: loss=1.4158217906951904\n",
            "step 21: loss=1.9610295295715332\n",
            "step 22: loss=1.9986664056777954\n",
            "step 23: loss=1.672990083694458\n",
            "step 24: loss=2.457414388656616\n",
            "step 25: loss=1.5557224750518799\n",
            "step 26: loss=2.3051018714904785\n",
            "step 27: loss=2.603790044784546\n",
            "step 28: loss=1.988590955734253\n",
            "step 29: loss=2.3711462020874023\n",
            "step 30: loss=1.6227275133132935\n",
            "step 31: loss=1.862285852432251\n",
            "step 32: loss=2.1689186096191406\n",
            "step 33: loss=1.754330039024353\n",
            "step 34: loss=1.313774824142456\n",
            "step 35: loss=1.7799018621444702\n",
            "step 36: loss=1.6797168254852295\n",
            "step 37: loss=2.9055237770080566\n",
            "step 38: loss=1.5091187953948975\n",
            "step 39: loss=2.635629892349243\n",
            "step 40: loss=1.9036000967025757\n",
            "step 41: loss=0.8337869644165039\n",
            "step 42: loss=2.479337453842163\n",
            "step 43: loss=2.0102226734161377\n",
            "step 44: loss=1.8198356628417969\n",
            "step 45: loss=2.361449956893921\n",
            "step 46: loss=1.0358593463897705\n",
            "step 47: loss=2.0157792568206787\n",
            "step 48: loss=1.810377597808838\n",
            "step 49: loss=3.01554536819458\n",
            "step 50: loss=1.4894685745239258\n",
            "step 51: loss=1.1730490922927856\n",
            "step 52: loss=1.2065200805664062\n",
            "step 53: loss=3.256063938140869\n",
            "step 54: loss=1.3365157842636108\n",
            "step 55: loss=1.2758262157440186\n",
            "step 56: loss=2.6482057571411133\n",
            "step 57: loss=2.540787696838379\n",
            "step 58: loss=1.3128734827041626\n",
            "step 59: loss=2.506495714187622\n",
            "step 60: loss=1.0716583728790283\n",
            "step 61: loss=0.8002644777297974\n",
            "step 62: loss=1.215010404586792\n",
            "step 63: loss=3.9132957458496094\n",
            "step 64: loss=2.513690233230591\n",
            "step 65: loss=2.3943910598754883\n",
            "step 66: loss=0.41791653633117676\n",
            "step 67: loss=0.9733765125274658\n",
            "step 68: loss=0.6354427337646484\n",
            "step 69: loss=3.076798439025879\n",
            "step 70: loss=1.3095070123672485\n",
            "step 71: loss=3.296731948852539\n",
            "step 72: loss=3.5359928607940674\n",
            "step 73: loss=0.5057768821716309\n",
            "step 74: loss=1.0847030878067017\n",
            "step 75: loss=0.8231465816497803\n",
            "step 76: loss=0.12777495384216309\n",
            "step 77: loss=2.528164863586426\n",
            "step 78: loss=2.6920502185821533\n",
            "step 79: loss=1.4572802782058716\n",
            "step 80: loss=0.3264431953430176\n",
            "step 81: loss=0.3224916458129883\n",
            "step 82: loss=0.880436897277832\n",
            "step 83: loss=1.6871346235275269\n",
            "step 84: loss=0.7689778804779053\n",
            "step 85: loss=2.236729621887207\n",
            "step 86: loss=0.39814305305480957\n",
            "step 87: loss=2.505406141281128\n",
            "step 88: loss=1.016530990600586\n",
            "step 89: loss=1.1767767667770386\n",
            "step 90: loss=1.6118593215942383\n",
            "step 91: loss=1.3020222187042236\n",
            "step 92: loss=1.0679250955581665\n",
            "step 93: loss=0.22047114372253418\n",
            "step 94: loss=1.8968682289123535\n",
            "step 95: loss=0.7754878997802734\n",
            "step 96: loss=2.2688071727752686\n",
            "step 97: loss=1.6609830856323242\n",
            "step 98: loss=1.714942216873169\n",
            "step 99: loss=0.8860684633255005\n",
            "step 100: loss=1.043237566947937\n",
            "step 101: loss=1.6429299116134644\n",
            "step 102: loss=1.3760747909545898\n",
            "step 103: loss=2.3424899578094482\n",
            "step 104: loss=0.7394161224365234\n",
            "step 105: loss=0.8303945064544678\n",
            "step 106: loss=1.6889162063598633\n",
            "step 107: loss=1.849713921546936\n",
            "step 108: loss=0.8009697198867798\n",
            "step 109: loss=1.9210385084152222\n",
            "step 110: loss=1.5583198070526123\n",
            "step 111: loss=0.6788965463638306\n",
            "step 112: loss=0.4137570858001709\n",
            "step 113: loss=1.6254088878631592\n",
            "step 114: loss=1.488877534866333\n",
            "step 115: loss=0.2103595733642578\n",
            "step 116: loss=0.9437799453735352\n",
            "step 117: loss=1.3162198066711426\n",
            "step 118: loss=0.7798060178756714\n",
            "step 119: loss=0.9784126281738281\n",
            "step 120: loss=1.2214597463607788\n",
            "step 121: loss=3.1054039001464844\n",
            "step 122: loss=1.6958223581314087\n",
            "step 123: loss=2.7126426696777344\n",
            "step 124: loss=0.5880532264709473\n",
            "step 125: loss=0.2147374153137207\n",
            "step 126: loss=1.3695894479751587\n",
            "step 127: loss=1.7903401851654053\n",
            "step 128: loss=0.8010847568511963\n",
            "step 129: loss=0.47148585319519043\n",
            "step 130: loss=0.42186427116394043\n",
            "step 131: loss=0.919746994972229\n",
            "step 132: loss=2.370388984680176\n",
            "step 133: loss=0.7040364742279053\n",
            "step 134: loss=0.6195166110992432\n",
            "step 135: loss=0.8623490333557129\n",
            "step 136: loss=0.2626211643218994\n",
            "step 137: loss=0.47997236251831055\n",
            "step 138: loss=0.6141399145126343\n",
            "step 139: loss=1.0177688598632812\n",
            "step 140: loss=1.6761692762374878\n",
            "step 141: loss=0.37014293670654297\n",
            "step 142: loss=2.50685977935791\n",
            "step 143: loss=0.473976731300354\n",
            "step 144: loss=1.8222389221191406\n",
            "step 145: loss=0.10611176490783691\n",
            "step 146: loss=1.3444867134094238\n",
            "step 147: loss=1.890256643295288\n",
            "step 148: loss=2.019212007522583\n",
            "step 149: loss=0.708979606628418\n",
            "step 150: loss=0.3448455333709717\n",
            "step 151: loss=1.4451547861099243\n",
            "step 152: loss=0.887959361076355\n",
            "step 153: loss=1.6465024948120117\n",
            "step 154: loss=1.2734510898590088\n",
            "step 155: loss=0.8892346620559692\n",
            "step 156: loss=0.8326750993728638\n",
            "step 157: loss=1.0322668552398682\n",
            "step 158: loss=1.7533605098724365\n",
            "step 159: loss=0.9787423610687256\n",
            "step 160: loss=1.5738203525543213\n",
            "step 161: loss=0.48970115184783936\n",
            "step 162: loss=0.15803837776184082\n",
            "step 163: loss=0.7275893688201904\n",
            "step 164: loss=0.5339555740356445\n",
            "step 165: loss=1.5419628620147705\n",
            "step 166: loss=0.4611380100250244\n",
            "step 167: loss=2.06067156791687\n",
            "step 168: loss=2.2312281131744385\n",
            "step 169: loss=0.656808614730835\n",
            "step 170: loss=0.4211156368255615\n",
            "step 171: loss=0.1600961685180664\n",
            "step 172: loss=0.6779335737228394\n",
            "step 173: loss=0.1400458812713623\n",
            "step 174: loss=0.7043989896774292\n",
            "step 175: loss=2.0440971851348877\n",
            "step 176: loss=0.5685373544692993\n",
            "step 177: loss=0.7488729953765869\n",
            "step 178: loss=0.2943863868713379\n",
            "step 179: loss=0.2867746353149414\n",
            "step 180: loss=2.0111541748046875\n",
            "step 181: loss=0.5551998615264893\n",
            "step 182: loss=0.42223358154296875\n",
            "step 183: loss=0.4737052917480469\n",
            "step 184: loss=2.3058722019195557\n",
            "step 185: loss=0.40650367736816406\n",
            "step 186: loss=0.4796102046966553\n",
            "step 187: loss=0.6871426105499268\n",
            "step 188: loss=2.680981159210205\n",
            "step 189: loss=0.7373931407928467\n",
            "step 190: loss=0.6501474380493164\n",
            "step 191: loss=0.44893431663513184\n",
            "step 192: loss=0.12216806411743164\n",
            "step 193: loss=0.12809205055236816\n",
            "step 194: loss=0.3562130928039551\n",
            "step 195: loss=0.060632944107055664\n",
            "step 196: loss=1.4033054113388062\n",
            "step 197: loss=1.2201976776123047\n",
            "step 198: loss=0.2856619358062744\n",
            "step 199: loss=2.5699079036712646\n",
            "step 200: loss=1.3816474676132202\n",
            "step 201: loss=0.4719282388687134\n",
            "step 202: loss=2.0478811264038086\n",
            "step 203: loss=0.4015340805053711\n",
            "step 204: loss=0.14312219619750977\n",
            "step 205: loss=0.49718499183654785\n",
            "step 206: loss=0.7516837120056152\n",
            "step 207: loss=0.17197680473327637\n",
            "step 208: loss=0.5553793907165527\n",
            "step 209: loss=0.3742920160293579\n",
            "step 210: loss=0.7562179565429688\n",
            "step 211: loss=0.15678930282592773\n",
            "step 212: loss=1.6474084854125977\n",
            "step 213: loss=1.2462327480316162\n",
            "step 214: loss=1.599760890007019\n",
            "step 215: loss=0.15593624114990234\n",
            "step 216: loss=0.29805707931518555\n",
            "step 217: loss=0.29372286796569824\n",
            "step 218: loss=0.15705633163452148\n",
            "step 219: loss=0.22482776641845703\n",
            "step 220: loss=0.9453972578048706\n",
            "step 221: loss=1.1685106754302979\n",
            "step 222: loss=2.3041555881500244\n",
            "step 223: loss=0.9989070892333984\n",
            "step 224: loss=0.7327048778533936\n",
            "step 225: loss=0.37357258796691895\n",
            "step 226: loss=0.6927950382232666\n",
            "step 227: loss=0.4200795888900757\n",
            "step 228: loss=0.5350275039672852\n",
            "step 229: loss=0.604094386100769\n",
            "step 230: loss=0.5697106122970581\n",
            "step 231: loss=0.8951528668403625\n",
            "step 232: loss=0.281496524810791\n",
            "step 233: loss=0.0047473907470703125\n",
            "step 234: loss=2.366014003753662\n",
            "step 235: loss=0.27408552169799805\n",
            "step 236: loss=1.1047792434692383\n",
            "step 237: loss=0.040659427642822266\n",
            "step 238: loss=0.22864484786987305\n",
            "step 239: loss=0.3796806335449219\n",
            "step 240: loss=0.34667110443115234\n",
            "step 241: loss=0.4054981470108032\n",
            "step 242: loss=0.1246330738067627\n",
            "step 243: loss=0.5560510158538818\n",
            "step 244: loss=0.44894957542419434\n",
            "step 245: loss=0.4151649475097656\n",
            "step 246: loss=0.9324600696563721\n",
            "step 247: loss=0.44788551330566406\n",
            "step 248: loss=3.057542085647583\n",
            "step 249: loss=0.05451393127441406\n",
            "step 250: loss=0.06193351745605469\n",
            "step 251: loss=2.078672170639038\n",
            "step 252: loss=0.5296133756637573\n",
            "step 253: loss=0.9901057481765747\n",
            "step 254: loss=0.09525799751281738\n",
            "step 255: loss=0.07671642303466797\n",
            "step 256: loss=0.489521861076355\n",
            "step 257: loss=2.7890677452087402\n",
            "step 258: loss=0.3984382152557373\n",
            "step 259: loss=0.8330893516540527\n",
            "step 260: loss=0.47151756286621094\n",
            "step 261: loss=0.983100175857544\n",
            "step 262: loss=2.7335660457611084\n",
            "step 263: loss=0.32002758979797363\n",
            "step 264: loss=0.1883378028869629\n",
            "step 265: loss=0.4457817077636719\n",
            "step 266: loss=0.6293996572494507\n",
            "step 267: loss=0.46929991245269775\n",
            "step 268: loss=0.7847534418106079\n",
            "step 269: loss=0.032675743103027344\n",
            "step 270: loss=0.6592978239059448\n",
            "step 271: loss=0.03919816017150879\n",
            "step 272: loss=0.7018625736236572\n",
            "step 273: loss=0.5264933109283447\n",
            "step 274: loss=0.12643933296203613\n",
            "step 275: loss=1.092840552330017\n",
            "step 276: loss=0.03591275215148926\n",
            "step 277: loss=0.29474616050720215\n",
            "step 278: loss=0.43141674995422363\n",
            "step 279: loss=0.4353444576263428\n",
            "step 280: loss=0.35237371921539307\n",
            "step 281: loss=0.08173036575317383\n",
            "step 282: loss=0.05996131896972656\n",
            "step 283: loss=1.4846298694610596\n",
            "step 284: loss=1.8887841701507568\n",
            "step 285: loss=0.4659993648529053\n",
            "step 286: loss=0.2334280014038086\n",
            "step 287: loss=0.15755367279052734\n",
            "step 288: loss=0.07255125045776367\n",
            "step 289: loss=2.1834232807159424\n",
            "step 290: loss=0.18325090408325195\n",
            "step 291: loss=0.37158799171447754\n",
            "step 292: loss=1.1927613019943237\n",
            "step 293: loss=0.044721126556396484\n",
            "step 294: loss=0.009671211242675781\n",
            "step 295: loss=0.8239340782165527\n",
            "step 296: loss=0.2812347412109375\n",
            "step 297: loss=0.29264354705810547\n",
            "step 298: loss=0.07653141021728516\n",
            "step 299: loss=0.012074947357177734\n",
            "step 300: loss=0.05053567886352539\n",
            "step 301: loss=0.006805896759033203\n",
            "step 302: loss=2.7376489639282227\n",
            "step 303: loss=0.9433908462524414\n",
            "step 304: loss=0.3315999507904053\n",
            "step 305: loss=0.4553706645965576\n",
            "step 306: loss=0.3246622085571289\n",
            "step 307: loss=0.9534530639648438\n",
            "step 308: loss=0.044396400451660156\n",
            "step 309: loss=0.4811127185821533\n",
            "step 310: loss=0.48668503761291504\n",
            "step 311: loss=2.132352828979492\n",
            "step 312: loss=0.17094135284423828\n",
            "step 313: loss=0.12753868103027344\n",
            "step 314: loss=0.551832914352417\n",
            "step 315: loss=0.07557868957519531\n",
            "step 316: loss=0.30028343200683594\n",
            "step 317: loss=0.33416497707366943\n",
            "step 318: loss=0.3379436731338501\n",
            "step 319: loss=0.21451210975646973\n",
            "step 320: loss=0.3522576093673706\n",
            "step 321: loss=0.8380552530288696\n",
            "step 322: loss=0.17711257934570312\n",
            "step 323: loss=0.09087562561035156\n",
            "step 324: loss=0.07382750511169434\n",
            "step 325: loss=0.23125982284545898\n",
            "step 326: loss=1.4439153671264648\n",
            "step 327: loss=0.47332286834716797\n",
            "step 328: loss=2.746384859085083\n",
            "step 329: loss=0.005428791046142578\n",
            "step 330: loss=0.5077652931213379\n",
            "step 331: loss=0.06275653839111328\n",
            "step 332: loss=0.3127434253692627\n",
            "step 333: loss=0.04568958282470703\n",
            "step 334: loss=0.0166778564453125\n",
            "step 335: loss=0.9150269031524658\n",
            "step 336: loss=0.34367990493774414\n",
            "step 337: loss=1.1579874753952026\n",
            "step 338: loss=0.33712172508239746\n",
            "step 339: loss=0.47450244426727295\n",
            "step 340: loss=0.0789024829864502\n",
            "step 341: loss=1.6881626844406128\n",
            "step 342: loss=0.3500051498413086\n",
            "step 343: loss=0.08574962615966797\n",
            "step 344: loss=1.064251184463501\n",
            "step 345: loss=0.7578305006027222\n",
            "step 346: loss=0.07385015487670898\n",
            "step 347: loss=0.2304244041442871\n",
            "step 348: loss=0.23037385940551758\n",
            "step 349: loss=0.35393309593200684\n",
            "step 350: loss=0.018471717834472656\n",
            "step 351: loss=0.36508679389953613\n",
            "step 352: loss=1.012511134147644\n",
            "step 353: loss=2.5447545051574707\n",
            "step 354: loss=0.7907414436340332\n",
            "step 355: loss=0.7476091384887695\n",
            "step 356: loss=0.012488365173339844\n",
            "step 357: loss=0.3724796772003174\n",
            "step 358: loss=1.1695151329040527\n",
            "step 359: loss=0.17859244346618652\n",
            "step 360: loss=0.08150219917297363\n",
            "step 361: loss=0.5774656534194946\n",
            "step 362: loss=0.12587261199951172\n",
            "step 363: loss=3.199237823486328\n",
            "step 364: loss=0.12036800384521484\n",
            "step 365: loss=1.0232316255569458\n",
            "step 366: loss=0.5209536552429199\n",
            "step 367: loss=0.47525346279144287\n",
            "step 368: loss=0.01161336898803711\n",
            "step 369: loss=0.19076108932495117\n",
            "step 370: loss=2.11970591545105\n",
            "step 371: loss=0.198927640914917\n",
            "step 372: loss=0.015585899353027344\n",
            "step 373: loss=0.014299869537353516\n",
            "step 374: loss=0.08033037185668945\n",
            "step 375: loss=1.9676306247711182\n",
            "step 376: loss=1.01432204246521\n",
            "step 377: loss=0.5524725914001465\n",
            "step 378: loss=0.629766583442688\n",
            "step 379: loss=3.3354454040527344\n",
            "step 380: loss=0.0397944450378418\n",
            "step 381: loss=0.008146286010742188\n",
            "step 382: loss=0.39034605026245117\n",
            "step 383: loss=0.3461265563964844\n",
            "step 384: loss=0.0007467269897460938\n",
            "step 385: loss=0.06762409210205078\n",
            "step 386: loss=0.09310150146484375\n",
            "step 387: loss=0.11734533309936523\n",
            "step 388: loss=0.42160606384277344\n",
            "step 389: loss=0.04375648498535156\n",
            "step 390: loss=0.6377222537994385\n",
            "step 391: loss=0.13796329498291016\n",
            "step 392: loss=0.9367666244506836\n",
            "step 393: loss=1.7518149614334106\n",
            "step 394: loss=3.9221692085266113\n",
            "step 395: loss=0.05047798156738281\n",
            "step 396: loss=0.25234508514404297\n",
            "step 397: loss=0.1284027099609375\n",
            "step 398: loss=0.9674408435821533\n",
            "step 399: loss=0.41379332542419434\n",
            "step 400: loss=0.029466629028320312\n",
            "step 401: loss=0.5387718677520752\n",
            "step 402: loss=0.44220614433288574\n",
            "step 403: loss=0.004082679748535156\n",
            "step 404: loss=0.01567983627319336\n",
            "step 405: loss=0.015554428100585938\n",
            "step 406: loss=0.6025831699371338\n",
            "step 407: loss=0.08917903900146484\n",
            "step 408: loss=0.018439769744873047\n",
            "step 409: loss=3.631787061691284\n",
            "step 410: loss=0.054201602935791016\n",
            "step 411: loss=0.5691431760787964\n",
            "step 412: loss=0.01025390625\n",
            "step 413: loss=0.16602587699890137\n",
            "step 414: loss=0.2280120849609375\n",
            "step 415: loss=0.033725738525390625\n",
            "step 416: loss=2.8930912017822266\n",
            "step 417: loss=0.4680633544921875\n",
            "step 418: loss=0.11901974678039551\n",
            "step 419: loss=1.3566269874572754\n",
            "step 420: loss=0.05841684341430664\n",
            "step 421: loss=0.28403472900390625\n",
            "step 422: loss=0.8525378704071045\n",
            "step 423: loss=0.2811431884765625\n",
            "step 424: loss=0.020607948303222656\n",
            "step 425: loss=0.8618518114089966\n",
            "step 426: loss=1.1827762126922607\n",
            "step 427: loss=0.8799014091491699\n",
            "step 428: loss=0.19707965850830078\n",
            "step 429: loss=0.2692444324493408\n",
            "step 430: loss=0.0019240379333496094\n",
            "step 431: loss=0.5260114669799805\n",
            "step 432: loss=0.2273731231689453\n",
            "step 433: loss=0.3134944438934326\n",
            "step 434: loss=1.1230807304382324\n",
            "step 435: loss=0.05976676940917969\n",
            "step 436: loss=0.18072032928466797\n",
            "step 437: loss=0.002167224884033203\n",
            "step 438: loss=0.01470041275024414\n",
            "step 439: loss=0.9843233823776245\n",
            "step 440: loss=0.011989116668701172\n",
            "step 441: loss=0.14016485214233398\n",
            "step 442: loss=0.7073942422866821\n",
            "step 443: loss=1.4848883152008057\n",
            "step 444: loss=1.0383212566375732\n",
            "step 445: loss=0.41783320903778076\n",
            "step 446: loss=0.7536642551422119\n",
            "step 447: loss=0.07221364974975586\n",
            "step 448: loss=0.05558300018310547\n",
            "step 449: loss=0.8591423034667969\n",
            "step 450: loss=3.564875364303589\n",
            "step 451: loss=0.006774425506591797\n",
            "step 452: loss=1.669120192527771\n",
            "step 453: loss=0.12167143821716309\n",
            "step 454: loss=3.2515270709991455\n",
            "step 455: loss=1.7813029289245605\n",
            "step 456: loss=1.3433032035827637\n",
            "step 457: loss=0.18211889266967773\n",
            "step 458: loss=0.7063822746276855\n",
            "step 459: loss=0.08410120010375977\n",
            "step 460: loss=0.03919649124145508\n",
            "step 461: loss=2.979452610015869\n",
            "step 462: loss=0.0026497840881347656\n",
            "step 463: loss=0.43689000606536865\n",
            "step 464: loss=0.07270312309265137\n",
            "step 465: loss=1.1039822101593018\n",
            "step 466: loss=0.03798484802246094\n",
            "step 467: loss=1.9705100059509277\n",
            "step 468: loss=0.11281657218933105\n",
            "step 469: loss=0.687421441078186\n",
            "step 470: loss=0.13335704803466797\n",
            "step 471: loss=2.6948351860046387\n",
            "step 472: loss=1.5483877658843994\n",
            "step 473: loss=0.11220932006835938\n",
            "step 474: loss=0.44478631019592285\n",
            "step 475: loss=0.013322830200195312\n",
            "step 476: loss=0.38378405570983887\n",
            "step 477: loss=0.04301595687866211\n",
            "step 478: loss=0.026033878326416016\n",
            "step 479: loss=0.1391465663909912\n",
            "step 480: loss=0.3230702877044678\n",
            "step 481: loss=0.24133872985839844\n",
            "step 482: loss=0.29029393196105957\n",
            "step 483: loss=0.4514617919921875\n",
            "step 484: loss=0.44598913192749023\n",
            "step 485: loss=1.3443806171417236\n",
            "step 486: loss=0.010129451751708984\n",
            "step 487: loss=0.008464813232421875\n",
            "step 488: loss=0.10316658020019531\n",
            "step 489: loss=0.14998960494995117\n",
            "step 490: loss=0.24096250534057617\n",
            "step 491: loss=0.0014219284057617188\n",
            "step 492: loss=0.05595660209655762\n",
            "step 493: loss=0.04845285415649414\n",
            "step 494: loss=0.3449218273162842\n",
            "step 495: loss=0.24515247344970703\n",
            "step 496: loss=1.8468502759933472\n",
            "step 497: loss=0.025726318359375\n",
            "step 498: loss=0.10625171661376953\n",
            "step 499: loss=3.827439308166504\n",
            "step 500: loss=0.8061550855636597\n",
            "step 501: loss=0.02237081527709961\n",
            "step 502: loss=0.3995704650878906\n",
            "step 503: loss=0.20708990097045898\n",
            "step 504: loss=0.045786142349243164\n",
            "step 505: loss=0.032854557037353516\n",
            "step 506: loss=0.20964956283569336\n",
            "step 507: loss=0.9962095022201538\n",
            "step 508: loss=0.16164588928222656\n",
            "step 509: loss=0.03909635543823242\n",
            "step 510: loss=0.10247039794921875\n",
            "step 511: loss=0.33330416679382324\n",
            "step 512: loss=0.12527990341186523\n",
            "step 513: loss=0.21143198013305664\n",
            "step 514: loss=0.3745746612548828\n",
            "step 515: loss=0.027648448944091797\n",
            "step 516: loss=0.08393144607543945\n",
            "step 517: loss=0.5669987201690674\n",
            "step 518: loss=0.957893967628479\n",
            "step 519: loss=0.02363729476928711\n",
            "step 520: loss=1.48310387134552\n",
            "step 521: loss=0.13979244232177734\n",
            "step 522: loss=0.042284488677978516\n",
            "step 523: loss=0.062416791915893555\n",
            "step 524: loss=0.03997659683227539\n",
            "step 525: loss=0.14003419876098633\n",
            "step 526: loss=3.1203131675720215\n",
            "step 527: loss=0.0059146881103515625\n",
            "step 528: loss=0.00048542022705078125\n",
            "step 529: loss=0.29979002475738525\n",
            "step 530: loss=0.05205678939819336\n",
            "step 531: loss=0.16924405097961426\n",
            "step 532: loss=0.0011043548583984375\n",
            "step 533: loss=0.22463774681091309\n",
            "step 534: loss=0.05318403244018555\n",
            "step 535: loss=0.08916521072387695\n",
            "step 536: loss=0.07006692886352539\n",
            "step 537: loss=0.07175874710083008\n",
            "step 538: loss=1.605657696723938\n",
            "step 539: loss=0.8576207160949707\n",
            "step 540: loss=3.2982354164123535\n",
            "step 541: loss=0.0028433799743652344\n",
            "step 542: loss=0.19463753700256348\n",
            "step 543: loss=0.8926622271537781\n",
            "step 544: loss=0.12177920341491699\n",
            "step 545: loss=0.7661073207855225\n",
            "step 546: loss=0.662636399269104\n",
            "step 547: loss=0.07155418395996094\n",
            "step 548: loss=0.01363372802734375\n",
            "step 549: loss=0.2269115447998047\n",
            "step 550: loss=0.19848036766052246\n",
            "step 551: loss=0.945902943611145\n",
            "step 552: loss=0.6102805137634277\n",
            "step 553: loss=0.054195404052734375\n",
            "step 554: loss=2.051149606704712\n",
            "step 555: loss=0.6343873739242554\n",
            "step 556: loss=1.581680178642273\n",
            "step 557: loss=0.35944652557373047\n",
            "step 558: loss=0.005198001861572266\n",
            "step 559: loss=0.0024509429931640625\n",
            "step 560: loss=0.09252715110778809\n",
            "step 561: loss=0.15797710418701172\n",
            "step 562: loss=0.5221741199493408\n",
            "step 563: loss=0.1967637538909912\n",
            "step 564: loss=0.16375017166137695\n",
            "step 565: loss=0.019205570220947266\n",
            "step 566: loss=2.8943355083465576\n",
            "step 567: loss=0.06458353996276855\n",
            "step 568: loss=0.2374105453491211\n",
            "step 569: loss=0.3637881278991699\n",
            "step 570: loss=0.05800652503967285\n",
            "step 571: loss=0.04111528396606445\n",
            "step 572: loss=0.05983877182006836\n",
            "step 573: loss=0.3058300018310547\n",
            "step 574: loss=1.237825632095337\n",
            "step 575: loss=0.28524351119995117\n",
            "step 576: loss=0.21417760848999023\n",
            "step 577: loss=0.4051241874694824\n",
            "step 578: loss=0.02850055694580078\n",
            "step 579: loss=0.01888132095336914\n",
            "step 580: loss=0.13847827911376953\n",
            "step 581: loss=0.016454219818115234\n",
            "step 582: loss=1.398078441619873\n",
            "step 583: loss=0.5642738342285156\n",
            "step 584: loss=0.09487390518188477\n",
            "step 585: loss=0.09597134590148926\n",
            "step 586: loss=1.049293041229248\n",
            "step 587: loss=0.47356557846069336\n",
            "step 588: loss=0.8383779525756836\n",
            "step 589: loss=3.177544116973877\n",
            "step 590: loss=0.4497392177581787\n",
            "step 591: loss=0.0792703628540039\n",
            "step 592: loss=1.357460856437683\n",
            "step 593: loss=0.2543766498565674\n",
            "step 594: loss=0.07349824905395508\n",
            "step 595: loss=2.8370463848114014\n",
            "step 596: loss=0.7364479303359985\n",
            "step 597: loss=0.7231221199035645\n",
            "step 598: loss=1.374111533164978\n",
            "step 599: loss=0.4321753978729248\n",
            "step 600: loss=0.17777228355407715\n",
            "step 601: loss=0.07130742073059082\n",
            "step 602: loss=0.2371366024017334\n",
            "step 603: loss=0.30576348304748535\n",
            "step 604: loss=2.668428421020508\n",
            "step 605: loss=1.1140674352645874\n",
            "step 606: loss=0.5520972013473511\n",
            "step 607: loss=0.8259549140930176\n",
            "step 608: loss=0.44512462615966797\n",
            "step 609: loss=0.7199970483779907\n",
            "step 610: loss=0.377687931060791\n",
            "step 611: loss=2.0839905738830566\n",
            "step 612: loss=1.1282179355621338\n",
            "step 613: loss=0.2851424217224121\n",
            "step 614: loss=0.04206252098083496\n",
            "step 615: loss=1.4286022186279297\n",
            "step 616: loss=0.5027270317077637\n",
            "step 617: loss=0.012112617492675781\n",
            "step 618: loss=0.7415882349014282\n",
            "step 619: loss=0.06085538864135742\n",
            "step 620: loss=0.5705982446670532\n",
            "step 621: loss=1.8605399131774902\n",
            "step 622: loss=0.23227739334106445\n",
            "step 623: loss=0.04270195960998535\n",
            "step 624: loss=2.679222822189331\n",
            "step 625: loss=0.2715725898742676\n",
            "step 626: loss=0.9293808937072754\n",
            "step 627: loss=0.16028642654418945\n",
            "step 628: loss=0.010308265686035156\n",
            "step 629: loss=1.290609359741211\n",
            "step 630: loss=0.08072900772094727\n",
            "step 631: loss=0.27584099769592285\n",
            "step 632: loss=0.2107226848602295\n",
            "step 633: loss=0.5600347518920898\n",
            "step 634: loss=0.011282920837402344\n",
            "step 635: loss=3.11643385887146\n",
            "step 636: loss=0.023334503173828125\n",
            "step 637: loss=0.49912166595458984\n",
            "step 638: loss=0.21662330627441406\n",
            "step 639: loss=0.0618898868560791\n",
            "step 640: loss=1.093286395072937\n",
            "step 641: loss=0.2558565139770508\n",
            "step 642: loss=0.010252952575683594\n",
            "step 643: loss=1.6186238527297974\n",
            "step 644: loss=0.011042594909667969\n",
            "step 645: loss=0.30058932304382324\n",
            "step 646: loss=0.09044694900512695\n",
            "step 647: loss=1.9067965745925903\n",
            "step 648: loss=0.12402224540710449\n",
            "step 649: loss=0.05713081359863281\n",
            "step 650: loss=0.10149407386779785\n",
            "step 651: loss=0.2526705265045166\n",
            "step 652: loss=0.6070406436920166\n",
            "step 653: loss=0.004278659820556641\n",
            "step 654: loss=0.7120640277862549\n",
            "step 655: loss=0.667755126953125\n",
            "step 656: loss=0.45967531204223633\n",
            "step 657: loss=0.05305337905883789\n",
            "step 658: loss=0.6365063190460205\n",
            "step 659: loss=0.12805700302124023\n",
            "step 660: loss=0.6878716945648193\n",
            "step 661: loss=0.004768848419189453\n",
            "step 662: loss=0.17098355293273926\n",
            "step 663: loss=1.0485873222351074\n",
            "step 664: loss=0.2143080234527588\n",
            "step 665: loss=0.040230751037597656\n",
            "step 666: loss=0.016267776489257812\n",
            "step 667: loss=0.011294364929199219\n",
            "step 668: loss=1.3694889545440674\n",
            "step 669: loss=2.1131973266601562\n",
            "step 670: loss=0.31995701789855957\n",
            "step 671: loss=1.4489619731903076\n",
            "step 672: loss=0.7029204368591309\n",
            "step 673: loss=1.19868803024292\n",
            "step 674: loss=0.05759167671203613\n",
            "step 675: loss=0.8192206621170044\n",
            "step 676: loss=0.05337262153625488\n",
            "step 677: loss=0.10202956199645996\n",
            "step 678: loss=0.4015166759490967\n",
            "step 679: loss=0.11560511589050293\n",
            "step 680: loss=0.0026750564575195312\n",
            "step 681: loss=1.3169834613800049\n",
            "step 682: loss=0.007153034210205078\n",
            "step 683: loss=0.580612063407898\n",
            "step 684: loss=1.1635642051696777\n",
            "step 685: loss=0.16467809677124023\n",
            "step 686: loss=0.029392719268798828\n",
            "step 687: loss=0.089141845703125\n",
            "step 688: loss=0.034459590911865234\n",
            "step 689: loss=1.5467545986175537\n",
            "step 690: loss=0.7600469589233398\n",
            "step 691: loss=0.3672494888305664\n",
            "step 692: loss=1.7930370569229126\n",
            "step 693: loss=0.04040336608886719\n",
            "step 694: loss=0.09868192672729492\n",
            "step 695: loss=0.09852075576782227\n",
            "step 696: loss=0.05927014350891113\n",
            "step 697: loss=1.2638287544250488\n",
            "step 698: loss=0.546238899230957\n",
            "step 699: loss=0.7163470983505249\n",
            "step 700: loss=0.0858759880065918\n",
            "step 701: loss=0.03256988525390625\n",
            "step 702: loss=0.02334737777709961\n",
            "step 703: loss=0.1107792854309082\n",
            "step 704: loss=0.030237197875976562\n",
            "step 705: loss=0.1586596965789795\n",
            "step 706: loss=0.14440345764160156\n",
            "step 707: loss=0.7704955339431763\n",
            "step 708: loss=0.273378849029541\n",
            "step 709: loss=0.12483501434326172\n",
            "step 710: loss=0.6951863765716553\n",
            "step 711: loss=1.7305415868759155\n",
            "step 712: loss=0.6946097612380981\n",
            "step 713: loss=0.1503126621246338\n",
            "step 714: loss=0.09028935432434082\n",
            "step 715: loss=0.06392288208007812\n",
            "step 716: loss=0.03419208526611328\n",
            "step 717: loss=0.28113722801208496\n",
            "step 718: loss=0.3789252042770386\n",
            "step 719: loss=0.010700225830078125\n",
            "step 720: loss=0.020720958709716797\n",
            "step 721: loss=0.061791181564331055\n",
            "step 722: loss=0.13160324096679688\n",
            "step 723: loss=0.022855758666992188\n",
            "step 724: loss=0.16715764999389648\n",
            "step 725: loss=4.071563243865967\n",
            "step 726: loss=0.454559326171875\n",
            "step 727: loss=0.00247955322265625\n",
            "step 728: loss=0.4241828918457031\n",
            "step 729: loss=1.0255894660949707\n",
            "step 730: loss=0.11156606674194336\n",
            "step 731: loss=0.061654090881347656\n",
            "step 732: loss=1.2832696437835693\n",
            "step 733: loss=0.6139696836471558\n",
            "step 734: loss=0.032423973083496094\n",
            "step 735: loss=0.8118246793746948\n",
            "step 736: loss=0.009581565856933594\n",
            "step 737: loss=0.28511953353881836\n",
            "step 738: loss=0.1188817024230957\n",
            "step 739: loss=0.0021009445190429688\n",
            "step 740: loss=0.11481595039367676\n",
            "step 741: loss=0.9182499647140503\n",
            "step 742: loss=5.225607872009277\n",
            "step 743: loss=1.704210877418518\n",
            "step 744: loss=0.04639291763305664\n",
            "step 745: loss=0.5776848793029785\n",
            "step 746: loss=0.01789712905883789\n",
            "step 747: loss=1.502614974975586\n",
            "step 748: loss=0.03278923034667969\n",
            "step 749: loss=0.01644754409790039\n",
            "step 750: loss=0.8263120651245117\n",
            "step 751: loss=0.04602527618408203\n",
            "step 752: loss=0.21524906158447266\n",
            "step 753: loss=0.061154842376708984\n",
            "step 754: loss=0.3718116283416748\n",
            "step 755: loss=0.28887057304382324\n",
            "step 756: loss=1.5567007064819336\n",
            "step 757: loss=0.028926372528076172\n",
            "step 758: loss=0.18595385551452637\n",
            "step 759: loss=0.07905268669128418\n",
            "step 760: loss=0.4115278720855713\n",
            "step 761: loss=0.008421897888183594\n",
            "step 762: loss=0.4036257266998291\n",
            "step 763: loss=0.9590024948120117\n",
            "step 764: loss=0.7133641242980957\n",
            "step 765: loss=0.48048698902130127\n",
            "step 766: loss=0.3986119031906128\n",
            "step 767: loss=0.13569283485412598\n",
            "step 768: loss=0.4674079418182373\n",
            "step 769: loss=0.00102996826171875\n",
            "step 770: loss=0.16486167907714844\n",
            "step 771: loss=0.43318843841552734\n",
            "step 772: loss=1.4921143054962158\n",
            "step 773: loss=0.08220386505126953\n",
            "step 774: loss=0.038150787353515625\n",
            "step 775: loss=0.013050556182861328\n",
            "step 776: loss=0.2003769874572754\n",
            "step 777: loss=0.05414152145385742\n",
            "step 778: loss=0.011188030242919922\n",
            "step 779: loss=0.014467239379882812\n",
            "step 780: loss=0.02270984649658203\n",
            "step 781: loss=0.0022478103637695312\n",
            "step 782: loss=0.07770586013793945\n",
            "step 783: loss=0.027918338775634766\n",
            "step 784: loss=0.04948854446411133\n",
            "step 785: loss=0.35353732109069824\n",
            "step 786: loss=0.013762474060058594\n",
            "step 787: loss=0.11862325668334961\n",
            "step 788: loss=1.4354572296142578\n",
            "step 789: loss=0.17925167083740234\n",
            "step 790: loss=0.4444906711578369\n",
            "step 791: loss=0.15286684036254883\n",
            "step 792: loss=0.09328126907348633\n",
            "step 793: loss=0.32610654830932617\n",
            "step 794: loss=0.9303807020187378\n",
            "step 795: loss=0.011697769165039062\n",
            "step 796: loss=0.029285907745361328\n",
            "step 797: loss=0.231231689453125\n",
            "step 798: loss=2.6338624954223633\n",
            "step 799: loss=0.5665862560272217\n",
            "step 800: loss=1.2449233531951904\n",
            "step 801: loss=0.013893604278564453\n",
            "step 802: loss=0.01511383056640625\n",
            "step 803: loss=0.12432408332824707\n",
            "step 804: loss=0.4973611831665039\n",
            "step 805: loss=0.0024280548095703125\n",
            "step 806: loss=0.002131938934326172\n",
            "step 807: loss=0.018868446350097656\n",
            "step 808: loss=0.002613067626953125\n",
            "step 809: loss=0.7255860567092896\n",
            "step 810: loss=0.0009622573852539062\n",
            "step 811: loss=0.9114747047424316\n",
            "step 812: loss=0.3157074451446533\n",
            "step 813: loss=0.07942962646484375\n",
            "step 814: loss=0.461492657661438\n",
            "step 815: loss=0.031975746154785156\n",
            "step 816: loss=0.2038557529449463\n",
            "step 817: loss=0.5515196323394775\n",
            "step 818: loss=0.2785799503326416\n",
            "step 819: loss=0.005415439605712891\n",
            "step 820: loss=0.17008376121520996\n",
            "step 821: loss=0.03214550018310547\n",
            "step 822: loss=0.037631988525390625\n",
            "step 823: loss=0.08344030380249023\n",
            "step 824: loss=0.22780489921569824\n",
            "step 825: loss=2.4254860877990723\n",
            "step 826: loss=0.014660835266113281\n",
            "step 827: loss=2.5519309043884277\n",
            "step 828: loss=0.01477813720703125\n",
            "step 829: loss=0.25369691848754883\n",
            "step 830: loss=0.11380338668823242\n",
            "step 831: loss=0.19821643829345703\n",
            "step 832: loss=0.005218505859375\n",
            "step 833: loss=0.15487146377563477\n",
            "step 834: loss=1.6532922983169556\n",
            "step 835: loss=0.43225550651550293\n",
            "step 836: loss=0.01710987091064453\n",
            "step 837: loss=0.015863895416259766\n",
            "step 838: loss=0.010437965393066406\n",
            "step 839: loss=0.03831672668457031\n",
            "step 840: loss=0.12788152694702148\n",
            "step 841: loss=0.005878925323486328\n",
            "step 842: loss=0.046411991119384766\n",
            "step 843: loss=0.01076507568359375\n",
            "step 844: loss=0.25150251388549805\n",
            "step 845: loss=0.0012178421020507812\n",
            "step 846: loss=0.2321016788482666\n",
            "step 847: loss=0.31379055976867676\n",
            "step 848: loss=0.06747221946716309\n",
            "step 849: loss=0.0008087158203125\n",
            "step 850: loss=0.07829785346984863\n",
            "step 851: loss=0.28147459030151367\n",
            "step 852: loss=0.3888612985610962\n",
            "step 853: loss=0.449662446975708\n",
            "step 854: loss=0.09763097763061523\n",
            "step 855: loss=1.7243692874908447\n",
            "step 856: loss=0.1643972396850586\n",
            "step 857: loss=0.11453390121459961\n",
            "step 858: loss=0.39929258823394775\n",
            "step 859: loss=0.16950511932373047\n",
            "step 860: loss=0.6861879825592041\n",
            "step 861: loss=0.02004098892211914\n",
            "step 862: loss=0.003284931182861328\n",
            "step 863: loss=0.10630512237548828\n",
            "step 864: loss=0.12004256248474121\n",
            "step 865: loss=0.8591494560241699\n",
            "step 866: loss=0.05156230926513672\n",
            "step 867: loss=0.7235795259475708\n",
            "step 868: loss=0.044901371002197266\n",
            "step 869: loss=2.3743138313293457\n",
            "step 870: loss=0.0057239532470703125\n",
            "step 871: loss=0.21019387245178223\n",
            "step 872: loss=0.1750349998474121\n",
            "step 873: loss=0.03566455841064453\n",
            "step 874: loss=1.4096548557281494\n",
            "step 875: loss=0.3176443576812744\n",
            "step 876: loss=1.5774856805801392\n",
            "step 877: loss=0.1490786075592041\n",
            "step 878: loss=0.14512395858764648\n",
            "step 879: loss=0.014568805694580078\n",
            "step 880: loss=0.1957552433013916\n",
            "step 881: loss=0.02507495880126953\n",
            "step 882: loss=0.11042499542236328\n",
            "step 883: loss=0.10820245742797852\n",
            "step 884: loss=0.013692378997802734\n",
            "step 885: loss=0.025472640991210938\n",
            "step 886: loss=1.4292902946472168\n",
            "step 887: loss=0.3023536205291748\n",
            "step 888: loss=0.16981220245361328\n",
            "step 889: loss=0.5759778022766113\n",
            "step 890: loss=0.39397764205932617\n",
            "step 891: loss=0.01878833770751953\n",
            "step 892: loss=0.0006103515625\n",
            "step 893: loss=0.14224529266357422\n",
            "step 894: loss=0.16683459281921387\n",
            "step 895: loss=0.02306652069091797\n",
            "step 896: loss=0.005665779113769531\n",
            "step 897: loss=0.26515984535217285\n",
            "step 898: loss=0.5112065076828003\n",
            "step 899: loss=0.01642131805419922\n",
            "step 900: loss=0.04950976371765137\n",
            "step 901: loss=0.10270214080810547\n",
            "step 902: loss=0.008306503295898438\n",
            "step 903: loss=1.2045660018920898\n",
            "step 904: loss=0.02616596221923828\n",
            "step 905: loss=0.5364210605621338\n",
            "step 906: loss=0.41507232189178467\n",
            "step 907: loss=0.6363639831542969\n",
            "step 908: loss=0.002647876739501953\n",
            "step 909: loss=0.06528759002685547\n",
            "step 910: loss=0.5764923095703125\n",
            "step 911: loss=0.29312682151794434\n",
            "step 912: loss=0.005534172058105469\n",
            "step 913: loss=0.04647350311279297\n",
            "step 914: loss=3.039876699447632\n",
            "step 915: loss=0.04770612716674805\n",
            "step 916: loss=0.23157477378845215\n",
            "step 917: loss=0.34737443923950195\n",
            "step 918: loss=0.5393333435058594\n",
            "step 919: loss=0.3387317657470703\n",
            "step 920: loss=0.10210371017456055\n",
            "step 921: loss=0.2000105381011963\n",
            "step 922: loss=0.46993470191955566\n",
            "step 923: loss=0.056276798248291016\n",
            "step 924: loss=0.006319999694824219\n",
            "step 925: loss=0.01444244384765625\n",
            "step 926: loss=0.02815389633178711\n",
            "step 927: loss=0.13592910766601562\n",
            "step 928: loss=2.0474705696105957\n",
            "step 929: loss=0.08091592788696289\n",
            "step 930: loss=0.23891758918762207\n",
            "step 931: loss=0.1295328140258789\n",
            "step 932: loss=0.03899955749511719\n",
            "step 933: loss=0.02909708023071289\n",
            "step 934: loss=0.022089481353759766\n",
            "step 935: loss=0.010469913482666016\n",
            "step 936: loss=1.1239633560180664\n",
            "step 937: loss=0.004912853240966797\n",
            "step 938: loss=0.24308156967163086\n",
            "step 939: loss=0.5980682373046875\n",
            "step 940: loss=0.12193965911865234\n",
            "step 941: loss=0.02315378189086914\n",
            "step 942: loss=0.37062692642211914\n",
            "step 943: loss=0.009302139282226562\n",
            "step 944: loss=0.011314868927001953\n",
            "step 945: loss=3.5264344215393066\n",
            "step 946: loss=0.0910792350769043\n",
            "step 947: loss=0.15913820266723633\n",
            "step 948: loss=0.01747608184814453\n",
            "step 949: loss=0.11133050918579102\n",
            "step 950: loss=0.09948945045471191\n",
            "step 951: loss=0.052641868591308594\n",
            "step 952: loss=0.00896453857421875\n",
            "step 953: loss=0.01663351058959961\n",
            "step 954: loss=0.05906391143798828\n",
            "step 955: loss=0.002895355224609375\n",
            "step 956: loss=0.07044506072998047\n",
            "step 957: loss=0.007376194000244141\n",
            "step 958: loss=0.2260730266571045\n",
            "step 959: loss=0.16261935234069824\n",
            "step 960: loss=0.6109166145324707\n",
            "step 961: loss=0.5834782123565674\n",
            "step 962: loss=0.05586504936218262\n",
            "step 963: loss=0.004221916198730469\n",
            "step 964: loss=0.033403873443603516\n",
            "step 965: loss=0.793737530708313\n",
            "step 966: loss=0.37858057022094727\n",
            "step 967: loss=0.0952444076538086\n",
            "step 968: loss=0.6360642910003662\n",
            "step 969: loss=0.03384065628051758\n",
            "step 970: loss=0.638343334197998\n",
            "step 971: loss=0.03069305419921875\n",
            "step 972: loss=0.14579248428344727\n",
            "step 973: loss=0.07487893104553223\n",
            "step 974: loss=1.3001110553741455\n",
            "step 975: loss=2.2975776195526123\n",
            "step 976: loss=0.01396322250366211\n",
            "step 977: loss=0.15135955810546875\n",
            "step 978: loss=0.8422958254814148\n",
            "step 979: loss=1.9849610328674316\n",
            "step 980: loss=0.5528028011322021\n",
            "step 981: loss=0.9595495462417603\n",
            "step 982: loss=0.15296173095703125\n",
            "step 983: loss=0.6124076843261719\n",
            "step 984: loss=0.04457712173461914\n",
            "step 985: loss=0.19276797771453857\n",
            "step 986: loss=0.14833617210388184\n",
            "step 987: loss=0.07393813133239746\n",
            "step 988: loss=0.07053279876708984\n",
            "step 989: loss=0.001033782958984375\n",
            "step 990: loss=0.31380701065063477\n",
            "step 991: loss=0.14849281311035156\n",
            "step 992: loss=0.2512540817260742\n",
            "step 993: loss=0.010190486907958984\n",
            "step 994: loss=0.709064245223999\n",
            "step 995: loss=0.2817842960357666\n",
            "step 996: loss=0.0027513504028320312\n",
            "step 997: loss=0.0024824142456054688\n",
            "step 998: loss=0.18323874473571777\n",
            "step 999: loss=0.29120421409606934\n",
            "step 1000: loss=0.002166748046875\n",
            "Mean loss        0.48416767\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c4d036d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_from_step = 1000  # e.g. resume from checkpoint at step 1000\n",
        "layer_sizes = [784, 128, 10]\n",
        "\n",
        "experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "template_state = create_training_state(layer_sizes, optimizer, jax.random.key(0))\n",
        "restored_state = checkpoint_manager.restore(\n",
        "    resume_from_step,\n",
        "    args=ocp.args.StandardRestore(template_state),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6352f95d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=1.940758228302002\n",
            "step 2: loss=2.1760780811309814\n",
            "step 3: loss=2.123516082763672\n",
            "step 4: loss=1.7909069061279297\n",
            "step 5: loss=2.7731266021728516\n",
            "step 6: loss=1.962531328201294\n",
            "step 7: loss=2.494056224822998\n",
            "step 8: loss=1.8388534784317017\n",
            "step 9: loss=1.8824307918548584\n",
            "step 10: loss=2.6979455947875977\n",
            "step 11: loss=2.197720766067505\n",
            "step 12: loss=2.1643967628479004\n",
            "step 13: loss=1.4617383480072021\n",
            "step 14: loss=1.5403313636779785\n",
            "step 15: loss=2.446437358856201\n",
            "step 16: loss=2.2722434997558594\n",
            "step 17: loss=1.3893849849700928\n",
            "step 18: loss=3.549886703491211\n",
            "step 19: loss=3.3583061695098877\n",
            "step 20: loss=1.5383902788162231\n",
            "step 21: loss=2.0145435333251953\n",
            "step 22: loss=1.8640811443328857\n",
            "step 23: loss=1.9871046543121338\n",
            "step 24: loss=2.46518611907959\n",
            "step 25: loss=2.6712143421173096\n",
            "step 26: loss=2.0123965740203857\n",
            "step 27: loss=3.0125784873962402\n",
            "step 28: loss=2.691230297088623\n",
            "step 29: loss=1.9900810718536377\n",
            "step 30: loss=1.6837618350982666\n",
            "step 31: loss=1.7050302028656006\n",
            "step 32: loss=2.1044046878814697\n",
            "step 33: loss=1.6550555229187012\n",
            "step 34: loss=2.254828691482544\n",
            "step 35: loss=1.9034671783447266\n",
            "step 36: loss=1.9907734394073486\n",
            "step 37: loss=2.0229430198669434\n",
            "step 38: loss=1.9300038814544678\n",
            "step 39: loss=2.7000436782836914\n",
            "step 40: loss=1.8529770374298096\n",
            "step 41: loss=1.8260843753814697\n",
            "step 42: loss=2.6148486137390137\n",
            "step 43: loss=1.9493474960327148\n",
            "step 44: loss=1.6372787952423096\n",
            "step 45: loss=2.0136427879333496\n",
            "step 46: loss=1.6119754314422607\n",
            "step 47: loss=2.0991406440734863\n",
            "step 48: loss=2.258373498916626\n",
            "step 49: loss=2.42681622505188\n",
            "step 50: loss=1.7030603885650635\n",
            "step 51: loss=1.8889999389648438\n",
            "step 52: loss=1.7781010866165161\n",
            "step 53: loss=3.225069284439087\n",
            "step 54: loss=1.3905025720596313\n",
            "step 55: loss=1.9320303201675415\n",
            "step 56: loss=2.2162458896636963\n",
            "step 57: loss=2.3552563190460205\n",
            "step 58: loss=1.6320664882659912\n",
            "step 59: loss=2.202133893966675\n",
            "step 60: loss=1.6967023611068726\n",
            "step 61: loss=1.246525764465332\n",
            "step 62: loss=1.6203155517578125\n",
            "step 63: loss=2.9075191020965576\n",
            "step 64: loss=2.1749255657196045\n",
            "step 65: loss=1.6314904689788818\n",
            "step 66: loss=0.9881222248077393\n",
            "step 67: loss=1.622375726699829\n",
            "step 68: loss=0.7821948528289795\n",
            "step 69: loss=2.6255650520324707\n",
            "step 70: loss=1.743849277496338\n",
            "step 71: loss=2.8109583854675293\n",
            "step 72: loss=2.7828750610351562\n",
            "step 73: loss=1.4112918376922607\n",
            "step 74: loss=1.306151032447815\n",
            "step 75: loss=0.8926953077316284\n",
            "step 76: loss=0.7406435012817383\n",
            "step 77: loss=2.758932590484619\n",
            "step 78: loss=2.4050614833831787\n",
            "step 79: loss=2.227637529373169\n",
            "step 80: loss=0.7494920492172241\n",
            "step 81: loss=0.23082280158996582\n",
            "step 82: loss=0.9866675138473511\n",
            "step 83: loss=2.122668981552124\n",
            "step 84: loss=0.6010141372680664\n",
            "step 85: loss=3.019683599472046\n",
            "step 86: loss=0.16074657440185547\n",
            "step 87: loss=1.9915804862976074\n",
            "step 88: loss=1.4108773469924927\n",
            "step 89: loss=1.5791131258010864\n",
            "step 90: loss=2.1513686180114746\n",
            "step 91: loss=1.6802482604980469\n",
            "step 92: loss=1.527822494506836\n",
            "step 93: loss=0.43253612518310547\n",
            "step 94: loss=2.0817129611968994\n",
            "step 95: loss=1.6460349559783936\n",
            "step 96: loss=2.0777554512023926\n",
            "step 97: loss=1.7677419185638428\n",
            "step 98: loss=2.6245574951171875\n",
            "step 99: loss=1.132123589515686\n",
            "step 100: loss=2.119290828704834\n",
            "step 101: loss=2.098219633102417\n",
            "step 102: loss=1.5714093446731567\n",
            "step 103: loss=2.2930195331573486\n",
            "step 104: loss=0.5091879367828369\n",
            "step 105: loss=0.4726097583770752\n",
            "step 106: loss=1.8006337881088257\n",
            "step 107: loss=2.1027603149414062\n",
            "step 108: loss=1.0504649877548218\n",
            "step 109: loss=1.6988062858581543\n",
            "step 110: loss=1.2320650815963745\n",
            "step 111: loss=1.6928997039794922\n",
            "step 112: loss=0.32294797897338867\n",
            "step 113: loss=1.959110975265503\n",
            "step 114: loss=2.2115509510040283\n",
            "step 115: loss=1.3057934045791626\n",
            "step 116: loss=1.507971167564392\n",
            "step 117: loss=2.102586030960083\n",
            "step 118: loss=0.9875845909118652\n",
            "step 119: loss=1.5989067554473877\n",
            "step 120: loss=1.7031259536743164\n",
            "step 121: loss=2.6286754608154297\n",
            "step 122: loss=1.5850002765655518\n",
            "step 123: loss=2.2829391956329346\n",
            "step 124: loss=1.293221116065979\n",
            "step 125: loss=0.8511708974838257\n",
            "step 126: loss=1.8777501583099365\n",
            "step 127: loss=2.0515291690826416\n",
            "step 128: loss=1.751558542251587\n",
            "step 129: loss=0.2879312038421631\n",
            "step 130: loss=0.9196925163269043\n",
            "step 131: loss=1.134814739227295\n",
            "step 132: loss=1.388135552406311\n",
            "step 133: loss=1.320654273033142\n",
            "step 134: loss=0.142578125\n",
            "step 135: loss=0.6125261783599854\n",
            "step 136: loss=1.151767373085022\n",
            "step 137: loss=0.8093137741088867\n",
            "step 138: loss=0.15493559837341309\n",
            "step 139: loss=1.6861603260040283\n",
            "step 140: loss=2.0366413593292236\n",
            "step 141: loss=0.7237839698791504\n",
            "step 142: loss=2.8405189514160156\n",
            "step 143: loss=0.6206004619598389\n",
            "step 144: loss=2.0611507892608643\n",
            "step 145: loss=0.9473954439163208\n",
            "step 146: loss=1.8579058647155762\n",
            "step 147: loss=2.705409049987793\n",
            "step 148: loss=1.6843841075897217\n",
            "step 149: loss=1.0625377893447876\n",
            "step 150: loss=0.9671682119369507\n",
            "step 151: loss=1.7277209758758545\n",
            "step 152: loss=0.5180060863494873\n",
            "step 153: loss=2.3091580867767334\n",
            "step 154: loss=1.3028345108032227\n",
            "step 155: loss=0.7140522003173828\n",
            "step 156: loss=1.1415538787841797\n",
            "step 157: loss=1.4793301820755005\n",
            "step 158: loss=1.6714287996292114\n",
            "step 159: loss=1.0126360654830933\n",
            "step 160: loss=2.005323886871338\n",
            "step 161: loss=1.0441279411315918\n",
            "step 162: loss=0.7576258182525635\n",
            "step 163: loss=0.9640073776245117\n",
            "step 164: loss=0.8265089988708496\n",
            "step 165: loss=1.7699790000915527\n",
            "step 166: loss=0.9343881607055664\n",
            "step 167: loss=1.606757640838623\n",
            "step 168: loss=2.594965696334839\n",
            "step 169: loss=1.0063514709472656\n",
            "step 170: loss=1.1417509317398071\n",
            "step 171: loss=0.4511137008666992\n",
            "step 172: loss=0.9041800498962402\n",
            "step 173: loss=0.5230724811553955\n",
            "step 174: loss=1.045677661895752\n",
            "step 175: loss=2.3977839946746826\n",
            "step 176: loss=1.2700635194778442\n",
            "step 177: loss=0.9969329833984375\n",
            "step 178: loss=1.160459280014038\n",
            "step 179: loss=0.4204108715057373\n",
            "step 180: loss=1.9843924045562744\n",
            "step 181: loss=0.2923877239227295\n",
            "step 182: loss=0.1150503158569336\n",
            "step 183: loss=1.0070995092391968\n",
            "step 184: loss=1.9128669500350952\n",
            "step 185: loss=0.47420787811279297\n",
            "step 186: loss=0.39101147651672363\n",
            "step 187: loss=2.231415271759033\n",
            "step 188: loss=2.5517473220825195\n",
            "step 189: loss=1.437174677848816\n",
            "step 190: loss=0.9578108787536621\n",
            "step 191: loss=0.08653736114501953\n",
            "step 192: loss=0.6862363815307617\n",
            "step 193: loss=0.3823404312133789\n",
            "step 194: loss=1.6281425952911377\n",
            "step 195: loss=0.3901338577270508\n",
            "step 196: loss=1.500989556312561\n",
            "step 197: loss=2.893779754638672\n",
            "step 198: loss=0.11595010757446289\n",
            "step 199: loss=2.770063877105713\n",
            "step 200: loss=0.5390090942382812\n",
            "step 201: loss=1.4554927349090576\n",
            "step 202: loss=1.9631141424179077\n",
            "step 203: loss=0.15400266647338867\n",
            "step 204: loss=0.10439205169677734\n",
            "step 205: loss=0.996600866317749\n",
            "step 206: loss=1.8002665042877197\n",
            "step 207: loss=0.7035231590270996\n",
            "step 208: loss=1.2461644411087036\n",
            "step 209: loss=1.3734639883041382\n",
            "step 210: loss=1.1962624788284302\n",
            "step 211: loss=0.047454833984375\n",
            "step 212: loss=3.489471912384033\n",
            "step 213: loss=1.5609102249145508\n",
            "step 214: loss=1.0745561122894287\n",
            "step 215: loss=0.06079435348510742\n",
            "step 216: loss=0.9788827896118164\n",
            "step 217: loss=0.6263360977172852\n",
            "step 218: loss=0.20669102668762207\n",
            "step 219: loss=0.9191924333572388\n",
            "step 220: loss=1.1311601400375366\n",
            "step 221: loss=1.6031124591827393\n",
            "step 222: loss=3.672095537185669\n",
            "step 223: loss=0.9712107181549072\n",
            "step 224: loss=0.8627488613128662\n",
            "step 225: loss=0.5474696159362793\n",
            "step 226: loss=1.2204034328460693\n",
            "step 227: loss=0.9344918727874756\n",
            "step 228: loss=0.4932880401611328\n",
            "step 229: loss=0.9787428379058838\n",
            "step 230: loss=0.5024271011352539\n",
            "step 231: loss=1.1683123111724854\n",
            "step 232: loss=0.7211713790893555\n",
            "step 233: loss=0.03919029235839844\n",
            "step 234: loss=0.9381096363067627\n",
            "step 235: loss=0.5780558586120605\n",
            "step 236: loss=0.9687178134918213\n",
            "step 237: loss=0.6189484596252441\n",
            "step 238: loss=0.41588544845581055\n",
            "step 239: loss=0.27318477630615234\n",
            "step 240: loss=0.9093091487884521\n",
            "step 241: loss=0.8887255191802979\n",
            "step 242: loss=0.22299766540527344\n",
            "step 243: loss=0.7276806831359863\n",
            "step 244: loss=0.2764899730682373\n",
            "step 245: loss=0.873140811920166\n",
            "step 246: loss=0.6811590194702148\n",
            "step 247: loss=0.46933579444885254\n",
            "step 248: loss=3.176146984100342\n",
            "step 249: loss=0.1948843002319336\n",
            "step 250: loss=0.022376537322998047\n",
            "step 251: loss=2.0302629470825195\n",
            "step 252: loss=0.7815532684326172\n",
            "step 253: loss=0.6058900356292725\n",
            "step 254: loss=0.32948780059814453\n",
            "step 255: loss=0.513887882232666\n",
            "step 256: loss=0.8629508018493652\n",
            "step 257: loss=2.4881768226623535\n",
            "step 258: loss=0.11145973205566406\n",
            "step 259: loss=1.424945592880249\n",
            "step 260: loss=1.1523075103759766\n",
            "step 261: loss=0.9763450622558594\n",
            "step 262: loss=3.0668561458587646\n",
            "step 263: loss=0.1405811309814453\n",
            "step 264: loss=0.4369082450866699\n",
            "step 265: loss=0.651355504989624\n",
            "step 266: loss=0.7770178318023682\n",
            "step 267: loss=0.5752403736114502\n",
            "step 268: loss=0.8714203834533691\n",
            "step 269: loss=0.2317209243774414\n",
            "step 270: loss=2.120420217514038\n",
            "step 271: loss=0.030502796173095703\n",
            "step 272: loss=2.8100504875183105\n",
            "step 273: loss=0.3450281620025635\n",
            "step 274: loss=0.4464294910430908\n",
            "step 275: loss=1.237307071685791\n",
            "step 276: loss=0.24210023880004883\n",
            "step 277: loss=0.6025030612945557\n",
            "step 278: loss=0.817582368850708\n",
            "step 279: loss=0.8477745056152344\n",
            "step 280: loss=0.6654469966888428\n",
            "step 281: loss=0.022299766540527344\n",
            "step 282: loss=0.05568695068359375\n",
            "step 283: loss=1.7557082176208496\n",
            "step 284: loss=2.3239552974700928\n",
            "step 285: loss=0.5763859748840332\n",
            "step 286: loss=0.27073144912719727\n",
            "step 287: loss=1.0109319686889648\n",
            "step 288: loss=0.11882925033569336\n",
            "step 289: loss=1.7510555982589722\n",
            "step 290: loss=0.2724437713623047\n",
            "step 291: loss=0.09409570693969727\n",
            "step 292: loss=0.36136913299560547\n",
            "step 293: loss=0.09254312515258789\n",
            "step 294: loss=0.005002021789550781\n",
            "step 295: loss=1.15247642993927\n",
            "step 296: loss=0.39642763137817383\n",
            "step 297: loss=0.6609292030334473\n",
            "step 298: loss=0.03882598876953125\n",
            "step 299: loss=0.06192445755004883\n",
            "step 300: loss=0.18926763534545898\n",
            "step 301: loss=0.016364097595214844\n",
            "step 302: loss=4.598568439483643\n",
            "step 303: loss=0.5449767112731934\n",
            "step 304: loss=0.5422558784484863\n",
            "step 305: loss=0.5100915431976318\n",
            "step 306: loss=0.38719916343688965\n",
            "step 307: loss=1.2490555047988892\n",
            "step 308: loss=0.025793075561523438\n",
            "step 309: loss=0.7196416854858398\n",
            "step 310: loss=0.616187334060669\n",
            "step 311: loss=1.2755953073501587\n",
            "step 312: loss=0.8189480304718018\n",
            "step 313: loss=0.06185770034790039\n",
            "step 314: loss=0.4077730178833008\n",
            "step 315: loss=0.28505587577819824\n",
            "step 316: loss=1.021296739578247\n",
            "step 317: loss=0.5595149993896484\n",
            "step 318: loss=1.267256736755371\n",
            "step 319: loss=0.2652921676635742\n",
            "step 320: loss=1.289210319519043\n",
            "step 321: loss=0.531684160232544\n",
            "step 322: loss=0.5415894985198975\n",
            "step 323: loss=0.012047767639160156\n",
            "step 324: loss=0.5780124664306641\n",
            "step 325: loss=0.49553704261779785\n",
            "step 326: loss=1.5517516136169434\n",
            "step 327: loss=0.5258686542510986\n",
            "step 328: loss=3.252657413482666\n",
            "step 329: loss=0.0019884109497070312\n",
            "step 330: loss=1.616234540939331\n",
            "step 331: loss=0.026736736297607422\n",
            "step 332: loss=0.16749238967895508\n",
            "step 333: loss=0.04065370559692383\n",
            "step 334: loss=0.4888019561767578\n",
            "step 335: loss=0.5071408748626709\n",
            "step 336: loss=0.34523630142211914\n",
            "step 337: loss=0.6336531639099121\n",
            "step 338: loss=0.41834449768066406\n",
            "step 339: loss=0.19454383850097656\n",
            "step 340: loss=0.08806180953979492\n",
            "step 341: loss=2.3574581146240234\n",
            "step 342: loss=0.47588229179382324\n",
            "step 343: loss=0.09826517105102539\n",
            "step 344: loss=0.7378146648406982\n",
            "step 345: loss=0.5165562629699707\n",
            "step 346: loss=0.12990188598632812\n",
            "step 347: loss=0.4433863162994385\n",
            "step 348: loss=0.19400978088378906\n",
            "step 349: loss=0.08053302764892578\n",
            "step 350: loss=0.02692890167236328\n",
            "step 351: loss=0.38886308670043945\n",
            "step 352: loss=1.523944616317749\n",
            "step 353: loss=1.557647943496704\n",
            "step 354: loss=0.7366688251495361\n",
            "step 355: loss=1.5161864757537842\n",
            "step 356: loss=0.018377304077148438\n",
            "step 357: loss=1.2819724082946777\n",
            "step 358: loss=0.48952770233154297\n",
            "step 359: loss=0.3830528259277344\n",
            "step 360: loss=0.2774205207824707\n",
            "step 361: loss=0.15614843368530273\n",
            "step 362: loss=0.026198387145996094\n",
            "step 363: loss=2.45873761177063\n",
            "step 364: loss=0.3456599712371826\n",
            "step 365: loss=0.384934663772583\n",
            "step 366: loss=1.2253036499023438\n",
            "step 367: loss=0.42652225494384766\n",
            "step 368: loss=0.0024127960205078125\n",
            "step 369: loss=0.29679250717163086\n",
            "step 370: loss=3.9339802265167236\n",
            "step 371: loss=1.1476609706878662\n",
            "step 372: loss=0.006028175354003906\n",
            "step 373: loss=0.025094985961914062\n",
            "step 374: loss=0.030088424682617188\n",
            "step 375: loss=0.2909841537475586\n",
            "step 376: loss=1.5548505783081055\n",
            "step 377: loss=0.18581438064575195\n",
            "step 378: loss=0.37858057022094727\n",
            "step 379: loss=3.816361427307129\n",
            "step 380: loss=0.1276102066040039\n",
            "step 381: loss=0.0203094482421875\n",
            "step 382: loss=0.6352777481079102\n",
            "step 383: loss=0.41879844665527344\n",
            "step 384: loss=0.0018863677978515625\n",
            "step 385: loss=0.0485844612121582\n",
            "step 386: loss=0.03948688507080078\n",
            "step 387: loss=0.19300317764282227\n",
            "step 388: loss=0.31698179244995117\n",
            "step 389: loss=0.3550095558166504\n",
            "step 390: loss=0.30381011962890625\n",
            "step 391: loss=0.5752663612365723\n",
            "step 392: loss=0.7523448467254639\n",
            "step 393: loss=0.9307587146759033\n",
            "step 394: loss=4.163150787353516\n",
            "step 395: loss=0.21659135818481445\n",
            "step 396: loss=0.5148525238037109\n",
            "step 397: loss=0.5608940124511719\n",
            "step 398: loss=1.8423759937286377\n",
            "step 399: loss=0.4655938148498535\n",
            "step 400: loss=0.04902029037475586\n",
            "step 401: loss=0.7953062057495117\n",
            "step 402: loss=0.34423065185546875\n",
            "step 403: loss=0.00025272369384765625\n",
            "step 404: loss=0.1343703269958496\n",
            "step 405: loss=0.1162419319152832\n",
            "step 406: loss=2.0450353622436523\n",
            "step 407: loss=0.22037982940673828\n",
            "step 408: loss=0.32031679153442383\n",
            "step 409: loss=4.15341854095459\n",
            "step 410: loss=0.015311241149902344\n",
            "step 411: loss=0.836982250213623\n",
            "step 412: loss=0.08687162399291992\n",
            "step 413: loss=1.182823896408081\n",
            "step 414: loss=0.6655287742614746\n",
            "step 415: loss=0.030199050903320312\n",
            "step 416: loss=1.7114145755767822\n",
            "step 417: loss=0.8987936973571777\n",
            "step 418: loss=0.1588435173034668\n",
            "step 419: loss=0.9800012111663818\n",
            "step 420: loss=0.14326953887939453\n",
            "step 421: loss=0.7330856323242188\n",
            "step 422: loss=0.5911045074462891\n",
            "step 423: loss=0.22020196914672852\n",
            "step 424: loss=0.02915668487548828\n",
            "step 425: loss=0.8084700107574463\n",
            "step 426: loss=1.3249101638793945\n",
            "step 427: loss=1.9790465831756592\n",
            "step 428: loss=0.49387168884277344\n",
            "step 429: loss=0.2029423713684082\n",
            "step 430: loss=0.010867118835449219\n",
            "step 431: loss=0.9041163921356201\n",
            "step 432: loss=0.7239291667938232\n",
            "step 433: loss=0.32804107666015625\n",
            "step 434: loss=2.6595869064331055\n",
            "step 435: loss=0.05801725387573242\n",
            "step 436: loss=0.2945094108581543\n",
            "step 437: loss=0.0058422088623046875\n",
            "step 438: loss=0.00118255615234375\n",
            "step 439: loss=0.9894213676452637\n",
            "step 440: loss=0.035610198974609375\n",
            "step 441: loss=0.07864665985107422\n",
            "step 442: loss=1.0972976684570312\n",
            "step 443: loss=2.7395873069763184\n",
            "step 444: loss=1.2684141397476196\n",
            "step 445: loss=0.8271484375\n",
            "step 446: loss=2.030148983001709\n",
            "step 447: loss=0.01706981658935547\n",
            "step 448: loss=0.14652442932128906\n",
            "step 449: loss=0.8165044784545898\n",
            "step 450: loss=2.9411308765411377\n",
            "step 451: loss=0.0037317276000976562\n",
            "step 452: loss=1.6171183586120605\n",
            "step 453: loss=0.05402040481567383\n",
            "step 454: loss=2.872264862060547\n",
            "step 455: loss=2.0467159748077393\n",
            "step 456: loss=1.5652191638946533\n",
            "step 457: loss=0.2162480354309082\n",
            "step 458: loss=2.43509578704834\n",
            "step 459: loss=0.12433147430419922\n",
            "step 460: loss=0.3025479316711426\n",
            "step 461: loss=4.552499294281006\n",
            "step 462: loss=0.032677650451660156\n",
            "step 463: loss=0.2710254192352295\n",
            "step 464: loss=0.08902263641357422\n",
            "step 465: loss=2.5139384269714355\n",
            "step 466: loss=0.0032968521118164062\n",
            "step 467: loss=2.8083159923553467\n",
            "step 468: loss=0.16899442672729492\n",
            "step 469: loss=0.6191864013671875\n",
            "step 470: loss=0.8607311248779297\n",
            "step 471: loss=1.9129371643066406\n",
            "step 472: loss=1.7333407402038574\n",
            "step 473: loss=0.3941974639892578\n",
            "step 474: loss=0.4605250358581543\n",
            "step 475: loss=0.028141021728515625\n",
            "step 476: loss=1.498698353767395\n",
            "step 477: loss=0.31674671173095703\n",
            "step 478: loss=0.00168609619140625\n",
            "step 479: loss=0.1409587860107422\n",
            "step 480: loss=0.22607660293579102\n",
            "step 481: loss=0.19871997833251953\n",
            "step 482: loss=0.45366716384887695\n",
            "step 483: loss=0.31646299362182617\n",
            "step 484: loss=0.9758138656616211\n",
            "step 485: loss=0.9316377639770508\n",
            "step 486: loss=0.0037441253662109375\n",
            "step 487: loss=0.0011358261108398438\n",
            "step 488: loss=0.045144081115722656\n",
            "step 489: loss=0.1974639892578125\n",
            "step 490: loss=0.6927015781402588\n",
            "step 491: loss=0.0006265640258789062\n",
            "step 492: loss=0.1433544158935547\n",
            "step 493: loss=0.2641420364379883\n",
            "step 494: loss=0.3066391944885254\n",
            "step 495: loss=1.760775089263916\n",
            "step 496: loss=0.8624153137207031\n",
            "step 497: loss=0.009818077087402344\n",
            "step 498: loss=0.0015277862548828125\n",
            "step 499: loss=3.7697038650512695\n",
            "step 500: loss=0.9256117343902588\n",
            "step 501: loss=0.03202962875366211\n",
            "step 502: loss=0.4472360610961914\n",
            "step 503: loss=0.25348567962646484\n",
            "step 504: loss=0.2319965362548828\n",
            "step 505: loss=0.016386985778808594\n",
            "step 506: loss=0.7594037055969238\n",
            "step 507: loss=2.1363370418548584\n",
            "step 508: loss=0.11826562881469727\n",
            "step 509: loss=0.07980871200561523\n",
            "step 510: loss=0.035796165466308594\n",
            "step 511: loss=0.22045469284057617\n",
            "step 512: loss=0.18213653564453125\n",
            "step 513: loss=0.5054140090942383\n",
            "step 514: loss=0.5509743690490723\n",
            "step 515: loss=0.019242286682128906\n",
            "step 516: loss=0.01963186264038086\n",
            "step 517: loss=1.2115912437438965\n",
            "step 518: loss=0.6631019115447998\n",
            "step 519: loss=0.08200645446777344\n",
            "step 520: loss=1.3339124917984009\n",
            "step 521: loss=0.1945514678955078\n",
            "step 522: loss=0.12208223342895508\n",
            "step 523: loss=0.018505096435546875\n",
            "step 524: loss=0.055393218994140625\n",
            "step 525: loss=0.09736061096191406\n",
            "step 526: loss=2.7697913646698\n",
            "step 527: loss=0.004085540771484375\n",
            "step 528: loss=0.004528045654296875\n",
            "step 529: loss=0.28116345405578613\n",
            "step 530: loss=0.05777549743652344\n",
            "step 531: loss=0.502511739730835\n",
            "step 532: loss=0.0038518905639648438\n",
            "step 533: loss=0.17044687271118164\n",
            "step 534: loss=0.051999568939208984\n",
            "step 535: loss=0.020951271057128906\n",
            "step 536: loss=0.3621788024902344\n",
            "step 537: loss=0.04747915267944336\n",
            "step 538: loss=0.6598091125488281\n",
            "step 539: loss=0.9023668766021729\n",
            "step 540: loss=2.9668502807617188\n",
            "step 541: loss=0.0038824081420898438\n",
            "step 542: loss=0.0684056282043457\n",
            "step 543: loss=0.49814701080322266\n",
            "step 544: loss=0.24260377883911133\n",
            "step 545: loss=0.6901845932006836\n",
            "step 546: loss=0.7440426349639893\n",
            "step 547: loss=0.17847728729248047\n",
            "step 548: loss=0.07002639770507812\n",
            "step 549: loss=0.09167671203613281\n",
            "step 550: loss=0.6276669502258301\n",
            "step 551: loss=1.7527539730072021\n",
            "step 552: loss=1.4664568901062012\n",
            "step 553: loss=0.01801156997680664\n",
            "step 554: loss=1.588718056678772\n",
            "step 555: loss=0.6365838050842285\n",
            "step 556: loss=2.359156608581543\n",
            "step 557: loss=0.2429332733154297\n",
            "step 558: loss=0.002788543701171875\n",
            "step 559: loss=0.0004901885986328125\n",
            "step 560: loss=0.14608001708984375\n",
            "step 561: loss=0.4102630615234375\n",
            "step 562: loss=0.8199503421783447\n",
            "step 563: loss=0.2971162796020508\n",
            "step 564: loss=0.2929973602294922\n",
            "step 565: loss=0.011035919189453125\n",
            "step 566: loss=2.90153169631958\n",
            "step 567: loss=0.06121063232421875\n",
            "step 568: loss=0.46771693229675293\n",
            "step 569: loss=0.2417287826538086\n",
            "step 570: loss=0.06613349914550781\n",
            "step 571: loss=0.046952247619628906\n",
            "step 572: loss=0.10270547866821289\n",
            "step 573: loss=0.10192632675170898\n",
            "step 574: loss=0.7308492660522461\n",
            "step 575: loss=0.18541479110717773\n",
            "step 576: loss=0.0673065185546875\n",
            "step 577: loss=1.1619279384613037\n",
            "step 578: loss=0.0492095947265625\n",
            "step 579: loss=0.010560035705566406\n",
            "step 580: loss=0.6665723323822021\n",
            "step 581: loss=0.06694650650024414\n",
            "step 582: loss=2.2689805030822754\n",
            "step 583: loss=0.22746658325195312\n",
            "step 584: loss=0.5360050201416016\n",
            "step 585: loss=0.14012861251831055\n",
            "step 586: loss=2.471245527267456\n",
            "step 587: loss=0.6285347938537598\n",
            "step 588: loss=0.49411869049072266\n",
            "step 589: loss=2.9138755798339844\n",
            "step 590: loss=0.5282862186431885\n",
            "step 591: loss=0.13997840881347656\n",
            "step 592: loss=2.677387237548828\n",
            "step 593: loss=0.6622674465179443\n",
            "step 594: loss=0.14083099365234375\n",
            "step 595: loss=1.5200077295303345\n",
            "step 596: loss=1.2404632568359375\n",
            "step 597: loss=0.9440689086914062\n",
            "step 598: loss=1.384364366531372\n",
            "step 599: loss=0.8591203689575195\n",
            "step 600: loss=0.14505529403686523\n",
            "step 601: loss=0.14066791534423828\n",
            "step 602: loss=0.21306133270263672\n",
            "step 603: loss=0.3978385925292969\n",
            "step 604: loss=2.1604790687561035\n",
            "step 605: loss=0.4126307964324951\n",
            "step 606: loss=1.072939157485962\n",
            "step 607: loss=0.3438701629638672\n",
            "step 608: loss=0.6456050872802734\n",
            "step 609: loss=0.12460613250732422\n",
            "step 610: loss=0.2161273956298828\n",
            "step 611: loss=1.4234001636505127\n",
            "step 612: loss=1.3905119895935059\n",
            "step 613: loss=0.44379472732543945\n",
            "step 614: loss=0.2622561454772949\n",
            "step 615: loss=1.1251068115234375\n",
            "step 616: loss=0.27696847915649414\n",
            "step 617: loss=0.03400135040283203\n",
            "step 618: loss=1.0506377220153809\n",
            "step 619: loss=0.0554356575012207\n",
            "step 620: loss=0.5775830745697021\n",
            "step 621: loss=2.9132144451141357\n",
            "step 622: loss=0.2057032585144043\n",
            "step 623: loss=0.12502622604370117\n",
            "step 624: loss=2.5381407737731934\n",
            "step 625: loss=0.5142438411712646\n",
            "step 626: loss=0.4413156509399414\n",
            "step 627: loss=0.21494150161743164\n",
            "step 628: loss=0.025819778442382812\n",
            "step 629: loss=1.5782334804534912\n",
            "step 630: loss=0.5100393295288086\n",
            "step 631: loss=0.10705423355102539\n",
            "step 632: loss=0.34012508392333984\n",
            "step 633: loss=2.570765733718872\n",
            "step 634: loss=0.02091503143310547\n",
            "step 635: loss=6.683933734893799\n",
            "step 636: loss=0.09868621826171875\n",
            "step 637: loss=0.9821426868438721\n",
            "step 638: loss=0.09847307205200195\n",
            "step 639: loss=0.23086905479431152\n",
            "step 640: loss=0.5108976364135742\n",
            "step 641: loss=0.6506447792053223\n",
            "step 642: loss=0.019531726837158203\n",
            "step 643: loss=1.9818276166915894\n",
            "step 644: loss=0.037041664123535156\n",
            "step 645: loss=0.25092124938964844\n",
            "step 646: loss=0.1673135757446289\n",
            "step 647: loss=3.2143805027008057\n",
            "step 648: loss=0.05027294158935547\n",
            "step 649: loss=0.0907602310180664\n",
            "step 650: loss=0.16423797607421875\n",
            "step 651: loss=0.8346233367919922\n",
            "step 652: loss=0.681767463684082\n",
            "step 653: loss=0.012887954711914062\n",
            "step 654: loss=0.21555805206298828\n",
            "step 655: loss=1.1414265632629395\n",
            "step 656: loss=0.4264535903930664\n",
            "step 657: loss=0.060373783111572266\n",
            "step 658: loss=0.3809828758239746\n",
            "step 659: loss=0.34478139877319336\n",
            "step 660: loss=0.5492372512817383\n",
            "step 661: loss=0.020505428314208984\n",
            "step 662: loss=0.17387819290161133\n",
            "step 663: loss=0.9730091094970703\n",
            "step 664: loss=0.41421985626220703\n",
            "step 665: loss=0.023694515228271484\n",
            "step 666: loss=0.09189701080322266\n",
            "step 667: loss=0.01885509490966797\n",
            "step 668: loss=1.4084246158599854\n",
            "step 669: loss=2.8859853744506836\n",
            "step 670: loss=0.2081911563873291\n",
            "step 671: loss=1.9136443138122559\n",
            "step 672: loss=0.5286903381347656\n",
            "step 673: loss=1.4348630905151367\n",
            "step 674: loss=0.16555047035217285\n",
            "step 675: loss=0.9397437572479248\n",
            "step 676: loss=0.03146553039550781\n",
            "step 677: loss=0.0074787139892578125\n",
            "step 678: loss=0.46175050735473633\n",
            "step 679: loss=0.09473705291748047\n",
            "step 680: loss=0.008462905883789062\n",
            "step 681: loss=1.46683931350708\n",
            "step 682: loss=0.058338165283203125\n",
            "step 683: loss=0.9928662776947021\n",
            "step 684: loss=0.0972757339477539\n",
            "step 685: loss=0.10205459594726562\n",
            "step 686: loss=0.03220033645629883\n",
            "step 687: loss=0.1822977066040039\n",
            "step 688: loss=0.18982315063476562\n",
            "step 689: loss=0.5775923728942871\n",
            "step 690: loss=0.3673434257507324\n",
            "step 691: loss=0.6741101741790771\n",
            "step 692: loss=2.0240063667297363\n",
            "step 693: loss=0.00860595703125\n",
            "step 694: loss=0.06125497817993164\n",
            "step 695: loss=0.13144350051879883\n",
            "step 696: loss=0.15802907943725586\n",
            "step 697: loss=2.6603844165802\n",
            "step 698: loss=0.852266788482666\n",
            "step 699: loss=0.5932917594909668\n",
            "step 700: loss=0.029884815216064453\n",
            "step 701: loss=0.12184667587280273\n",
            "step 702: loss=0.0046176910400390625\n",
            "step 703: loss=0.038036346435546875\n",
            "step 704: loss=0.03569364547729492\n",
            "step 705: loss=0.05467510223388672\n",
            "step 706: loss=0.17128419876098633\n",
            "step 707: loss=0.9932460784912109\n",
            "step 708: loss=0.39558982849121094\n",
            "step 709: loss=0.26425933837890625\n",
            "step 710: loss=1.4729747772216797\n",
            "step 711: loss=0.7270638942718506\n",
            "step 712: loss=2.2098217010498047\n",
            "step 713: loss=0.18172788619995117\n",
            "step 714: loss=0.12151336669921875\n",
            "step 715: loss=0.17783498764038086\n",
            "step 716: loss=1.0094122886657715\n",
            "step 717: loss=0.2937312126159668\n",
            "step 718: loss=0.27271175384521484\n",
            "step 719: loss=0.02951812744140625\n",
            "step 720: loss=0.026482105255126953\n",
            "step 721: loss=0.4348630905151367\n",
            "step 722: loss=0.04599809646606445\n",
            "step 723: loss=0.06534814834594727\n",
            "step 724: loss=0.30994606018066406\n",
            "step 725: loss=6.10931921005249\n",
            "step 726: loss=2.822141408920288\n",
            "step 727: loss=0.19489383697509766\n",
            "step 728: loss=1.1917390823364258\n",
            "step 729: loss=1.0231399536132812\n",
            "step 730: loss=0.22848963737487793\n",
            "step 731: loss=0.23949384689331055\n",
            "step 732: loss=0.35466933250427246\n",
            "step 733: loss=0.8382441997528076\n",
            "step 734: loss=0.14731597900390625\n",
            "step 735: loss=0.8544707298278809\n",
            "step 736: loss=0.0206451416015625\n",
            "step 737: loss=0.2973334789276123\n",
            "step 738: loss=0.04121255874633789\n",
            "step 739: loss=0.009181976318359375\n",
            "step 740: loss=1.2764720916748047\n",
            "step 741: loss=1.273885726928711\n",
            "step 742: loss=4.272341728210449\n",
            "step 743: loss=1.4487228393554688\n",
            "step 744: loss=0.06347942352294922\n",
            "step 745: loss=0.4986743927001953\n",
            "step 746: loss=0.007968902587890625\n",
            "step 747: loss=1.4382514953613281\n",
            "step 748: loss=0.11493587493896484\n",
            "step 749: loss=0.006710052490234375\n",
            "step 750: loss=0.43000030517578125\n",
            "step 751: loss=0.38118982315063477\n",
            "step 752: loss=0.4966104030609131\n",
            "step 753: loss=0.11653280258178711\n",
            "step 754: loss=0.46392297744750977\n",
            "step 755: loss=0.023680686950683594\n",
            "step 756: loss=0.7851626873016357\n",
            "step 757: loss=0.008550643920898438\n",
            "step 758: loss=1.274908423423767\n",
            "step 759: loss=0.08631372451782227\n",
            "step 760: loss=0.4114065170288086\n",
            "step 761: loss=0.030434608459472656\n",
            "step 762: loss=0.4647047519683838\n",
            "step 763: loss=0.25783538818359375\n",
            "step 764: loss=0.12276840209960938\n",
            "step 765: loss=0.34235191345214844\n",
            "step 766: loss=0.16598224639892578\n",
            "step 767: loss=0.08156108856201172\n",
            "step 768: loss=0.8856585025787354\n",
            "step 769: loss=0.001708984375\n",
            "step 770: loss=0.06731557846069336\n",
            "step 771: loss=0.34869885444641113\n",
            "step 772: loss=1.9329450130462646\n",
            "step 773: loss=0.0950460433959961\n",
            "step 774: loss=0.09786367416381836\n",
            "step 775: loss=0.035532474517822266\n",
            "step 776: loss=0.15671443939208984\n",
            "step 777: loss=0.0809788703918457\n",
            "step 778: loss=0.01971292495727539\n",
            "step 779: loss=0.02810382843017578\n",
            "step 780: loss=0.016853809356689453\n",
            "step 781: loss=0.0031080245971679688\n",
            "step 782: loss=0.024465560913085938\n",
            "step 783: loss=0.6460561752319336\n",
            "step 784: loss=0.018276691436767578\n",
            "step 785: loss=0.08486080169677734\n",
            "step 786: loss=0.032095909118652344\n",
            "step 787: loss=0.23444843292236328\n",
            "step 788: loss=0.6643266677856445\n",
            "step 789: loss=0.015120983123779297\n",
            "step 790: loss=0.5551629066467285\n",
            "step 791: loss=0.2002096176147461\n",
            "step 792: loss=0.6620044708251953\n",
            "step 793: loss=0.10877704620361328\n",
            "step 794: loss=0.6455426216125488\n",
            "step 795: loss=0.04246807098388672\n",
            "step 796: loss=0.02660989761352539\n",
            "step 797: loss=0.2708439826965332\n",
            "step 798: loss=2.9363293647766113\n",
            "step 799: loss=0.481719970703125\n",
            "step 800: loss=0.5913913249969482\n",
            "step 801: loss=0.1209716796875\n",
            "step 802: loss=0.004852294921875\n",
            "step 803: loss=0.11364507675170898\n",
            "step 804: loss=2.612170696258545\n",
            "step 805: loss=0.011935234069824219\n",
            "step 806: loss=0.0046215057373046875\n",
            "step 807: loss=0.041417598724365234\n",
            "step 808: loss=0.007549285888671875\n",
            "step 809: loss=0.5903716087341309\n",
            "step 810: loss=0.00115966796875\n",
            "step 811: loss=1.6003857851028442\n",
            "step 812: loss=0.2842984199523926\n",
            "step 813: loss=0.04505729675292969\n",
            "step 814: loss=0.9899144172668457\n",
            "step 815: loss=0.025193214416503906\n",
            "step 816: loss=0.6489248275756836\n",
            "step 817: loss=0.29872918128967285\n",
            "step 818: loss=0.8774468898773193\n",
            "step 819: loss=0.00337982177734375\n",
            "step 820: loss=0.18364381790161133\n",
            "step 821: loss=0.13299942016601562\n",
            "step 822: loss=0.04888343811035156\n",
            "step 823: loss=0.0027322769165039062\n",
            "step 824: loss=0.5085883140563965\n",
            "step 825: loss=3.0380735397338867\n",
            "step 826: loss=0.0025577545166015625\n",
            "step 827: loss=1.741243600845337\n",
            "step 828: loss=0.020491600036621094\n",
            "step 829: loss=0.4415397644042969\n",
            "step 830: loss=0.15727806091308594\n",
            "step 831: loss=0.6046748161315918\n",
            "step 832: loss=0.01106405258178711\n",
            "step 833: loss=0.29731130599975586\n",
            "step 834: loss=3.7791943550109863\n",
            "step 835: loss=0.3176240921020508\n",
            "step 836: loss=0.008383750915527344\n",
            "step 837: loss=0.0025529861450195312\n",
            "step 838: loss=0.0021047592163085938\n",
            "step 839: loss=0.04311656951904297\n",
            "step 840: loss=0.02485513687133789\n",
            "step 841: loss=0.0011339187622070312\n",
            "step 842: loss=0.04321432113647461\n",
            "step 843: loss=0.02466869354248047\n",
            "step 844: loss=0.21531963348388672\n",
            "step 845: loss=0.0077533721923828125\n",
            "step 846: loss=0.15342044830322266\n",
            "step 847: loss=1.3729257583618164\n",
            "step 848: loss=0.12397575378417969\n",
            "step 849: loss=0.0162353515625\n",
            "step 850: loss=0.06940364837646484\n",
            "step 851: loss=0.1914076805114746\n",
            "step 852: loss=0.3683433532714844\n",
            "step 853: loss=0.5249056816101074\n",
            "step 854: loss=0.10742568969726562\n",
            "step 855: loss=3.3022565841674805\n",
            "step 856: loss=0.11857175827026367\n",
            "step 857: loss=0.09141826629638672\n",
            "step 858: loss=0.12316131591796875\n",
            "step 859: loss=0.09176254272460938\n",
            "step 860: loss=0.511023998260498\n",
            "step 861: loss=0.023911476135253906\n",
            "step 862: loss=0.004899024963378906\n",
            "step 863: loss=0.1987757682800293\n",
            "step 864: loss=0.36321258544921875\n",
            "step 865: loss=1.4952280521392822\n",
            "step 866: loss=0.47373294830322266\n",
            "step 867: loss=0.6865699291229248\n",
            "step 868: loss=0.2302231788635254\n",
            "step 869: loss=2.550868272781372\n",
            "step 870: loss=8.58306884765625e-05\n",
            "step 871: loss=0.4573197364807129\n",
            "step 872: loss=0.5439281463623047\n",
            "step 873: loss=0.288485050201416\n",
            "step 874: loss=0.9280276298522949\n",
            "step 875: loss=0.36085987091064453\n",
            "step 876: loss=1.9587894678115845\n",
            "step 877: loss=0.4587221145629883\n",
            "step 878: loss=0.19086813926696777\n",
            "step 879: loss=0.11730003356933594\n",
            "step 880: loss=0.2756223678588867\n",
            "step 881: loss=0.43172788619995117\n",
            "step 882: loss=0.05208110809326172\n",
            "step 883: loss=0.2842135429382324\n",
            "step 884: loss=0.13697004318237305\n",
            "step 885: loss=0.036350250244140625\n",
            "step 886: loss=1.500255823135376\n",
            "step 887: loss=0.07356500625610352\n",
            "step 888: loss=0.17042112350463867\n",
            "step 889: loss=0.3357248306274414\n",
            "step 890: loss=0.006404399871826172\n",
            "step 891: loss=0.030520915985107422\n",
            "step 892: loss=0.0009679794311523438\n",
            "step 893: loss=0.046398162841796875\n",
            "step 894: loss=0.15717458724975586\n",
            "step 895: loss=0.06026315689086914\n",
            "step 896: loss=0.04476022720336914\n",
            "step 897: loss=0.6424202919006348\n",
            "step 898: loss=0.7885024547576904\n",
            "step 899: loss=0.017273426055908203\n",
            "step 900: loss=0.014695167541503906\n",
            "step 901: loss=0.030878543853759766\n",
            "step 902: loss=0.004338264465332031\n",
            "step 903: loss=1.2929409742355347\n",
            "step 904: loss=0.09101390838623047\n",
            "step 905: loss=0.08078718185424805\n",
            "step 906: loss=1.155379295349121\n",
            "step 907: loss=0.33635473251342773\n",
            "step 908: loss=0.006482124328613281\n",
            "step 909: loss=0.30418872833251953\n",
            "step 910: loss=0.06952381134033203\n",
            "step 911: loss=0.6127896308898926\n",
            "step 912: loss=0.15243005752563477\n",
            "step 913: loss=0.028969287872314453\n",
            "step 914: loss=4.3818135261535645\n",
            "step 915: loss=0.01670217514038086\n",
            "step 916: loss=0.7612361907958984\n",
            "step 917: loss=0.8915224075317383\n",
            "step 918: loss=0.9248113632202148\n",
            "step 919: loss=1.341148853302002\n",
            "step 920: loss=0.3162088394165039\n",
            "step 921: loss=0.4000864028930664\n",
            "step 922: loss=1.9502477645874023\n",
            "step 923: loss=0.08072280883789062\n",
            "step 924: loss=0.011460304260253906\n",
            "step 925: loss=0.011565685272216797\n",
            "step 926: loss=0.051601409912109375\n",
            "step 927: loss=0.1639399528503418\n",
            "step 928: loss=2.8063321113586426\n",
            "step 929: loss=0.29902219772338867\n",
            "step 930: loss=0.15179920196533203\n",
            "step 931: loss=0.5291564464569092\n",
            "step 932: loss=0.3462502956390381\n",
            "step 933: loss=0.06670761108398438\n",
            "step 934: loss=0.009406089782714844\n",
            "step 935: loss=0.02409839630126953\n",
            "step 936: loss=0.46036291122436523\n",
            "step 937: loss=0.03229331970214844\n",
            "step 938: loss=0.046222686767578125\n",
            "step 939: loss=0.3735795021057129\n",
            "step 940: loss=0.20336627960205078\n",
            "step 941: loss=0.024906158447265625\n",
            "step 942: loss=0.3062772750854492\n",
            "step 943: loss=0.019220352172851562\n",
            "step 944: loss=0.0009450912475585938\n",
            "step 945: loss=2.307795763015747\n",
            "step 946: loss=0.24263477325439453\n",
            "step 947: loss=0.03453683853149414\n",
            "step 948: loss=0.044391632080078125\n",
            "step 949: loss=0.3814692497253418\n",
            "step 950: loss=0.22207140922546387\n",
            "step 951: loss=0.14189815521240234\n",
            "step 952: loss=0.0019931793212890625\n",
            "step 953: loss=0.22443532943725586\n",
            "step 954: loss=0.08647441864013672\n",
            "step 955: loss=7.724761962890625e-05\n",
            "step 956: loss=0.20012760162353516\n",
            "step 957: loss=0.005602836608886719\n",
            "step 958: loss=0.8122437000274658\n",
            "step 959: loss=0.2339034080505371\n",
            "step 960: loss=0.4978032112121582\n",
            "step 961: loss=0.9441063404083252\n",
            "step 962: loss=0.059866905212402344\n",
            "step 963: loss=0.05092620849609375\n",
            "step 964: loss=0.0217132568359375\n",
            "step 965: loss=1.3094550371170044\n",
            "step 966: loss=0.08177995681762695\n",
            "step 967: loss=0.5009264945983887\n",
            "step 968: loss=2.086491584777832\n",
            "step 969: loss=0.11873388290405273\n",
            "step 970: loss=0.23385000228881836\n",
            "step 971: loss=0.008561134338378906\n",
            "step 972: loss=0.25334978103637695\n",
            "step 973: loss=0.1501774787902832\n",
            "step 974: loss=1.9254882335662842\n",
            "step 975: loss=1.4295926094055176\n",
            "step 976: loss=0.00786447525024414\n",
            "step 977: loss=0.11253166198730469\n",
            "step 978: loss=1.1687957048416138\n",
            "step 979: loss=0.7309257984161377\n",
            "step 980: loss=0.24778318405151367\n",
            "step 981: loss=0.5749430656433105\n",
            "step 982: loss=0.6188721656799316\n",
            "step 983: loss=0.26140594482421875\n",
            "step 984: loss=0.02825021743774414\n",
            "step 985: loss=1.3639967441558838\n",
            "step 986: loss=0.08413457870483398\n",
            "step 987: loss=0.06549072265625\n",
            "step 988: loss=0.2288646697998047\n",
            "step 989: loss=3.528594970703125e-05\n",
            "step 990: loss=0.08487510681152344\n",
            "step 991: loss=0.1023263931274414\n",
            "step 992: loss=0.6776700019836426\n",
            "step 993: loss=0.003749847412109375\n",
            "step 994: loss=0.17822027206420898\n",
            "step 995: loss=0.6524538993835449\n",
            "step 996: loss=0.00047206878662109375\n",
            "step 997: loss=0.0029249191284179688\n",
            "step 998: loss=0.32758665084838867\n",
            "step 999: loss=0.05590200424194336\n",
            "step 1000: loss=0.00032520294189453125\n",
            "[low-rank] Mean loss        0.5849002\n",
            "[low-rank] True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "[low-rank] Predictions:     [2 0 4 8 7 6 0 8 3 1]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "rank = 32\n",
        "layer_sizes = (784, 128, 10)\n",
        "\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "params_lowrank = train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    use_lowrank=True,\n",
        "    rank=rank,\n",
        "    layer_sizes=layer_sizes,\n",
        ")\n",
        "evaluate_mlp(\n",
        "    test_data,\n",
        "    params_lowrank,\n",
        "    use_lowrank=True,\n",
        "    rank=rank,\n",
        "    layer_sizes=layer_sizes,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f61693",
      "metadata": {},
      "source": [
        "GANs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "dc99c571",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gan_discriminator_loss(d_real_logits, d_fake_logits):\n",
        "    real_targets = jnp.ones_like(d_real_logits)\n",
        "    fake_targets = jnp.zeros_like(d_fake_logits)\n",
        "    real_loss = optax.sigmoid_binary_cross_entropy(d_real_logits, real_targets)\n",
        "    fake_loss = optax.sigmoid_binary_cross_entropy(d_fake_logits, fake_targets)\n",
        "    return (real_loss.mean() + fake_loss.mean())\n",
        "\n",
        "\n",
        "def gan_generator_loss(d_fake_logits):\n",
        "    targets = jnp.ones_like(d_fake_logits)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(d_fake_logits, targets)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "98852b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def discriminator_loss(real_logits, fake_logits):\n",
        "    \"\"\"\n",
        "    Discriminator loss.\n",
        "    Real -> 1, Fake -> 0\n",
        "    \"\"\"\n",
        "    real_targets = jnp.ones_like(real_logits)\n",
        "    fake_targets = jnp.zeros_like(fake_logits)\n",
        "\n",
        "    loss_real = optax.sigmoid_binary_cross_entropy(\n",
        "        logits=real_logits, labels=real_targets\n",
        "    ).mean()\n",
        "\n",
        "    loss_fake = optax.sigmoid_binary_cross_entropy(\n",
        "        logits=fake_logits, labels=fake_targets\n",
        "    ).mean()\n",
        "\n",
        "    return loss_real + loss_fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "da36a1c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generator_loss_saturating(fake_logits):\n",
        "    \"\"\"\n",
        "    Generator minimax loss.\n",
        "    Literal mirror of discriminator fake term.\n",
        "    (Saturates when D(fake) â‰ˆ 0)\n",
        "    \"\"\"\n",
        "    fake_targets = jnp.zeros_like(fake_logits)\n",
        "\n",
        "    return optax.sigmoid_binary_cross_entropy(\n",
        "        logits=fake_logits, labels=fake_targets\n",
        "    ).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "17ce2a96",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generator_loss_non_saturating(fake_logits):\n",
        "    \"\"\"\n",
        "    Generator surrogate loss.\n",
        "    Train G so that D(fake) -> 1 (non-saturating).\n",
        "    \"\"\"\n",
        "    real_targets = jnp.ones_like(fake_logits)\n",
        "\n",
        "    return optax.sigmoid_binary_cross_entropy(\n",
        "        logits=fake_logits, labels=real_targets\n",
        "    ).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3523c4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D loss: 1.386295\n",
            "G loss (saturating): 0.6931475\n",
            "G loss (non-saturating): 0.6931475\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# D outputs 0.5 for both real and fake\n",
        "real_logits = jnp.zeros((batch_size, 1), dtype=jnp.float32)\n",
        "fake_logits = jnp.zeros((batch_size, 1), dtype=jnp.float32)\n",
        "\n",
        "print(\"D loss:\",\n",
        "      discriminator_loss(real_logits, fake_logits))\n",
        "# log(2) + log(2) = ~1.386\n",
        "\n",
        "print(\"G loss (saturating):\",\n",
        "      generator_loss_saturating(fake_logits))\n",
        "# -log(1 - 0.5) = ~0.693\n",
        "\n",
        "print(\"G loss (non-saturating):\",\n",
        "      generator_loss_non_saturating(fake_logits))\n",
        "# -log(0.5) = ~0.693\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
