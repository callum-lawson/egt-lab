{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, tree_util, vmap\n",
    "from jax.nn import relu\n",
    "from jax.scipy.special import logsumexp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764616896.260375   10649 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1764616896.277408   10649 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data_jax[data_type] = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data_jax[data_type] = flatten_image_for_mlp(data_jax[data_type])\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e662dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-12-01 19:21:38,900:jax._src.xla_bridge:850: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "dataset_tf = \"train\"\n",
    "all_data_tf = mnist_data[dataset_tf]\n",
    "all_data_jax = prepare_data(all_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d5ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_data_jax[\"image\"]\n",
    "labels = all_data_jax[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5e364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (60000, 784)\n",
      "Labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239a4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_preactivations(x, W, b):\n",
    "    \"\"\"\n",
    "    x is a row vector (single sample) - shape: (784,)\n",
    "    W is the weights matrix - shape: (784, 128)\n",
    "    b is the bias vector - shape: (128,)\n",
    "    Returns: row vector - shape: (128,)\n",
    "    \"\"\"\n",
    "    return x @ W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7568be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, params_list):\n",
    "    \"\"\"Forward pass through all layers: inputs and outputs are vectors\"\"\"\n",
    "    for layer_number, params in enumerate(params_list):\n",
    "        W, b = params[\"W\"], params[\"b\"]\n",
    "        x = calculate_preactivations(x, W, b)\n",
    "        if layer_number != (len(params_list) - 1):\n",
    "            x = relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046c9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions_logits, observed_label):\n",
    "    log_probs = predictions_logits - logsumexp(predictions_logits)\n",
    "    return -log_probs[observed_label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4c7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass_batch = vmap(forward_pass, in_axes=(0, None))\n",
    "calculate_loss_batch = vmap(calculate_loss, in_axes=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_params(dims, key, scale=0.1):\n",
    "    return scale * random.normal(key, dims)\n",
    "\n",
    "def initialise_layer(m, n, key):\n",
    "    \"\"\"Initialize weights and biases for one layer\"\"\"\n",
    "    w_key, b_key = random.split(key)\n",
    "    return {\n",
    "        \"W\": initialise_params((m, n), w_key),\n",
    "        \"b\": initialise_params((n,), b_key)\n",
    "    }\n",
    "\n",
    "def initialise_network(sizes, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    params_list = []\n",
    "    keys = random.split(key, len(sizes) - 1)\n",
    "    \n",
    "    for i in range(len(sizes) - 1):\n",
    "        input_size = sizes[i]\n",
    "        output_size = sizes[i + 1]\n",
    "        layer_key = keys[i]\n",
    "        layer_params = initialise_layer(input_size, output_size, layer_key)\n",
    "        params_list.append(layer_params)\n",
    "    \n",
    "    return params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4cbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_loss_batch(params, images, labels):\n",
    "    logits = forward_pass_batch(images, params)\n",
    "    loss = calculate_loss_batch(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "    return mean_loss\n",
    "    \n",
    "calculate_gradients_by_param = grad(calculate_mean_loss_batch)\n",
    "\n",
    "def update_parameters(params, gradients_by_param, learning_rate):\n",
    "    return tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, \n",
    "        params, \n",
    "        gradients_by_param\n",
    "    )\n",
    "\n",
    "@jit\n",
    "def take_training_step(params, images, labels, learning_rate=0.1):\n",
    "    gradients_by_param = calculate_gradients_by_param(params, images, labels)\n",
    "    params_new = update_parameters(params, gradients_by_param, learning_rate)\n",
    "    return params_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(images, labels, n_steps, layer_sizes, key):\n",
    "    params = initialise_network(layer_sizes, key)\n",
    "    for step in range(n_steps):\n",
    "        params = take_training_step(params, images, labels)\n",
    "        loss = calculate_mean_loss_batch(params, images, labels)\n",
    "        print(f\"step {step} complete: loss = {loss}\")\n",
    "    print(f\"training finished after {n_steps}: final loss = {loss}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 complete: loss = 1.7302042245864868\n",
      "step 1 complete: loss = 1.4029744863510132\n",
      "step 2 complete: loss = 1.1516979932785034\n",
      "step 3 complete: loss = 0.9586992263793945\n",
      "step 4 complete: loss = 0.8078511357307434\n",
      "training finished after 5: final loss = 0.8078511357307434\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [784, 128, 10]\n",
    "key = random.key(0)\n",
    "\n",
    "trial_set_size = 20\n",
    "test_images = images[:trial_set_size]\n",
    "test_labels = labels[:trial_set_size]\n",
    "\n",
    "# Train the model\n",
    "params = run_training(test_images, test_labels, n_steps=5, layer_sizes=layer_sizes, key=key)\n",
    "\n",
    "# Then use it\n",
    "logits = forward_pass_batch(test_images, params)\n",
    "loss = calculate_loss_batch(logits, test_labels)\n",
    "predictions = jnp.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e344ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:     [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1]\n",
      "Predictions:     [7 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 7 1]\n",
      "Match:           [False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True]\n",
      "Loss             [1.9643625  0.53660583 1.072345   0.45093012 0.9924766  0.38990068\n",
      " 1.3392543  0.36016273 0.3386774  0.85717463 1.1086264  1.5149956\n",
      " 0.34358168 0.6781447  0.74361444 0.48400784 0.31850433 1.1063684\n",
      " 1.2447559  0.31253457]\n"
     ]
    }
   ],
   "source": [
    "print(\"True labels:    \", test_labels)\n",
    "print(\"Predictions:    \", predictions)\n",
    "print(\"Match:          \", predictions == test_labels)\n",
    "print(\"Loss            \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87189ac9",
   "metadata": {},
   "source": [
    "### Data details\n",
    "- n cols x n cells x n colour channels\n",
    "- logits output (10 classes)\n",
    "\n",
    "### Tutorial\n",
    "- https://docs.jax.dev/en/latest/notebooks/neural_network_with_tfds_data.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
