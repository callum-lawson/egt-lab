{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bffd5b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Key differences between NNX and Linen:**\n",
    "\n",
    "* **State model:** NNX modules are *stateful Python objects*; Linen modules are *stateless templates* whose state lives in external PyTrees.\n",
    "* **Initialization:** NNX creates parameters when you **instantiate** the module; Linen requires a separate **`model.init(rng, x)`** step.\n",
    "* **Calling convention:** NNX uses **direct calls** (`y = model(x)`); Linen uses **`apply`** (`y = model.apply(params, x)`).\n",
    "* **Mental model:** NNX behaves more like **normal Python classes with live attributes**; Linen enforces a **pure-functional style** with explicit data flow.\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "from flax import nnx\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764797987.289718    8223 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1764797987.293828    8223 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data_jax[data_type] = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data_jax[data_type] = flatten_image_for_mlp(data_jax[data_type])\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e662dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-12-03 21:39:49,306:jax._src.xla_bridge:854: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "dataset_tf = \"train\"\n",
    "all_data_tf = mnist_data[dataset_tf]\n",
    "all_data_jax = prepare_data(all_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d5ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_data_jax[\"image\"]\n",
    "labels = all_data_jax[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5e364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (60000, 784)\n",
      "Labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9813eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    def __init__(self, input_size, output_size, *, rngs, init_sd=0.05):\n",
    "        self.weights = nnx.Param(rngs.params.normal((input_size, output_size)) *init_sd)\n",
    "        self.biases = nnx.Param(jnp.zeros((output_size,)))\n",
    "\n",
    "    def __call__(self, input_activations):\n",
    "        return input_activations @ self.weights + self.biases\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "    def __init__(self, layer_sizes, *, rngs):\n",
    "        layers = []\n",
    "        input_sizes = layer_sizes[:-1]\n",
    "        output_sizes = layer_sizes[1:]\n",
    "        for input_size, output_size in zip(input_sizes, output_sizes):\n",
    "            layers.append(Linear(input_size, output_size, rngs=rngs))\n",
    "        self.layers = nnx.List(layers)\n",
    "\n",
    "    def __call__(self, activations):\n",
    "        for layer_number, layer in enumerate(self.layers):\n",
    "            activations = layer(activations)\n",
    "            if layer_number != (len(self.layers) - 1):\n",
    "                activations = jax.nn.relu(activations)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_loss_batch(model, images, labels):\n",
    "    logits = model(images) # FORWARD PASS\n",
    "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def take_training_step(model, optimizer, images, labels):\n",
    "    loss, grads = nnx.value_and_grad(calculate_mean_loss_batch)(model, images, labels)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(layer_sizes, images, labels, n_steps, initial_learning_rate=1e-3):\n",
    "    model = MLP(layer_sizes, rngs=nnx.Rngs(0))\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(initial_learning_rate), wrt=nnx.Param)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        loss = take_training_step(model, optimizer, images, labels)\n",
    "        print(f\"step {step}: loss={loss}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=2.2502477169036865\n",
      "step 1: loss=2.117492198944092\n",
      "step 2: loss=1.9914255142211914\n",
      "step 3: loss=1.8695420026779175\n",
      "step 4: loss=1.7506182193756104\n"
     ]
    }
   ],
   "source": [
    "trial_set_size = 20\n",
    "test_images = images[:trial_set_size]\n",
    "test_labels = labels[:trial_set_size]\n",
    "\n",
    "layer_sizes = [784, 128, 10]\n",
    "model = run_training(layer_sizes, test_images, test_labels, n_steps=5, initial_learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b7d5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:     [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1]\n",
      "Predictions:  s   [7 1 7 7 8 1 2 7 1 2 6 4 7 7 3 3 7 7 7 1]\n",
      "Match:           [False  True False  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False False  True]\n",
      "Loss             [2.3186948 1.6746998 2.1503305 1.1054006 1.7520564 1.7254624 1.4025867\n",
      " 1.3423988 1.4489025 2.0254958 1.939018  1.9059012 1.325226  1.439785\n",
      " 1.5715833 1.4464514 1.1044837 1.7482777 1.7558821 1.5099177]\n"
     ]
    }
   ],
   "source": [
    "logits = model(test_images)\n",
    "loss = optax.softmax_cross_entropy_with_integer_labels(logits, test_labels)\n",
    "\n",
    "predictions = jnp.argmax(logits, axis=1)\n",
    "\n",
    "print(\"True labels:    \", test_labels)\n",
    "print(\"Predictions:  s  \", predictions)\n",
    "print(\"Match:          \", predictions == test_labels)\n",
    "print(\"Loss            \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
