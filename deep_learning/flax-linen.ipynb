{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bffd5b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Key differences from JAX implementation:**  \n",
    "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
    "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
    "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, tree_util, vmap\n",
    "from jax.scipy.special import logsumexp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data_jax[data_type] = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data_jax[data_type] = flatten_image_for_mlp(data_jax[data_type])\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e662dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tf = \"train\"\n",
    "all_data_tf = mnist_data[dataset_tf]\n",
    "all_data_jax = prepare_data(all_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "71d5ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_data_jax[\"image\"]\n",
    "labels = all_data_jax[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cc5e364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (60000, 784)\n",
      "Labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9813eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    layer_sizes: Sequence[int]  # Proper annotation for Flax modules\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, activations):\n",
    "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
    "            activations = nn.Dense(\n",
    "                layer_size,\n",
    "                kernel_init=nn.initializers.normal(0.1),\n",
    "                bias_init=nn.initializers.normal(0.1)\n",
    "            )(activations)\n",
    "\n",
    "            if layer_number != (len(self.layer_sizes) - 1):\n",
    "                activations = nn.relu(activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_network_params(layer_sizes, key, model):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    input_shape_dummy = jnp.ones((1, layer_sizes[0]))\n",
    "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "244a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions_logits, observed_label):\n",
    "    log_probs = predictions_logits - logsumexp(predictions_logits)\n",
    "    return -log_probs[observed_label] \n",
    "\n",
    "calculate_loss_batch = vmap(calculate_loss, in_axes=(0, 0))\n",
    "\n",
    "def calculate_mean_loss_batch(params, images, labels, model):\n",
    "    logits = model.apply({\"params\": params}, images) # foward pass\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1f4cbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def take_training_step(params, images, labels, optimizer_state):\n",
    "    \"\"\"\n",
    "    Single training step \n",
    "    `model` and `optimizer` are defined outside the function because they can't be jitted\n",
    "    \"\"\"\n",
    "    calculate_gradients_by_param = grad(calculate_mean_loss_batch)\n",
    "    gradients_by_param = calculate_gradients_by_param(params, images, labels)\n",
    "    updates_by_param, optimizer_state = optimizer.update(gradients_by_param, optimizer_state)\n",
    "    params = optax.apply_updates(params, updates_by_param)\n",
    "    return params, optimizer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fa6224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def take_training_step(state, images, labels):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, images)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "\n",
    "    grads = grad(loss_fn)(state.params)\n",
    "    return state.apply_gradients(grads=grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(images, labels, n_steps, layer_sizes, key, lr=1e-3):\n",
    "    model = MLP(layer_sizes=layer_sizes[1:])\n",
    "    params = initialise_network_params(layer_sizes, key, model)\n",
    "\n",
    "    optimizer = optax.adam(lr)\n",
    "    state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        state = take_training_step(state, images, labels)\n",
    "        loss = calculate_mean_loss_batch(state.params, images, labels, model)\n",
    "        print(f\"step {step}: loss={loss}\")\n",
    "\n",
    "    return state.params, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=2.1574771404266357\n",
      "step 1: loss=1.9256445169448853\n",
      "step 2: loss=1.7224485874176025\n",
      "step 3: loss=1.5373908281326294\n",
      "step 4: loss=1.3704960346221924\n"
     ]
    }
   ],
   "source": [
    "trial_set_size = 20\n",
    "test_images = images[:trial_set_size]\n",
    "test_labels = labels[:trial_set_size]\n",
    "\n",
    "layer_sizes = [784, 128, 10]\n",
    "key = random.key(0)\n",
    "\n",
    "params, model = run_training(test_images, test_labels, n_steps=5, layer_sizes=layer_sizes, key=key)\n",
    "\n",
    "logits = model.apply({\"params\": params}, test_images)\n",
    "loss = calculate_loss_batch(logits, test_labels)\n",
    "predictions = jnp.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e344ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:     [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1]\n",
      "Predictions:     [1 1 6 7 8 1 2 9 1 6 6 1 7 7 3 3 7 9 9 1]\n",
      "Match:           [False  True False  True  True  True  True False  True  True  True False\n",
      "  True  True  True  True  True  True  True  True]\n",
      "Loss             [1.7879559  1.1756442  2.1219096  0.80904436 1.1157752  1.352685\n",
      " 1.3984716  1.574126   0.9887434  1.3136357  1.7677958  2.639235\n",
      " 1.1510949  1.5284108  0.9676604  1.4889581  0.8168993  1.0736182\n",
      " 1.2909293  1.0473256 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"True labels:    \", test_labels)\n",
    "print(\"Predictions:    \", predictions)\n",
    "print(\"Match:          \", predictions == test_labels)\n",
    "print(\"Loss            \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
