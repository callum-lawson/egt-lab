{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bffd5b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Key differences from JAX implementation:**  \n",
    "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
    "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
    "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data = flatten_image_for_mlp(data)\n",
    "        if subsample_size > 0:\n",
    "            data = data[:subsample_size]\n",
    "        data_jax[data_type] = data\n",
    "\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    layer_sizes: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, activations):\n",
    "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
    "            activations = nn.Dense(\n",
    "                layer_size,\n",
    "                kernel_init=nn.initializers.normal(0.1),\n",
    "                bias_init=nn.initializers.normal(0.1)\n",
    "            )(activations)\n",
    "\n",
    "            if layer_number != (len(self.layer_sizes) - 1):\n",
    "                activations = nn.relu(activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_network_params(model, input_layer_size, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
    "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
    "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
    "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def take_training_step(training_state, images, labels):\n",
    "    \"\"\"\n",
    "    Single training step \n",
    "    The model and optimiser are passed in the training state\n",
    "    returns a training state\n",
    "    \"\"\"\n",
    "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
    "    grads_by_params = grads_by_params_fn(\n",
    "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
    "        training_state.apply_fn,\n",
    "        images,\n",
    "        labels,\n",
    "    )\n",
    "    return training_state.apply_gradients(grads=grads_by_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(images, labels, n_batches):\n",
    "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
    "    n_samples = len(images)\n",
    "    assert len(images) == len(labels)\n",
    "    assert n_samples >= n_batches\n",
    "    assert n_batches > 0\n",
    "    n_samples_per_batch = n_samples // n_batches\n",
    "    start = 0\n",
    "    end = n_samples_per_batch\n",
    "    while end <= n_samples: \n",
    "        yield (images[start:end], labels[start:end])\n",
    "        start += n_samples_per_batch\n",
    "        end += n_samples_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experiment_name(layer_sizes, optimizer):\n",
    "    layer_part = \"mlp_\" + \"-\".join(str(s) for s in layer_sizes)\n",
    "    opt_name = optimizer.__class__.__name__\n",
    "    return f\"{layer_part}_{opt_name}\"\n",
    "\n",
    "def initialise_checkpoint_manager(experiment_name: str = \"mlp\", max_to_keep=20):\n",
    "    project_root = Path().resolve()\n",
    "    base_dir = project_root / \"checkpoints\"\n",
    "    checkpoint_dir = base_dir / experiment_name\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_manager = ocp.CheckpointManager(\n",
    "        directory=str(checkpoint_dir),\n",
    "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
    "    )\n",
    "    return checkpoint_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f245d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_state(layer_sizes, optimizer, key):\n",
    "    input_layer_size = layer_sizes[0]\n",
    "    network_layer_sizes = layer_sizes[1:]\n",
    "    model = MLP(layer_sizes=network_layer_sizes)\n",
    "    apply_fn = model.apply\n",
    "    params = initialise_network_params(model, input_layer_size, key)\n",
    "    training_state = train_state.TrainState.create(\n",
    "        apply_fn=apply_fn,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "    )\n",
    "    return training_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    images, \n",
    "    labels, \n",
    "    n_steps, \n",
    "    layer_sizes, \n",
    "    optimizer, \n",
    "    checkpoint_manager, \n",
    "    key, \n",
    "    steps_per_save, \n",
    "    training_state\n",
    "    ):\n",
    "    \"\"\"\n",
    "    The training state ('state') is an instance of TrainState that holds:\n",
    "    - apply_fn: the model's apply function, used for forward passes\n",
    "    - params: the parameters of the neural network\n",
    "    - tx: the optimizers (Optax transformation) for parameter updates\n",
    "    - opt_state: the state of the optimizer\n",
    "    \"\"\"\n",
    "    if training_state is None:\n",
    "        training_state = create_training_state(layer_sizes, optimizer, key)\n",
    "\n",
    "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
    "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
    "        step = training_state.step\n",
    "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
    "        print(f\"step {step}: loss={loss}\")\n",
    "        if step == 1 or step % steps_per_save == 0:\n",
    "            step_dir = step\n",
    "            checkpoint_manager.save(\n",
    "                step_dir,\n",
    "                args=ocp.args.StandardSave(training_state)\n",
    "                )\n",
    "\n",
    "    return training_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(train_data, optimizer, n_steps=10**3, steps_per_save=100, training_state=None, key=jax.random.key(0)):\n",
    "    layer_sizes = [784, 128, 10]\n",
    "    experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
    "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
    "    final_params = run_training(\n",
    "        images=train_data[\"image\"], \n",
    "        labels=train_data[\"label\"], \n",
    "        n_steps=n_steps, \n",
    "        layer_sizes=layer_sizes, \n",
    "        optimizer=optimizer,\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        key=key,\n",
    "        steps_per_save=steps_per_save, \n",
    "        training_state=training_state\n",
    "        )\n",
    "    return final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_sizes(params):\n",
    "    layer_sizes = []\n",
    "    for layer, layer_params in enumerate(params.values()):\n",
    "        if layer == 0:\n",
    "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
    "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
    "        else:\n",
    "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
    "    return layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlp(test_data, params, n_examples=10):\n",
    "    layer_sizes = extract_layer_sizes(params)\n",
    "    model = MLP(layer_sizes=layer_sizes[1:])\n",
    "    apply_fn = model.apply\n",
    "\n",
    "    images = test_data[\"image\"]\n",
    "    labels = test_data[\"label\"]\n",
    "\n",
    "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
    "    example_images = images[:n_examples]\n",
    "    example_labels = labels[:n_examples]\n",
    "    logits = apply_fn({\"params\": params}, example_images)\n",
    "    example_predictions = jnp.argmax(logits, axis=1)\n",
    "\n",
    "    print(\"Mean loss       \", mean_loss)\n",
    "    print(\"True labels:    \", example_labels)\n",
    "    print(\"Predictions:    \", example_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876ad27",
   "metadata": {},
   "source": [
    "1. Learning rate decay\n",
    "2. Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
    "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.2421865463256836\n",
      "step 2: loss=1.4549814462661743\n",
      "step 3: loss=3.0767364501953125\n",
      "step 4: loss=1.6018766164779663\n",
      "step 5: loss=1.922335147857666\n",
      "step 6: loss=1.5518207550048828\n",
      "step 7: loss=2.2047572135925293\n",
      "step 8: loss=2.28387188911438\n",
      "step 9: loss=1.3409204483032227\n",
      "step 10: loss=2.344090700149536\n",
      "step 11: loss=2.6371214389801025\n",
      "step 12: loss=3.4621236324310303\n",
      "step 13: loss=1.713283658027649\n",
      "step 14: loss=1.9781702756881714\n",
      "step 15: loss=3.3416006565093994\n",
      "step 16: loss=2.9397127628326416\n",
      "step 17: loss=1.0316027402877808\n",
      "step 18: loss=2.92775821685791\n",
      "step 19: loss=2.7445106506347656\n",
      "step 20: loss=1.4158217906951904\n",
      "step 21: loss=1.9610295295715332\n",
      "step 22: loss=1.9986664056777954\n",
      "step 23: loss=1.672990083694458\n",
      "step 24: loss=2.457414388656616\n",
      "step 25: loss=1.5557224750518799\n",
      "step 26: loss=2.3051018714904785\n",
      "step 27: loss=2.603790044784546\n",
      "step 28: loss=1.988590955734253\n",
      "step 29: loss=2.3711462020874023\n",
      "step 30: loss=1.6227275133132935\n",
      "step 31: loss=1.862285852432251\n",
      "step 32: loss=2.1689186096191406\n",
      "step 33: loss=1.754330039024353\n",
      "step 34: loss=1.313774824142456\n",
      "step 35: loss=1.7799018621444702\n",
      "step 36: loss=1.6797168254852295\n",
      "step 37: loss=2.9055237770080566\n",
      "step 38: loss=1.5091187953948975\n",
      "step 39: loss=2.635629892349243\n",
      "step 40: loss=1.9036000967025757\n",
      "step 41: loss=0.8337869644165039\n",
      "step 42: loss=2.479337453842163\n",
      "step 43: loss=2.0102226734161377\n",
      "step 44: loss=1.8198356628417969\n",
      "step 45: loss=2.361449956893921\n",
      "step 46: loss=1.0358593463897705\n",
      "step 47: loss=2.0157792568206787\n",
      "step 48: loss=1.810377597808838\n",
      "step 49: loss=3.01554536819458\n",
      "step 50: loss=1.4894685745239258\n",
      "step 51: loss=1.1730490922927856\n",
      "step 52: loss=1.2065200805664062\n",
      "step 53: loss=3.256063938140869\n",
      "step 54: loss=1.3365157842636108\n",
      "step 55: loss=1.2758262157440186\n",
      "step 56: loss=2.6482057571411133\n",
      "step 57: loss=2.540787696838379\n",
      "step 58: loss=1.3128734827041626\n",
      "step 59: loss=2.506495714187622\n",
      "step 60: loss=1.0716583728790283\n",
      "step 61: loss=0.8002644777297974\n",
      "step 62: loss=1.215010404586792\n",
      "step 63: loss=3.9132957458496094\n",
      "step 64: loss=2.513690233230591\n",
      "step 65: loss=2.3943910598754883\n",
      "step 66: loss=0.41791653633117676\n",
      "step 67: loss=0.9733765125274658\n",
      "step 68: loss=0.6354427337646484\n",
      "step 69: loss=3.076798439025879\n",
      "step 70: loss=1.3095070123672485\n",
      "step 71: loss=3.296731948852539\n",
      "step 72: loss=3.5359928607940674\n",
      "step 73: loss=0.5057768821716309\n",
      "step 74: loss=1.0847030878067017\n",
      "step 75: loss=0.8231465816497803\n",
      "step 76: loss=0.12777495384216309\n",
      "step 77: loss=2.528164863586426\n",
      "step 78: loss=2.6920502185821533\n",
      "step 79: loss=1.4572802782058716\n",
      "step 80: loss=0.3264431953430176\n",
      "step 81: loss=0.3224916458129883\n",
      "step 82: loss=0.880436897277832\n",
      "step 83: loss=1.6871346235275269\n",
      "step 84: loss=0.7689778804779053\n",
      "step 85: loss=2.236729621887207\n",
      "step 86: loss=0.39814305305480957\n",
      "step 87: loss=2.505406141281128\n",
      "step 88: loss=1.016530990600586\n",
      "step 89: loss=1.1767767667770386\n",
      "step 90: loss=1.6118593215942383\n",
      "step 91: loss=1.3020222187042236\n",
      "step 92: loss=1.0679250955581665\n",
      "step 93: loss=0.22047114372253418\n",
      "step 94: loss=1.8968682289123535\n",
      "step 95: loss=0.7754878997802734\n",
      "step 96: loss=2.2688071727752686\n",
      "step 97: loss=1.6609830856323242\n",
      "step 98: loss=1.714942216873169\n",
      "step 99: loss=0.8860684633255005\n",
      "step 100: loss=1.043237566947937\n",
      "step 101: loss=1.6429299116134644\n",
      "step 102: loss=1.3760747909545898\n",
      "step 103: loss=2.3424899578094482\n",
      "step 104: loss=0.7394161224365234\n",
      "step 105: loss=0.8303945064544678\n",
      "step 106: loss=1.6889162063598633\n",
      "step 107: loss=1.849713921546936\n",
      "step 108: loss=0.8009697198867798\n",
      "step 109: loss=1.9210385084152222\n",
      "step 110: loss=1.5583198070526123\n",
      "step 111: loss=0.6788965463638306\n",
      "step 112: loss=0.4137570858001709\n",
      "step 113: loss=1.6254088878631592\n",
      "step 114: loss=1.488877534866333\n",
      "step 115: loss=0.2103595733642578\n",
      "step 116: loss=0.9437799453735352\n",
      "step 117: loss=1.3162198066711426\n",
      "step 118: loss=0.7798060178756714\n",
      "step 119: loss=0.9784126281738281\n",
      "step 120: loss=1.2214597463607788\n",
      "step 121: loss=3.1054039001464844\n",
      "step 122: loss=1.6958223581314087\n",
      "step 123: loss=2.7126426696777344\n",
      "step 124: loss=0.5880532264709473\n",
      "step 125: loss=0.2147374153137207\n",
      "step 126: loss=1.3695894479751587\n",
      "step 127: loss=1.7903401851654053\n",
      "step 128: loss=0.8010847568511963\n",
      "step 129: loss=0.47148585319519043\n",
      "step 130: loss=0.42186427116394043\n",
      "step 131: loss=0.919746994972229\n",
      "step 132: loss=2.370388984680176\n",
      "step 133: loss=0.7040364742279053\n",
      "step 134: loss=0.6195166110992432\n",
      "step 135: loss=0.8623490333557129\n",
      "step 136: loss=0.2626211643218994\n",
      "step 137: loss=0.47997236251831055\n",
      "step 138: loss=0.6141399145126343\n",
      "step 139: loss=1.0177688598632812\n",
      "step 140: loss=1.6761692762374878\n",
      "step 141: loss=0.37014293670654297\n",
      "step 142: loss=2.50685977935791\n",
      "step 143: loss=0.473976731300354\n",
      "step 144: loss=1.8222389221191406\n",
      "step 145: loss=0.10611176490783691\n",
      "step 146: loss=1.3444867134094238\n",
      "step 147: loss=1.890256643295288\n",
      "step 148: loss=2.019212007522583\n",
      "step 149: loss=0.708979606628418\n",
      "step 150: loss=0.3448455333709717\n",
      "step 151: loss=1.4451547861099243\n",
      "step 152: loss=0.887959361076355\n",
      "step 153: loss=1.6465024948120117\n",
      "step 154: loss=1.2734510898590088\n",
      "step 155: loss=0.8892346620559692\n",
      "step 156: loss=0.8326750993728638\n",
      "step 157: loss=1.0322668552398682\n",
      "step 158: loss=1.7533605098724365\n",
      "step 159: loss=0.9787423610687256\n",
      "step 160: loss=1.5738203525543213\n",
      "step 161: loss=0.48970115184783936\n",
      "step 162: loss=0.15803837776184082\n",
      "step 163: loss=0.7275893688201904\n",
      "step 164: loss=0.5339555740356445\n",
      "step 165: loss=1.5419628620147705\n",
      "step 166: loss=0.4611380100250244\n",
      "step 167: loss=2.06067156791687\n",
      "step 168: loss=2.2312281131744385\n",
      "step 169: loss=0.656808614730835\n",
      "step 170: loss=0.4211156368255615\n",
      "step 171: loss=0.1600961685180664\n",
      "step 172: loss=0.6779335737228394\n",
      "step 173: loss=0.1400458812713623\n",
      "step 174: loss=0.7043989896774292\n",
      "step 175: loss=2.0440971851348877\n",
      "step 176: loss=0.5685373544692993\n",
      "step 177: loss=0.7488729953765869\n",
      "step 178: loss=0.2943863868713379\n",
      "step 179: loss=0.2867746353149414\n",
      "step 180: loss=2.0111541748046875\n",
      "step 181: loss=0.5551998615264893\n",
      "step 182: loss=0.42223358154296875\n",
      "step 183: loss=0.4737052917480469\n",
      "step 184: loss=2.3058722019195557\n",
      "step 185: loss=0.40650367736816406\n",
      "step 186: loss=0.4796102046966553\n",
      "step 187: loss=0.6871426105499268\n",
      "step 188: loss=2.680981159210205\n",
      "step 189: loss=0.7373931407928467\n",
      "step 190: loss=0.6501474380493164\n",
      "step 191: loss=0.44893431663513184\n",
      "step 192: loss=0.12216806411743164\n",
      "step 193: loss=0.12809205055236816\n",
      "step 194: loss=0.3562130928039551\n",
      "step 195: loss=0.060632944107055664\n",
      "step 196: loss=1.4033054113388062\n",
      "step 197: loss=1.2201976776123047\n",
      "step 198: loss=0.2856619358062744\n",
      "step 199: loss=2.5699079036712646\n",
      "step 200: loss=1.3816474676132202\n",
      "step 201: loss=0.4719282388687134\n",
      "step 202: loss=2.0478811264038086\n",
      "step 203: loss=0.4015340805053711\n",
      "step 204: loss=0.14312219619750977\n",
      "step 205: loss=0.49718499183654785\n",
      "step 206: loss=0.7516837120056152\n",
      "step 207: loss=0.17197680473327637\n",
      "step 208: loss=0.5553793907165527\n",
      "step 209: loss=0.3742920160293579\n",
      "step 210: loss=0.7562179565429688\n",
      "step 211: loss=0.15678930282592773\n",
      "step 212: loss=1.6474084854125977\n",
      "step 213: loss=1.2462327480316162\n",
      "step 214: loss=1.599760890007019\n",
      "step 215: loss=0.15593624114990234\n",
      "step 216: loss=0.29805707931518555\n",
      "step 217: loss=0.29372286796569824\n",
      "step 218: loss=0.15705633163452148\n",
      "step 219: loss=0.22482776641845703\n",
      "step 220: loss=0.9453972578048706\n",
      "step 221: loss=1.1685106754302979\n",
      "step 222: loss=2.3041555881500244\n",
      "step 223: loss=0.9989070892333984\n",
      "step 224: loss=0.7327048778533936\n",
      "step 225: loss=0.37357258796691895\n",
      "step 226: loss=0.6927950382232666\n",
      "step 227: loss=0.4200795888900757\n",
      "step 228: loss=0.5350275039672852\n",
      "step 229: loss=0.604094386100769\n",
      "step 230: loss=0.5697106122970581\n",
      "step 231: loss=0.8951528668403625\n",
      "step 232: loss=0.281496524810791\n",
      "step 233: loss=0.0047473907470703125\n",
      "step 234: loss=2.366014003753662\n",
      "step 235: loss=0.27408552169799805\n",
      "step 236: loss=1.1047792434692383\n",
      "step 237: loss=0.040659427642822266\n",
      "step 238: loss=0.22864484786987305\n",
      "step 239: loss=0.3796806335449219\n",
      "step 240: loss=0.34667110443115234\n",
      "step 241: loss=0.4054981470108032\n",
      "step 242: loss=0.1246330738067627\n",
      "step 243: loss=0.5560510158538818\n",
      "step 244: loss=0.44894957542419434\n",
      "step 245: loss=0.4151649475097656\n",
      "step 246: loss=0.9324600696563721\n",
      "step 247: loss=0.44788551330566406\n",
      "step 248: loss=3.057542085647583\n",
      "step 249: loss=0.05451393127441406\n",
      "step 250: loss=0.06193351745605469\n",
      "step 251: loss=2.078672170639038\n",
      "step 252: loss=0.5296133756637573\n",
      "step 253: loss=0.9901057481765747\n",
      "step 254: loss=0.09525799751281738\n",
      "step 255: loss=0.07671642303466797\n",
      "step 256: loss=0.489521861076355\n",
      "step 257: loss=2.7890677452087402\n",
      "step 258: loss=0.3984382152557373\n",
      "step 259: loss=0.8330893516540527\n",
      "step 260: loss=0.47151756286621094\n",
      "step 261: loss=0.983100175857544\n",
      "step 262: loss=2.7335660457611084\n",
      "step 263: loss=0.32002758979797363\n",
      "step 264: loss=0.1883378028869629\n",
      "step 265: loss=0.4457817077636719\n",
      "step 266: loss=0.6293996572494507\n",
      "step 267: loss=0.46929991245269775\n",
      "step 268: loss=0.7847534418106079\n",
      "step 269: loss=0.032675743103027344\n",
      "step 270: loss=0.6592978239059448\n",
      "step 271: loss=0.03919816017150879\n",
      "step 272: loss=0.7018625736236572\n",
      "step 273: loss=0.5264933109283447\n",
      "step 274: loss=0.12643933296203613\n",
      "step 275: loss=1.092840552330017\n",
      "step 276: loss=0.03591275215148926\n",
      "step 277: loss=0.29474616050720215\n",
      "step 278: loss=0.43141674995422363\n",
      "step 279: loss=0.4353444576263428\n",
      "step 280: loss=0.35237371921539307\n",
      "step 281: loss=0.08173036575317383\n",
      "step 282: loss=0.05996131896972656\n",
      "step 283: loss=1.4846298694610596\n",
      "step 284: loss=1.8887841701507568\n",
      "step 285: loss=0.4659993648529053\n",
      "step 286: loss=0.2334280014038086\n",
      "step 287: loss=0.15755367279052734\n",
      "step 288: loss=0.07255125045776367\n",
      "step 289: loss=2.1834232807159424\n",
      "step 290: loss=0.18325090408325195\n",
      "step 291: loss=0.37158799171447754\n",
      "step 292: loss=1.1927613019943237\n",
      "step 293: loss=0.044721126556396484\n",
      "step 294: loss=0.009671211242675781\n",
      "step 295: loss=0.8239340782165527\n",
      "step 296: loss=0.2812347412109375\n",
      "step 297: loss=0.29264354705810547\n",
      "step 298: loss=0.07653141021728516\n",
      "step 299: loss=0.012074947357177734\n",
      "step 300: loss=0.05053567886352539\n",
      "step 301: loss=0.006805896759033203\n",
      "step 302: loss=2.7376489639282227\n",
      "step 303: loss=0.9433908462524414\n",
      "step 304: loss=0.3315999507904053\n",
      "step 305: loss=0.4553706645965576\n",
      "step 306: loss=0.3246622085571289\n",
      "step 307: loss=0.9534530639648438\n",
      "step 308: loss=0.044396400451660156\n",
      "step 309: loss=0.4811127185821533\n",
      "step 310: loss=0.48668503761291504\n",
      "step 311: loss=2.132352828979492\n",
      "step 312: loss=0.17094135284423828\n",
      "step 313: loss=0.12753868103027344\n",
      "step 314: loss=0.551832914352417\n",
      "step 315: loss=0.07557868957519531\n",
      "step 316: loss=0.30028343200683594\n",
      "step 317: loss=0.33416497707366943\n",
      "step 318: loss=0.3379436731338501\n",
      "step 319: loss=0.21451210975646973\n",
      "step 320: loss=0.3522576093673706\n",
      "step 321: loss=0.8380552530288696\n",
      "step 322: loss=0.17711257934570312\n",
      "step 323: loss=0.09087562561035156\n",
      "step 324: loss=0.07382750511169434\n",
      "step 325: loss=0.23125982284545898\n",
      "step 326: loss=1.4439153671264648\n",
      "step 327: loss=0.47332286834716797\n",
      "step 328: loss=2.746384859085083\n",
      "step 329: loss=0.005428791046142578\n",
      "step 330: loss=0.5077652931213379\n",
      "step 331: loss=0.06275653839111328\n",
      "step 332: loss=0.3127434253692627\n",
      "step 333: loss=0.04568958282470703\n",
      "step 334: loss=0.0166778564453125\n",
      "step 335: loss=0.9150269031524658\n",
      "step 336: loss=0.34367990493774414\n",
      "step 337: loss=1.1579874753952026\n",
      "step 338: loss=0.33712172508239746\n",
      "step 339: loss=0.47450244426727295\n",
      "step 340: loss=0.0789024829864502\n",
      "step 341: loss=1.6881626844406128\n",
      "step 342: loss=0.3500051498413086\n",
      "step 343: loss=0.08574962615966797\n",
      "step 344: loss=1.064251184463501\n",
      "step 345: loss=0.7578305006027222\n",
      "step 346: loss=0.07385015487670898\n",
      "step 347: loss=0.2304244041442871\n",
      "step 348: loss=0.23037385940551758\n",
      "step 349: loss=0.35393309593200684\n",
      "step 350: loss=0.018471717834472656\n",
      "step 351: loss=0.36508679389953613\n",
      "step 352: loss=1.012511134147644\n",
      "step 353: loss=2.5447545051574707\n",
      "step 354: loss=0.7907414436340332\n",
      "step 355: loss=0.7476091384887695\n",
      "step 356: loss=0.012488365173339844\n",
      "step 357: loss=0.3724796772003174\n",
      "step 358: loss=1.1695151329040527\n",
      "step 359: loss=0.17859244346618652\n",
      "step 360: loss=0.08150219917297363\n",
      "step 361: loss=0.5774656534194946\n",
      "step 362: loss=0.12587261199951172\n",
      "step 363: loss=3.199237823486328\n",
      "step 364: loss=0.12036800384521484\n",
      "step 365: loss=1.0232316255569458\n",
      "step 366: loss=0.5209536552429199\n",
      "step 367: loss=0.47525346279144287\n",
      "step 368: loss=0.01161336898803711\n",
      "step 369: loss=0.19076108932495117\n",
      "step 370: loss=2.11970591545105\n",
      "step 371: loss=0.198927640914917\n",
      "step 372: loss=0.015585899353027344\n",
      "step 373: loss=0.014299869537353516\n",
      "step 374: loss=0.08033037185668945\n",
      "step 375: loss=1.9676306247711182\n",
      "step 376: loss=1.01432204246521\n",
      "step 377: loss=0.5524725914001465\n",
      "step 378: loss=0.629766583442688\n",
      "step 379: loss=3.3354454040527344\n",
      "step 380: loss=0.0397944450378418\n",
      "step 381: loss=0.008146286010742188\n",
      "step 382: loss=0.39034605026245117\n",
      "step 383: loss=0.3461265563964844\n",
      "step 384: loss=0.0007467269897460938\n",
      "step 385: loss=0.06762409210205078\n",
      "step 386: loss=0.09310150146484375\n",
      "step 387: loss=0.11734533309936523\n",
      "step 388: loss=0.42160606384277344\n",
      "step 389: loss=0.04375648498535156\n",
      "step 390: loss=0.6377222537994385\n",
      "step 391: loss=0.13796329498291016\n",
      "step 392: loss=0.9367666244506836\n",
      "step 393: loss=1.7518149614334106\n",
      "step 394: loss=3.9221692085266113\n",
      "step 395: loss=0.05047798156738281\n",
      "step 396: loss=0.25234508514404297\n",
      "step 397: loss=0.1284027099609375\n",
      "step 398: loss=0.9674408435821533\n",
      "step 399: loss=0.41379332542419434\n",
      "step 400: loss=0.029466629028320312\n",
      "step 401: loss=0.5387718677520752\n",
      "step 402: loss=0.44220614433288574\n",
      "step 403: loss=0.004082679748535156\n",
      "step 404: loss=0.01567983627319336\n",
      "step 405: loss=0.015554428100585938\n",
      "step 406: loss=0.6025831699371338\n",
      "step 407: loss=0.08917903900146484\n",
      "step 408: loss=0.018439769744873047\n",
      "step 409: loss=3.631787061691284\n",
      "step 410: loss=0.054201602935791016\n",
      "step 411: loss=0.5691431760787964\n",
      "step 412: loss=0.01025390625\n",
      "step 413: loss=0.16602587699890137\n",
      "step 414: loss=0.2280120849609375\n",
      "step 415: loss=0.033725738525390625\n",
      "step 416: loss=2.8930912017822266\n",
      "step 417: loss=0.4680633544921875\n",
      "step 418: loss=0.11901974678039551\n",
      "step 419: loss=1.3566269874572754\n",
      "step 420: loss=0.05841684341430664\n",
      "step 421: loss=0.28403472900390625\n",
      "step 422: loss=0.8525378704071045\n",
      "step 423: loss=0.2811431884765625\n",
      "step 424: loss=0.020607948303222656\n",
      "step 425: loss=0.8618518114089966\n",
      "step 426: loss=1.1827762126922607\n",
      "step 427: loss=0.8799014091491699\n",
      "step 428: loss=0.19707965850830078\n",
      "step 429: loss=0.2692444324493408\n",
      "step 430: loss=0.0019240379333496094\n",
      "step 431: loss=0.5260114669799805\n",
      "step 432: loss=0.2273731231689453\n",
      "step 433: loss=0.3134944438934326\n",
      "step 434: loss=1.1230807304382324\n",
      "step 435: loss=0.05976676940917969\n",
      "step 436: loss=0.18072032928466797\n",
      "step 437: loss=0.002167224884033203\n",
      "step 438: loss=0.01470041275024414\n",
      "step 439: loss=0.9843233823776245\n",
      "step 440: loss=0.011989116668701172\n",
      "step 441: loss=0.14016485214233398\n",
      "step 442: loss=0.7073942422866821\n",
      "step 443: loss=1.4848883152008057\n",
      "step 444: loss=1.0383212566375732\n",
      "step 445: loss=0.41783320903778076\n",
      "step 446: loss=0.7536642551422119\n",
      "step 447: loss=0.07221364974975586\n",
      "step 448: loss=0.05558300018310547\n",
      "step 449: loss=0.8591423034667969\n",
      "step 450: loss=3.564875364303589\n",
      "step 451: loss=0.006774425506591797\n",
      "step 452: loss=1.669120192527771\n",
      "step 453: loss=0.12167143821716309\n",
      "step 454: loss=3.2515270709991455\n",
      "step 455: loss=1.7813029289245605\n",
      "step 456: loss=1.3433032035827637\n",
      "step 457: loss=0.18211889266967773\n",
      "step 458: loss=0.7063822746276855\n",
      "step 459: loss=0.08410120010375977\n",
      "step 460: loss=0.03919649124145508\n",
      "step 461: loss=2.979452610015869\n",
      "step 462: loss=0.0026497840881347656\n",
      "step 463: loss=0.43689000606536865\n",
      "step 464: loss=0.07270312309265137\n",
      "step 465: loss=1.1039822101593018\n",
      "step 466: loss=0.03798484802246094\n",
      "step 467: loss=1.9705100059509277\n",
      "step 468: loss=0.11281657218933105\n",
      "step 469: loss=0.687421441078186\n",
      "step 470: loss=0.13335704803466797\n",
      "step 471: loss=2.6948351860046387\n",
      "step 472: loss=1.5483877658843994\n",
      "step 473: loss=0.11220932006835938\n",
      "step 474: loss=0.44478631019592285\n",
      "step 475: loss=0.013322830200195312\n",
      "step 476: loss=0.38378405570983887\n",
      "step 477: loss=0.04301595687866211\n",
      "step 478: loss=0.026033878326416016\n",
      "step 479: loss=0.1391465663909912\n",
      "step 480: loss=0.3230702877044678\n",
      "step 481: loss=0.24133872985839844\n",
      "step 482: loss=0.29029393196105957\n",
      "step 483: loss=0.4514617919921875\n",
      "step 484: loss=0.44598913192749023\n",
      "step 485: loss=1.3443806171417236\n",
      "step 486: loss=0.010129451751708984\n",
      "step 487: loss=0.008464813232421875\n",
      "step 488: loss=0.10316658020019531\n",
      "step 489: loss=0.14998960494995117\n",
      "step 490: loss=0.24096250534057617\n",
      "step 491: loss=0.0014219284057617188\n",
      "step 492: loss=0.05595660209655762\n",
      "step 493: loss=0.04845285415649414\n",
      "step 494: loss=0.3449218273162842\n",
      "step 495: loss=0.24515247344970703\n",
      "step 496: loss=1.8468502759933472\n",
      "step 497: loss=0.025726318359375\n",
      "step 498: loss=0.10625171661376953\n",
      "step 499: loss=3.827439308166504\n",
      "step 500: loss=0.8061550855636597\n",
      "step 501: loss=0.02237081527709961\n",
      "step 502: loss=0.3995704650878906\n",
      "step 503: loss=0.20708990097045898\n",
      "step 504: loss=0.045786142349243164\n",
      "step 505: loss=0.032854557037353516\n",
      "step 506: loss=0.20964956283569336\n",
      "step 507: loss=0.9962095022201538\n",
      "step 508: loss=0.16164588928222656\n",
      "step 509: loss=0.03909635543823242\n",
      "step 510: loss=0.10247039794921875\n",
      "step 511: loss=0.33330416679382324\n",
      "step 512: loss=0.12527990341186523\n",
      "step 513: loss=0.21143198013305664\n",
      "step 514: loss=0.3745746612548828\n",
      "step 515: loss=0.027648448944091797\n",
      "step 516: loss=0.08393144607543945\n",
      "step 517: loss=0.5669987201690674\n",
      "step 518: loss=0.957893967628479\n",
      "step 519: loss=0.02363729476928711\n",
      "step 520: loss=1.48310387134552\n",
      "step 521: loss=0.13979244232177734\n",
      "step 522: loss=0.042284488677978516\n",
      "step 523: loss=0.062416791915893555\n",
      "step 524: loss=0.03997659683227539\n",
      "step 525: loss=0.14003419876098633\n",
      "step 526: loss=3.1203131675720215\n",
      "step 527: loss=0.0059146881103515625\n",
      "step 528: loss=0.00048542022705078125\n",
      "step 529: loss=0.29979002475738525\n",
      "step 530: loss=0.05205678939819336\n",
      "step 531: loss=0.16924405097961426\n",
      "step 532: loss=0.0011043548583984375\n",
      "step 533: loss=0.22463774681091309\n",
      "step 534: loss=0.05318403244018555\n",
      "step 535: loss=0.08916521072387695\n",
      "step 536: loss=0.07006692886352539\n",
      "step 537: loss=0.07175874710083008\n",
      "step 538: loss=1.605657696723938\n",
      "step 539: loss=0.8576207160949707\n",
      "step 540: loss=3.2982354164123535\n",
      "step 541: loss=0.0028433799743652344\n",
      "step 542: loss=0.19463753700256348\n",
      "step 543: loss=0.8926622271537781\n",
      "step 544: loss=0.12177920341491699\n",
      "step 545: loss=0.7661073207855225\n",
      "step 546: loss=0.662636399269104\n",
      "step 547: loss=0.07155418395996094\n",
      "step 548: loss=0.01363372802734375\n",
      "step 549: loss=0.2269115447998047\n",
      "step 550: loss=0.19848036766052246\n",
      "step 551: loss=0.945902943611145\n",
      "step 552: loss=0.6102805137634277\n",
      "step 553: loss=0.054195404052734375\n",
      "step 554: loss=2.051149606704712\n",
      "step 555: loss=0.6343873739242554\n",
      "step 556: loss=1.581680178642273\n",
      "step 557: loss=0.35944652557373047\n",
      "step 558: loss=0.005198001861572266\n",
      "step 559: loss=0.0024509429931640625\n",
      "step 560: loss=0.09252715110778809\n",
      "step 561: loss=0.15797710418701172\n",
      "step 562: loss=0.5221741199493408\n",
      "step 563: loss=0.1967637538909912\n",
      "step 564: loss=0.16375017166137695\n",
      "step 565: loss=0.019205570220947266\n",
      "step 566: loss=2.8943355083465576\n",
      "step 567: loss=0.06458353996276855\n",
      "step 568: loss=0.2374105453491211\n",
      "step 569: loss=0.3637881278991699\n",
      "step 570: loss=0.05800652503967285\n",
      "step 571: loss=0.04111528396606445\n",
      "step 572: loss=0.05983877182006836\n",
      "step 573: loss=0.3058300018310547\n",
      "step 574: loss=1.237825632095337\n",
      "step 575: loss=0.28524351119995117\n",
      "step 576: loss=0.21417760848999023\n",
      "step 577: loss=0.4051241874694824\n",
      "step 578: loss=0.02850055694580078\n",
      "step 579: loss=0.01888132095336914\n",
      "step 580: loss=0.13847827911376953\n",
      "step 581: loss=0.016454219818115234\n",
      "step 582: loss=1.398078441619873\n",
      "step 583: loss=0.5642738342285156\n",
      "step 584: loss=0.09487390518188477\n",
      "step 585: loss=0.09597134590148926\n",
      "step 586: loss=1.049293041229248\n",
      "step 587: loss=0.47356557846069336\n",
      "step 588: loss=0.8383779525756836\n",
      "step 589: loss=3.177544116973877\n",
      "step 590: loss=0.4497392177581787\n",
      "step 591: loss=0.0792703628540039\n",
      "step 592: loss=1.357460856437683\n",
      "step 593: loss=0.2543766498565674\n",
      "step 594: loss=0.07349824905395508\n",
      "step 595: loss=2.8370463848114014\n",
      "step 596: loss=0.7364479303359985\n",
      "step 597: loss=0.7231221199035645\n",
      "step 598: loss=1.374111533164978\n",
      "step 599: loss=0.4321753978729248\n",
      "step 600: loss=0.17777228355407715\n",
      "step 601: loss=0.07130742073059082\n",
      "step 602: loss=0.2371366024017334\n",
      "step 603: loss=0.30576348304748535\n",
      "step 604: loss=2.668428421020508\n",
      "step 605: loss=1.1140674352645874\n",
      "step 606: loss=0.5520972013473511\n",
      "step 607: loss=0.8259549140930176\n",
      "step 608: loss=0.44512462615966797\n",
      "step 609: loss=0.7199970483779907\n",
      "step 610: loss=0.377687931060791\n",
      "step 611: loss=2.0839905738830566\n",
      "step 612: loss=1.1282179355621338\n",
      "step 613: loss=0.2851424217224121\n",
      "step 614: loss=0.04206252098083496\n",
      "step 615: loss=1.4286022186279297\n",
      "step 616: loss=0.5027270317077637\n",
      "step 617: loss=0.012112617492675781\n",
      "step 618: loss=0.7415882349014282\n",
      "step 619: loss=0.06085538864135742\n",
      "step 620: loss=0.5705982446670532\n",
      "step 621: loss=1.8605399131774902\n",
      "step 622: loss=0.23227739334106445\n",
      "step 623: loss=0.04270195960998535\n",
      "step 624: loss=2.679222822189331\n",
      "step 625: loss=0.2715725898742676\n",
      "step 626: loss=0.9293808937072754\n",
      "step 627: loss=0.16028642654418945\n",
      "step 628: loss=0.010308265686035156\n",
      "step 629: loss=1.290609359741211\n",
      "step 630: loss=0.08072900772094727\n",
      "step 631: loss=0.27584099769592285\n",
      "step 632: loss=0.2107226848602295\n",
      "step 633: loss=0.5600347518920898\n",
      "step 634: loss=0.011282920837402344\n",
      "step 635: loss=3.11643385887146\n",
      "step 636: loss=0.023334503173828125\n",
      "step 637: loss=0.49912166595458984\n",
      "step 638: loss=0.21662330627441406\n",
      "step 639: loss=0.0618898868560791\n",
      "step 640: loss=1.093286395072937\n",
      "step 641: loss=0.2558565139770508\n",
      "step 642: loss=0.010252952575683594\n",
      "step 643: loss=1.6186238527297974\n",
      "step 644: loss=0.011042594909667969\n",
      "step 645: loss=0.30058932304382324\n",
      "step 646: loss=0.09044694900512695\n",
      "step 647: loss=1.9067965745925903\n",
      "step 648: loss=0.12402224540710449\n",
      "step 649: loss=0.05713081359863281\n",
      "step 650: loss=0.10149407386779785\n",
      "step 651: loss=0.2526705265045166\n",
      "step 652: loss=0.6070406436920166\n",
      "step 653: loss=0.004278659820556641\n",
      "step 654: loss=0.7120640277862549\n",
      "step 655: loss=0.667755126953125\n",
      "step 656: loss=0.45967531204223633\n",
      "step 657: loss=0.05305337905883789\n",
      "step 658: loss=0.6365063190460205\n",
      "step 659: loss=0.12805700302124023\n",
      "step 660: loss=0.6878716945648193\n",
      "step 661: loss=0.004768848419189453\n",
      "step 662: loss=0.17098355293273926\n",
      "step 663: loss=1.0485873222351074\n",
      "step 664: loss=0.2143080234527588\n",
      "step 665: loss=0.040230751037597656\n",
      "step 666: loss=0.016267776489257812\n",
      "step 667: loss=0.011294364929199219\n",
      "step 668: loss=1.3694889545440674\n",
      "step 669: loss=2.1131973266601562\n",
      "step 670: loss=0.31995701789855957\n",
      "step 671: loss=1.4489619731903076\n",
      "step 672: loss=0.7029204368591309\n",
      "step 673: loss=1.19868803024292\n",
      "step 674: loss=0.05759167671203613\n",
      "step 675: loss=0.8192206621170044\n",
      "step 676: loss=0.05337262153625488\n",
      "step 677: loss=0.10202956199645996\n",
      "step 678: loss=0.4015166759490967\n",
      "step 679: loss=0.11560511589050293\n",
      "step 680: loss=0.0026750564575195312\n",
      "step 681: loss=1.3169834613800049\n",
      "step 682: loss=0.007153034210205078\n",
      "step 683: loss=0.580612063407898\n",
      "step 684: loss=1.1635642051696777\n",
      "step 685: loss=0.16467809677124023\n",
      "step 686: loss=0.029392719268798828\n",
      "step 687: loss=0.089141845703125\n",
      "step 688: loss=0.034459590911865234\n",
      "step 689: loss=1.5467545986175537\n",
      "step 690: loss=0.7600469589233398\n",
      "step 691: loss=0.3672494888305664\n",
      "step 692: loss=1.7930370569229126\n",
      "step 693: loss=0.04040336608886719\n",
      "step 694: loss=0.09868192672729492\n",
      "step 695: loss=0.09852075576782227\n",
      "step 696: loss=0.05927014350891113\n",
      "step 697: loss=1.2638287544250488\n",
      "step 698: loss=0.546238899230957\n",
      "step 699: loss=0.7163470983505249\n",
      "step 700: loss=0.0858759880065918\n",
      "step 701: loss=0.03256988525390625\n",
      "step 702: loss=0.02334737777709961\n",
      "step 703: loss=0.1107792854309082\n",
      "step 704: loss=0.030237197875976562\n",
      "step 705: loss=0.1586596965789795\n",
      "step 706: loss=0.14440345764160156\n",
      "step 707: loss=0.7704955339431763\n",
      "step 708: loss=0.273378849029541\n",
      "step 709: loss=0.12483501434326172\n",
      "step 710: loss=0.6951863765716553\n",
      "step 711: loss=1.7305415868759155\n",
      "step 712: loss=0.6946097612380981\n",
      "step 713: loss=0.1503126621246338\n",
      "step 714: loss=0.09028935432434082\n",
      "step 715: loss=0.06392288208007812\n",
      "step 716: loss=0.03419208526611328\n",
      "step 717: loss=0.28113722801208496\n",
      "step 718: loss=0.3789252042770386\n",
      "step 719: loss=0.010700225830078125\n",
      "step 720: loss=0.020720958709716797\n",
      "step 721: loss=0.061791181564331055\n",
      "step 722: loss=0.13160324096679688\n",
      "step 723: loss=0.022855758666992188\n",
      "step 724: loss=0.16715764999389648\n",
      "step 725: loss=4.071563243865967\n",
      "step 726: loss=0.454559326171875\n",
      "step 727: loss=0.00247955322265625\n",
      "step 728: loss=0.4241828918457031\n",
      "step 729: loss=1.0255894660949707\n",
      "step 730: loss=0.11156606674194336\n",
      "step 731: loss=0.061654090881347656\n",
      "step 732: loss=1.2832696437835693\n",
      "step 733: loss=0.6139696836471558\n",
      "step 734: loss=0.032423973083496094\n",
      "step 735: loss=0.8118246793746948\n",
      "step 736: loss=0.009581565856933594\n",
      "step 737: loss=0.28511953353881836\n",
      "step 738: loss=0.1188817024230957\n",
      "step 739: loss=0.0021009445190429688\n",
      "step 740: loss=0.11481595039367676\n",
      "step 741: loss=0.9182499647140503\n",
      "step 742: loss=5.225607872009277\n",
      "step 743: loss=1.704210877418518\n",
      "step 744: loss=0.04639291763305664\n",
      "step 745: loss=0.5776848793029785\n",
      "step 746: loss=0.01789712905883789\n",
      "step 747: loss=1.502614974975586\n",
      "step 748: loss=0.03278923034667969\n",
      "step 749: loss=0.01644754409790039\n",
      "step 750: loss=0.8263120651245117\n",
      "step 751: loss=0.04602527618408203\n",
      "step 752: loss=0.21524906158447266\n",
      "step 753: loss=0.061154842376708984\n",
      "step 754: loss=0.3718116283416748\n",
      "step 755: loss=0.28887057304382324\n",
      "step 756: loss=1.5567007064819336\n",
      "step 757: loss=0.028926372528076172\n",
      "step 758: loss=0.18595385551452637\n",
      "step 759: loss=0.07905268669128418\n",
      "step 760: loss=0.4115278720855713\n",
      "step 761: loss=0.008421897888183594\n",
      "step 762: loss=0.4036257266998291\n",
      "step 763: loss=0.9590024948120117\n",
      "step 764: loss=0.7133641242980957\n",
      "step 765: loss=0.48048698902130127\n",
      "step 766: loss=0.3986119031906128\n",
      "step 767: loss=0.13569283485412598\n",
      "step 768: loss=0.4674079418182373\n",
      "step 769: loss=0.00102996826171875\n",
      "step 770: loss=0.16486167907714844\n",
      "step 771: loss=0.43318843841552734\n",
      "step 772: loss=1.4921143054962158\n",
      "step 773: loss=0.08220386505126953\n",
      "step 774: loss=0.038150787353515625\n",
      "step 775: loss=0.013050556182861328\n",
      "step 776: loss=0.2003769874572754\n",
      "step 777: loss=0.05414152145385742\n",
      "step 778: loss=0.011188030242919922\n",
      "step 779: loss=0.014467239379882812\n",
      "step 780: loss=0.02270984649658203\n",
      "step 781: loss=0.0022478103637695312\n",
      "step 782: loss=0.07770586013793945\n",
      "step 783: loss=0.027918338775634766\n",
      "step 784: loss=0.04948854446411133\n",
      "step 785: loss=0.35353732109069824\n",
      "step 786: loss=0.013762474060058594\n",
      "step 787: loss=0.11862325668334961\n",
      "step 788: loss=1.4354572296142578\n",
      "step 789: loss=0.17925167083740234\n",
      "step 790: loss=0.4444906711578369\n",
      "step 791: loss=0.15286684036254883\n",
      "step 792: loss=0.09328126907348633\n",
      "step 793: loss=0.32610654830932617\n",
      "step 794: loss=0.9303807020187378\n",
      "step 795: loss=0.011697769165039062\n",
      "step 796: loss=0.029285907745361328\n",
      "step 797: loss=0.231231689453125\n",
      "step 798: loss=2.6338624954223633\n",
      "step 799: loss=0.5665862560272217\n",
      "step 800: loss=1.2449233531951904\n",
      "step 801: loss=0.013893604278564453\n",
      "step 802: loss=0.01511383056640625\n",
      "step 803: loss=0.12432408332824707\n",
      "step 804: loss=0.4973611831665039\n",
      "step 805: loss=0.0024280548095703125\n",
      "step 806: loss=0.002131938934326172\n",
      "step 807: loss=0.018868446350097656\n",
      "step 808: loss=0.002613067626953125\n",
      "step 809: loss=0.7255860567092896\n",
      "step 810: loss=0.0009622573852539062\n",
      "step 811: loss=0.9114747047424316\n",
      "step 812: loss=0.3157074451446533\n",
      "step 813: loss=0.07942962646484375\n",
      "step 814: loss=0.461492657661438\n",
      "step 815: loss=0.031975746154785156\n",
      "step 816: loss=0.2038557529449463\n",
      "step 817: loss=0.5515196323394775\n",
      "step 818: loss=0.2785799503326416\n",
      "step 819: loss=0.005415439605712891\n",
      "step 820: loss=0.17008376121520996\n",
      "step 821: loss=0.03214550018310547\n",
      "step 822: loss=0.037631988525390625\n",
      "step 823: loss=0.08344030380249023\n",
      "step 824: loss=0.22780489921569824\n",
      "step 825: loss=2.4254860877990723\n",
      "step 826: loss=0.014660835266113281\n",
      "step 827: loss=2.5519309043884277\n",
      "step 828: loss=0.01477813720703125\n",
      "step 829: loss=0.25369691848754883\n",
      "step 830: loss=0.11380338668823242\n",
      "step 831: loss=0.19821643829345703\n",
      "step 832: loss=0.005218505859375\n",
      "step 833: loss=0.15487146377563477\n",
      "step 834: loss=1.6532922983169556\n",
      "step 835: loss=0.43225550651550293\n",
      "step 836: loss=0.01710987091064453\n",
      "step 837: loss=0.015863895416259766\n",
      "step 838: loss=0.010437965393066406\n",
      "step 839: loss=0.03831672668457031\n",
      "step 840: loss=0.12788152694702148\n",
      "step 841: loss=0.005878925323486328\n",
      "step 842: loss=0.046411991119384766\n",
      "step 843: loss=0.01076507568359375\n",
      "step 844: loss=0.25150251388549805\n",
      "step 845: loss=0.0012178421020507812\n",
      "step 846: loss=0.2321016788482666\n",
      "step 847: loss=0.31379055976867676\n",
      "step 848: loss=0.06747221946716309\n",
      "step 849: loss=0.0008087158203125\n",
      "step 850: loss=0.07829785346984863\n",
      "step 851: loss=0.28147459030151367\n",
      "step 852: loss=0.3888612985610962\n",
      "step 853: loss=0.449662446975708\n",
      "step 854: loss=0.09763097763061523\n",
      "step 855: loss=1.7243692874908447\n",
      "step 856: loss=0.1643972396850586\n",
      "step 857: loss=0.11453390121459961\n",
      "step 858: loss=0.39929258823394775\n",
      "step 859: loss=0.16950511932373047\n",
      "step 860: loss=0.6861879825592041\n",
      "step 861: loss=0.02004098892211914\n",
      "step 862: loss=0.003284931182861328\n",
      "step 863: loss=0.10630512237548828\n",
      "step 864: loss=0.12004256248474121\n",
      "step 865: loss=0.8591494560241699\n",
      "step 866: loss=0.05156230926513672\n",
      "step 867: loss=0.7235795259475708\n",
      "step 868: loss=0.044901371002197266\n",
      "step 869: loss=2.3743138313293457\n",
      "step 870: loss=0.0057239532470703125\n",
      "step 871: loss=0.21019387245178223\n",
      "step 872: loss=0.1750349998474121\n",
      "step 873: loss=0.03566455841064453\n",
      "step 874: loss=1.4096548557281494\n",
      "step 875: loss=0.3176443576812744\n",
      "step 876: loss=1.5774856805801392\n",
      "step 877: loss=0.1490786075592041\n",
      "step 878: loss=0.14512395858764648\n",
      "step 879: loss=0.014568805694580078\n",
      "step 880: loss=0.1957552433013916\n",
      "step 881: loss=0.02507495880126953\n",
      "step 882: loss=0.11042499542236328\n",
      "step 883: loss=0.10820245742797852\n",
      "step 884: loss=0.013692378997802734\n",
      "step 885: loss=0.025472640991210938\n",
      "step 886: loss=1.4292902946472168\n",
      "step 887: loss=0.3023536205291748\n",
      "step 888: loss=0.16981220245361328\n",
      "step 889: loss=0.5759778022766113\n",
      "step 890: loss=0.39397764205932617\n",
      "step 891: loss=0.01878833770751953\n",
      "step 892: loss=0.0006103515625\n",
      "step 893: loss=0.14224529266357422\n",
      "step 894: loss=0.16683459281921387\n",
      "step 895: loss=0.02306652069091797\n",
      "step 896: loss=0.005665779113769531\n",
      "step 897: loss=0.26515984535217285\n",
      "step 898: loss=0.5112065076828003\n",
      "step 899: loss=0.01642131805419922\n",
      "step 900: loss=0.04950976371765137\n",
      "step 901: loss=0.10270214080810547\n",
      "step 902: loss=0.008306503295898438\n",
      "step 903: loss=1.2045660018920898\n",
      "step 904: loss=0.02616596221923828\n",
      "step 905: loss=0.5364210605621338\n",
      "step 906: loss=0.41507232189178467\n",
      "step 907: loss=0.6363639831542969\n",
      "step 908: loss=0.002647876739501953\n",
      "step 909: loss=0.06528759002685547\n",
      "step 910: loss=0.5764923095703125\n",
      "step 911: loss=0.29312682151794434\n",
      "step 912: loss=0.005534172058105469\n",
      "step 913: loss=0.04647350311279297\n",
      "step 914: loss=3.039876699447632\n",
      "step 915: loss=0.04770612716674805\n",
      "step 916: loss=0.23157477378845215\n",
      "step 917: loss=0.34737443923950195\n",
      "step 918: loss=0.5393333435058594\n",
      "step 919: loss=0.3387317657470703\n",
      "step 920: loss=0.10210371017456055\n",
      "step 921: loss=0.2000105381011963\n",
      "step 922: loss=0.46993470191955566\n",
      "step 923: loss=0.056276798248291016\n",
      "step 924: loss=0.006319999694824219\n",
      "step 925: loss=0.01444244384765625\n",
      "step 926: loss=0.02815389633178711\n",
      "step 927: loss=0.13592910766601562\n",
      "step 928: loss=2.0474705696105957\n",
      "step 929: loss=0.08091592788696289\n",
      "step 930: loss=0.23891758918762207\n",
      "step 931: loss=0.1295328140258789\n",
      "step 932: loss=0.03899955749511719\n",
      "step 933: loss=0.02909708023071289\n",
      "step 934: loss=0.022089481353759766\n",
      "step 935: loss=0.010469913482666016\n",
      "step 936: loss=1.1239633560180664\n",
      "step 937: loss=0.004912853240966797\n",
      "step 938: loss=0.24308156967163086\n",
      "step 939: loss=0.5980682373046875\n",
      "step 940: loss=0.12193965911865234\n",
      "step 941: loss=0.02315378189086914\n",
      "step 942: loss=0.37062692642211914\n",
      "step 943: loss=0.009302139282226562\n",
      "step 944: loss=0.011314868927001953\n",
      "step 945: loss=3.5264344215393066\n",
      "step 946: loss=0.0910792350769043\n",
      "step 947: loss=0.15913820266723633\n",
      "step 948: loss=0.01747608184814453\n",
      "step 949: loss=0.11133050918579102\n",
      "step 950: loss=0.09948945045471191\n",
      "step 951: loss=0.052641868591308594\n",
      "step 952: loss=0.00896453857421875\n",
      "step 953: loss=0.01663351058959961\n",
      "step 954: loss=0.05906391143798828\n",
      "step 955: loss=0.002895355224609375\n",
      "step 956: loss=0.07044506072998047\n",
      "step 957: loss=0.007376194000244141\n",
      "step 958: loss=0.2260730266571045\n",
      "step 959: loss=0.16261935234069824\n",
      "step 960: loss=0.6109166145324707\n",
      "step 961: loss=0.5834782123565674\n",
      "step 962: loss=0.05586504936218262\n",
      "step 963: loss=0.004221916198730469\n",
      "step 964: loss=0.033403873443603516\n",
      "step 965: loss=0.793737530708313\n",
      "step 966: loss=0.37858057022094727\n",
      "step 967: loss=0.0952444076538086\n",
      "step 968: loss=0.6360642910003662\n",
      "step 969: loss=0.03384065628051758\n",
      "step 970: loss=0.638343334197998\n",
      "step 971: loss=0.03069305419921875\n",
      "step 972: loss=0.14579248428344727\n",
      "step 973: loss=0.07487893104553223\n",
      "step 974: loss=1.3001110553741455\n",
      "step 975: loss=2.2975776195526123\n",
      "step 976: loss=0.01396322250366211\n",
      "step 977: loss=0.15135955810546875\n",
      "step 978: loss=0.8422958254814148\n",
      "step 979: loss=1.9849610328674316\n",
      "step 980: loss=0.5528028011322021\n",
      "step 981: loss=0.9595495462417603\n",
      "step 982: loss=0.15296173095703125\n",
      "step 983: loss=0.6124076843261719\n",
      "step 984: loss=0.04457712173461914\n",
      "step 985: loss=0.19276797771453857\n",
      "step 986: loss=0.14833617210388184\n",
      "step 987: loss=0.07393813133239746\n",
      "step 988: loss=0.07053279876708984\n",
      "step 989: loss=0.001033782958984375\n",
      "step 990: loss=0.31380701065063477\n",
      "step 991: loss=0.14849281311035156\n",
      "step 992: loss=0.2512540817260742\n",
      "step 993: loss=0.010190486907958984\n",
      "step 994: loss=0.709064245223999\n",
      "step 995: loss=0.2817842960357666\n",
      "step 996: loss=0.0027513504028320312\n",
      "step 997: loss=0.0024824142456054688\n",
      "step 998: loss=0.18323874473571777\n",
      "step 999: loss=0.29120421409606934\n",
      "step 1000: loss=0.002166748046875\n",
      "Mean loss        0.48416767\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d036d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_step = 1000  # e.g. resume from checkpoint at step 1000\n",
    "layer_sizes = [784, 128, 10]\n",
    "\n",
    "experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
    "checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
    "\n",
    "template_state = create_training_state(layer_sizes, optimizer, jax.random.key(0))\n",
    "restored_state = checkpoint_manager.restore(\n",
    "    resume_from_step,\n",
    "    args=ocp.args.StandardRestore(template_state),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1001: loss=0.1626293659210205\n",
      "step 1002: loss=0.017407894134521484\n",
      "step 1003: loss=0.031241416931152344\n",
      "step 1004: loss=0.004973888397216797\n",
      "step 1005: loss=0.10784530639648438\n",
      "step 1006: loss=0.002956390380859375\n",
      "step 1007: loss=0.0009908676147460938\n",
      "step 1008: loss=0.5485451221466064\n",
      "step 1009: loss=0.004882335662841797\n",
      "step 1010: loss=0.015553951263427734\n",
      "step 1011: loss=0.22939682006835938\n",
      "step 1012: loss=0.03225088119506836\n",
      "step 1013: loss=0.014636039733886719\n",
      "step 1014: loss=0.19443297386169434\n",
      "step 1015: loss=0.0006361007690429688\n",
      "step 1016: loss=9.5367431640625e-05\n",
      "step 1017: loss=0.0055103302001953125\n",
      "step 1018: loss=0.4422445297241211\n",
      "step 1019: loss=0.8210334777832031\n",
      "step 1020: loss=0.043196678161621094\n",
      "step 1021: loss=0.042943477630615234\n",
      "step 1022: loss=0.1428360939025879\n",
      "step 1023: loss=0.09797310829162598\n",
      "step 1024: loss=0.16956233978271484\n",
      "step 1025: loss=0.3048539161682129\n",
      "step 1026: loss=1.570849061012268\n",
      "step 1027: loss=0.314772367477417\n",
      "step 1028: loss=0.14598894119262695\n",
      "step 1029: loss=0.13607382774353027\n",
      "step 1030: loss=0.09300756454467773\n",
      "step 1031: loss=0.0002117156982421875\n",
      "step 1032: loss=0.016544342041015625\n",
      "step 1033: loss=0.06578230857849121\n",
      "step 1034: loss=0.3234424591064453\n",
      "step 1035: loss=0.011977195739746094\n",
      "step 1036: loss=0.8655931949615479\n",
      "step 1037: loss=0.039923667907714844\n",
      "step 1038: loss=0.01228475570678711\n",
      "step 1039: loss=0.7493517398834229\n",
      "step 1040: loss=0.04069232940673828\n",
      "step 1041: loss=0.037724971771240234\n",
      "step 1042: loss=0.08974981307983398\n",
      "step 1043: loss=0.018084049224853516\n",
      "step 1044: loss=0.047277212142944336\n",
      "step 1045: loss=0.6408088207244873\n",
      "step 1046: loss=0.011878013610839844\n",
      "step 1047: loss=0.00961923599243164\n",
      "step 1048: loss=0.7152836322784424\n",
      "step 1049: loss=0.007767200469970703\n",
      "step 1050: loss=0.20940804481506348\n",
      "step 1051: loss=0.07469987869262695\n",
      "step 1052: loss=0.210099458694458\n",
      "step 1053: loss=0.09630346298217773\n",
      "step 1054: loss=0.020104408264160156\n",
      "step 1055: loss=0.07259464263916016\n",
      "step 1056: loss=1.2613219022750854\n",
      "step 1057: loss=0.012250423431396484\n",
      "step 1058: loss=0.20248746871948242\n",
      "step 1059: loss=4.059324741363525\n",
      "step 1060: loss=0.19527053833007812\n",
      "step 1061: loss=0.007721900939941406\n",
      "step 1062: loss=0.027651309967041016\n",
      "step 1063: loss=0.603323221206665\n",
      "step 1064: loss=0.7820124626159668\n",
      "step 1065: loss=0.9905705451965332\n",
      "step 1066: loss=0.0049571990966796875\n",
      "step 1067: loss=0.18484926223754883\n",
      "step 1068: loss=0.008664608001708984\n",
      "step 1069: loss=0.452229380607605\n",
      "step 1070: loss=0.09458589553833008\n",
      "step 1071: loss=0.3984771966934204\n",
      "step 1072: loss=0.25646424293518066\n",
      "step 1073: loss=0.01741933822631836\n",
      "step 1074: loss=0.002857685089111328\n",
      "step 1075: loss=0.02662944793701172\n",
      "step 1076: loss=0.0009775161743164062\n",
      "step 1077: loss=0.3767528533935547\n",
      "step 1078: loss=1.8105497360229492\n",
      "step 1079: loss=0.02627086639404297\n",
      "step 1080: loss=0.0006084442138671875\n",
      "step 1081: loss=0.00379180908203125\n",
      "step 1082: loss=0.0077114105224609375\n",
      "step 1083: loss=0.008121967315673828\n",
      "step 1084: loss=0.03461647033691406\n",
      "step 1085: loss=1.4824302196502686\n",
      "step 1086: loss=0.03733491897583008\n",
      "step 1087: loss=0.5462534427642822\n",
      "step 1088: loss=0.005248069763183594\n",
      "step 1089: loss=0.014200210571289062\n",
      "step 1090: loss=0.7933633327484131\n",
      "step 1091: loss=0.004519462585449219\n",
      "step 1092: loss=0.06622552871704102\n",
      "step 1093: loss=0.004139423370361328\n",
      "step 1094: loss=0.7655787467956543\n",
      "step 1095: loss=0.0005903244018554688\n",
      "step 1096: loss=0.011120319366455078\n",
      "step 1097: loss=0.03551673889160156\n",
      "step 1098: loss=0.046636104583740234\n",
      "step 1099: loss=0.10104990005493164\n",
      "step 1100: loss=0.03680706024169922\n",
      "step 1101: loss=0.0600128173828125\n",
      "step 1102: loss=0.0012173652648925781\n",
      "step 1103: loss=1.513902187347412\n",
      "step 1104: loss=0.009312152862548828\n",
      "step 1105: loss=1.0278410911560059\n",
      "step 1106: loss=0.3108689785003662\n",
      "step 1107: loss=0.028818607330322266\n",
      "step 1108: loss=0.02547907829284668\n",
      "step 1109: loss=0.002213001251220703\n",
      "step 1110: loss=0.20375561714172363\n",
      "step 1111: loss=0.05109596252441406\n",
      "step 1112: loss=0.03166913986206055\n",
      "step 1113: loss=0.43865108489990234\n",
      "step 1114: loss=0.02792215347290039\n",
      "step 1115: loss=0.00046634674072265625\n",
      "step 1116: loss=0.09184002876281738\n",
      "step 1117: loss=0.004589557647705078\n",
      "step 1118: loss=0.0801236629486084\n",
      "step 1119: loss=0.6003773212432861\n",
      "step 1120: loss=0.4056210517883301\n",
      "step 1121: loss=1.2839908599853516\n",
      "step 1122: loss=0.03129911422729492\n",
      "step 1123: loss=0.14172148704528809\n",
      "step 1124: loss=0.009854793548583984\n",
      "step 1125: loss=0.00176239013671875\n",
      "step 1126: loss=0.037995338439941406\n",
      "step 1127: loss=0.09078598022460938\n",
      "step 1128: loss=0.007198810577392578\n",
      "step 1129: loss=0.11002230644226074\n",
      "step 1130: loss=0.08499717712402344\n",
      "step 1131: loss=0.0327606201171875\n",
      "step 1132: loss=1.2765183448791504\n",
      "step 1133: loss=0.014400959014892578\n",
      "step 1134: loss=0.02146005630493164\n",
      "step 1135: loss=0.09001016616821289\n",
      "step 1136: loss=0.005741119384765625\n",
      "step 1137: loss=0.13014841079711914\n",
      "step 1138: loss=0.1533496379852295\n",
      "step 1139: loss=0.016679763793945312\n",
      "step 1140: loss=0.0036249160766601562\n",
      "step 1141: loss=0.02324962615966797\n",
      "step 1142: loss=0.32183265686035156\n",
      "step 1143: loss=0.004617214202880859\n",
      "step 1144: loss=1.0366684198379517\n",
      "step 1145: loss=0.004954814910888672\n",
      "step 1146: loss=0.07909965515136719\n",
      "step 1147: loss=0.25406646728515625\n",
      "step 1148: loss=1.552024483680725\n",
      "step 1149: loss=0.09420561790466309\n",
      "step 1150: loss=0.04807567596435547\n",
      "step 1151: loss=0.02412891387939453\n",
      "step 1152: loss=0.016863346099853516\n",
      "step 1153: loss=0.0028200149536132812\n",
      "step 1154: loss=0.0025496482849121094\n",
      "step 1155: loss=0.689828634262085\n",
      "step 1156: loss=0.2728095054626465\n",
      "step 1157: loss=0.07658123970031738\n",
      "step 1158: loss=0.012423038482666016\n",
      "step 1159: loss=0.004768848419189453\n",
      "step 1160: loss=0.028670310974121094\n",
      "step 1161: loss=0.0003113746643066406\n",
      "step 1162: loss=0.007377147674560547\n",
      "step 1163: loss=0.0026369094848632812\n",
      "step 1164: loss=0.07589292526245117\n",
      "step 1165: loss=0.1468658447265625\n",
      "step 1166: loss=0.004059791564941406\n",
      "step 1167: loss=0.048050880432128906\n",
      "step 1168: loss=0.15915870666503906\n",
      "step 1169: loss=0.017077922821044922\n",
      "step 1170: loss=0.012436866760253906\n",
      "step 1171: loss=0.007636070251464844\n",
      "step 1172: loss=0.3201608657836914\n",
      "step 1173: loss=0.002017498016357422\n",
      "step 1174: loss=0.040223121643066406\n",
      "step 1175: loss=0.3871796131134033\n",
      "step 1176: loss=0.022149085998535156\n",
      "step 1177: loss=0.03946399688720703\n",
      "step 1178: loss=0.00220489501953125\n",
      "step 1179: loss=0.020730018615722656\n",
      "step 1180: loss=1.604821801185608\n",
      "step 1181: loss=0.08331894874572754\n",
      "step 1182: loss=0.022668838500976562\n",
      "step 1183: loss=0.014053821563720703\n",
      "step 1184: loss=0.055033206939697266\n",
      "step 1185: loss=0.08323001861572266\n",
      "step 1186: loss=0.03009796142578125\n",
      "step 1187: loss=0.01802825927734375\n",
      "step 1188: loss=0.4612252712249756\n",
      "step 1189: loss=0.01162576675415039\n",
      "step 1190: loss=0.005026817321777344\n",
      "step 1191: loss=0.11319828033447266\n",
      "step 1192: loss=0.0016803741455078125\n",
      "step 1193: loss=0.00034809112548828125\n",
      "step 1194: loss=0.0603790283203125\n",
      "step 1195: loss=0.0018815994262695312\n",
      "step 1196: loss=0.16170263290405273\n",
      "step 1197: loss=0.2908773422241211\n",
      "step 1198: loss=0.031519412994384766\n",
      "step 1199: loss=2.1463234424591064\n",
      "step 1200: loss=0.9627663493156433\n",
      "step 1201: loss=0.0038123130798339844\n",
      "step 1202: loss=0.4058964252471924\n",
      "step 1203: loss=0.18172216415405273\n",
      "step 1204: loss=0.020883560180664062\n",
      "step 1205: loss=0.008228778839111328\n",
      "step 1206: loss=0.008556365966796875\n",
      "step 1207: loss=0.025165557861328125\n",
      "step 1208: loss=0.03114461898803711\n",
      "step 1209: loss=0.0013537406921386719\n",
      "step 1210: loss=0.26059818267822266\n",
      "step 1211: loss=0.025119304656982422\n",
      "step 1212: loss=0.2758338451385498\n",
      "step 1213: loss=0.674568772315979\n",
      "step 1214: loss=0.3330986499786377\n",
      "step 1215: loss=0.0001277923583984375\n",
      "step 1216: loss=0.043561458587646484\n",
      "step 1217: loss=0.012444019317626953\n",
      "step 1218: loss=0.008404254913330078\n",
      "step 1219: loss=0.005790233612060547\n",
      "step 1220: loss=0.027063369750976562\n",
      "step 1221: loss=0.04086017608642578\n",
      "step 1222: loss=1.4971646070480347\n",
      "step 1223: loss=0.07079529762268066\n",
      "step 1224: loss=0.030503511428833008\n",
      "step 1225: loss=0.040850162506103516\n",
      "step 1226: loss=0.12685680389404297\n",
      "step 1227: loss=0.11972236633300781\n",
      "step 1228: loss=0.0685582160949707\n",
      "step 1229: loss=0.09670543670654297\n",
      "step 1230: loss=0.035756826400756836\n",
      "step 1231: loss=0.08947992324829102\n",
      "step 1232: loss=0.22990059852600098\n",
      "step 1233: loss=6.29425048828125e-05\n",
      "step 1234: loss=0.7096219062805176\n",
      "step 1235: loss=0.7265791893005371\n",
      "step 1236: loss=0.15836286544799805\n",
      "step 1237: loss=0.001552581787109375\n",
      "step 1238: loss=0.010675907135009766\n",
      "step 1239: loss=0.011548995971679688\n",
      "step 1240: loss=0.07656431198120117\n",
      "step 1241: loss=0.10735702514648438\n",
      "step 1242: loss=0.010092735290527344\n",
      "step 1243: loss=0.1826789379119873\n",
      "step 1244: loss=0.02172994613647461\n",
      "step 1245: loss=0.0033774375915527344\n",
      "step 1246: loss=0.03788018226623535\n",
      "step 1247: loss=0.05229949951171875\n",
      "step 1248: loss=1.2361040115356445\n",
      "step 1249: loss=0.00010013580322265625\n",
      "step 1250: loss=0.008226871490478516\n",
      "step 1251: loss=0.3423879146575928\n",
      "step 1252: loss=0.024322032928466797\n",
      "step 1253: loss=0.035619258880615234\n",
      "step 1254: loss=0.011382579803466797\n",
      "step 1255: loss=0.01534128189086914\n",
      "step 1256: loss=0.01314401626586914\n",
      "step 1257: loss=1.006316900253296\n",
      "step 1258: loss=0.20354461669921875\n",
      "step 1259: loss=0.5348777770996094\n",
      "step 1260: loss=0.03265190124511719\n",
      "step 1261: loss=0.21750664710998535\n",
      "step 1262: loss=0.5395522117614746\n",
      "step 1263: loss=0.0076904296875\n",
      "step 1264: loss=0.08109116554260254\n",
      "step 1265: loss=0.1924576759338379\n",
      "step 1266: loss=0.18837404251098633\n",
      "step 1267: loss=0.044295310974121094\n",
      "step 1268: loss=0.03443002700805664\n",
      "step 1269: loss=0.0007271766662597656\n",
      "step 1270: loss=0.2871263027191162\n",
      "step 1271: loss=1.049041748046875e-05\n",
      "step 1272: loss=0.012858390808105469\n",
      "step 1273: loss=0.0019707679748535156\n",
      "step 1274: loss=0.0023412704467773438\n",
      "step 1275: loss=0.1732947826385498\n",
      "step 1276: loss=0.000484466552734375\n",
      "step 1277: loss=0.0677332878112793\n",
      "step 1278: loss=0.01694202423095703\n",
      "step 1279: loss=0.08466601371765137\n",
      "step 1280: loss=0.08986449241638184\n",
      "step 1281: loss=0.012970447540283203\n",
      "step 1282: loss=0.014156341552734375\n",
      "step 1283: loss=0.32639002799987793\n",
      "step 1284: loss=1.217644214630127\n",
      "step 1285: loss=0.02835559844970703\n",
      "step 1286: loss=0.0026140213012695312\n",
      "step 1287: loss=0.02348470687866211\n",
      "step 1288: loss=0.02862548828125\n",
      "step 1289: loss=0.21062707901000977\n",
      "step 1290: loss=0.0009860992431640625\n",
      "step 1291: loss=0.0016298294067382812\n",
      "step 1292: loss=0.1581568717956543\n",
      "step 1293: loss=0.017374038696289062\n",
      "step 1294: loss=0.00049591064453125\n",
      "step 1295: loss=0.08648204803466797\n",
      "step 1296: loss=0.0482335090637207\n",
      "step 1297: loss=0.001617431640625\n",
      "step 1298: loss=0.0020036697387695312\n",
      "step 1299: loss=0.008901596069335938\n",
      "step 1300: loss=0.04480552673339844\n",
      "step 1301: loss=0.000213623046875\n",
      "step 1302: loss=3.8289921283721924\n",
      "step 1303: loss=0.29468441009521484\n",
      "step 1304: loss=0.00345611572265625\n",
      "step 1305: loss=0.0020771026611328125\n",
      "step 1306: loss=0.18143939971923828\n",
      "step 1307: loss=0.2595984935760498\n",
      "step 1308: loss=0.0023488998413085938\n",
      "step 1309: loss=0.056387901306152344\n",
      "step 1310: loss=0.3872394561767578\n",
      "step 1311: loss=0.1813364028930664\n",
      "step 1312: loss=0.0037927627563476562\n",
      "step 1313: loss=0.022809982299804688\n",
      "step 1314: loss=0.12825298309326172\n",
      "step 1315: loss=0.0013566017150878906\n",
      "step 1316: loss=0.022032737731933594\n",
      "step 1317: loss=0.004200458526611328\n",
      "step 1318: loss=0.004429340362548828\n",
      "step 1319: loss=0.1249535083770752\n",
      "step 1320: loss=0.012188434600830078\n",
      "step 1321: loss=0.16175103187561035\n",
      "step 1322: loss=0.022179126739501953\n",
      "step 1323: loss=0.009984016418457031\n",
      "step 1324: loss=0.0012793540954589844\n",
      "step 1325: loss=0.004088401794433594\n",
      "step 1326: loss=0.6569926738739014\n",
      "step 1327: loss=0.020417213439941406\n",
      "step 1328: loss=1.4942398071289062\n",
      "step 1329: loss=0.0006995201110839844\n",
      "step 1330: loss=0.3135833740234375\n",
      "step 1331: loss=0.003319263458251953\n",
      "step 1332: loss=0.024753093719482422\n",
      "step 1333: loss=0.016324520111083984\n",
      "step 1334: loss=0.0006103515625\n",
      "step 1335: loss=0.6373841762542725\n",
      "step 1336: loss=0.1016387939453125\n",
      "step 1337: loss=0.028322696685791016\n",
      "step 1338: loss=0.036763668060302734\n",
      "step 1339: loss=0.17965984344482422\n",
      "step 1340: loss=0.01766681671142578\n",
      "step 1341: loss=0.7193061709403992\n",
      "step 1342: loss=0.08399200439453125\n",
      "step 1343: loss=0.018961429595947266\n",
      "step 1344: loss=0.045099735260009766\n",
      "step 1345: loss=0.005125522613525391\n",
      "step 1346: loss=0.028477191925048828\n",
      "step 1347: loss=0.010256290435791016\n",
      "step 1348: loss=0.004501819610595703\n",
      "step 1349: loss=0.01276540756225586\n",
      "step 1350: loss=0.0016283988952636719\n",
      "step 1351: loss=0.0024313926696777344\n",
      "step 1352: loss=0.09863829612731934\n",
      "step 1353: loss=0.45129191875457764\n",
      "step 1354: loss=0.033548831939697266\n",
      "step 1355: loss=0.3980836868286133\n",
      "step 1356: loss=0.0045337677001953125\n",
      "step 1357: loss=0.03186774253845215\n",
      "step 1358: loss=0.005653858184814453\n",
      "step 1359: loss=0.008360862731933594\n",
      "step 1360: loss=0.0010175704956054688\n",
      "step 1361: loss=0.04345130920410156\n",
      "step 1362: loss=0.06328296661376953\n",
      "step 1363: loss=2.1239264011383057\n",
      "step 1364: loss=0.07636404037475586\n",
      "step 1365: loss=0.5956707000732422\n",
      "step 1366: loss=0.14943361282348633\n",
      "step 1367: loss=0.08498930931091309\n",
      "step 1368: loss=0.002578258514404297\n",
      "step 1369: loss=0.00014209747314453125\n",
      "step 1370: loss=0.7903897762298584\n",
      "step 1371: loss=0.0773000717163086\n",
      "step 1372: loss=0.028435230255126953\n",
      "step 1373: loss=0.0018863677978515625\n",
      "step 1374: loss=0.008478641510009766\n",
      "step 1375: loss=0.27088260650634766\n",
      "step 1376: loss=0.25244903564453125\n",
      "step 1377: loss=0.025552988052368164\n",
      "step 1378: loss=0.1546785831451416\n",
      "step 1379: loss=0.38130760192871094\n",
      "step 1380: loss=0.0011224746704101562\n",
      "step 1381: loss=0.001132965087890625\n",
      "step 1382: loss=0.053763389587402344\n",
      "step 1383: loss=0.01424264907836914\n",
      "step 1384: loss=1.621246337890625e-05\n",
      "step 1385: loss=0.08846426010131836\n",
      "step 1386: loss=0.0068340301513671875\n",
      "step 1387: loss=0.005295753479003906\n",
      "step 1388: loss=0.07916641235351562\n",
      "step 1389: loss=0.000637054443359375\n",
      "step 1390: loss=0.06777739524841309\n",
      "step 1391: loss=0.027057647705078125\n",
      "step 1392: loss=0.16372370719909668\n",
      "step 1393: loss=0.16978907585144043\n",
      "step 1394: loss=2.819176197052002\n",
      "step 1395: loss=0.0005025863647460938\n",
      "step 1396: loss=0.04915618896484375\n",
      "step 1397: loss=0.042624473571777344\n",
      "step 1398: loss=0.06264448165893555\n",
      "step 1399: loss=0.0442051887512207\n",
      "step 1400: loss=0.004332065582275391\n",
      "step 1401: loss=0.13192105293273926\n",
      "step 1402: loss=0.0004520416259765625\n",
      "step 1403: loss=0.0003199577331542969\n",
      "step 1404: loss=0.0013957023620605469\n",
      "step 1405: loss=0.0006384849548339844\n",
      "step 1406: loss=0.24060654640197754\n",
      "step 1407: loss=0.0015306472778320312\n",
      "step 1408: loss=0.0006685256958007812\n",
      "step 1409: loss=3.178748369216919\n",
      "step 1410: loss=0.01769542694091797\n",
      "step 1411: loss=0.05989360809326172\n",
      "step 1412: loss=0.0004711151123046875\n",
      "step 1413: loss=0.014223575592041016\n",
      "step 1414: loss=0.010881423950195312\n",
      "step 1415: loss=0.0029511451721191406\n",
      "step 1416: loss=1.204953670501709\n",
      "step 1417: loss=0.014825820922851562\n",
      "step 1418: loss=0.014616966247558594\n",
      "step 1419: loss=0.4398317337036133\n",
      "step 1420: loss=0.08567070960998535\n",
      "step 1421: loss=0.029613971710205078\n",
      "step 1422: loss=0.16761207580566406\n",
      "step 1423: loss=0.015293598175048828\n",
      "step 1424: loss=0.003395557403564453\n",
      "step 1425: loss=0.2690110206604004\n",
      "step 1426: loss=0.2226696014404297\n",
      "step 1427: loss=0.1279745101928711\n",
      "step 1428: loss=0.1160275936126709\n",
      "step 1429: loss=0.35317516326904297\n",
      "step 1430: loss=7.62939453125e-05\n",
      "step 1431: loss=0.018709659576416016\n",
      "step 1432: loss=0.2945125102996826\n",
      "step 1433: loss=0.11767315864562988\n",
      "step 1434: loss=0.12093925476074219\n",
      "step 1435: loss=0.012970447540283203\n",
      "step 1436: loss=0.18533062934875488\n",
      "step 1437: loss=5.340576171875e-05\n",
      "step 1438: loss=0.001495361328125\n",
      "step 1439: loss=0.3231240510940552\n",
      "step 1440: loss=0.0005245208740234375\n",
      "step 1441: loss=0.02144145965576172\n",
      "step 1442: loss=0.03689265251159668\n",
      "step 1443: loss=0.07671570777893066\n",
      "step 1444: loss=0.3404301404953003\n",
      "step 1445: loss=0.04117584228515625\n",
      "step 1446: loss=0.11533355712890625\n",
      "step 1447: loss=0.011414051055908203\n",
      "step 1448: loss=0.018762588500976562\n",
      "step 1449: loss=0.03667593002319336\n",
      "step 1450: loss=0.6469244956970215\n",
      "step 1451: loss=0.0010986328125\n",
      "step 1452: loss=0.3163294792175293\n",
      "step 1453: loss=0.030060768127441406\n",
      "step 1454: loss=0.8069682121276855\n",
      "step 1455: loss=0.3349423408508301\n",
      "step 1456: loss=1.7350786924362183\n",
      "step 1457: loss=0.03682851791381836\n",
      "step 1458: loss=0.09995269775390625\n",
      "step 1459: loss=0.01538705825805664\n",
      "step 1460: loss=0.006072044372558594\n",
      "step 1461: loss=0.8176702260971069\n",
      "step 1462: loss=0.0019655227661132812\n",
      "step 1463: loss=0.06472039222717285\n",
      "step 1464: loss=0.0028214454650878906\n",
      "step 1465: loss=0.6776237487792969\n",
      "step 1466: loss=0.0041866302490234375\n",
      "step 1467: loss=0.569083571434021\n",
      "step 1468: loss=0.05303621292114258\n",
      "step 1469: loss=0.04572129249572754\n",
      "step 1470: loss=0.0050792694091796875\n",
      "step 1471: loss=0.5556769371032715\n",
      "step 1472: loss=0.6054384708404541\n",
      "step 1473: loss=0.016014575958251953\n",
      "step 1474: loss=0.11725997924804688\n",
      "step 1475: loss=0.0011010169982910156\n",
      "step 1476: loss=0.07707905769348145\n",
      "step 1477: loss=0.016223907470703125\n",
      "step 1478: loss=0.0010433197021484375\n",
      "step 1479: loss=0.0739741325378418\n",
      "step 1480: loss=0.028263568878173828\n",
      "step 1481: loss=0.014484882354736328\n",
      "step 1482: loss=0.02749776840209961\n",
      "step 1483: loss=0.019678592681884766\n",
      "step 1484: loss=0.03412318229675293\n",
      "step 1485: loss=0.3073434829711914\n",
      "step 1486: loss=0.0014429092407226562\n",
      "step 1487: loss=8.296966552734375e-05\n",
      "step 1488: loss=0.0024118423461914062\n",
      "step 1489: loss=0.006085872650146484\n",
      "step 1490: loss=0.14802050590515137\n",
      "step 1491: loss=0.0007772445678710938\n",
      "step 1492: loss=0.000522613525390625\n",
      "step 1493: loss=0.015981674194335938\n",
      "step 1494: loss=0.0436711311340332\n",
      "step 1495: loss=0.008254051208496094\n",
      "step 1496: loss=0.2353816032409668\n",
      "step 1497: loss=0.005907535552978516\n",
      "step 1498: loss=0.004480838775634766\n",
      "step 1499: loss=0.9267098903656006\n",
      "step 1500: loss=0.1269702911376953\n",
      "step 1501: loss=0.0011110305786132812\n",
      "step 1502: loss=0.0768890380859375\n",
      "step 1503: loss=0.012132644653320312\n",
      "step 1504: loss=0.025094985961914062\n",
      "step 1505: loss=0.003726959228515625\n",
      "step 1506: loss=0.17690587043762207\n",
      "step 1507: loss=0.182356595993042\n",
      "step 1508: loss=0.009600639343261719\n",
      "step 1509: loss=0.0022687911987304688\n",
      "step 1510: loss=0.019440650939941406\n",
      "step 1511: loss=0.07108449935913086\n",
      "step 1512: loss=0.05274677276611328\n",
      "step 1513: loss=0.01139068603515625\n",
      "step 1514: loss=0.20629453659057617\n",
      "step 1515: loss=0.0014576911926269531\n",
      "step 1516: loss=0.012168407440185547\n",
      "step 1517: loss=0.30356109142303467\n",
      "step 1518: loss=0.10848736763000488\n",
      "step 1519: loss=0.007010936737060547\n",
      "step 1520: loss=0.12987732887268066\n",
      "step 1521: loss=0.022357463836669922\n",
      "step 1522: loss=0.013411521911621094\n",
      "step 1523: loss=0.005294322967529297\n",
      "step 1524: loss=0.005818367004394531\n",
      "step 1525: loss=0.001972675323486328\n",
      "step 1526: loss=3.3895387649536133\n",
      "step 1527: loss=0.0002899169921875\n",
      "step 1528: loss=7.62939453125e-05\n",
      "step 1529: loss=0.10421061515808105\n",
      "step 1530: loss=0.0032825469970703125\n",
      "step 1531: loss=0.005353450775146484\n",
      "step 1532: loss=4.100799560546875e-05\n",
      "step 1533: loss=0.014390945434570312\n",
      "step 1534: loss=0.0040836334228515625\n",
      "step 1535: loss=0.041461944580078125\n",
      "step 1536: loss=0.0023102760314941406\n",
      "step 1537: loss=0.021305084228515625\n",
      "step 1538: loss=0.633580207824707\n",
      "step 1539: loss=0.560760498046875\n",
      "step 1540: loss=1.6962655782699585\n",
      "step 1541: loss=0.0012602806091308594\n",
      "step 1542: loss=0.04548168182373047\n",
      "step 1543: loss=0.08443713188171387\n",
      "step 1544: loss=0.008582592010498047\n",
      "step 1545: loss=0.2191098928451538\n",
      "step 1546: loss=0.23373770713806152\n",
      "step 1547: loss=0.008641719818115234\n",
      "step 1548: loss=0.0007681846618652344\n",
      "step 1549: loss=0.03319740295410156\n",
      "step 1550: loss=0.28096914291381836\n",
      "step 1551: loss=0.6629800796508789\n",
      "step 1552: loss=0.10477828979492188\n",
      "step 1553: loss=0.002971172332763672\n",
      "step 1554: loss=0.50518798828125\n",
      "step 1555: loss=0.23905658721923828\n",
      "step 1556: loss=1.043480634689331\n",
      "step 1557: loss=0.19914507865905762\n",
      "step 1558: loss=9.632110595703125e-05\n",
      "step 1559: loss=0.0014085769653320312\n",
      "step 1560: loss=0.08930253982543945\n",
      "step 1561: loss=0.030561447143554688\n",
      "step 1562: loss=0.16022777557373047\n",
      "step 1563: loss=0.17746973037719727\n",
      "step 1564: loss=0.016654491424560547\n",
      "step 1565: loss=0.008440971374511719\n",
      "step 1566: loss=0.5582807064056396\n",
      "step 1567: loss=0.024689674377441406\n",
      "step 1568: loss=0.05111336708068848\n",
      "step 1569: loss=0.03325366973876953\n",
      "step 1570: loss=0.018563270568847656\n",
      "step 1571: loss=0.009358882904052734\n",
      "step 1572: loss=0.015969276428222656\n",
      "step 1573: loss=0.21262407302856445\n",
      "step 1574: loss=0.20110130310058594\n",
      "step 1575: loss=0.2016611099243164\n",
      "step 1576: loss=0.06192350387573242\n",
      "step 1577: loss=0.08449459075927734\n",
      "step 1578: loss=0.0019202232360839844\n",
      "step 1579: loss=0.012054920196533203\n",
      "step 1580: loss=0.01990032196044922\n",
      "step 1581: loss=0.004056453704833984\n",
      "step 1582: loss=0.46944499015808105\n",
      "step 1583: loss=0.18107128143310547\n",
      "step 1584: loss=0.0025949478149414062\n",
      "step 1585: loss=0.028498172760009766\n",
      "step 1586: loss=0.2936685085296631\n",
      "step 1587: loss=0.06951570510864258\n",
      "step 1588: loss=0.4252622127532959\n",
      "step 1589: loss=0.8792774677276611\n",
      "step 1590: loss=0.06547975540161133\n",
      "step 1591: loss=0.007937908172607422\n",
      "step 1592: loss=0.39698755741119385\n",
      "step 1593: loss=0.031064510345458984\n",
      "step 1594: loss=0.013475894927978516\n",
      "step 1595: loss=2.1847925186157227\n",
      "step 1596: loss=0.5230252742767334\n",
      "step 1597: loss=0.21626710891723633\n",
      "step 1598: loss=0.1911168098449707\n",
      "step 1599: loss=0.04807901382446289\n",
      "step 1600: loss=0.027595043182373047\n",
      "step 1601: loss=0.0052280426025390625\n",
      "step 1602: loss=0.05585074424743652\n",
      "step 1603: loss=0.20243167877197266\n",
      "step 1604: loss=1.2042088508605957\n",
      "step 1605: loss=0.06678557395935059\n",
      "step 1606: loss=0.08559155464172363\n",
      "step 1607: loss=0.2045135498046875\n",
      "step 1608: loss=0.06025123596191406\n",
      "step 1609: loss=0.11030864715576172\n",
      "step 1610: loss=0.06580829620361328\n",
      "step 1611: loss=1.032108187675476\n",
      "step 1612: loss=0.009571552276611328\n",
      "step 1613: loss=0.027226924896240234\n",
      "step 1614: loss=0.06454634666442871\n",
      "step 1615: loss=0.029736995697021484\n",
      "step 1616: loss=0.07435250282287598\n",
      "step 1617: loss=0.00044727325439453125\n",
      "step 1618: loss=0.16203045845031738\n",
      "step 1619: loss=0.07793307304382324\n",
      "step 1620: loss=0.11376667022705078\n",
      "step 1621: loss=0.298598051071167\n",
      "step 1622: loss=0.03981733322143555\n",
      "step 1623: loss=0.04098224639892578\n",
      "step 1624: loss=0.6176102161407471\n",
      "step 1625: loss=1.0984222888946533\n",
      "step 1626: loss=0.14606952667236328\n",
      "step 1627: loss=0.026809215545654297\n",
      "step 1628: loss=0.0001201629638671875\n",
      "step 1629: loss=0.43259143829345703\n",
      "step 1630: loss=0.0017147064208984375\n",
      "step 1631: loss=0.20094680786132812\n",
      "step 1632: loss=0.24129843711853027\n",
      "step 1633: loss=0.15503883361816406\n",
      "step 1634: loss=0.0002727508544921875\n",
      "step 1635: loss=0.4966268539428711\n",
      "step 1636: loss=0.00026607513427734375\n",
      "step 1637: loss=0.44066619873046875\n",
      "step 1638: loss=0.06043243408203125\n",
      "step 1639: loss=0.002414703369140625\n",
      "step 1640: loss=0.1826009750366211\n",
      "step 1641: loss=0.1027994155883789\n",
      "step 1642: loss=0.003440380096435547\n",
      "step 1643: loss=0.5506304502487183\n",
      "step 1644: loss=0.008540153503417969\n",
      "step 1645: loss=0.01575756072998047\n",
      "step 1646: loss=0.0011467933654785156\n",
      "step 1647: loss=0.13507437705993652\n",
      "step 1648: loss=0.02973794937133789\n",
      "step 1649: loss=0.0007567405700683594\n",
      "step 1650: loss=0.005146503448486328\n",
      "step 1651: loss=0.009072303771972656\n",
      "step 1652: loss=0.2804841995239258\n",
      "step 1653: loss=0.0010547637939453125\n",
      "step 1654: loss=0.11556434631347656\n",
      "step 1655: loss=0.5471842288970947\n",
      "step 1656: loss=0.018390655517578125\n",
      "step 1657: loss=0.005279064178466797\n",
      "step 1658: loss=0.2854125499725342\n",
      "step 1659: loss=0.10133934020996094\n",
      "step 1660: loss=0.14499926567077637\n",
      "step 1661: loss=0.00057220458984375\n",
      "step 1662: loss=0.0362401008605957\n",
      "step 1663: loss=0.0419154167175293\n",
      "step 1664: loss=0.03362703323364258\n",
      "step 1665: loss=0.008584976196289062\n",
      "step 1666: loss=0.0005311965942382812\n",
      "step 1667: loss=0.0011043548583984375\n",
      "step 1668: loss=0.5511074066162109\n",
      "step 1669: loss=0.36252593994140625\n",
      "step 1670: loss=0.15012526512145996\n",
      "step 1671: loss=0.2701714038848877\n",
      "step 1672: loss=0.42069387435913086\n",
      "step 1673: loss=0.8218390941619873\n",
      "step 1674: loss=0.034110069274902344\n",
      "step 1675: loss=0.6801662445068359\n",
      "step 1676: loss=0.018695831298828125\n",
      "step 1677: loss=0.009753704071044922\n",
      "step 1678: loss=0.09774327278137207\n",
      "step 1679: loss=0.032511234283447266\n",
      "step 1680: loss=0.00012683868408203125\n",
      "step 1681: loss=0.13346290588378906\n",
      "step 1682: loss=0.00014400482177734375\n",
      "step 1683: loss=0.08217620849609375\n",
      "step 1684: loss=0.22934794425964355\n",
      "step 1685: loss=0.12432575225830078\n",
      "step 1686: loss=0.0053577423095703125\n",
      "step 1687: loss=0.01948070526123047\n",
      "step 1688: loss=0.00985860824584961\n",
      "step 1689: loss=0.4117429256439209\n",
      "step 1690: loss=0.20614194869995117\n",
      "step 1691: loss=0.09371733665466309\n",
      "step 1692: loss=0.9308011531829834\n",
      "step 1693: loss=0.005922794342041016\n",
      "step 1694: loss=0.012776374816894531\n",
      "step 1695: loss=0.008655548095703125\n",
      "step 1696: loss=0.015024662017822266\n",
      "step 1697: loss=0.5668759346008301\n",
      "step 1698: loss=0.05148029327392578\n",
      "step 1699: loss=0.1288611888885498\n",
      "step 1700: loss=0.0530400276184082\n",
      "step 1701: loss=0.0011267662048339844\n",
      "step 1702: loss=0.003935813903808594\n",
      "step 1703: loss=0.0409088134765625\n",
      "step 1704: loss=0.006133079528808594\n",
      "step 1705: loss=0.031178951263427734\n",
      "step 1706: loss=0.03152275085449219\n",
      "step 1707: loss=0.43497276306152344\n",
      "step 1708: loss=0.11254167556762695\n",
      "step 1709: loss=0.027180194854736328\n",
      "step 1710: loss=0.025602340698242188\n",
      "step 1711: loss=0.3667844533920288\n",
      "step 1712: loss=0.17866086959838867\n",
      "step 1713: loss=0.04136824607849121\n",
      "step 1714: loss=0.030861616134643555\n",
      "step 1715: loss=0.003639698028564453\n",
      "step 1716: loss=0.004277229309082031\n",
      "step 1717: loss=0.04141688346862793\n",
      "step 1718: loss=0.04918408393859863\n",
      "step 1719: loss=0.00034427642822265625\n",
      "step 1720: loss=0.0034394264221191406\n",
      "step 1721: loss=0.006712436676025391\n",
      "step 1722: loss=0.010517120361328125\n",
      "step 1723: loss=0.00024890899658203125\n",
      "step 1724: loss=0.22164535522460938\n",
      "step 1725: loss=3.8018200397491455\n",
      "step 1726: loss=0.08279275894165039\n",
      "step 1727: loss=0.00037384033203125\n",
      "step 1728: loss=0.04691290855407715\n",
      "step 1729: loss=0.13104796409606934\n",
      "step 1730: loss=0.023840904235839844\n",
      "step 1731: loss=0.007630825042724609\n",
      "step 1732: loss=0.4625493288040161\n",
      "step 1733: loss=0.21915912628173828\n",
      "step 1734: loss=0.003639698028564453\n",
      "step 1735: loss=0.2976408004760742\n",
      "step 1736: loss=0.004973888397216797\n",
      "step 1737: loss=0.20421290397644043\n",
      "step 1738: loss=0.019153118133544922\n",
      "step 1739: loss=5.245208740234375e-05\n",
      "step 1740: loss=0.054657936096191406\n",
      "step 1741: loss=0.07558655738830566\n",
      "step 1742: loss=5.26342248916626\n",
      "step 1743: loss=0.31380319595336914\n",
      "step 1744: loss=0.003200531005859375\n",
      "step 1745: loss=0.22080516815185547\n",
      "step 1746: loss=0.009223461151123047\n",
      "step 1747: loss=0.4722909927368164\n",
      "step 1748: loss=0.009346485137939453\n",
      "step 1749: loss=0.0032668113708496094\n",
      "step 1750: loss=0.9367046356201172\n",
      "step 1751: loss=0.004542827606201172\n",
      "step 1752: loss=0.19883036613464355\n",
      "step 1753: loss=0.030304431915283203\n",
      "step 1754: loss=0.10064077377319336\n",
      "step 1755: loss=0.1117866039276123\n",
      "step 1756: loss=0.22506427764892578\n",
      "step 1757: loss=0.0016832351684570312\n",
      "step 1758: loss=0.09720849990844727\n",
      "step 1759: loss=0.07725334167480469\n",
      "step 1760: loss=0.02687215805053711\n",
      "step 1761: loss=0.000583648681640625\n",
      "step 1762: loss=0.0722355842590332\n",
      "step 1763: loss=0.14926719665527344\n",
      "step 1764: loss=0.08098077774047852\n",
      "step 1765: loss=0.07811355590820312\n",
      "step 1766: loss=0.16404354572296143\n",
      "step 1767: loss=0.09472155570983887\n",
      "step 1768: loss=0.012936115264892578\n",
      "step 1769: loss=7.05718994140625e-05\n",
      "step 1770: loss=0.12040185928344727\n",
      "step 1771: loss=0.2715296745300293\n",
      "step 1772: loss=1.3782072067260742\n",
      "step 1773: loss=0.017582416534423828\n",
      "step 1774: loss=0.023566246032714844\n",
      "step 1775: loss=0.0020427703857421875\n",
      "step 1776: loss=0.030818462371826172\n",
      "step 1777: loss=0.0083770751953125\n",
      "step 1778: loss=0.008779525756835938\n",
      "step 1779: loss=0.004837989807128906\n",
      "step 1780: loss=0.009705066680908203\n",
      "step 1781: loss=9.441375732421875e-05\n",
      "step 1782: loss=0.0312957763671875\n",
      "step 1783: loss=0.0017952919006347656\n",
      "step 1784: loss=0.0017518997192382812\n",
      "step 1785: loss=0.01167917251586914\n",
      "step 1786: loss=0.006351947784423828\n",
      "step 1787: loss=0.18572139739990234\n",
      "step 1788: loss=0.22350692749023438\n",
      "step 1789: loss=0.03586721420288086\n",
      "step 1790: loss=0.03142404556274414\n",
      "step 1791: loss=0.057220458984375\n",
      "step 1792: loss=0.023380756378173828\n",
      "step 1793: loss=0.2261967658996582\n",
      "step 1794: loss=0.1636042594909668\n",
      "step 1795: loss=0.0008311271667480469\n",
      "step 1796: loss=0.009186267852783203\n",
      "step 1797: loss=0.05345296859741211\n",
      "step 1798: loss=2.1842336654663086\n",
      "step 1799: loss=0.23951339721679688\n",
      "step 1800: loss=0.46537208557128906\n",
      "step 1801: loss=0.006124019622802734\n",
      "step 1802: loss=0.0038728713989257812\n",
      "step 1803: loss=0.024188995361328125\n",
      "step 1804: loss=0.160139799118042\n",
      "step 1805: loss=0.0003528594970703125\n",
      "step 1806: loss=0.00042057037353515625\n",
      "step 1807: loss=0.0033197402954101562\n",
      "step 1808: loss=0.0014467239379882812\n",
      "step 1809: loss=0.2608311176300049\n",
      "step 1810: loss=0.00026416778564453125\n",
      "step 1811: loss=0.608456015586853\n",
      "step 1812: loss=0.15521621704101562\n",
      "step 1813: loss=0.025329113006591797\n",
      "step 1814: loss=0.9537088871002197\n",
      "step 1815: loss=0.012410163879394531\n",
      "step 1816: loss=0.04104948043823242\n",
      "step 1817: loss=0.14691758155822754\n",
      "step 1818: loss=0.020838260650634766\n",
      "step 1819: loss=0.0043392181396484375\n",
      "step 1820: loss=0.02861928939819336\n",
      "step 1821: loss=0.020590782165527344\n",
      "step 1822: loss=0.007065773010253906\n",
      "step 1823: loss=0.03475332260131836\n",
      "step 1824: loss=0.13762950897216797\n",
      "step 1825: loss=0.3033792972564697\n",
      "step 1826: loss=0.0031065940856933594\n",
      "step 1827: loss=0.904409646987915\n",
      "step 1828: loss=0.002132415771484375\n",
      "step 1829: loss=0.048436641693115234\n",
      "step 1830: loss=0.05208635330200195\n",
      "step 1831: loss=0.0666966438293457\n",
      "step 1832: loss=0.000125885009765625\n",
      "step 1833: loss=0.02478313446044922\n",
      "step 1834: loss=0.21184587478637695\n",
      "step 1835: loss=0.3109102249145508\n",
      "step 1836: loss=0.0035266876220703125\n",
      "step 1837: loss=0.008171558380126953\n",
      "step 1838: loss=0.0017652511596679688\n",
      "step 1839: loss=0.0040950775146484375\n",
      "step 1840: loss=0.06879138946533203\n",
      "step 1841: loss=0.0003528594970703125\n",
      "step 1842: loss=0.0267181396484375\n",
      "step 1843: loss=0.0008697509765625\n",
      "step 1844: loss=0.03314352035522461\n",
      "step 1845: loss=0.00011730194091796875\n",
      "step 1846: loss=0.056177616119384766\n",
      "step 1847: loss=0.23808813095092773\n",
      "step 1848: loss=0.09098553657531738\n",
      "step 1849: loss=0.0001678466796875\n",
      "step 1850: loss=0.030178070068359375\n",
      "step 1851: loss=0.008967876434326172\n",
      "step 1852: loss=0.36790359020233154\n",
      "step 1853: loss=0.07280945777893066\n",
      "step 1854: loss=0.008320331573486328\n",
      "step 1855: loss=0.42780542373657227\n",
      "step 1856: loss=0.038349151611328125\n",
      "step 1857: loss=0.021057605743408203\n",
      "step 1858: loss=0.009790897369384766\n",
      "step 1859: loss=0.045013427734375\n",
      "step 1860: loss=0.2187962532043457\n",
      "step 1861: loss=0.009824752807617188\n",
      "step 1862: loss=0.0001583099365234375\n",
      "step 1863: loss=0.03968954086303711\n",
      "step 1864: loss=0.0054836273193359375\n",
      "step 1865: loss=0.1407923698425293\n",
      "step 1866: loss=0.013606548309326172\n",
      "step 1867: loss=0.17886734008789062\n",
      "step 1868: loss=0.005841732025146484\n",
      "step 1869: loss=1.4918911457061768\n",
      "step 1870: loss=0.0005960464477539062\n",
      "step 1871: loss=0.04952812194824219\n",
      "step 1872: loss=0.02548360824584961\n",
      "step 1873: loss=0.0071849822998046875\n",
      "step 1874: loss=0.3841726779937744\n",
      "step 1875: loss=0.07076501846313477\n",
      "step 1876: loss=0.4331812858581543\n",
      "step 1877: loss=0.04628753662109375\n",
      "step 1878: loss=0.03896903991699219\n",
      "step 1879: loss=0.0006542205810546875\n",
      "step 1880: loss=0.1489114761352539\n",
      "step 1881: loss=0.006114482879638672\n",
      "step 1882: loss=0.088775634765625\n",
      "step 1883: loss=0.020968914031982422\n",
      "step 1884: loss=0.0011320114135742188\n",
      "step 1885: loss=0.01263570785522461\n",
      "step 1886: loss=0.4453902244567871\n",
      "step 1887: loss=0.03040599822998047\n",
      "step 1888: loss=0.03289365768432617\n",
      "step 1889: loss=0.18955421447753906\n",
      "step 1890: loss=0.13664793968200684\n",
      "step 1891: loss=0.004312038421630859\n",
      "step 1892: loss=2.384185791015625e-05\n",
      "step 1893: loss=0.07896852493286133\n",
      "step 1894: loss=0.07677531242370605\n",
      "step 1895: loss=0.0010290145874023438\n",
      "step 1896: loss=0.0012936592102050781\n",
      "step 1897: loss=0.19965672492980957\n",
      "step 1898: loss=0.0755147933959961\n",
      "step 1899: loss=0.0032405853271484375\n",
      "step 1900: loss=0.005822181701660156\n",
      "step 1901: loss=0.08955764770507812\n",
      "step 1902: loss=0.0026230812072753906\n",
      "step 1903: loss=0.20334601402282715\n",
      "step 1904: loss=0.008885383605957031\n",
      "step 1905: loss=0.25542783737182617\n",
      "step 1906: loss=0.03086256980895996\n",
      "step 1907: loss=0.2506754398345947\n",
      "step 1908: loss=0.0007014274597167969\n",
      "step 1909: loss=0.0051727294921875\n",
      "step 1910: loss=0.11113548278808594\n",
      "step 1911: loss=0.04044389724731445\n",
      "step 1912: loss=0.0004215240478515625\n",
      "step 1913: loss=0.019912242889404297\n",
      "step 1914: loss=1.4262325763702393\n",
      "step 1915: loss=0.027172088623046875\n",
      "step 1916: loss=0.09154462814331055\n",
      "step 1917: loss=0.010875701904296875\n",
      "step 1918: loss=0.10902118682861328\n",
      "step 1919: loss=0.11564230918884277\n",
      "step 1920: loss=0.06446361541748047\n",
      "step 1921: loss=0.08886265754699707\n",
      "step 1922: loss=0.1756899356842041\n",
      "step 1923: loss=0.023624897003173828\n",
      "step 1924: loss=0.006579399108886719\n",
      "step 1925: loss=0.006858348846435547\n",
      "step 1926: loss=0.00543212890625\n",
      "step 1927: loss=0.0240020751953125\n",
      "step 1928: loss=0.6833474636077881\n",
      "step 1929: loss=0.0577082633972168\n",
      "step 1930: loss=0.017292022705078125\n",
      "step 1931: loss=0.040328025817871094\n",
      "step 1932: loss=0.005287647247314453\n",
      "step 1933: loss=0.012613296508789062\n",
      "step 1934: loss=0.004623889923095703\n",
      "step 1935: loss=0.0011153221130371094\n",
      "step 1936: loss=0.5298681259155273\n",
      "step 1937: loss=0.00030517578125\n",
      "step 1938: loss=0.1638965606689453\n",
      "step 1939: loss=0.34738266468048096\n",
      "step 1940: loss=0.013697147369384766\n",
      "step 1941: loss=0.0030884742736816406\n",
      "step 1942: loss=0.13126540184020996\n",
      "step 1943: loss=0.0009889602661132812\n",
      "step 1944: loss=0.000640869140625\n",
      "step 1945: loss=2.8810253143310547\n",
      "step 1946: loss=0.001873016357421875\n",
      "step 1947: loss=0.09017419815063477\n",
      "step 1948: loss=0.0250396728515625\n",
      "step 1949: loss=0.15851640701293945\n",
      "step 1950: loss=0.03221940994262695\n",
      "step 1951: loss=0.031409263610839844\n",
      "step 1952: loss=0.0022249221801757812\n",
      "step 1953: loss=0.0010695457458496094\n",
      "step 1954: loss=0.07157421112060547\n",
      "step 1955: loss=0.00015926361083984375\n",
      "step 1956: loss=0.019190311431884766\n",
      "step 1957: loss=0.0012798309326171875\n",
      "step 1958: loss=0.17901945114135742\n",
      "step 1959: loss=0.09918451309204102\n",
      "step 1960: loss=0.12117528915405273\n",
      "step 1961: loss=0.26049208641052246\n",
      "step 1962: loss=0.019834041595458984\n",
      "step 1963: loss=0.000431060791015625\n",
      "step 1964: loss=0.04895210266113281\n",
      "step 1965: loss=0.29978060722351074\n",
      "step 1966: loss=0.20575571060180664\n",
      "step 1967: loss=0.025646209716796875\n",
      "step 1968: loss=0.1696157455444336\n",
      "step 1969: loss=0.008367061614990234\n",
      "step 1970: loss=0.2836923599243164\n",
      "step 1971: loss=0.0077667236328125\n",
      "step 1972: loss=0.0913543701171875\n",
      "step 1973: loss=0.03208160400390625\n",
      "step 1974: loss=0.25241875648498535\n",
      "step 1975: loss=0.6923378705978394\n",
      "step 1976: loss=0.0060482025146484375\n",
      "step 1977: loss=0.04904794692993164\n",
      "step 1978: loss=0.5755470991134644\n",
      "step 1979: loss=1.2452046871185303\n",
      "step 1980: loss=0.019805908203125\n",
      "step 1981: loss=0.1442852020263672\n",
      "step 1982: loss=0.1641380786895752\n",
      "step 1983: loss=0.22363948822021484\n",
      "step 1984: loss=0.021692276000976562\n",
      "step 1985: loss=0.0656898021697998\n",
      "step 1986: loss=0.028370380401611328\n",
      "step 1987: loss=0.016373634338378906\n",
      "step 1988: loss=0.04315376281738281\n",
      "step 1989: loss=0.00011348724365234375\n",
      "step 1990: loss=0.03131103515625\n",
      "step 1991: loss=0.02651691436767578\n",
      "step 1992: loss=0.13248801231384277\n",
      "step 1993: loss=0.0017499923706054688\n",
      "step 1994: loss=0.18218255043029785\n",
      "step 1995: loss=0.0590970516204834\n",
      "step 1996: loss=0.0019855499267578125\n",
      "step 1997: loss=0.0009751319885253906\n",
      "step 1998: loss=0.1408083438873291\n",
      "step 1999: loss=0.04166698455810547\n",
      "step 2000: loss=0.0007457733154296875\n",
      "Mean loss        0.37612873\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
     ]
    }
   ],
   "source": [
    "extra_steps = 1000\n",
    "key = jax.random.key(1)\n",
    "params = train_mlp(\n",
    "    train_data=train_data, \n",
    "    optimizer=optimizer, \n",
    "    training_state=restored_state,\n",
    "    )\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092763fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223949670791626\n",
      "step 2: loss=1.4270418882369995\n",
      "step 3: loss=1.1914063692092896\n",
      "step 4: loss=0.8947293162345886\n",
      "step 5: loss=0.8820065259933472\n",
      "step 6: loss=0.6795811057090759\n",
      "step 7: loss=0.6839590072631836\n",
      "step 8: loss=0.7954467535018921\n",
      "step 9: loss=0.7858410477638245\n",
      "step 10: loss=0.7066607475280762\n",
      "step 11: loss=0.508007824420929\n",
      "step 12: loss=0.6086064577102661\n",
      "step 13: loss=0.6874796152114868\n",
      "step 14: loss=0.6014252305030823\n",
      "step 15: loss=0.6547794938087463\n",
      "step 16: loss=0.4428560435771942\n",
      "step 17: loss=0.35594701766967773\n",
      "step 18: loss=0.3600447475910187\n",
      "step 19: loss=0.4020954668521881\n",
      "step 20: loss=0.3246818482875824\n",
      "Mean loss        0.5156536\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1775729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=7.921195030212402\n",
      "step 2: loss=22.573795318603516\n",
      "step 3: loss=15.713532447814941\n",
      "step 4: loss=14.277660369873047\n",
      "step 5: loss=4.392229080200195\n",
      "step 6: loss=2.0070979595184326\n",
      "step 7: loss=1.3166155815124512\n",
      "step 8: loss=1.6341989040374756\n",
      "step 9: loss=1.6222692728042603\n",
      "step 10: loss=1.4166510105133057\n",
      "step 11: loss=1.3110309839248657\n",
      "step 12: loss=1.5402616262435913\n",
      "step 13: loss=1.504196286201477\n",
      "step 14: loss=1.6209816932678223\n",
      "step 15: loss=1.6276339292526245\n",
      "step 16: loss=1.2568767070770264\n",
      "step 17: loss=0.8523489236831665\n",
      "step 18: loss=1.3203682899475098\n",
      "step 19: loss=1.4638503789901733\n",
      "step 20: loss=1.3379669189453125\n",
      "Mean loss        1.8293804\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 3 7 8 3 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6626627",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2 # for all subsequent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f508a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223958015441895\n",
      "step 2: loss=1.4270424842834473\n",
      "step 3: loss=1.191407322883606\n",
      "step 4: loss=0.8947314023971558\n",
      "step 5: loss=0.8820069432258606\n",
      "step 6: loss=0.6795834898948669\n",
      "step 7: loss=0.6839614510536194\n",
      "step 8: loss=0.7954472303390503\n",
      "step 9: loss=0.7858410477638245\n",
      "step 10: loss=0.7066599726676941\n",
      "step 11: loss=0.5080094337463379\n",
      "step 12: loss=0.6086087822914124\n",
      "step 13: loss=0.687477707862854\n",
      "step 14: loss=0.6014255285263062\n",
      "step 15: loss=0.6547796726226807\n",
      "step 16: loss=0.44285672903060913\n",
      "step 17: loss=0.35594889521598816\n",
      "step 18: loss=0.36004677414894104\n",
      "step 19: loss=0.4020947813987732\n",
      "step 20: loss=0.3246852457523346\n",
      "Mean loss        0.51565444\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.adamw(learning_rate, weight_decay=1e-4)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223958015441895\n",
      "step 2: loss=1.4270424842834473\n",
      "step 3: loss=1.1914074420928955\n",
      "step 4: loss=0.8947315812110901\n",
      "step 5: loss=0.8820069432258606\n",
      "step 6: loss=0.6795834302902222\n",
      "step 7: loss=0.6839614510536194\n",
      "step 8: loss=0.795447051525116\n",
      "step 9: loss=0.7858409881591797\n",
      "step 10: loss=0.7066601514816284\n",
      "step 11: loss=0.5080092549324036\n",
      "step 12: loss=0.6086088418960571\n",
      "step 13: loss=0.687477707862854\n",
      "step 14: loss=0.6014255881309509\n",
      "step 15: loss=0.6547797918319702\n",
      "step 16: loss=0.44285669922828674\n",
      "step 17: loss=0.3559488356113434\n",
      "step 18: loss=0.3600468337535858\n",
      "step 19: loss=0.4020947813987732\n",
      "step 20: loss=0.3246852159500122\n",
      "Mean loss        0.5156544\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "mask_fn = lambda p: jax.tree_util.tree_map(lambda x: x.ndim != 1, p) # mask biases\n",
    "optimizer = optax.adamw(learning_rate, weight_decay=1e-4, mask=mask_fn)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5964dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.329394578933716\n",
      "step 2: loss=2.111168384552002\n",
      "step 3: loss=2.098027229309082\n",
      "step 4: loss=2.115138530731201\n",
      "step 5: loss=2.1225409507751465\n",
      "step 6: loss=1.9438470602035522\n",
      "step 7: loss=1.989481806755066\n",
      "step 8: loss=1.8919346332550049\n",
      "step 9: loss=1.8656792640686035\n",
      "step 10: loss=1.854970932006836\n",
      "step 11: loss=1.867348551750183\n",
      "step 12: loss=1.8192609548568726\n",
      "step 13: loss=1.8986468315124512\n",
      "step 14: loss=1.8100045919418335\n",
      "step 15: loss=1.870030164718628\n",
      "step 16: loss=1.7674648761749268\n",
      "step 17: loss=1.6689090728759766\n",
      "step 18: loss=1.7513816356658936\n",
      "step 19: loss=1.673409342765808\n",
      "step 20: loss=1.7518361806869507\n",
      "Mean loss        1.7164931\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "lr = optax.cosine_decay_schedule(init_value=1e-3, decay_steps=20)\n",
    "optimizer = optax.adamw(lr, weight_decay=1e-4, mask=mask_fn)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd672339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.4460508823394775\n",
      "step 2: loss=2.2159481048583984\n",
      "step 3: loss=2.226898193359375\n",
      "step 4: loss=2.282470703125\n",
      "step 5: loss=2.2132441997528076\n",
      "step 6: loss=2.1017580032348633\n",
      "step 7: loss=2.186561346054077\n",
      "step 8: loss=2.037569284439087\n",
      "step 9: loss=2.0478765964508057\n",
      "step 10: loss=2.033282995223999\n",
      "step 11: loss=2.013596534729004\n",
      "step 12: loss=1.9634274244308472\n",
      "step 13: loss=2.019657850265503\n",
      "step 14: loss=1.8717783689498901\n",
      "step 15: loss=1.937285304069519\n",
      "step 16: loss=1.8528590202331543\n",
      "step 17: loss=1.723109245300293\n",
      "step 18: loss=1.7739412784576416\n",
      "step 19: loss=1.659662127494812\n",
      "step 20: loss=1.698677659034729\n",
      "Mean loss        1.7045716\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.contrib.muon(learning_rate=learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
