{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bffd5b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Key differences from JAX implementation:**  \n",
    "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
    "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
    "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data = flatten_image_for_mlp(data)\n",
    "        if subsample_size > 0:\n",
    "            data = data[:subsample_size]\n",
    "        data_jax[data_type] = data\n",
    "\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9813eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    layer_sizes: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, activations):\n",
    "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
    "            activations = nn.Dense(\n",
    "                layer_size,\n",
    "                kernel_init=nn.initializers.normal(0.1),\n",
    "                bias_init=nn.initializers.normal(0.1)\n",
    "            )(activations)\n",
    "\n",
    "            if layer_number != (len(self.layer_sizes) - 1):\n",
    "                activations = nn.relu(activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_network_params(model, input_layer_size, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
    "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "244a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
    "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
    "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fa6224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def take_training_step(training_state, images, labels):\n",
    "    \"\"\"\n",
    "    Single training step \n",
    "    The model and optimiser are passed in the training state\n",
    "    returns a training state\n",
    "    \"\"\"\n",
    "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
    "    grads_by_params = grads_by_params_fn(\n",
    "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
    "        training_state.apply_fn,\n",
    "        images,\n",
    "        labels,\n",
    "    )\n",
    "    return training_state.apply_gradients(grads=grads_by_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4d625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(images, labels, n_batches):\n",
    "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
    "    n_samples = len(images)\n",
    "    assert len(images) == len(labels)\n",
    "    assert n_samples >= n_batches\n",
    "    assert n_batches > 0\n",
    "    n_samples_per_batch = n_samples // n_batches\n",
    "    start = 0\n",
    "    end = n_samples_per_batch\n",
    "    while end <= n_samples: \n",
    "        yield (images[start:end], labels[start:end])\n",
    "        start += n_samples_per_batch\n",
    "        end += n_samples_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(images, labels, n_steps, layer_sizes, optimizer, key):\n",
    "    \"\"\"\n",
    "    The training state ('state') is an instance of TrainState that holds:\n",
    "    - apply_fn: the model's apply function, used for forward passes\n",
    "    - params: the parameters of the neural network\n",
    "    - tx: the optimizers (Optax transformation) for parameter updates\n",
    "    - opt_state: the state of the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer_size = layer_sizes[0]\n",
    "    network_layer_sizes = layer_sizes[1:]\n",
    "    model = MLP(layer_sizes=network_layer_sizes)\n",
    "    params = initialise_network_params(model, input_layer_size, key)\n",
    "    apply_fn = model.apply\n",
    "    \n",
    "    training_state = train_state.TrainState.create(\n",
    "        apply_fn=apply_fn, \n",
    "        params=params, \n",
    "        tx=optimizer,\n",
    "        )\n",
    "\n",
    "    step = 1\n",
    "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
    "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
    "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
    "        print(f\"step {step}: loss={loss}\")\n",
    "        step += 1\n",
    "\n",
    "    return training_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4575e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
    "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(train_data, optimizer):\n",
    "    n_steps = 20\n",
    "    layer_sizes = [784, 128, 10]\n",
    "    key = jax.random.key(0)\n",
    "    final_params = run_training(\n",
    "        train_data[\"image\"], \n",
    "        train_data[\"label\"], \n",
    "        n_steps, \n",
    "        layer_sizes, \n",
    "        optimizer,\n",
    "        key,\n",
    "        )\n",
    "    return final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_sizes(params):\n",
    "    layer_sizes = []\n",
    "    for layer, layer_params in enumerate(params.values()):\n",
    "        if layer == 0:\n",
    "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
    "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
    "        else:\n",
    "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
    "    return layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlp(test_data, params, n_examples=10):\n",
    "    layer_sizes = extract_layer_sizes(params)\n",
    "    model = MLP(layer_sizes=layer_sizes[1:])\n",
    "    apply_fn = model.apply\n",
    "\n",
    "    images = test_data[\"image\"]\n",
    "    labels = test_data[\"label\"]\n",
    "\n",
    "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
    "    example_images = images[:n_examples]\n",
    "    example_labels = labels[:n_examples]\n",
    "    logits = apply_fn({\"params\": params}, example_images)\n",
    "    example_predictions = jnp.argmax(logits, axis=1)\n",
    "\n",
    "    print(\"Mean loss       \", mean_loss)\n",
    "    print(\"True labels:    \", example_labels)\n",
    "    print(\"Predictions:    \", example_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876ad27",
   "metadata": {},
   "source": [
    "1. Learning rate decay\n",
    "2. Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b492873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.329394817352295\n",
      "step 2: loss=2.110353708267212\n",
      "step 3: loss=2.0951199531555176\n",
      "step 4: loss=2.107095956802368\n",
      "step 5: loss=2.1107473373413086\n",
      "step 6: loss=1.921804428100586\n",
      "step 7: loss=1.9533743858337402\n",
      "step 8: loss=1.84724760055542\n",
      "step 9: loss=1.7940917015075684\n",
      "step 10: loss=1.764140248298645\n",
      "step 11: loss=1.751694917678833\n",
      "step 12: loss=1.6859773397445679\n",
      "step 13: loss=1.7524176836013794\n",
      "step 14: loss=1.642892599105835\n",
      "step 15: loss=1.6448626518249512\n",
      "step 16: loss=1.460929274559021\n",
      "step 17: loss=1.3634432554244995\n",
      "step 18: loss=1.413408875465393\n",
      "step 19: loss=1.2928463220596313\n",
      "step 20: loss=1.3445155620574951\n",
      "Mean loss        1.3096466\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "092763fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223949670791626\n",
      "step 2: loss=1.4270418882369995\n",
      "step 3: loss=1.1914063692092896\n",
      "step 4: loss=0.8947293162345886\n",
      "step 5: loss=0.8820065259933472\n",
      "step 6: loss=0.6795811057090759\n",
      "step 7: loss=0.6839590072631836\n",
      "step 8: loss=0.7954467535018921\n",
      "step 9: loss=0.7858410477638245\n",
      "step 10: loss=0.7066607475280762\n",
      "step 11: loss=0.508007824420929\n",
      "step 12: loss=0.6086064577102661\n",
      "step 13: loss=0.6874796152114868\n",
      "step 14: loss=0.6014252305030823\n",
      "step 15: loss=0.6547794938087463\n",
      "step 16: loss=0.4428560435771942\n",
      "step 17: loss=0.35594701766967773\n",
      "step 18: loss=0.3600447475910187\n",
      "step 19: loss=0.4020954668521881\n",
      "step 20: loss=0.3246818482875824\n",
      "Mean loss        0.5156536\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d1775729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=7.921195030212402\n",
      "step 2: loss=22.573795318603516\n",
      "step 3: loss=15.713532447814941\n",
      "step 4: loss=14.277660369873047\n",
      "step 5: loss=4.392229080200195\n",
      "step 6: loss=2.0070979595184326\n",
      "step 7: loss=1.3166155815124512\n",
      "step 8: loss=1.6341989040374756\n",
      "step 9: loss=1.6222692728042603\n",
      "step 10: loss=1.4166510105133057\n",
      "step 11: loss=1.3110309839248657\n",
      "step 12: loss=1.5402616262435913\n",
      "step 13: loss=1.504196286201477\n",
      "step 14: loss=1.6209816932678223\n",
      "step 15: loss=1.6276339292526245\n",
      "step 16: loss=1.2568767070770264\n",
      "step 17: loss=0.8523489236831665\n",
      "step 18: loss=1.3203682899475098\n",
      "step 19: loss=1.4638503789901733\n",
      "step 20: loss=1.3379669189453125\n",
      "Mean loss        1.8293804\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 3 7 8 3 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b6626627",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2 # for all subsequent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1f508a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223958015441895\n",
      "step 2: loss=1.4270424842834473\n",
      "step 3: loss=1.191407322883606\n",
      "step 4: loss=0.8947314023971558\n",
      "step 5: loss=0.8820069432258606\n",
      "step 6: loss=0.6795834898948669\n",
      "step 7: loss=0.6839614510536194\n",
      "step 8: loss=0.7954472303390503\n",
      "step 9: loss=0.7858410477638245\n",
      "step 10: loss=0.7066599726676941\n",
      "step 11: loss=0.5080094337463379\n",
      "step 12: loss=0.6086087822914124\n",
      "step 13: loss=0.687477707862854\n",
      "step 14: loss=0.6014255285263062\n",
      "step 15: loss=0.6547796726226807\n",
      "step 16: loss=0.44285672903060913\n",
      "step 17: loss=0.35594889521598816\n",
      "step 18: loss=0.36004677414894104\n",
      "step 19: loss=0.4020947813987732\n",
      "step 20: loss=0.3246852457523346\n",
      "Mean loss        0.51565444\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.adamw(learning_rate, weight_decay=1e-4)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8017b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223958015441895\n",
      "step 2: loss=1.4270424842834473\n",
      "step 3: loss=1.1914074420928955\n",
      "step 4: loss=0.8947315812110901\n",
      "step 5: loss=0.8820069432258606\n",
      "step 6: loss=0.6795834302902222\n",
      "step 7: loss=0.6839614510536194\n",
      "step 8: loss=0.795447051525116\n",
      "step 9: loss=0.7858409881591797\n",
      "step 10: loss=0.7066601514816284\n",
      "step 11: loss=0.5080092549324036\n",
      "step 12: loss=0.6086088418960571\n",
      "step 13: loss=0.687477707862854\n",
      "step 14: loss=0.6014255881309509\n",
      "step 15: loss=0.6547797918319702\n",
      "step 16: loss=0.44285669922828674\n",
      "step 17: loss=0.3559488356113434\n",
      "step 18: loss=0.3600468337535858\n",
      "step 19: loss=0.4020947813987732\n",
      "step 20: loss=0.3246852159500122\n",
      "Mean loss        0.5156544\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "mask_fn = lambda p: jax.tree_util.tree_map(lambda x: x.ndim != 1, p) # mask biases\n",
    "optimizer = optax.adamw(learning_rate, weight_decay=1e-4, mask=mask_fn)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f5964dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.329394578933716\n",
      "step 2: loss=2.111168384552002\n",
      "step 3: loss=2.098027229309082\n",
      "step 4: loss=2.115138530731201\n",
      "step 5: loss=2.1225409507751465\n",
      "step 6: loss=1.9438470602035522\n",
      "step 7: loss=1.989481806755066\n",
      "step 8: loss=1.8919346332550049\n",
      "step 9: loss=1.8656792640686035\n",
      "step 10: loss=1.854970932006836\n",
      "step 11: loss=1.867348551750183\n",
      "step 12: loss=1.8192609548568726\n",
      "step 13: loss=1.8986468315124512\n",
      "step 14: loss=1.8100045919418335\n",
      "step 15: loss=1.870030164718628\n",
      "step 16: loss=1.7674648761749268\n",
      "step 17: loss=1.6689090728759766\n",
      "step 18: loss=1.7513816356658936\n",
      "step 19: loss=1.673409342765808\n",
      "step 20: loss=1.7518361806869507\n",
      "Mean loss        1.7164931\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "lr = optax.cosine_decay_schedule(init_value=1e-3, decay_steps=20)\n",
    "optimizer = optax.adamw(lr, weight_decay=1e-4, mask=mask_fn)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "dd672339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.4460508823394775\n",
      "step 2: loss=2.2159481048583984\n",
      "step 3: loss=2.226898193359375\n",
      "step 4: loss=2.282470703125\n",
      "step 5: loss=2.2132441997528076\n",
      "step 6: loss=2.1017580032348633\n",
      "step 7: loss=2.186561346054077\n",
      "step 8: loss=2.037569284439087\n",
      "step 9: loss=2.0478765964508057\n",
      "step 10: loss=2.033282995223999\n",
      "step 11: loss=2.013596534729004\n",
      "step 12: loss=1.9634274244308472\n",
      "step 13: loss=2.019657850265503\n",
      "step 14: loss=1.8717783689498901\n",
      "step 15: loss=1.937285304069519\n",
      "step 16: loss=1.8528590202331543\n",
      "step 17: loss=1.723109245300293\n",
      "step 18: loss=1.7739412784576416\n",
      "step 19: loss=1.659662127494812\n",
      "step 20: loss=1.698677659034729\n",
      "Mean loss        1.7045716\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.contrib.muon(learning_rate=learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
