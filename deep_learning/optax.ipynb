{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bffd5b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Key differences from JAX implementation:**  \n",
    "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
    "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
    "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data_jax[data_type] = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data_jax[data_type] = flatten_image_for_mlp(data_jax[data_type])\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e662dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (60000, 784)\n",
      "Labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "prepared_data = {key: prepare_data(value) for key, value in mnist_data.items()}\n",
    "train_data = prepared_data[\"train\"]\n",
    "\n",
    "images_train = train_data[\"image\"]\n",
    "labels_train = train_data[\"label\"]\n",
    "\n",
    "print(\"Images shape:\", images_train.shape)\n",
    "print(\"Labels shape:\", labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9813eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    layer_sizes: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, activations):\n",
    "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
    "            activations = nn.Dense(\n",
    "                layer_size,\n",
    "                kernel_init=nn.initializers.normal(0.1),\n",
    "                bias_init=nn.initializers.normal(0.1)\n",
    "            )(activations)\n",
    "\n",
    "            if layer_number != (len(self.layer_sizes) - 1):\n",
    "                activations = nn.relu(activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_network_params(model, input_layer_size, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
    "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "244a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
    "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
    "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa6224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def take_training_step(training_state, images, labels):\n",
    "    \"\"\"\n",
    "    Single training step \n",
    "    The model and optimiser are passed in the training state\n",
    "    returns a training state\n",
    "    \"\"\"\n",
    "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
    "    grads_by_params = grads_by_params_fn(\n",
    "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
    "        training_state.apply_fn,\n",
    "        images,\n",
    "        labels,\n",
    "    )\n",
    "    return training_state.apply_gradients(grads=grads_by_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4d625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(images, labels, n_batches):\n",
    "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
    "    n_samples = len(images)\n",
    "    assert len(images) == len(labels)\n",
    "    assert n_samples >= n_batches\n",
    "    assert n_batches >= 0\n",
    "    n_samples_per_batch = n_samples // n_batches\n",
    "    start = 0\n",
    "    end = n_samples_per_batch\n",
    "    while end <= n_samples: \n",
    "        yield (images[start:end], labels[start:end])\n",
    "        start += n_samples_per_batch\n",
    "        end += n_samples_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(images, labels, n_steps, layer_sizes, key, initial_learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    The training state ('state') is an instance of TrainState that holds:\n",
    "    - apply_fn: the model's apply function, used for forward passes\n",
    "    - params: the parameters of the neural network\n",
    "    - tx: the optimizers (Optax transformation) for parameter updates\n",
    "    - opt_state: the state of the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer_size = layer_sizes[0]\n",
    "    network_layer_sizes = layer_sizes[1:]\n",
    "    model = MLP(layer_sizes=network_layer_sizes)\n",
    "    params = initialise_network_params(model, input_layer_size, key)\n",
    "    apply_fn = model.apply\n",
    "    \n",
    "    optimizer = optax.adam(initial_learning_rate)\n",
    "    \n",
    "    training_state = train_state.TrainState.create(apply_fn=apply_fn, params=params, tx=optimizer)\n",
    "\n",
    "    step = 0\n",
    "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
    "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
    "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
    "        print(f\"step {step}: loss={loss}\")\n",
    "        step += 1\n",
    "\n",
    "    return training_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f265eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_set_size = 100\n",
    "images = images_train[:trial_set_size]\n",
    "labels = labels_train[:trial_set_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=2.1574771404266357\n",
      "step 1: loss=2.396851062774658\n",
      "step 2: loss=2.1652634143829346\n",
      "step 3: loss=2.0755467414855957\n",
      "step 4: loss=1.9482983350753784\n"
     ]
    }
   ],
   "source": [
    "n_steps = 5\n",
    "layer_sizes = [784, 128, 10]\n",
    "key = jax.random.key(0)\n",
    "final_params = run_training(images, labels, n_steps=n_steps, layer_sizes=layer_sizes, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b7d5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:     [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3 0 9 4 9 0\n",
      " 6 8 4 7 2 6 0 3 1 1 7 2 4 4 6 5 1 9 3 2 4 3 4 4 7 5 8 1 1 4 1 5 3 5 8 4 1\n",
      " 1 4 5 3 2 4 1 4 8 1 2 1 9 0 7 6 7 4 4 9 7 5 6 8 4 6]\n",
      "Predictions:     [1 1 4 7 8 1 7 4 1 6 5 1 4 1 3 8 7 4 9 1 4 5 6 2 9 7 1 1 2 9 6 4 4 9 4 4 7\n",
      " 6 1 2 7 1 9 7 2 1 1 9 6 1 4 2 9 1 9 7 9 9 9 4 9 7 7 6 7 1 1 1 7 3 1 1 4 1\n",
      " 1 4 5 1 9 4 1 0 1 1 1 1 4 9 7 6 1 7 9 9 9 4 5 2 1 1]\n",
      "Match:           [False  True False  True  True  True False False  True  True False False\n",
      " False False  True False  True False  True  True False False  True False\n",
      "  True False False False False False False False False  True  True False\n",
      " False  True False False  True False False False False  True  True False\n",
      " False False  True False False  True  True False False False False  True\n",
      " False  True False False False  True False  True False  True False False\n",
      "  True  True  True  True  True False False  True  True False False  True\n",
      " False  True False False  True  True False False False  True False False\n",
      " False False False False]\n",
      "Loss             [1.8369937 1.3738154 2.3659039 1.2163229 1.5227491 1.6644053 1.8822132\n",
      " 2.032792  1.3072567 1.6062896 1.9321811 2.773177  1.7305741 1.908079\n",
      " 1.6027377 1.9490368 1.2384518 1.7350551 1.7458477 1.4829874 2.1223588\n",
      " 2.167745  1.8577945 2.0400758 1.2688496 1.9230553 2.4840755 2.0128176\n",
      " 2.150177  2.0954435 2.3015523 2.1303782 2.62747   1.2789067 1.3813876\n",
      " 2.151094  3.000436  1.8836695 2.5216718 2.1835272 1.2316529 1.9851203\n",
      " 2.3744712 2.278901  2.5968804 1.3833638 1.5266513 2.0297737 2.457388\n",
      " 1.9199667 1.4500232 1.9739356 2.122565  1.5256329 1.5933899 2.4044464\n",
      " 2.8308182 1.8077492 2.5859833 1.4811258 1.72035   1.8402826 2.844078\n",
      " 2.4045289 2.1024241 1.139522  1.8649151 1.4561619 2.7567425 1.8346052\n",
      " 2.7438037 3.5333564 1.2698894 1.6959596 1.724895  0.8610239 1.7981209\n",
      " 2.6237268 2.3453038 1.2700464 1.2965983 1.7555163 2.358637  1.5739381\n",
      " 2.3804522 1.5286454 2.159108  2.1362276 1.7584543 1.8034613 1.8348383\n",
      " 2.0205479 1.6608655 1.568011  1.5500056 2.5253048 2.5418186 2.2373714\n",
      " 2.1156514 2.1605146]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(layer_sizes=layer_sizes[1:])\n",
    "logits = model.apply({\"params\": final_params}, images)\n",
    "loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "predictions = jnp.argmax(logits, axis=1)\n",
    "\n",
    "print(\"True labels:    \", labels)\n",
    "print(\"Predictions:    \", predictions)\n",
    "print(\"Match:          \", predictions == labels)\n",
    "print(\"Loss            \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
