{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "76bffd5b",
      "metadata": {},
      "source": [
        "<small>\n",
        "\n",
        "**Key differences from JAX implementation:**  \n",
        "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
        "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
        "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
        "\n",
        "</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_datasets as tfds\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "558de4e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST from TensorFlow Datasets\n",
        "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0fb40841",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalise(x, x_max=255.0):\n",
        "    return x / x_max\n",
        "\n",
        "def convert_to_jax(data_np, data_type):\n",
        "    if data_type == \"image\":\n",
        "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
        "    elif data_type == \"label\":\n",
        "        data_jax = jnp.array(data_np)\n",
        "    else:\n",
        "        raise ValueError(\"not image or label\")\n",
        "    return data_jax\n",
        "\n",
        "def flatten_image_for_mlp(data_jax):\n",
        "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
        "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
        "    data_flattened = data_jax.reshape(n_batch, -1)\n",
        "    return data_flattened\n",
        "\n",
        "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
        "    data_jax = {}\n",
        "    for data_type, data_tf in data_dict.items():\n",
        "        data_numpy = data_tf.numpy()\n",
        "        data = convert_to_jax(data_numpy, data_type)\n",
        "        if data_type == \"image\":\n",
        "            data = flatten_image_for_mlp(data)\n",
        "        if subsample_size > 0:\n",
        "            data = data[:subsample_size]\n",
        "        data_jax[data_type] = data\n",
        "\n",
        "    return data_jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "244a19b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
        "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
        "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
        "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
        "    return cross_entropy_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fa6224c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_training_step(training_state, images, labels):\n",
        "    \"\"\"\n",
        "    Single training step \n",
        "    The model and optimiser are passed in the training state\n",
        "    returns a training state\n",
        "    \"\"\"\n",
        "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
        "        training_state.apply_fn,\n",
        "        images,\n",
        "        labels,\n",
        "    )\n",
        "    return training_state.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batches(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "27fcb22b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(layer_sizes, optimizer):\n",
        "    layer_part = \"mlp_\" + \"-\".join(str(s) for s in layer_sizes)\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"{layer_part}_{opt_name}\"\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"mlp\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "72c149a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(\n",
        "    images,\n",
        "    labels,\n",
        "    n_steps,\n",
        "    layer_sizes,\n",
        "    optimizer,\n",
        "    checkpoint_manager,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    training_state,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    ): \n",
        "    \"\"\"\n",
        "    The training state ('state') is an instance of TrainState that holds:\n",
        "    - apply_fn: the model's apply function, used for forward passes\n",
        "    - params: the parameters of the neural network\n",
        "    - tx: the optimizers (Optax transformation) for parameter updates\n",
        "    - opt_state: the state of the optimizer\n",
        "    \"\"\"\n",
        "    if training_state is None:\n",
        "        training_state = create_training_state(\n",
        "            layer_sizes,\n",
        "            optimizer,\n",
        "            key,\n",
        "            use_lowrank=use_lowrank,\n",
        "            rank=rank,\n",
        "        )\n",
        "\n",
        "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
        "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
        "        step = training_state.step\n",
        "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
        "        print(f\"step {step}: loss={loss}\")\n",
        "        if step == 1 or step % steps_per_save == 0:\n",
        "            step_dir = step\n",
        "            checkpoint_manager.save(\n",
        "                step_dir,\n",
        "                args=ocp.args.StandardSave(training_state)\n",
        "                )\n",
        "\n",
        "    return training_state.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "852ef6e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    n_steps=10**3,\n",
        "    steps_per_save=100,\n",
        "    training_state=None,\n",
        "    key=jax.random.key(0),\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=(784, 128, 10),\n",
        "):\n",
        "    layer_sizes = list(layer_sizes)\n",
        "    experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        experiment_name = experiment_name + f\"_lowrank-r{rank}\"\n",
        "\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    final_params = run_training(\n",
        "        images=train_data[\"image\"],\n",
        "        labels=train_data[\"label\"],\n",
        "        n_steps=n_steps,\n",
        "        layer_sizes=layer_sizes,\n",
        "        optimizer=optimizer,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        training_state=training_state,\n",
        "        use_lowrank=use_lowrank,\n",
        "        rank=rank,\n",
        "    )\n",
        "    return final_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "dca913e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_layer_sizes(params):\n",
        "    layer_sizes = []\n",
        "    for layer, layer_params in enumerate(params.values()):\n",
        "        if layer == 0:\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
        "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
        "        else:\n",
        "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
        "    return layer_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1b7d5d50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mlp(\n",
        "    test_data,\n",
        "    params,\n",
        "    n_examples=10,\n",
        "    use_lowrank: bool = False,\n",
        "    rank: int | None = None,\n",
        "    layer_sizes=None,\n",
        "):\n",
        "    images = test_data[\"image\"]\n",
        "    labels = test_data[\"label\"]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if layer_sizes is None:\n",
        "            raise ValueError(\"layer_sizes must be provided when use_lowrank=True\")\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=layer_sizes[1:], rank=rank)\n",
        "    else:\n",
        "        layer_sizes = extract_layer_sizes(params)\n",
        "        model = MLP(layer_sizes=layer_sizes[1:])\n",
        "\n",
        "    apply_fn = model.apply\n",
        "\n",
        "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
        "    example_images = images[:n_examples]\n",
        "    example_labels = labels[:n_examples]\n",
        "    logits = apply_fn({\"params\": params}, example_images)\n",
        "    example_predictions = jnp.argmax(logits, axis=1)\n",
        "\n",
        "    prefix = \"[low-rank] \" if use_lowrank else \"\"\n",
        "    print(prefix + \"Mean loss       \", mean_loss)\n",
        "    print(prefix + \"True labels:    \", example_labels)\n",
        "    print(prefix + \"Predictions:    \", example_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b876ad27",
      "metadata": {},
      "source": [
        "1. Learning rate decay\n",
        "2. Weight decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
        "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "b492873c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=2.2421865463256836\n",
            "step 2: loss=1.4549814462661743\n",
            "step 3: loss=3.0767364501953125\n",
            "step 4: loss=1.6018766164779663\n",
            "step 5: loss=1.922335147857666\n",
            "step 6: loss=1.5518207550048828\n",
            "step 7: loss=2.2047572135925293\n",
            "step 8: loss=2.28387188911438\n",
            "step 9: loss=1.3409204483032227\n",
            "step 10: loss=2.344090700149536\n",
            "step 11: loss=2.6371214389801025\n",
            "step 12: loss=3.4621236324310303\n",
            "step 13: loss=1.713283658027649\n",
            "step 14: loss=1.9781702756881714\n",
            "step 15: loss=3.3416006565093994\n",
            "step 16: loss=2.9397127628326416\n",
            "step 17: loss=1.0316027402877808\n",
            "step 18: loss=2.92775821685791\n",
            "step 19: loss=2.7445106506347656\n",
            "step 20: loss=1.4158217906951904\n",
            "step 21: loss=1.9610295295715332\n",
            "step 22: loss=1.9986664056777954\n",
            "step 23: loss=1.672990083694458\n",
            "step 24: loss=2.457414388656616\n",
            "step 25: loss=1.5557224750518799\n",
            "step 26: loss=2.3051018714904785\n",
            "step 27: loss=2.603790044784546\n",
            "step 28: loss=1.988590955734253\n",
            "step 29: loss=2.3711462020874023\n",
            "step 30: loss=1.6227275133132935\n",
            "step 31: loss=1.862285852432251\n",
            "step 32: loss=2.1689186096191406\n",
            "step 33: loss=1.754330039024353\n",
            "step 34: loss=1.313774824142456\n",
            "step 35: loss=1.7799018621444702\n",
            "step 36: loss=1.6797168254852295\n",
            "step 37: loss=2.9055237770080566\n",
            "step 38: loss=1.5091187953948975\n",
            "step 39: loss=2.635629892349243\n",
            "step 40: loss=1.9036000967025757\n",
            "step 41: loss=0.8337869644165039\n",
            "step 42: loss=2.479337453842163\n",
            "step 43: loss=2.0102226734161377\n",
            "step 44: loss=1.8198356628417969\n",
            "step 45: loss=2.361449956893921\n",
            "step 46: loss=1.0358593463897705\n",
            "step 47: loss=2.0157792568206787\n",
            "step 48: loss=1.810377597808838\n",
            "step 49: loss=3.01554536819458\n",
            "step 50: loss=1.4894685745239258\n",
            "step 51: loss=1.1730490922927856\n",
            "step 52: loss=1.2065200805664062\n",
            "step 53: loss=3.256063938140869\n",
            "step 54: loss=1.3365157842636108\n",
            "step 55: loss=1.2758262157440186\n",
            "step 56: loss=2.6482057571411133\n",
            "step 57: loss=2.540787696838379\n",
            "step 58: loss=1.3128734827041626\n",
            "step 59: loss=2.506495714187622\n",
            "step 60: loss=1.0716583728790283\n",
            "step 61: loss=0.8002644777297974\n",
            "step 62: loss=1.215010404586792\n",
            "step 63: loss=3.9132957458496094\n",
            "step 64: loss=2.513690233230591\n",
            "step 65: loss=2.3943910598754883\n",
            "step 66: loss=0.41791653633117676\n",
            "step 67: loss=0.9733765125274658\n",
            "step 68: loss=0.6354427337646484\n",
            "step 69: loss=3.076798439025879\n",
            "step 70: loss=1.3095070123672485\n",
            "step 71: loss=3.296731948852539\n",
            "step 72: loss=3.5359928607940674\n",
            "step 73: loss=0.5057768821716309\n",
            "step 74: loss=1.0847030878067017\n",
            "step 75: loss=0.8231465816497803\n",
            "step 76: loss=0.12777495384216309\n",
            "step 77: loss=2.528164863586426\n",
            "step 78: loss=2.6920502185821533\n",
            "step 79: loss=1.4572802782058716\n",
            "step 80: loss=0.3264431953430176\n",
            "step 81: loss=0.3224916458129883\n",
            "step 82: loss=0.880436897277832\n",
            "step 83: loss=1.6871346235275269\n",
            "step 84: loss=0.7689778804779053\n",
            "step 85: loss=2.236729621887207\n",
            "step 86: loss=0.39814305305480957\n",
            "step 87: loss=2.505406141281128\n",
            "step 88: loss=1.016530990600586\n",
            "step 89: loss=1.1767767667770386\n",
            "step 90: loss=1.6118593215942383\n",
            "step 91: loss=1.3020222187042236\n",
            "step 92: loss=1.0679250955581665\n",
            "step 93: loss=0.22047114372253418\n",
            "step 94: loss=1.8968682289123535\n",
            "step 95: loss=0.7754878997802734\n",
            "step 96: loss=2.2688071727752686\n",
            "step 97: loss=1.6609830856323242\n",
            "step 98: loss=1.714942216873169\n",
            "step 99: loss=0.8860684633255005\n",
            "step 100: loss=1.043237566947937\n",
            "step 101: loss=1.6429299116134644\n",
            "step 102: loss=1.3760747909545898\n",
            "step 103: loss=2.3424899578094482\n",
            "step 104: loss=0.7394161224365234\n",
            "step 105: loss=0.8303945064544678\n",
            "step 106: loss=1.6889162063598633\n",
            "step 107: loss=1.849713921546936\n",
            "step 108: loss=0.8009697198867798\n",
            "step 109: loss=1.9210385084152222\n",
            "step 110: loss=1.5583198070526123\n",
            "step 111: loss=0.6788965463638306\n",
            "step 112: loss=0.4137570858001709\n",
            "step 113: loss=1.6254088878631592\n",
            "step 114: loss=1.488877534866333\n",
            "step 115: loss=0.2103595733642578\n",
            "step 116: loss=0.9437799453735352\n",
            "step 117: loss=1.3162198066711426\n",
            "step 118: loss=0.7798060178756714\n",
            "step 119: loss=0.9784126281738281\n",
            "step 120: loss=1.2214597463607788\n",
            "step 121: loss=3.1054039001464844\n",
            "step 122: loss=1.6958223581314087\n",
            "step 123: loss=2.7126426696777344\n",
            "step 124: loss=0.5880532264709473\n",
            "step 125: loss=0.2147374153137207\n",
            "step 126: loss=1.3695894479751587\n",
            "step 127: loss=1.7903401851654053\n",
            "step 128: loss=0.8010847568511963\n",
            "step 129: loss=0.47148585319519043\n",
            "step 130: loss=0.42186427116394043\n",
            "step 131: loss=0.919746994972229\n",
            "step 132: loss=2.370388984680176\n",
            "step 133: loss=0.7040364742279053\n",
            "step 134: loss=0.6195166110992432\n",
            "step 135: loss=0.8623490333557129\n",
            "step 136: loss=0.2626211643218994\n",
            "step 137: loss=0.47997236251831055\n",
            "step 138: loss=0.6141399145126343\n",
            "step 139: loss=1.0177688598632812\n",
            "step 140: loss=1.6761692762374878\n",
            "step 141: loss=0.37014293670654297\n",
            "step 142: loss=2.50685977935791\n",
            "step 143: loss=0.473976731300354\n",
            "step 144: loss=1.8222389221191406\n",
            "step 145: loss=0.10611176490783691\n",
            "step 146: loss=1.3444867134094238\n",
            "step 147: loss=1.890256643295288\n",
            "step 148: loss=2.019212007522583\n",
            "step 149: loss=0.708979606628418\n",
            "step 150: loss=0.3448455333709717\n",
            "step 151: loss=1.4451547861099243\n",
            "step 152: loss=0.887959361076355\n",
            "step 153: loss=1.6465024948120117\n",
            "step 154: loss=1.2734510898590088\n",
            "step 155: loss=0.8892346620559692\n",
            "step 156: loss=0.8326750993728638\n",
            "step 157: loss=1.0322668552398682\n",
            "step 158: loss=1.7533605098724365\n",
            "step 159: loss=0.9787423610687256\n",
            "step 160: loss=1.5738203525543213\n",
            "step 161: loss=0.48970115184783936\n",
            "step 162: loss=0.15803837776184082\n",
            "step 163: loss=0.7275893688201904\n",
            "step 164: loss=0.5339555740356445\n",
            "step 165: loss=1.5419628620147705\n",
            "step 166: loss=0.4611380100250244\n",
            "step 167: loss=2.06067156791687\n",
            "step 168: loss=2.2312281131744385\n",
            "step 169: loss=0.656808614730835\n",
            "step 170: loss=0.4211156368255615\n",
            "step 171: loss=0.1600961685180664\n",
            "step 172: loss=0.6779335737228394\n",
            "step 173: loss=0.1400458812713623\n",
            "step 174: loss=0.7043989896774292\n",
            "step 175: loss=2.0440971851348877\n",
            "step 176: loss=0.5685373544692993\n",
            "step 177: loss=0.7488729953765869\n",
            "step 178: loss=0.2943863868713379\n",
            "step 179: loss=0.2867746353149414\n",
            "step 180: loss=2.0111541748046875\n",
            "step 181: loss=0.5551998615264893\n",
            "step 182: loss=0.42223358154296875\n",
            "step 183: loss=0.4737052917480469\n",
            "step 184: loss=2.3058722019195557\n",
            "step 185: loss=0.40650367736816406\n",
            "step 186: loss=0.4796102046966553\n",
            "step 187: loss=0.6871426105499268\n",
            "step 188: loss=2.680981159210205\n",
            "step 189: loss=0.7373931407928467\n",
            "step 190: loss=0.6501474380493164\n",
            "step 191: loss=0.44893431663513184\n",
            "step 192: loss=0.12216806411743164\n",
            "step 193: loss=0.12809205055236816\n",
            "step 194: loss=0.3562130928039551\n",
            "step 195: loss=0.060632944107055664\n",
            "step 196: loss=1.4033054113388062\n",
            "step 197: loss=1.2201976776123047\n",
            "step 198: loss=0.2856619358062744\n",
            "step 199: loss=2.5699079036712646\n",
            "step 200: loss=1.3816474676132202\n",
            "step 201: loss=0.4719282388687134\n",
            "step 202: loss=2.0478811264038086\n",
            "step 203: loss=0.4015340805053711\n",
            "step 204: loss=0.14312219619750977\n",
            "step 205: loss=0.49718499183654785\n",
            "step 206: loss=0.7516837120056152\n",
            "step 207: loss=0.17197680473327637\n",
            "step 208: loss=0.5553793907165527\n",
            "step 209: loss=0.3742920160293579\n",
            "step 210: loss=0.7562179565429688\n",
            "step 211: loss=0.15678930282592773\n",
            "step 212: loss=1.6474084854125977\n",
            "step 213: loss=1.2462327480316162\n",
            "step 214: loss=1.599760890007019\n",
            "step 215: loss=0.15593624114990234\n",
            "step 216: loss=0.29805707931518555\n",
            "step 217: loss=0.29372286796569824\n",
            "step 218: loss=0.15705633163452148\n",
            "step 219: loss=0.22482776641845703\n",
            "step 220: loss=0.9453972578048706\n",
            "step 221: loss=1.1685106754302979\n",
            "step 222: loss=2.3041555881500244\n",
            "step 223: loss=0.9989070892333984\n",
            "step 224: loss=0.7327048778533936\n",
            "step 225: loss=0.37357258796691895\n",
            "step 226: loss=0.6927950382232666\n",
            "step 227: loss=0.4200795888900757\n",
            "step 228: loss=0.5350275039672852\n",
            "step 229: loss=0.604094386100769\n",
            "step 230: loss=0.5697106122970581\n",
            "step 231: loss=0.8951528668403625\n",
            "step 232: loss=0.281496524810791\n",
            "step 233: loss=0.0047473907470703125\n",
            "step 234: loss=2.366014003753662\n",
            "step 235: loss=0.27408552169799805\n",
            "step 236: loss=1.1047792434692383\n",
            "step 237: loss=0.040659427642822266\n",
            "step 238: loss=0.22864484786987305\n",
            "step 239: loss=0.3796806335449219\n",
            "step 240: loss=0.34667110443115234\n",
            "step 241: loss=0.4054981470108032\n",
            "step 242: loss=0.1246330738067627\n",
            "step 243: loss=0.5560510158538818\n",
            "step 244: loss=0.44894957542419434\n",
            "step 245: loss=0.4151649475097656\n",
            "step 246: loss=0.9324600696563721\n",
            "step 247: loss=0.44788551330566406\n",
            "step 248: loss=3.057542085647583\n",
            "step 249: loss=0.05451393127441406\n",
            "step 250: loss=0.06193351745605469\n",
            "step 251: loss=2.078672170639038\n",
            "step 252: loss=0.5296133756637573\n",
            "step 253: loss=0.9901057481765747\n",
            "step 254: loss=0.09525799751281738\n",
            "step 255: loss=0.07671642303466797\n",
            "step 256: loss=0.489521861076355\n",
            "step 257: loss=2.7890677452087402\n",
            "step 258: loss=0.3984382152557373\n",
            "step 259: loss=0.8330893516540527\n",
            "step 260: loss=0.47151756286621094\n",
            "step 261: loss=0.983100175857544\n",
            "step 262: loss=2.7335660457611084\n",
            "step 263: loss=0.32002758979797363\n",
            "step 264: loss=0.1883378028869629\n",
            "step 265: loss=0.4457817077636719\n",
            "step 266: loss=0.6293996572494507\n",
            "step 267: loss=0.46929991245269775\n",
            "step 268: loss=0.7847534418106079\n",
            "step 269: loss=0.032675743103027344\n",
            "step 270: loss=0.6592978239059448\n",
            "step 271: loss=0.03919816017150879\n",
            "step 272: loss=0.7018625736236572\n",
            "step 273: loss=0.5264933109283447\n",
            "step 274: loss=0.12643933296203613\n",
            "step 275: loss=1.092840552330017\n",
            "step 276: loss=0.03591275215148926\n",
            "step 277: loss=0.29474616050720215\n",
            "step 278: loss=0.43141674995422363\n",
            "step 279: loss=0.4353444576263428\n",
            "step 280: loss=0.35237371921539307\n",
            "step 281: loss=0.08173036575317383\n",
            "step 282: loss=0.05996131896972656\n",
            "step 283: loss=1.4846298694610596\n",
            "step 284: loss=1.8887841701507568\n",
            "step 285: loss=0.4659993648529053\n",
            "step 286: loss=0.2334280014038086\n",
            "step 287: loss=0.15755367279052734\n",
            "step 288: loss=0.07255125045776367\n",
            "step 289: loss=2.1834232807159424\n",
            "step 290: loss=0.18325090408325195\n",
            "step 291: loss=0.37158799171447754\n",
            "step 292: loss=1.1927613019943237\n",
            "step 293: loss=0.044721126556396484\n",
            "step 294: loss=0.009671211242675781\n",
            "step 295: loss=0.8239340782165527\n",
            "step 296: loss=0.2812347412109375\n",
            "step 297: loss=0.29264354705810547\n",
            "step 298: loss=0.07653141021728516\n",
            "step 299: loss=0.012074947357177734\n",
            "step 300: loss=0.05053567886352539\n",
            "step 301: loss=0.006805896759033203\n",
            "step 302: loss=2.7376489639282227\n",
            "step 303: loss=0.9433908462524414\n",
            "step 304: loss=0.3315999507904053\n",
            "step 305: loss=0.4553706645965576\n",
            "step 306: loss=0.3246622085571289\n",
            "step 307: loss=0.9534530639648438\n",
            "step 308: loss=0.044396400451660156\n",
            "step 309: loss=0.4811127185821533\n",
            "step 310: loss=0.48668503761291504\n",
            "step 311: loss=2.132352828979492\n",
            "step 312: loss=0.17094135284423828\n",
            "step 313: loss=0.12753868103027344\n",
            "step 314: loss=0.551832914352417\n",
            "step 315: loss=0.07557868957519531\n",
            "step 316: loss=0.30028343200683594\n",
            "step 317: loss=0.33416497707366943\n",
            "step 318: loss=0.3379436731338501\n",
            "step 319: loss=0.21451210975646973\n",
            "step 320: loss=0.3522576093673706\n",
            "step 321: loss=0.8380552530288696\n",
            "step 322: loss=0.17711257934570312\n",
            "step 323: loss=0.09087562561035156\n",
            "step 324: loss=0.07382750511169434\n",
            "step 325: loss=0.23125982284545898\n",
            "step 326: loss=1.4439153671264648\n",
            "step 327: loss=0.47332286834716797\n",
            "step 328: loss=2.746384859085083\n",
            "step 329: loss=0.005428791046142578\n",
            "step 330: loss=0.5077652931213379\n",
            "step 331: loss=0.06275653839111328\n",
            "step 332: loss=0.3127434253692627\n",
            "step 333: loss=0.04568958282470703\n",
            "step 334: loss=0.0166778564453125\n",
            "step 335: loss=0.9150269031524658\n",
            "step 336: loss=0.34367990493774414\n",
            "step 337: loss=1.1579874753952026\n",
            "step 338: loss=0.33712172508239746\n",
            "step 339: loss=0.47450244426727295\n",
            "step 340: loss=0.0789024829864502\n",
            "step 341: loss=1.6881626844406128\n",
            "step 342: loss=0.3500051498413086\n",
            "step 343: loss=0.08574962615966797\n",
            "step 344: loss=1.064251184463501\n",
            "step 345: loss=0.7578305006027222\n",
            "step 346: loss=0.07385015487670898\n",
            "step 347: loss=0.2304244041442871\n",
            "step 348: loss=0.23037385940551758\n",
            "step 349: loss=0.35393309593200684\n",
            "step 350: loss=0.018471717834472656\n",
            "step 351: loss=0.36508679389953613\n",
            "step 352: loss=1.012511134147644\n",
            "step 353: loss=2.5447545051574707\n",
            "step 354: loss=0.7907414436340332\n",
            "step 355: loss=0.7476091384887695\n",
            "step 356: loss=0.012488365173339844\n",
            "step 357: loss=0.3724796772003174\n",
            "step 358: loss=1.1695151329040527\n",
            "step 359: loss=0.17859244346618652\n",
            "step 360: loss=0.08150219917297363\n",
            "step 361: loss=0.5774656534194946\n",
            "step 362: loss=0.12587261199951172\n",
            "step 363: loss=3.199237823486328\n",
            "step 364: loss=0.12036800384521484\n",
            "step 365: loss=1.0232316255569458\n",
            "step 366: loss=0.5209536552429199\n",
            "step 367: loss=0.47525346279144287\n",
            "step 368: loss=0.01161336898803711\n",
            "step 369: loss=0.19076108932495117\n",
            "step 370: loss=2.11970591545105\n",
            "step 371: loss=0.198927640914917\n",
            "step 372: loss=0.015585899353027344\n",
            "step 373: loss=0.014299869537353516\n",
            "step 374: loss=0.08033037185668945\n",
            "step 375: loss=1.9676306247711182\n",
            "step 376: loss=1.01432204246521\n",
            "step 377: loss=0.5524725914001465\n",
            "step 378: loss=0.629766583442688\n",
            "step 379: loss=3.3354454040527344\n",
            "step 380: loss=0.0397944450378418\n",
            "step 381: loss=0.008146286010742188\n",
            "step 382: loss=0.39034605026245117\n",
            "step 383: loss=0.3461265563964844\n",
            "step 384: loss=0.0007467269897460938\n",
            "step 385: loss=0.06762409210205078\n",
            "step 386: loss=0.09310150146484375\n",
            "step 387: loss=0.11734533309936523\n",
            "step 388: loss=0.42160606384277344\n",
            "step 389: loss=0.04375648498535156\n",
            "step 390: loss=0.6377222537994385\n",
            "step 391: loss=0.13796329498291016\n",
            "step 392: loss=0.9367666244506836\n",
            "step 393: loss=1.7518149614334106\n",
            "step 394: loss=3.9221692085266113\n",
            "step 395: loss=0.05047798156738281\n",
            "step 396: loss=0.25234508514404297\n",
            "step 397: loss=0.1284027099609375\n",
            "step 398: loss=0.9674408435821533\n",
            "step 399: loss=0.41379332542419434\n",
            "step 400: loss=0.029466629028320312\n",
            "step 401: loss=0.5387718677520752\n",
            "step 402: loss=0.44220614433288574\n",
            "step 403: loss=0.004082679748535156\n",
            "step 404: loss=0.01567983627319336\n",
            "step 405: loss=0.015554428100585938\n",
            "step 406: loss=0.6025831699371338\n",
            "step 407: loss=0.08917903900146484\n",
            "step 408: loss=0.018439769744873047\n",
            "step 409: loss=3.631787061691284\n",
            "step 410: loss=0.054201602935791016\n",
            "step 411: loss=0.5691431760787964\n",
            "step 412: loss=0.01025390625\n",
            "step 413: loss=0.16602587699890137\n",
            "step 414: loss=0.2280120849609375\n",
            "step 415: loss=0.033725738525390625\n",
            "step 416: loss=2.8930912017822266\n",
            "step 417: loss=0.4680633544921875\n",
            "step 418: loss=0.11901974678039551\n",
            "step 419: loss=1.3566269874572754\n",
            "step 420: loss=0.05841684341430664\n",
            "step 421: loss=0.28403472900390625\n",
            "step 422: loss=0.8525378704071045\n",
            "step 423: loss=0.2811431884765625\n",
            "step 424: loss=0.020607948303222656\n",
            "step 425: loss=0.8618518114089966\n",
            "step 426: loss=1.1827762126922607\n",
            "step 427: loss=0.8799014091491699\n",
            "step 428: loss=0.19707965850830078\n",
            "step 429: loss=0.2692444324493408\n",
            "step 430: loss=0.0019240379333496094\n",
            "step 431: loss=0.5260114669799805\n",
            "step 432: loss=0.2273731231689453\n",
            "step 433: loss=0.3134944438934326\n",
            "step 434: loss=1.1230807304382324\n",
            "step 435: loss=0.05976676940917969\n",
            "step 436: loss=0.18072032928466797\n",
            "step 437: loss=0.002167224884033203\n",
            "step 438: loss=0.01470041275024414\n",
            "step 439: loss=0.9843233823776245\n",
            "step 440: loss=0.011989116668701172\n",
            "step 441: loss=0.14016485214233398\n",
            "step 442: loss=0.7073942422866821\n",
            "step 443: loss=1.4848883152008057\n",
            "step 444: loss=1.0383212566375732\n",
            "step 445: loss=0.41783320903778076\n",
            "step 446: loss=0.7536642551422119\n",
            "step 447: loss=0.07221364974975586\n",
            "step 448: loss=0.05558300018310547\n",
            "step 449: loss=0.8591423034667969\n",
            "step 450: loss=3.564875364303589\n",
            "step 451: loss=0.006774425506591797\n",
            "step 452: loss=1.669120192527771\n",
            "step 453: loss=0.12167143821716309\n",
            "step 454: loss=3.2515270709991455\n",
            "step 455: loss=1.7813029289245605\n",
            "step 456: loss=1.3433032035827637\n",
            "step 457: loss=0.18211889266967773\n",
            "step 458: loss=0.7063822746276855\n",
            "step 459: loss=0.08410120010375977\n",
            "step 460: loss=0.03919649124145508\n",
            "step 461: loss=2.979452610015869\n",
            "step 462: loss=0.0026497840881347656\n",
            "step 463: loss=0.43689000606536865\n",
            "step 464: loss=0.07270312309265137\n",
            "step 465: loss=1.1039822101593018\n",
            "step 466: loss=0.03798484802246094\n",
            "step 467: loss=1.9705100059509277\n",
            "step 468: loss=0.11281657218933105\n",
            "step 469: loss=0.687421441078186\n",
            "step 470: loss=0.13335704803466797\n",
            "step 471: loss=2.6948351860046387\n",
            "step 472: loss=1.5483877658843994\n",
            "step 473: loss=0.11220932006835938\n",
            "step 474: loss=0.44478631019592285\n",
            "step 475: loss=0.013322830200195312\n",
            "step 476: loss=0.38378405570983887\n",
            "step 477: loss=0.04301595687866211\n",
            "step 478: loss=0.026033878326416016\n",
            "step 479: loss=0.1391465663909912\n",
            "step 480: loss=0.3230702877044678\n",
            "step 481: loss=0.24133872985839844\n",
            "step 482: loss=0.29029393196105957\n",
            "step 483: loss=0.4514617919921875\n",
            "step 484: loss=0.44598913192749023\n",
            "step 485: loss=1.3443806171417236\n",
            "step 486: loss=0.010129451751708984\n",
            "step 487: loss=0.008464813232421875\n",
            "step 488: loss=0.10316658020019531\n",
            "step 489: loss=0.14998960494995117\n",
            "step 490: loss=0.24096250534057617\n",
            "step 491: loss=0.0014219284057617188\n",
            "step 492: loss=0.05595660209655762\n",
            "step 493: loss=0.04845285415649414\n",
            "step 494: loss=0.3449218273162842\n",
            "step 495: loss=0.24515247344970703\n",
            "step 496: loss=1.8468502759933472\n",
            "step 497: loss=0.025726318359375\n",
            "step 498: loss=0.10625171661376953\n",
            "step 499: loss=3.827439308166504\n",
            "step 500: loss=0.8061550855636597\n",
            "step 501: loss=0.02237081527709961\n",
            "step 502: loss=0.3995704650878906\n",
            "step 503: loss=0.20708990097045898\n",
            "step 504: loss=0.045786142349243164\n",
            "step 505: loss=0.032854557037353516\n",
            "step 506: loss=0.20964956283569336\n",
            "step 507: loss=0.9962095022201538\n",
            "step 508: loss=0.16164588928222656\n",
            "step 509: loss=0.03909635543823242\n",
            "step 510: loss=0.10247039794921875\n",
            "step 511: loss=0.33330416679382324\n",
            "step 512: loss=0.12527990341186523\n",
            "step 513: loss=0.21143198013305664\n",
            "step 514: loss=0.3745746612548828\n",
            "step 515: loss=0.027648448944091797\n",
            "step 516: loss=0.08393144607543945\n",
            "step 517: loss=0.5669987201690674\n",
            "step 518: loss=0.957893967628479\n",
            "step 519: loss=0.02363729476928711\n",
            "step 520: loss=1.48310387134552\n",
            "step 521: loss=0.13979244232177734\n",
            "step 522: loss=0.042284488677978516\n",
            "step 523: loss=0.062416791915893555\n",
            "step 524: loss=0.03997659683227539\n",
            "step 525: loss=0.14003419876098633\n",
            "step 526: loss=3.1203131675720215\n",
            "step 527: loss=0.0059146881103515625\n",
            "step 528: loss=0.00048542022705078125\n",
            "step 529: loss=0.29979002475738525\n",
            "step 530: loss=0.05205678939819336\n",
            "step 531: loss=0.16924405097961426\n",
            "step 532: loss=0.0011043548583984375\n",
            "step 533: loss=0.22463774681091309\n",
            "step 534: loss=0.05318403244018555\n",
            "step 535: loss=0.08916521072387695\n",
            "step 536: loss=0.07006692886352539\n",
            "step 537: loss=0.07175874710083008\n",
            "step 538: loss=1.605657696723938\n",
            "step 539: loss=0.8576207160949707\n",
            "step 540: loss=3.2982354164123535\n",
            "step 541: loss=0.0028433799743652344\n",
            "step 542: loss=0.19463753700256348\n",
            "step 543: loss=0.8926622271537781\n",
            "step 544: loss=0.12177920341491699\n",
            "step 545: loss=0.7661073207855225\n",
            "step 546: loss=0.662636399269104\n",
            "step 547: loss=0.07155418395996094\n",
            "step 548: loss=0.01363372802734375\n",
            "step 549: loss=0.2269115447998047\n",
            "step 550: loss=0.19848036766052246\n",
            "step 551: loss=0.945902943611145\n",
            "step 552: loss=0.6102805137634277\n",
            "step 553: loss=0.054195404052734375\n",
            "step 554: loss=2.051149606704712\n",
            "step 555: loss=0.6343873739242554\n",
            "step 556: loss=1.581680178642273\n",
            "step 557: loss=0.35944652557373047\n",
            "step 558: loss=0.005198001861572266\n",
            "step 559: loss=0.0024509429931640625\n",
            "step 560: loss=0.09252715110778809\n",
            "step 561: loss=0.15797710418701172\n",
            "step 562: loss=0.5221741199493408\n",
            "step 563: loss=0.1967637538909912\n",
            "step 564: loss=0.16375017166137695\n",
            "step 565: loss=0.019205570220947266\n",
            "step 566: loss=2.8943355083465576\n",
            "step 567: loss=0.06458353996276855\n",
            "step 568: loss=0.2374105453491211\n",
            "step 569: loss=0.3637881278991699\n",
            "step 570: loss=0.05800652503967285\n",
            "step 571: loss=0.04111528396606445\n",
            "step 572: loss=0.05983877182006836\n",
            "step 573: loss=0.3058300018310547\n",
            "step 574: loss=1.237825632095337\n",
            "step 575: loss=0.28524351119995117\n",
            "step 576: loss=0.21417760848999023\n",
            "step 577: loss=0.4051241874694824\n",
            "step 578: loss=0.02850055694580078\n",
            "step 579: loss=0.01888132095336914\n",
            "step 580: loss=0.13847827911376953\n",
            "step 581: loss=0.016454219818115234\n",
            "step 582: loss=1.398078441619873\n",
            "step 583: loss=0.5642738342285156\n",
            "step 584: loss=0.09487390518188477\n",
            "step 585: loss=0.09597134590148926\n",
            "step 586: loss=1.049293041229248\n",
            "step 587: loss=0.47356557846069336\n",
            "step 588: loss=0.8383779525756836\n",
            "step 589: loss=3.177544116973877\n",
            "step 590: loss=0.4497392177581787\n",
            "step 591: loss=0.0792703628540039\n",
            "step 592: loss=1.357460856437683\n",
            "step 593: loss=0.2543766498565674\n",
            "step 594: loss=0.07349824905395508\n",
            "step 595: loss=2.8370463848114014\n",
            "step 596: loss=0.7364479303359985\n",
            "step 597: loss=0.7231221199035645\n",
            "step 598: loss=1.374111533164978\n",
            "step 599: loss=0.4321753978729248\n",
            "step 600: loss=0.17777228355407715\n",
            "step 601: loss=0.07130742073059082\n",
            "step 602: loss=0.2371366024017334\n",
            "step 603: loss=0.30576348304748535\n",
            "step 604: loss=2.668428421020508\n",
            "step 605: loss=1.1140674352645874\n",
            "step 606: loss=0.5520972013473511\n",
            "step 607: loss=0.8259549140930176\n",
            "step 608: loss=0.44512462615966797\n",
            "step 609: loss=0.7199970483779907\n",
            "step 610: loss=0.377687931060791\n",
            "step 611: loss=2.0839905738830566\n",
            "step 612: loss=1.1282179355621338\n",
            "step 613: loss=0.2851424217224121\n",
            "step 614: loss=0.04206252098083496\n",
            "step 615: loss=1.4286022186279297\n",
            "step 616: loss=0.5027270317077637\n",
            "step 617: loss=0.012112617492675781\n",
            "step 618: loss=0.7415882349014282\n",
            "step 619: loss=0.06085538864135742\n",
            "step 620: loss=0.5705982446670532\n",
            "step 621: loss=1.8605399131774902\n",
            "step 622: loss=0.23227739334106445\n",
            "step 623: loss=0.04270195960998535\n",
            "step 624: loss=2.679222822189331\n",
            "step 625: loss=0.2715725898742676\n",
            "step 626: loss=0.9293808937072754\n",
            "step 627: loss=0.16028642654418945\n",
            "step 628: loss=0.010308265686035156\n",
            "step 629: loss=1.290609359741211\n",
            "step 630: loss=0.08072900772094727\n",
            "step 631: loss=0.27584099769592285\n",
            "step 632: loss=0.2107226848602295\n",
            "step 633: loss=0.5600347518920898\n",
            "step 634: loss=0.011282920837402344\n",
            "step 635: loss=3.11643385887146\n",
            "step 636: loss=0.023334503173828125\n",
            "step 637: loss=0.49912166595458984\n",
            "step 638: loss=0.21662330627441406\n",
            "step 639: loss=0.0618898868560791\n",
            "step 640: loss=1.093286395072937\n",
            "step 641: loss=0.2558565139770508\n",
            "step 642: loss=0.010252952575683594\n",
            "step 643: loss=1.6186238527297974\n",
            "step 644: loss=0.011042594909667969\n",
            "step 645: loss=0.30058932304382324\n",
            "step 646: loss=0.09044694900512695\n",
            "step 647: loss=1.9067965745925903\n",
            "step 648: loss=0.12402224540710449\n",
            "step 649: loss=0.05713081359863281\n",
            "step 650: loss=0.10149407386779785\n",
            "step 651: loss=0.2526705265045166\n",
            "step 652: loss=0.6070406436920166\n",
            "step 653: loss=0.004278659820556641\n",
            "step 654: loss=0.7120640277862549\n",
            "step 655: loss=0.667755126953125\n",
            "step 656: loss=0.45967531204223633\n",
            "step 657: loss=0.05305337905883789\n",
            "step 658: loss=0.6365063190460205\n",
            "step 659: loss=0.12805700302124023\n",
            "step 660: loss=0.6878716945648193\n",
            "step 661: loss=0.004768848419189453\n",
            "step 662: loss=0.17098355293273926\n",
            "step 663: loss=1.0485873222351074\n",
            "step 664: loss=0.2143080234527588\n",
            "step 665: loss=0.040230751037597656\n",
            "step 666: loss=0.016267776489257812\n",
            "step 667: loss=0.011294364929199219\n",
            "step 668: loss=1.3694889545440674\n",
            "step 669: loss=2.1131973266601562\n",
            "step 670: loss=0.31995701789855957\n",
            "step 671: loss=1.4489619731903076\n",
            "step 672: loss=0.7029204368591309\n",
            "step 673: loss=1.19868803024292\n",
            "step 674: loss=0.05759167671203613\n",
            "step 675: loss=0.8192206621170044\n",
            "step 676: loss=0.05337262153625488\n",
            "step 677: loss=0.10202956199645996\n",
            "step 678: loss=0.4015166759490967\n",
            "step 679: loss=0.11560511589050293\n",
            "step 680: loss=0.0026750564575195312\n",
            "step 681: loss=1.3169834613800049\n",
            "step 682: loss=0.007153034210205078\n",
            "step 683: loss=0.580612063407898\n",
            "step 684: loss=1.1635642051696777\n",
            "step 685: loss=0.16467809677124023\n",
            "step 686: loss=0.029392719268798828\n",
            "step 687: loss=0.089141845703125\n",
            "step 688: loss=0.034459590911865234\n",
            "step 689: loss=1.5467545986175537\n",
            "step 690: loss=0.7600469589233398\n",
            "step 691: loss=0.3672494888305664\n",
            "step 692: loss=1.7930370569229126\n",
            "step 693: loss=0.04040336608886719\n",
            "step 694: loss=0.09868192672729492\n",
            "step 695: loss=0.09852075576782227\n",
            "step 696: loss=0.05927014350891113\n",
            "step 697: loss=1.2638287544250488\n",
            "step 698: loss=0.546238899230957\n",
            "step 699: loss=0.7163470983505249\n",
            "step 700: loss=0.0858759880065918\n",
            "step 701: loss=0.03256988525390625\n",
            "step 702: loss=0.02334737777709961\n",
            "step 703: loss=0.1107792854309082\n",
            "step 704: loss=0.030237197875976562\n",
            "step 705: loss=0.1586596965789795\n",
            "step 706: loss=0.14440345764160156\n",
            "step 707: loss=0.7704955339431763\n",
            "step 708: loss=0.273378849029541\n",
            "step 709: loss=0.12483501434326172\n",
            "step 710: loss=0.6951863765716553\n",
            "step 711: loss=1.7305415868759155\n",
            "step 712: loss=0.6946097612380981\n",
            "step 713: loss=0.1503126621246338\n",
            "step 714: loss=0.09028935432434082\n",
            "step 715: loss=0.06392288208007812\n",
            "step 716: loss=0.03419208526611328\n",
            "step 717: loss=0.28113722801208496\n",
            "step 718: loss=0.3789252042770386\n",
            "step 719: loss=0.010700225830078125\n",
            "step 720: loss=0.020720958709716797\n",
            "step 721: loss=0.061791181564331055\n",
            "step 722: loss=0.13160324096679688\n",
            "step 723: loss=0.022855758666992188\n",
            "step 724: loss=0.16715764999389648\n",
            "step 725: loss=4.071563243865967\n",
            "step 726: loss=0.454559326171875\n",
            "step 727: loss=0.00247955322265625\n",
            "step 728: loss=0.4241828918457031\n",
            "step 729: loss=1.0255894660949707\n",
            "step 730: loss=0.11156606674194336\n",
            "step 731: loss=0.061654090881347656\n",
            "step 732: loss=1.2832696437835693\n",
            "step 733: loss=0.6139696836471558\n",
            "step 734: loss=0.032423973083496094\n",
            "step 735: loss=0.8118246793746948\n",
            "step 736: loss=0.009581565856933594\n",
            "step 737: loss=0.28511953353881836\n",
            "step 738: loss=0.1188817024230957\n",
            "step 739: loss=0.0021009445190429688\n",
            "step 740: loss=0.11481595039367676\n",
            "step 741: loss=0.9182499647140503\n",
            "step 742: loss=5.225607872009277\n",
            "step 743: loss=1.704210877418518\n",
            "step 744: loss=0.04639291763305664\n",
            "step 745: loss=0.5776848793029785\n",
            "step 746: loss=0.01789712905883789\n",
            "step 747: loss=1.502614974975586\n",
            "step 748: loss=0.03278923034667969\n",
            "step 749: loss=0.01644754409790039\n",
            "step 750: loss=0.8263120651245117\n",
            "step 751: loss=0.04602527618408203\n",
            "step 752: loss=0.21524906158447266\n",
            "step 753: loss=0.061154842376708984\n",
            "step 754: loss=0.3718116283416748\n",
            "step 755: loss=0.28887057304382324\n",
            "step 756: loss=1.5567007064819336\n",
            "step 757: loss=0.028926372528076172\n",
            "step 758: loss=0.18595385551452637\n",
            "step 759: loss=0.07905268669128418\n",
            "step 760: loss=0.4115278720855713\n",
            "step 761: loss=0.008421897888183594\n",
            "step 762: loss=0.4036257266998291\n",
            "step 763: loss=0.9590024948120117\n",
            "step 764: loss=0.7133641242980957\n",
            "step 765: loss=0.48048698902130127\n",
            "step 766: loss=0.3986119031906128\n",
            "step 767: loss=0.13569283485412598\n",
            "step 768: loss=0.4674079418182373\n",
            "step 769: loss=0.00102996826171875\n",
            "step 770: loss=0.16486167907714844\n",
            "step 771: loss=0.43318843841552734\n",
            "step 772: loss=1.4921143054962158\n",
            "step 773: loss=0.08220386505126953\n",
            "step 774: loss=0.038150787353515625\n",
            "step 775: loss=0.013050556182861328\n",
            "step 776: loss=0.2003769874572754\n",
            "step 777: loss=0.05414152145385742\n",
            "step 778: loss=0.011188030242919922\n",
            "step 779: loss=0.014467239379882812\n",
            "step 780: loss=0.02270984649658203\n",
            "step 781: loss=0.0022478103637695312\n",
            "step 782: loss=0.07770586013793945\n",
            "step 783: loss=0.027918338775634766\n",
            "step 784: loss=0.04948854446411133\n",
            "step 785: loss=0.35353732109069824\n",
            "step 786: loss=0.013762474060058594\n",
            "step 787: loss=0.11862325668334961\n",
            "step 788: loss=1.4354572296142578\n",
            "step 789: loss=0.17925167083740234\n",
            "step 790: loss=0.4444906711578369\n",
            "step 791: loss=0.15286684036254883\n",
            "step 792: loss=0.09328126907348633\n",
            "step 793: loss=0.32610654830932617\n",
            "step 794: loss=0.9303807020187378\n",
            "step 795: loss=0.011697769165039062\n",
            "step 796: loss=0.029285907745361328\n",
            "step 797: loss=0.231231689453125\n",
            "step 798: loss=2.6338624954223633\n",
            "step 799: loss=0.5665862560272217\n",
            "step 800: loss=1.2449233531951904\n",
            "step 801: loss=0.013893604278564453\n",
            "step 802: loss=0.01511383056640625\n",
            "step 803: loss=0.12432408332824707\n",
            "step 804: loss=0.4973611831665039\n",
            "step 805: loss=0.0024280548095703125\n",
            "step 806: loss=0.002131938934326172\n",
            "step 807: loss=0.018868446350097656\n",
            "step 808: loss=0.002613067626953125\n",
            "step 809: loss=0.7255860567092896\n",
            "step 810: loss=0.0009622573852539062\n",
            "step 811: loss=0.9114747047424316\n",
            "step 812: loss=0.3157074451446533\n",
            "step 813: loss=0.07942962646484375\n",
            "step 814: loss=0.461492657661438\n",
            "step 815: loss=0.031975746154785156\n",
            "step 816: loss=0.2038557529449463\n",
            "step 817: loss=0.5515196323394775\n",
            "step 818: loss=0.2785799503326416\n",
            "step 819: loss=0.005415439605712891\n",
            "step 820: loss=0.17008376121520996\n",
            "step 821: loss=0.03214550018310547\n",
            "step 822: loss=0.037631988525390625\n",
            "step 823: loss=0.08344030380249023\n",
            "step 824: loss=0.22780489921569824\n",
            "step 825: loss=2.4254860877990723\n",
            "step 826: loss=0.014660835266113281\n",
            "step 827: loss=2.5519309043884277\n",
            "step 828: loss=0.01477813720703125\n",
            "step 829: loss=0.25369691848754883\n",
            "step 830: loss=0.11380338668823242\n",
            "step 831: loss=0.19821643829345703\n",
            "step 832: loss=0.005218505859375\n",
            "step 833: loss=0.15487146377563477\n",
            "step 834: loss=1.6532922983169556\n",
            "step 835: loss=0.43225550651550293\n",
            "step 836: loss=0.01710987091064453\n",
            "step 837: loss=0.015863895416259766\n",
            "step 838: loss=0.010437965393066406\n",
            "step 839: loss=0.03831672668457031\n",
            "step 840: loss=0.12788152694702148\n",
            "step 841: loss=0.005878925323486328\n",
            "step 842: loss=0.046411991119384766\n",
            "step 843: loss=0.01076507568359375\n",
            "step 844: loss=0.25150251388549805\n",
            "step 845: loss=0.0012178421020507812\n",
            "step 846: loss=0.2321016788482666\n",
            "step 847: loss=0.31379055976867676\n",
            "step 848: loss=0.06747221946716309\n",
            "step 849: loss=0.0008087158203125\n",
            "step 850: loss=0.07829785346984863\n",
            "step 851: loss=0.28147459030151367\n",
            "step 852: loss=0.3888612985610962\n",
            "step 853: loss=0.449662446975708\n",
            "step 854: loss=0.09763097763061523\n",
            "step 855: loss=1.7243692874908447\n",
            "step 856: loss=0.1643972396850586\n",
            "step 857: loss=0.11453390121459961\n",
            "step 858: loss=0.39929258823394775\n",
            "step 859: loss=0.16950511932373047\n",
            "step 860: loss=0.6861879825592041\n",
            "step 861: loss=0.02004098892211914\n",
            "step 862: loss=0.003284931182861328\n",
            "step 863: loss=0.10630512237548828\n",
            "step 864: loss=0.12004256248474121\n",
            "step 865: loss=0.8591494560241699\n",
            "step 866: loss=0.05156230926513672\n",
            "step 867: loss=0.7235795259475708\n",
            "step 868: loss=0.044901371002197266\n",
            "step 869: loss=2.3743138313293457\n",
            "step 870: loss=0.0057239532470703125\n",
            "step 871: loss=0.21019387245178223\n",
            "step 872: loss=0.1750349998474121\n",
            "step 873: loss=0.03566455841064453\n",
            "step 874: loss=1.4096548557281494\n",
            "step 875: loss=0.3176443576812744\n",
            "step 876: loss=1.5774856805801392\n",
            "step 877: loss=0.1490786075592041\n",
            "step 878: loss=0.14512395858764648\n",
            "step 879: loss=0.014568805694580078\n",
            "step 880: loss=0.1957552433013916\n",
            "step 881: loss=0.02507495880126953\n",
            "step 882: loss=0.11042499542236328\n",
            "step 883: loss=0.10820245742797852\n",
            "step 884: loss=0.013692378997802734\n",
            "step 885: loss=0.025472640991210938\n",
            "step 886: loss=1.4292902946472168\n",
            "step 887: loss=0.3023536205291748\n",
            "step 888: loss=0.16981220245361328\n",
            "step 889: loss=0.5759778022766113\n",
            "step 890: loss=0.39397764205932617\n",
            "step 891: loss=0.01878833770751953\n",
            "step 892: loss=0.0006103515625\n",
            "step 893: loss=0.14224529266357422\n",
            "step 894: loss=0.16683459281921387\n",
            "step 895: loss=0.02306652069091797\n",
            "step 896: loss=0.005665779113769531\n",
            "step 897: loss=0.26515984535217285\n",
            "step 898: loss=0.5112065076828003\n",
            "step 899: loss=0.01642131805419922\n",
            "step 900: loss=0.04950976371765137\n",
            "step 901: loss=0.10270214080810547\n",
            "step 902: loss=0.008306503295898438\n",
            "step 903: loss=1.2045660018920898\n",
            "step 904: loss=0.02616596221923828\n",
            "step 905: loss=0.5364210605621338\n",
            "step 906: loss=0.41507232189178467\n",
            "step 907: loss=0.6363639831542969\n",
            "step 908: loss=0.002647876739501953\n",
            "step 909: loss=0.06528759002685547\n",
            "step 910: loss=0.5764923095703125\n",
            "step 911: loss=0.29312682151794434\n",
            "step 912: loss=0.005534172058105469\n",
            "step 913: loss=0.04647350311279297\n",
            "step 914: loss=3.039876699447632\n",
            "step 915: loss=0.04770612716674805\n",
            "step 916: loss=0.23157477378845215\n",
            "step 917: loss=0.34737443923950195\n",
            "step 918: loss=0.5393333435058594\n",
            "step 919: loss=0.3387317657470703\n",
            "step 920: loss=0.10210371017456055\n",
            "step 921: loss=0.2000105381011963\n",
            "step 922: loss=0.46993470191955566\n",
            "step 923: loss=0.056276798248291016\n",
            "step 924: loss=0.006319999694824219\n",
            "step 925: loss=0.01444244384765625\n",
            "step 926: loss=0.02815389633178711\n",
            "step 927: loss=0.13592910766601562\n",
            "step 928: loss=2.0474705696105957\n",
            "step 929: loss=0.08091592788696289\n",
            "step 930: loss=0.23891758918762207\n",
            "step 931: loss=0.1295328140258789\n",
            "step 932: loss=0.03899955749511719\n",
            "step 933: loss=0.02909708023071289\n",
            "step 934: loss=0.022089481353759766\n",
            "step 935: loss=0.010469913482666016\n",
            "step 936: loss=1.1239633560180664\n",
            "step 937: loss=0.004912853240966797\n",
            "step 938: loss=0.24308156967163086\n",
            "step 939: loss=0.5980682373046875\n",
            "step 940: loss=0.12193965911865234\n",
            "step 941: loss=0.02315378189086914\n",
            "step 942: loss=0.37062692642211914\n",
            "step 943: loss=0.009302139282226562\n",
            "step 944: loss=0.011314868927001953\n",
            "step 945: loss=3.5264344215393066\n",
            "step 946: loss=0.0910792350769043\n",
            "step 947: loss=0.15913820266723633\n",
            "step 948: loss=0.01747608184814453\n",
            "step 949: loss=0.11133050918579102\n",
            "step 950: loss=0.09948945045471191\n",
            "step 951: loss=0.052641868591308594\n",
            "step 952: loss=0.00896453857421875\n",
            "step 953: loss=0.01663351058959961\n",
            "step 954: loss=0.05906391143798828\n",
            "step 955: loss=0.002895355224609375\n",
            "step 956: loss=0.07044506072998047\n",
            "step 957: loss=0.007376194000244141\n",
            "step 958: loss=0.2260730266571045\n",
            "step 959: loss=0.16261935234069824\n",
            "step 960: loss=0.6109166145324707\n",
            "step 961: loss=0.5834782123565674\n",
            "step 962: loss=0.05586504936218262\n",
            "step 963: loss=0.004221916198730469\n",
            "step 964: loss=0.033403873443603516\n",
            "step 965: loss=0.793737530708313\n",
            "step 966: loss=0.37858057022094727\n",
            "step 967: loss=0.0952444076538086\n",
            "step 968: loss=0.6360642910003662\n",
            "step 969: loss=0.03384065628051758\n",
            "step 970: loss=0.638343334197998\n",
            "step 971: loss=0.03069305419921875\n",
            "step 972: loss=0.14579248428344727\n",
            "step 973: loss=0.07487893104553223\n",
            "step 974: loss=1.3001110553741455\n",
            "step 975: loss=2.2975776195526123\n",
            "step 976: loss=0.01396322250366211\n",
            "step 977: loss=0.15135955810546875\n",
            "step 978: loss=0.8422958254814148\n",
            "step 979: loss=1.9849610328674316\n",
            "step 980: loss=0.5528028011322021\n",
            "step 981: loss=0.9595495462417603\n",
            "step 982: loss=0.15296173095703125\n",
            "step 983: loss=0.6124076843261719\n",
            "step 984: loss=0.04457712173461914\n",
            "step 985: loss=0.19276797771453857\n",
            "step 986: loss=0.14833617210388184\n",
            "step 987: loss=0.07393813133239746\n",
            "step 988: loss=0.07053279876708984\n",
            "step 989: loss=0.001033782958984375\n",
            "step 990: loss=0.31380701065063477\n",
            "step 991: loss=0.14849281311035156\n",
            "step 992: loss=0.2512540817260742\n",
            "step 993: loss=0.010190486907958984\n",
            "step 994: loss=0.709064245223999\n",
            "step 995: loss=0.2817842960357666\n",
            "step 996: loss=0.0027513504028320312\n",
            "step 997: loss=0.0024824142456054688\n",
            "step 998: loss=0.18323874473571777\n",
            "step 999: loss=0.29120421409606934\n",
            "step 1000: loss=0.002166748046875\n",
            "Mean loss        0.48416767\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c4d036d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_from_step = 1000  # e.g. resume from checkpoint at step 1000\n",
        "layer_sizes = [784, 128, 10]\n",
        "\n",
        "experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
        "checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "template_state = create_training_state(layer_sizes, optimizer, jax.random.key(0))\n",
        "restored_state = checkpoint_manager.restore(\n",
        "    resume_from_step,\n",
        "    args=ocp.args.StandardRestore(template_state),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "da53fb51",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1001: loss=0.1626293659210205\n",
            "step 1002: loss=0.017407894134521484\n",
            "step 1003: loss=0.031241416931152344\n",
            "step 1004: loss=0.004973888397216797\n",
            "step 1005: loss=0.10784530639648438\n",
            "step 1006: loss=0.002956390380859375\n",
            "step 1007: loss=0.0009908676147460938\n",
            "step 1008: loss=0.5485451221466064\n",
            "step 1009: loss=0.004882335662841797\n",
            "step 1010: loss=0.015553951263427734\n",
            "step 1011: loss=0.22939682006835938\n",
            "step 1012: loss=0.03225088119506836\n",
            "step 1013: loss=0.014636039733886719\n",
            "step 1014: loss=0.19443297386169434\n",
            "step 1015: loss=0.0006361007690429688\n",
            "step 1016: loss=9.5367431640625e-05\n",
            "step 1017: loss=0.0055103302001953125\n",
            "step 1018: loss=0.4422445297241211\n",
            "step 1019: loss=0.8210334777832031\n",
            "step 1020: loss=0.043196678161621094\n",
            "step 1021: loss=0.042943477630615234\n",
            "step 1022: loss=0.1428360939025879\n",
            "step 1023: loss=0.09797310829162598\n",
            "step 1024: loss=0.16956233978271484\n",
            "step 1025: loss=0.3048539161682129\n",
            "step 1026: loss=1.570849061012268\n",
            "step 1027: loss=0.314772367477417\n",
            "step 1028: loss=0.14598894119262695\n",
            "step 1029: loss=0.13607382774353027\n",
            "step 1030: loss=0.09300756454467773\n",
            "step 1031: loss=0.0002117156982421875\n",
            "step 1032: loss=0.016544342041015625\n",
            "step 1033: loss=0.06578230857849121\n",
            "step 1034: loss=0.3234424591064453\n",
            "step 1035: loss=0.011977195739746094\n",
            "step 1036: loss=0.8655931949615479\n",
            "step 1037: loss=0.039923667907714844\n",
            "step 1038: loss=0.01228475570678711\n",
            "step 1039: loss=0.7493517398834229\n",
            "step 1040: loss=0.04069232940673828\n",
            "step 1041: loss=0.037724971771240234\n",
            "step 1042: loss=0.08974981307983398\n",
            "step 1043: loss=0.018084049224853516\n",
            "step 1044: loss=0.047277212142944336\n",
            "step 1045: loss=0.6408088207244873\n",
            "step 1046: loss=0.011878013610839844\n",
            "step 1047: loss=0.00961923599243164\n",
            "step 1048: loss=0.7152836322784424\n",
            "step 1049: loss=0.007767200469970703\n",
            "step 1050: loss=0.20940804481506348\n",
            "step 1051: loss=0.07469987869262695\n",
            "step 1052: loss=0.210099458694458\n",
            "step 1053: loss=0.09630346298217773\n",
            "step 1054: loss=0.020104408264160156\n",
            "step 1055: loss=0.07259464263916016\n",
            "step 1056: loss=1.2613219022750854\n",
            "step 1057: loss=0.012250423431396484\n",
            "step 1058: loss=0.20248746871948242\n",
            "step 1059: loss=4.059324741363525\n",
            "step 1060: loss=0.19527053833007812\n",
            "step 1061: loss=0.007721900939941406\n",
            "step 1062: loss=0.027651309967041016\n",
            "step 1063: loss=0.603323221206665\n",
            "step 1064: loss=0.7820124626159668\n",
            "step 1065: loss=0.9905705451965332\n",
            "step 1066: loss=0.0049571990966796875\n",
            "step 1067: loss=0.18484926223754883\n",
            "step 1068: loss=0.008664608001708984\n",
            "step 1069: loss=0.452229380607605\n",
            "step 1070: loss=0.09458589553833008\n",
            "step 1071: loss=0.3984771966934204\n",
            "step 1072: loss=0.25646424293518066\n",
            "step 1073: loss=0.01741933822631836\n",
            "step 1074: loss=0.002857685089111328\n",
            "step 1075: loss=0.02662944793701172\n",
            "step 1076: loss=0.0009775161743164062\n",
            "step 1077: loss=0.3767528533935547\n",
            "step 1078: loss=1.8105497360229492\n",
            "step 1079: loss=0.02627086639404297\n",
            "step 1080: loss=0.0006084442138671875\n",
            "step 1081: loss=0.00379180908203125\n",
            "step 1082: loss=0.0077114105224609375\n",
            "step 1083: loss=0.008121967315673828\n",
            "step 1084: loss=0.03461647033691406\n",
            "step 1085: loss=1.4824302196502686\n",
            "step 1086: loss=0.03733491897583008\n",
            "step 1087: loss=0.5462534427642822\n",
            "step 1088: loss=0.005248069763183594\n",
            "step 1089: loss=0.014200210571289062\n",
            "step 1090: loss=0.7933633327484131\n",
            "step 1091: loss=0.004519462585449219\n",
            "step 1092: loss=0.06622552871704102\n",
            "step 1093: loss=0.004139423370361328\n",
            "step 1094: loss=0.7655787467956543\n",
            "step 1095: loss=0.0005903244018554688\n",
            "step 1096: loss=0.011120319366455078\n",
            "step 1097: loss=0.03551673889160156\n",
            "step 1098: loss=0.046636104583740234\n",
            "step 1099: loss=0.10104990005493164\n",
            "step 1100: loss=0.03680706024169922\n",
            "step 1101: loss=0.0600128173828125\n",
            "step 1102: loss=0.0012173652648925781\n",
            "step 1103: loss=1.513902187347412\n",
            "step 1104: loss=0.009312152862548828\n",
            "step 1105: loss=1.0278410911560059\n",
            "step 1106: loss=0.3108689785003662\n",
            "step 1107: loss=0.028818607330322266\n",
            "step 1108: loss=0.02547907829284668\n",
            "step 1109: loss=0.002213001251220703\n",
            "step 1110: loss=0.20375561714172363\n",
            "step 1111: loss=0.05109596252441406\n",
            "step 1112: loss=0.03166913986206055\n",
            "step 1113: loss=0.43865108489990234\n",
            "step 1114: loss=0.02792215347290039\n",
            "step 1115: loss=0.00046634674072265625\n",
            "step 1116: loss=0.09184002876281738\n",
            "step 1117: loss=0.004589557647705078\n",
            "step 1118: loss=0.0801236629486084\n",
            "step 1119: loss=0.6003773212432861\n",
            "step 1120: loss=0.4056210517883301\n",
            "step 1121: loss=1.2839908599853516\n",
            "step 1122: loss=0.03129911422729492\n",
            "step 1123: loss=0.14172148704528809\n",
            "step 1124: loss=0.009854793548583984\n",
            "step 1125: loss=0.00176239013671875\n",
            "step 1126: loss=0.037995338439941406\n",
            "step 1127: loss=0.09078598022460938\n",
            "step 1128: loss=0.007198810577392578\n",
            "step 1129: loss=0.11002230644226074\n",
            "step 1130: loss=0.08499717712402344\n",
            "step 1131: loss=0.0327606201171875\n",
            "step 1132: loss=1.2765183448791504\n",
            "step 1133: loss=0.014400959014892578\n",
            "step 1134: loss=0.02146005630493164\n",
            "step 1135: loss=0.09001016616821289\n",
            "step 1136: loss=0.005741119384765625\n",
            "step 1137: loss=0.13014841079711914\n",
            "step 1138: loss=0.1533496379852295\n",
            "step 1139: loss=0.016679763793945312\n",
            "step 1140: loss=0.0036249160766601562\n",
            "step 1141: loss=0.02324962615966797\n",
            "step 1142: loss=0.32183265686035156\n",
            "step 1143: loss=0.004617214202880859\n",
            "step 1144: loss=1.0366684198379517\n",
            "step 1145: loss=0.004954814910888672\n",
            "step 1146: loss=0.07909965515136719\n",
            "step 1147: loss=0.25406646728515625\n",
            "step 1148: loss=1.552024483680725\n",
            "step 1149: loss=0.09420561790466309\n",
            "step 1150: loss=0.04807567596435547\n",
            "step 1151: loss=0.02412891387939453\n",
            "step 1152: loss=0.016863346099853516\n",
            "step 1153: loss=0.0028200149536132812\n",
            "step 1154: loss=0.0025496482849121094\n",
            "step 1155: loss=0.689828634262085\n",
            "step 1156: loss=0.2728095054626465\n",
            "step 1157: loss=0.07658123970031738\n",
            "step 1158: loss=0.012423038482666016\n",
            "step 1159: loss=0.004768848419189453\n",
            "step 1160: loss=0.028670310974121094\n",
            "step 1161: loss=0.0003113746643066406\n",
            "step 1162: loss=0.007377147674560547\n",
            "step 1163: loss=0.0026369094848632812\n",
            "step 1164: loss=0.07589292526245117\n",
            "step 1165: loss=0.1468658447265625\n",
            "step 1166: loss=0.004059791564941406\n",
            "step 1167: loss=0.048050880432128906\n",
            "step 1168: loss=0.15915870666503906\n",
            "step 1169: loss=0.017077922821044922\n",
            "step 1170: loss=0.012436866760253906\n",
            "step 1171: loss=0.007636070251464844\n",
            "step 1172: loss=0.3201608657836914\n",
            "step 1173: loss=0.002017498016357422\n",
            "step 1174: loss=0.040223121643066406\n",
            "step 1175: loss=0.3871796131134033\n",
            "step 1176: loss=0.022149085998535156\n",
            "step 1177: loss=0.03946399688720703\n",
            "step 1178: loss=0.00220489501953125\n",
            "step 1179: loss=0.020730018615722656\n",
            "step 1180: loss=1.604821801185608\n",
            "step 1181: loss=0.08331894874572754\n",
            "step 1182: loss=0.022668838500976562\n",
            "step 1183: loss=0.014053821563720703\n",
            "step 1184: loss=0.055033206939697266\n",
            "step 1185: loss=0.08323001861572266\n",
            "step 1186: loss=0.03009796142578125\n",
            "step 1187: loss=0.01802825927734375\n",
            "step 1188: loss=0.4612252712249756\n",
            "step 1189: loss=0.01162576675415039\n",
            "step 1190: loss=0.005026817321777344\n",
            "step 1191: loss=0.11319828033447266\n",
            "step 1192: loss=0.0016803741455078125\n",
            "step 1193: loss=0.00034809112548828125\n",
            "step 1194: loss=0.0603790283203125\n",
            "step 1195: loss=0.0018815994262695312\n",
            "step 1196: loss=0.16170263290405273\n",
            "step 1197: loss=0.2908773422241211\n",
            "step 1198: loss=0.031519412994384766\n",
            "step 1199: loss=2.1463234424591064\n",
            "step 1200: loss=0.9627663493156433\n",
            "step 1201: loss=0.0038123130798339844\n",
            "step 1202: loss=0.4058964252471924\n",
            "step 1203: loss=0.18172216415405273\n",
            "step 1204: loss=0.020883560180664062\n",
            "step 1205: loss=0.008228778839111328\n",
            "step 1206: loss=0.008556365966796875\n",
            "step 1207: loss=0.025165557861328125\n",
            "step 1208: loss=0.03114461898803711\n",
            "step 1209: loss=0.0013537406921386719\n",
            "step 1210: loss=0.26059818267822266\n",
            "step 1211: loss=0.025119304656982422\n",
            "step 1212: loss=0.2758338451385498\n",
            "step 1213: loss=0.674568772315979\n",
            "step 1214: loss=0.3330986499786377\n",
            "step 1215: loss=0.0001277923583984375\n",
            "step 1216: loss=0.043561458587646484\n",
            "step 1217: loss=0.012444019317626953\n",
            "step 1218: loss=0.008404254913330078\n",
            "step 1219: loss=0.005790233612060547\n",
            "step 1220: loss=0.027063369750976562\n",
            "step 1221: loss=0.04086017608642578\n",
            "step 1222: loss=1.4971646070480347\n",
            "step 1223: loss=0.07079529762268066\n",
            "step 1224: loss=0.030503511428833008\n",
            "step 1225: loss=0.040850162506103516\n",
            "step 1226: loss=0.12685680389404297\n",
            "step 1227: loss=0.11972236633300781\n",
            "step 1228: loss=0.0685582160949707\n",
            "step 1229: loss=0.09670543670654297\n",
            "step 1230: loss=0.035756826400756836\n",
            "step 1231: loss=0.08947992324829102\n",
            "step 1232: loss=0.22990059852600098\n",
            "step 1233: loss=6.29425048828125e-05\n",
            "step 1234: loss=0.7096219062805176\n",
            "step 1235: loss=0.7265791893005371\n",
            "step 1236: loss=0.15836286544799805\n",
            "step 1237: loss=0.001552581787109375\n",
            "step 1238: loss=0.010675907135009766\n",
            "step 1239: loss=0.011548995971679688\n",
            "step 1240: loss=0.07656431198120117\n",
            "step 1241: loss=0.10735702514648438\n",
            "step 1242: loss=0.010092735290527344\n",
            "step 1243: loss=0.1826789379119873\n",
            "step 1244: loss=0.02172994613647461\n",
            "step 1245: loss=0.0033774375915527344\n",
            "step 1246: loss=0.03788018226623535\n",
            "step 1247: loss=0.05229949951171875\n",
            "step 1248: loss=1.2361040115356445\n",
            "step 1249: loss=0.00010013580322265625\n",
            "step 1250: loss=0.008226871490478516\n",
            "step 1251: loss=0.3423879146575928\n",
            "step 1252: loss=0.024322032928466797\n",
            "step 1253: loss=0.035619258880615234\n",
            "step 1254: loss=0.011382579803466797\n",
            "step 1255: loss=0.01534128189086914\n",
            "step 1256: loss=0.01314401626586914\n",
            "step 1257: loss=1.006316900253296\n",
            "step 1258: loss=0.20354461669921875\n",
            "step 1259: loss=0.5348777770996094\n",
            "step 1260: loss=0.03265190124511719\n",
            "step 1261: loss=0.21750664710998535\n",
            "step 1262: loss=0.5395522117614746\n",
            "step 1263: loss=0.0076904296875\n",
            "step 1264: loss=0.08109116554260254\n",
            "step 1265: loss=0.1924576759338379\n",
            "step 1266: loss=0.18837404251098633\n",
            "step 1267: loss=0.044295310974121094\n",
            "step 1268: loss=0.03443002700805664\n",
            "step 1269: loss=0.0007271766662597656\n",
            "step 1270: loss=0.2871263027191162\n",
            "step 1271: loss=1.049041748046875e-05\n",
            "step 1272: loss=0.012858390808105469\n",
            "step 1273: loss=0.0019707679748535156\n",
            "step 1274: loss=0.0023412704467773438\n",
            "step 1275: loss=0.1732947826385498\n",
            "step 1276: loss=0.000484466552734375\n",
            "step 1277: loss=0.0677332878112793\n",
            "step 1278: loss=0.01694202423095703\n",
            "step 1279: loss=0.08466601371765137\n",
            "step 1280: loss=0.08986449241638184\n",
            "step 1281: loss=0.012970447540283203\n",
            "step 1282: loss=0.014156341552734375\n",
            "step 1283: loss=0.32639002799987793\n",
            "step 1284: loss=1.217644214630127\n",
            "step 1285: loss=0.02835559844970703\n",
            "step 1286: loss=0.0026140213012695312\n",
            "step 1287: loss=0.02348470687866211\n",
            "step 1288: loss=0.02862548828125\n",
            "step 1289: loss=0.21062707901000977\n",
            "step 1290: loss=0.0009860992431640625\n",
            "step 1291: loss=0.0016298294067382812\n",
            "step 1292: loss=0.1581568717956543\n",
            "step 1293: loss=0.017374038696289062\n",
            "step 1294: loss=0.00049591064453125\n",
            "step 1295: loss=0.08648204803466797\n",
            "step 1296: loss=0.0482335090637207\n",
            "step 1297: loss=0.001617431640625\n",
            "step 1298: loss=0.0020036697387695312\n",
            "step 1299: loss=0.008901596069335938\n",
            "step 1300: loss=0.04480552673339844\n",
            "step 1301: loss=0.000213623046875\n",
            "step 1302: loss=3.8289921283721924\n",
            "step 1303: loss=0.29468441009521484\n",
            "step 1304: loss=0.00345611572265625\n",
            "step 1305: loss=0.0020771026611328125\n",
            "step 1306: loss=0.18143939971923828\n",
            "step 1307: loss=0.2595984935760498\n",
            "step 1308: loss=0.0023488998413085938\n",
            "step 1309: loss=0.056387901306152344\n",
            "step 1310: loss=0.3872394561767578\n",
            "step 1311: loss=0.1813364028930664\n",
            "step 1312: loss=0.0037927627563476562\n",
            "step 1313: loss=0.022809982299804688\n",
            "step 1314: loss=0.12825298309326172\n",
            "step 1315: loss=0.0013566017150878906\n",
            "step 1316: loss=0.022032737731933594\n",
            "step 1317: loss=0.004200458526611328\n",
            "step 1318: loss=0.004429340362548828\n",
            "step 1319: loss=0.1249535083770752\n",
            "step 1320: loss=0.012188434600830078\n",
            "step 1321: loss=0.16175103187561035\n",
            "step 1322: loss=0.022179126739501953\n",
            "step 1323: loss=0.009984016418457031\n",
            "step 1324: loss=0.0012793540954589844\n",
            "step 1325: loss=0.004088401794433594\n",
            "step 1326: loss=0.6569926738739014\n",
            "step 1327: loss=0.020417213439941406\n",
            "step 1328: loss=1.4942398071289062\n",
            "step 1329: loss=0.0006995201110839844\n",
            "step 1330: loss=0.3135833740234375\n",
            "step 1331: loss=0.003319263458251953\n",
            "step 1332: loss=0.024753093719482422\n",
            "step 1333: loss=0.016324520111083984\n",
            "step 1334: loss=0.0006103515625\n",
            "step 1335: loss=0.6373841762542725\n",
            "step 1336: loss=0.1016387939453125\n",
            "step 1337: loss=0.028322696685791016\n",
            "step 1338: loss=0.036763668060302734\n",
            "step 1339: loss=0.17965984344482422\n",
            "step 1340: loss=0.01766681671142578\n",
            "step 1341: loss=0.7193061709403992\n",
            "step 1342: loss=0.08399200439453125\n",
            "step 1343: loss=0.018961429595947266\n",
            "step 1344: loss=0.045099735260009766\n",
            "step 1345: loss=0.005125522613525391\n",
            "step 1346: loss=0.028477191925048828\n",
            "step 1347: loss=0.010256290435791016\n",
            "step 1348: loss=0.004501819610595703\n",
            "step 1349: loss=0.01276540756225586\n",
            "step 1350: loss=0.0016283988952636719\n",
            "step 1351: loss=0.0024313926696777344\n",
            "step 1352: loss=0.09863829612731934\n",
            "step 1353: loss=0.45129191875457764\n",
            "step 1354: loss=0.033548831939697266\n",
            "step 1355: loss=0.3980836868286133\n",
            "step 1356: loss=0.0045337677001953125\n",
            "step 1357: loss=0.03186774253845215\n",
            "step 1358: loss=0.005653858184814453\n",
            "step 1359: loss=0.008360862731933594\n",
            "step 1360: loss=0.0010175704956054688\n",
            "step 1361: loss=0.04345130920410156\n",
            "step 1362: loss=0.06328296661376953\n",
            "step 1363: loss=2.1239264011383057\n",
            "step 1364: loss=0.07636404037475586\n",
            "step 1365: loss=0.5956707000732422\n",
            "step 1366: loss=0.14943361282348633\n",
            "step 1367: loss=0.08498930931091309\n",
            "step 1368: loss=0.002578258514404297\n",
            "step 1369: loss=0.00014209747314453125\n",
            "step 1370: loss=0.7903897762298584\n",
            "step 1371: loss=0.0773000717163086\n",
            "step 1372: loss=0.028435230255126953\n",
            "step 1373: loss=0.0018863677978515625\n",
            "step 1374: loss=0.008478641510009766\n",
            "step 1375: loss=0.27088260650634766\n",
            "step 1376: loss=0.25244903564453125\n",
            "step 1377: loss=0.025552988052368164\n",
            "step 1378: loss=0.1546785831451416\n",
            "step 1379: loss=0.38130760192871094\n",
            "step 1380: loss=0.0011224746704101562\n",
            "step 1381: loss=0.001132965087890625\n",
            "step 1382: loss=0.053763389587402344\n",
            "step 1383: loss=0.01424264907836914\n",
            "step 1384: loss=1.621246337890625e-05\n",
            "step 1385: loss=0.08846426010131836\n",
            "step 1386: loss=0.0068340301513671875\n",
            "step 1387: loss=0.005295753479003906\n",
            "step 1388: loss=0.07916641235351562\n",
            "step 1389: loss=0.000637054443359375\n",
            "step 1390: loss=0.06777739524841309\n",
            "step 1391: loss=0.027057647705078125\n",
            "step 1392: loss=0.16372370719909668\n",
            "step 1393: loss=0.16978907585144043\n",
            "step 1394: loss=2.819176197052002\n",
            "step 1395: loss=0.0005025863647460938\n",
            "step 1396: loss=0.04915618896484375\n",
            "step 1397: loss=0.042624473571777344\n",
            "step 1398: loss=0.06264448165893555\n",
            "step 1399: loss=0.0442051887512207\n",
            "step 1400: loss=0.004332065582275391\n",
            "step 1401: loss=0.13192105293273926\n",
            "step 1402: loss=0.0004520416259765625\n",
            "step 1403: loss=0.0003199577331542969\n",
            "step 1404: loss=0.0013957023620605469\n",
            "step 1405: loss=0.0006384849548339844\n",
            "step 1406: loss=0.24060654640197754\n",
            "step 1407: loss=0.0015306472778320312\n",
            "step 1408: loss=0.0006685256958007812\n",
            "step 1409: loss=3.178748369216919\n",
            "step 1410: loss=0.01769542694091797\n",
            "step 1411: loss=0.05989360809326172\n",
            "step 1412: loss=0.0004711151123046875\n",
            "step 1413: loss=0.014223575592041016\n",
            "step 1414: loss=0.010881423950195312\n",
            "step 1415: loss=0.0029511451721191406\n",
            "step 1416: loss=1.204953670501709\n",
            "step 1417: loss=0.014825820922851562\n",
            "step 1418: loss=0.014616966247558594\n",
            "step 1419: loss=0.4398317337036133\n",
            "step 1420: loss=0.08567070960998535\n",
            "step 1421: loss=0.029613971710205078\n",
            "step 1422: loss=0.16761207580566406\n",
            "step 1423: loss=0.015293598175048828\n",
            "step 1424: loss=0.003395557403564453\n",
            "step 1425: loss=0.2690110206604004\n",
            "step 1426: loss=0.2226696014404297\n",
            "step 1427: loss=0.1279745101928711\n",
            "step 1428: loss=0.1160275936126709\n",
            "step 1429: loss=0.35317516326904297\n",
            "step 1430: loss=7.62939453125e-05\n",
            "step 1431: loss=0.018709659576416016\n",
            "step 1432: loss=0.2945125102996826\n",
            "step 1433: loss=0.11767315864562988\n",
            "step 1434: loss=0.12093925476074219\n",
            "step 1435: loss=0.012970447540283203\n",
            "step 1436: loss=0.18533062934875488\n",
            "step 1437: loss=5.340576171875e-05\n",
            "step 1438: loss=0.001495361328125\n",
            "step 1439: loss=0.3231240510940552\n",
            "step 1440: loss=0.0005245208740234375\n",
            "step 1441: loss=0.02144145965576172\n",
            "step 1442: loss=0.03689265251159668\n",
            "step 1443: loss=0.07671570777893066\n",
            "step 1444: loss=0.3404301404953003\n",
            "step 1445: loss=0.04117584228515625\n",
            "step 1446: loss=0.11533355712890625\n",
            "step 1447: loss=0.011414051055908203\n",
            "step 1448: loss=0.018762588500976562\n",
            "step 1449: loss=0.03667593002319336\n",
            "step 1450: loss=0.6469244956970215\n",
            "step 1451: loss=0.0010986328125\n",
            "step 1452: loss=0.3163294792175293\n",
            "step 1453: loss=0.030060768127441406\n",
            "step 1454: loss=0.8069682121276855\n",
            "step 1455: loss=0.3349423408508301\n",
            "step 1456: loss=1.7350786924362183\n",
            "step 1457: loss=0.03682851791381836\n",
            "step 1458: loss=0.09995269775390625\n",
            "step 1459: loss=0.01538705825805664\n",
            "step 1460: loss=0.006072044372558594\n",
            "step 1461: loss=0.8176702260971069\n",
            "step 1462: loss=0.0019655227661132812\n",
            "step 1463: loss=0.06472039222717285\n",
            "step 1464: loss=0.0028214454650878906\n",
            "step 1465: loss=0.6776237487792969\n",
            "step 1466: loss=0.0041866302490234375\n",
            "step 1467: loss=0.569083571434021\n",
            "step 1468: loss=0.05303621292114258\n",
            "step 1469: loss=0.04572129249572754\n",
            "step 1470: loss=0.0050792694091796875\n",
            "step 1471: loss=0.5556769371032715\n",
            "step 1472: loss=0.6054384708404541\n",
            "step 1473: loss=0.016014575958251953\n",
            "step 1474: loss=0.11725997924804688\n",
            "step 1475: loss=0.0011010169982910156\n",
            "step 1476: loss=0.07707905769348145\n",
            "step 1477: loss=0.016223907470703125\n",
            "step 1478: loss=0.0010433197021484375\n",
            "step 1479: loss=0.0739741325378418\n",
            "step 1480: loss=0.028263568878173828\n",
            "step 1481: loss=0.014484882354736328\n",
            "step 1482: loss=0.02749776840209961\n",
            "step 1483: loss=0.019678592681884766\n",
            "step 1484: loss=0.03412318229675293\n",
            "step 1485: loss=0.3073434829711914\n",
            "step 1486: loss=0.0014429092407226562\n",
            "step 1487: loss=8.296966552734375e-05\n",
            "step 1488: loss=0.0024118423461914062\n",
            "step 1489: loss=0.006085872650146484\n",
            "step 1490: loss=0.14802050590515137\n",
            "step 1491: loss=0.0007772445678710938\n",
            "step 1492: loss=0.000522613525390625\n",
            "step 1493: loss=0.015981674194335938\n",
            "step 1494: loss=0.0436711311340332\n",
            "step 1495: loss=0.008254051208496094\n",
            "step 1496: loss=0.2353816032409668\n",
            "step 1497: loss=0.005907535552978516\n",
            "step 1498: loss=0.004480838775634766\n",
            "step 1499: loss=0.9267098903656006\n",
            "step 1500: loss=0.1269702911376953\n",
            "step 1501: loss=0.0011110305786132812\n",
            "step 1502: loss=0.0768890380859375\n",
            "step 1503: loss=0.012132644653320312\n",
            "step 1504: loss=0.025094985961914062\n",
            "step 1505: loss=0.003726959228515625\n",
            "step 1506: loss=0.17690587043762207\n",
            "step 1507: loss=0.182356595993042\n",
            "step 1508: loss=0.009600639343261719\n",
            "step 1509: loss=0.0022687911987304688\n",
            "step 1510: loss=0.019440650939941406\n",
            "step 1511: loss=0.07108449935913086\n",
            "step 1512: loss=0.05274677276611328\n",
            "step 1513: loss=0.01139068603515625\n",
            "step 1514: loss=0.20629453659057617\n",
            "step 1515: loss=0.0014576911926269531\n",
            "step 1516: loss=0.012168407440185547\n",
            "step 1517: loss=0.30356109142303467\n",
            "step 1518: loss=0.10848736763000488\n",
            "step 1519: loss=0.007010936737060547\n",
            "step 1520: loss=0.12987732887268066\n",
            "step 1521: loss=0.022357463836669922\n",
            "step 1522: loss=0.013411521911621094\n",
            "step 1523: loss=0.005294322967529297\n",
            "step 1524: loss=0.005818367004394531\n",
            "step 1525: loss=0.001972675323486328\n",
            "step 1526: loss=3.3895387649536133\n",
            "step 1527: loss=0.0002899169921875\n",
            "step 1528: loss=7.62939453125e-05\n",
            "step 1529: loss=0.10421061515808105\n",
            "step 1530: loss=0.0032825469970703125\n",
            "step 1531: loss=0.005353450775146484\n",
            "step 1532: loss=4.100799560546875e-05\n",
            "step 1533: loss=0.014390945434570312\n",
            "step 1534: loss=0.0040836334228515625\n",
            "step 1535: loss=0.041461944580078125\n",
            "step 1536: loss=0.0023102760314941406\n",
            "step 1537: loss=0.021305084228515625\n",
            "step 1538: loss=0.633580207824707\n",
            "step 1539: loss=0.560760498046875\n",
            "step 1540: loss=1.6962655782699585\n",
            "step 1541: loss=0.0012602806091308594\n",
            "step 1542: loss=0.04548168182373047\n",
            "step 1543: loss=0.08443713188171387\n",
            "step 1544: loss=0.008582592010498047\n",
            "step 1545: loss=0.2191098928451538\n",
            "step 1546: loss=0.23373770713806152\n",
            "step 1547: loss=0.008641719818115234\n",
            "step 1548: loss=0.0007681846618652344\n",
            "step 1549: loss=0.03319740295410156\n",
            "step 1550: loss=0.28096914291381836\n",
            "step 1551: loss=0.6629800796508789\n",
            "step 1552: loss=0.10477828979492188\n",
            "step 1553: loss=0.002971172332763672\n",
            "step 1554: loss=0.50518798828125\n",
            "step 1555: loss=0.23905658721923828\n",
            "step 1556: loss=1.043480634689331\n",
            "step 1557: loss=0.19914507865905762\n",
            "step 1558: loss=9.632110595703125e-05\n",
            "step 1559: loss=0.0014085769653320312\n",
            "step 1560: loss=0.08930253982543945\n",
            "step 1561: loss=0.030561447143554688\n",
            "step 1562: loss=0.16022777557373047\n",
            "step 1563: loss=0.17746973037719727\n",
            "step 1564: loss=0.016654491424560547\n",
            "step 1565: loss=0.008440971374511719\n",
            "step 1566: loss=0.5582807064056396\n",
            "step 1567: loss=0.024689674377441406\n",
            "step 1568: loss=0.05111336708068848\n",
            "step 1569: loss=0.03325366973876953\n",
            "step 1570: loss=0.018563270568847656\n",
            "step 1571: loss=0.009358882904052734\n",
            "step 1572: loss=0.015969276428222656\n",
            "step 1573: loss=0.21262407302856445\n",
            "step 1574: loss=0.20110130310058594\n",
            "step 1575: loss=0.2016611099243164\n",
            "step 1576: loss=0.06192350387573242\n",
            "step 1577: loss=0.08449459075927734\n",
            "step 1578: loss=0.0019202232360839844\n",
            "step 1579: loss=0.012054920196533203\n",
            "step 1580: loss=0.01990032196044922\n",
            "step 1581: loss=0.004056453704833984\n",
            "step 1582: loss=0.46944499015808105\n",
            "step 1583: loss=0.18107128143310547\n",
            "step 1584: loss=0.0025949478149414062\n",
            "step 1585: loss=0.028498172760009766\n",
            "step 1586: loss=0.2936685085296631\n",
            "step 1587: loss=0.06951570510864258\n",
            "step 1588: loss=0.4252622127532959\n",
            "step 1589: loss=0.8792774677276611\n",
            "step 1590: loss=0.06547975540161133\n",
            "step 1591: loss=0.007937908172607422\n",
            "step 1592: loss=0.39698755741119385\n",
            "step 1593: loss=0.031064510345458984\n",
            "step 1594: loss=0.013475894927978516\n",
            "step 1595: loss=2.1847925186157227\n",
            "step 1596: loss=0.5230252742767334\n",
            "step 1597: loss=0.21626710891723633\n",
            "step 1598: loss=0.1911168098449707\n",
            "step 1599: loss=0.04807901382446289\n",
            "step 1600: loss=0.027595043182373047\n",
            "step 1601: loss=0.0052280426025390625\n",
            "step 1602: loss=0.05585074424743652\n",
            "step 1603: loss=0.20243167877197266\n",
            "step 1604: loss=1.2042088508605957\n",
            "step 1605: loss=0.06678557395935059\n",
            "step 1606: loss=0.08559155464172363\n",
            "step 1607: loss=0.2045135498046875\n",
            "step 1608: loss=0.06025123596191406\n",
            "step 1609: loss=0.11030864715576172\n",
            "step 1610: loss=0.06580829620361328\n",
            "step 1611: loss=1.032108187675476\n",
            "step 1612: loss=0.009571552276611328\n",
            "step 1613: loss=0.027226924896240234\n",
            "step 1614: loss=0.06454634666442871\n",
            "step 1615: loss=0.029736995697021484\n",
            "step 1616: loss=0.07435250282287598\n",
            "step 1617: loss=0.00044727325439453125\n",
            "step 1618: loss=0.16203045845031738\n",
            "step 1619: loss=0.07793307304382324\n",
            "step 1620: loss=0.11376667022705078\n",
            "step 1621: loss=0.298598051071167\n",
            "step 1622: loss=0.03981733322143555\n",
            "step 1623: loss=0.04098224639892578\n",
            "step 1624: loss=0.6176102161407471\n",
            "step 1625: loss=1.0984222888946533\n",
            "step 1626: loss=0.14606952667236328\n",
            "step 1627: loss=0.026809215545654297\n",
            "step 1628: loss=0.0001201629638671875\n",
            "step 1629: loss=0.43259143829345703\n",
            "step 1630: loss=0.0017147064208984375\n",
            "step 1631: loss=0.20094680786132812\n",
            "step 1632: loss=0.24129843711853027\n",
            "step 1633: loss=0.15503883361816406\n",
            "step 1634: loss=0.0002727508544921875\n",
            "step 1635: loss=0.4966268539428711\n",
            "step 1636: loss=0.00026607513427734375\n",
            "step 1637: loss=0.44066619873046875\n",
            "step 1638: loss=0.06043243408203125\n",
            "step 1639: loss=0.002414703369140625\n",
            "step 1640: loss=0.1826009750366211\n",
            "step 1641: loss=0.1027994155883789\n",
            "step 1642: loss=0.003440380096435547\n",
            "step 1643: loss=0.5506304502487183\n",
            "step 1644: loss=0.008540153503417969\n",
            "step 1645: loss=0.01575756072998047\n",
            "step 1646: loss=0.0011467933654785156\n",
            "step 1647: loss=0.13507437705993652\n",
            "step 1648: loss=0.02973794937133789\n",
            "step 1649: loss=0.0007567405700683594\n",
            "step 1650: loss=0.005146503448486328\n",
            "step 1651: loss=0.009072303771972656\n",
            "step 1652: loss=0.2804841995239258\n",
            "step 1653: loss=0.0010547637939453125\n",
            "step 1654: loss=0.11556434631347656\n",
            "step 1655: loss=0.5471842288970947\n",
            "step 1656: loss=0.018390655517578125\n",
            "step 1657: loss=0.005279064178466797\n",
            "step 1658: loss=0.2854125499725342\n",
            "step 1659: loss=0.10133934020996094\n",
            "step 1660: loss=0.14499926567077637\n",
            "step 1661: loss=0.00057220458984375\n",
            "step 1662: loss=0.0362401008605957\n",
            "step 1663: loss=0.0419154167175293\n",
            "step 1664: loss=0.03362703323364258\n",
            "step 1665: loss=0.008584976196289062\n",
            "step 1666: loss=0.0005311965942382812\n",
            "step 1667: loss=0.0011043548583984375\n",
            "step 1668: loss=0.5511074066162109\n",
            "step 1669: loss=0.36252593994140625\n",
            "step 1670: loss=0.15012526512145996\n",
            "step 1671: loss=0.2701714038848877\n",
            "step 1672: loss=0.42069387435913086\n",
            "step 1673: loss=0.8218390941619873\n",
            "step 1674: loss=0.034110069274902344\n",
            "step 1675: loss=0.6801662445068359\n",
            "step 1676: loss=0.018695831298828125\n",
            "step 1677: loss=0.009753704071044922\n",
            "step 1678: loss=0.09774327278137207\n",
            "step 1679: loss=0.032511234283447266\n",
            "step 1680: loss=0.00012683868408203125\n",
            "step 1681: loss=0.13346290588378906\n",
            "step 1682: loss=0.00014400482177734375\n",
            "step 1683: loss=0.08217620849609375\n",
            "step 1684: loss=0.22934794425964355\n",
            "step 1685: loss=0.12432575225830078\n",
            "step 1686: loss=0.0053577423095703125\n",
            "step 1687: loss=0.01948070526123047\n",
            "step 1688: loss=0.00985860824584961\n",
            "step 1689: loss=0.4117429256439209\n",
            "step 1690: loss=0.20614194869995117\n",
            "step 1691: loss=0.09371733665466309\n",
            "step 1692: loss=0.9308011531829834\n",
            "step 1693: loss=0.005922794342041016\n",
            "step 1694: loss=0.012776374816894531\n",
            "step 1695: loss=0.008655548095703125\n",
            "step 1696: loss=0.015024662017822266\n",
            "step 1697: loss=0.5668759346008301\n",
            "step 1698: loss=0.05148029327392578\n",
            "step 1699: loss=0.1288611888885498\n",
            "step 1700: loss=0.0530400276184082\n",
            "step 1701: loss=0.0011267662048339844\n",
            "step 1702: loss=0.003935813903808594\n",
            "step 1703: loss=0.0409088134765625\n",
            "step 1704: loss=0.006133079528808594\n",
            "step 1705: loss=0.031178951263427734\n",
            "step 1706: loss=0.03152275085449219\n",
            "step 1707: loss=0.43497276306152344\n",
            "step 1708: loss=0.11254167556762695\n",
            "step 1709: loss=0.027180194854736328\n",
            "step 1710: loss=0.025602340698242188\n",
            "step 1711: loss=0.3667844533920288\n",
            "step 1712: loss=0.17866086959838867\n",
            "step 1713: loss=0.04136824607849121\n",
            "step 1714: loss=0.030861616134643555\n",
            "step 1715: loss=0.003639698028564453\n",
            "step 1716: loss=0.004277229309082031\n",
            "step 1717: loss=0.04141688346862793\n",
            "step 1718: loss=0.04918408393859863\n",
            "step 1719: loss=0.00034427642822265625\n",
            "step 1720: loss=0.0034394264221191406\n",
            "step 1721: loss=0.006712436676025391\n",
            "step 1722: loss=0.010517120361328125\n",
            "step 1723: loss=0.00024890899658203125\n",
            "step 1724: loss=0.22164535522460938\n",
            "step 1725: loss=3.8018200397491455\n",
            "step 1726: loss=0.08279275894165039\n",
            "step 1727: loss=0.00037384033203125\n",
            "step 1728: loss=0.04691290855407715\n",
            "step 1729: loss=0.13104796409606934\n",
            "step 1730: loss=0.023840904235839844\n",
            "step 1731: loss=0.007630825042724609\n",
            "step 1732: loss=0.4625493288040161\n",
            "step 1733: loss=0.21915912628173828\n",
            "step 1734: loss=0.003639698028564453\n",
            "step 1735: loss=0.2976408004760742\n",
            "step 1736: loss=0.004973888397216797\n",
            "step 1737: loss=0.20421290397644043\n",
            "step 1738: loss=0.019153118133544922\n",
            "step 1739: loss=5.245208740234375e-05\n",
            "step 1740: loss=0.054657936096191406\n",
            "step 1741: loss=0.07558655738830566\n",
            "step 1742: loss=5.26342248916626\n",
            "step 1743: loss=0.31380319595336914\n",
            "step 1744: loss=0.003200531005859375\n",
            "step 1745: loss=0.22080516815185547\n",
            "step 1746: loss=0.009223461151123047\n",
            "step 1747: loss=0.4722909927368164\n",
            "step 1748: loss=0.009346485137939453\n",
            "step 1749: loss=0.0032668113708496094\n",
            "step 1750: loss=0.9367046356201172\n",
            "step 1751: loss=0.004542827606201172\n",
            "step 1752: loss=0.19883036613464355\n",
            "step 1753: loss=0.030304431915283203\n",
            "step 1754: loss=0.10064077377319336\n",
            "step 1755: loss=0.1117866039276123\n",
            "step 1756: loss=0.22506427764892578\n",
            "step 1757: loss=0.0016832351684570312\n",
            "step 1758: loss=0.09720849990844727\n",
            "step 1759: loss=0.07725334167480469\n",
            "step 1760: loss=0.02687215805053711\n",
            "step 1761: loss=0.000583648681640625\n",
            "step 1762: loss=0.0722355842590332\n",
            "step 1763: loss=0.14926719665527344\n",
            "step 1764: loss=0.08098077774047852\n",
            "step 1765: loss=0.07811355590820312\n",
            "step 1766: loss=0.16404354572296143\n",
            "step 1767: loss=0.09472155570983887\n",
            "step 1768: loss=0.012936115264892578\n",
            "step 1769: loss=7.05718994140625e-05\n",
            "step 1770: loss=0.12040185928344727\n",
            "step 1771: loss=0.2715296745300293\n",
            "step 1772: loss=1.3782072067260742\n",
            "step 1773: loss=0.017582416534423828\n",
            "step 1774: loss=0.023566246032714844\n",
            "step 1775: loss=0.0020427703857421875\n",
            "step 1776: loss=0.030818462371826172\n",
            "step 1777: loss=0.0083770751953125\n",
            "step 1778: loss=0.008779525756835938\n",
            "step 1779: loss=0.004837989807128906\n",
            "step 1780: loss=0.009705066680908203\n",
            "step 1781: loss=9.441375732421875e-05\n",
            "step 1782: loss=0.0312957763671875\n",
            "step 1783: loss=0.0017952919006347656\n",
            "step 1784: loss=0.0017518997192382812\n",
            "step 1785: loss=0.01167917251586914\n",
            "step 1786: loss=0.006351947784423828\n",
            "step 1787: loss=0.18572139739990234\n",
            "step 1788: loss=0.22350692749023438\n",
            "step 1789: loss=0.03586721420288086\n",
            "step 1790: loss=0.03142404556274414\n",
            "step 1791: loss=0.057220458984375\n",
            "step 1792: loss=0.023380756378173828\n",
            "step 1793: loss=0.2261967658996582\n",
            "step 1794: loss=0.1636042594909668\n",
            "step 1795: loss=0.0008311271667480469\n",
            "step 1796: loss=0.009186267852783203\n",
            "step 1797: loss=0.05345296859741211\n",
            "step 1798: loss=2.1842336654663086\n",
            "step 1799: loss=0.23951339721679688\n",
            "step 1800: loss=0.46537208557128906\n",
            "step 1801: loss=0.006124019622802734\n",
            "step 1802: loss=0.0038728713989257812\n",
            "step 1803: loss=0.024188995361328125\n",
            "step 1804: loss=0.160139799118042\n",
            "step 1805: loss=0.0003528594970703125\n",
            "step 1806: loss=0.00042057037353515625\n",
            "step 1807: loss=0.0033197402954101562\n",
            "step 1808: loss=0.0014467239379882812\n",
            "step 1809: loss=0.2608311176300049\n",
            "step 1810: loss=0.00026416778564453125\n",
            "step 1811: loss=0.608456015586853\n",
            "step 1812: loss=0.15521621704101562\n",
            "step 1813: loss=0.025329113006591797\n",
            "step 1814: loss=0.9537088871002197\n",
            "step 1815: loss=0.012410163879394531\n",
            "step 1816: loss=0.04104948043823242\n",
            "step 1817: loss=0.14691758155822754\n",
            "step 1818: loss=0.020838260650634766\n",
            "step 1819: loss=0.0043392181396484375\n",
            "step 1820: loss=0.02861928939819336\n",
            "step 1821: loss=0.020590782165527344\n",
            "step 1822: loss=0.007065773010253906\n",
            "step 1823: loss=0.03475332260131836\n",
            "step 1824: loss=0.13762950897216797\n",
            "step 1825: loss=0.3033792972564697\n",
            "step 1826: loss=0.0031065940856933594\n",
            "step 1827: loss=0.904409646987915\n",
            "step 1828: loss=0.002132415771484375\n",
            "step 1829: loss=0.048436641693115234\n",
            "step 1830: loss=0.05208635330200195\n",
            "step 1831: loss=0.0666966438293457\n",
            "step 1832: loss=0.000125885009765625\n",
            "step 1833: loss=0.02478313446044922\n",
            "step 1834: loss=0.21184587478637695\n",
            "step 1835: loss=0.3109102249145508\n",
            "step 1836: loss=0.0035266876220703125\n",
            "step 1837: loss=0.008171558380126953\n",
            "step 1838: loss=0.0017652511596679688\n",
            "step 1839: loss=0.0040950775146484375\n",
            "step 1840: loss=0.06879138946533203\n",
            "step 1841: loss=0.0003528594970703125\n",
            "step 1842: loss=0.0267181396484375\n",
            "step 1843: loss=0.0008697509765625\n",
            "step 1844: loss=0.03314352035522461\n",
            "step 1845: loss=0.00011730194091796875\n",
            "step 1846: loss=0.056177616119384766\n",
            "step 1847: loss=0.23808813095092773\n",
            "step 1848: loss=0.09098553657531738\n",
            "step 1849: loss=0.0001678466796875\n",
            "step 1850: loss=0.030178070068359375\n",
            "step 1851: loss=0.008967876434326172\n",
            "step 1852: loss=0.36790359020233154\n",
            "step 1853: loss=0.07280945777893066\n",
            "step 1854: loss=0.008320331573486328\n",
            "step 1855: loss=0.42780542373657227\n",
            "step 1856: loss=0.038349151611328125\n",
            "step 1857: loss=0.021057605743408203\n",
            "step 1858: loss=0.009790897369384766\n",
            "step 1859: loss=0.045013427734375\n",
            "step 1860: loss=0.2187962532043457\n",
            "step 1861: loss=0.009824752807617188\n",
            "step 1862: loss=0.0001583099365234375\n",
            "step 1863: loss=0.03968954086303711\n",
            "step 1864: loss=0.0054836273193359375\n",
            "step 1865: loss=0.1407923698425293\n",
            "step 1866: loss=0.013606548309326172\n",
            "step 1867: loss=0.17886734008789062\n",
            "step 1868: loss=0.005841732025146484\n",
            "step 1869: loss=1.4918911457061768\n",
            "step 1870: loss=0.0005960464477539062\n",
            "step 1871: loss=0.04952812194824219\n",
            "step 1872: loss=0.02548360824584961\n",
            "step 1873: loss=0.0071849822998046875\n",
            "step 1874: loss=0.3841726779937744\n",
            "step 1875: loss=0.07076501846313477\n",
            "step 1876: loss=0.4331812858581543\n",
            "step 1877: loss=0.04628753662109375\n",
            "step 1878: loss=0.03896903991699219\n",
            "step 1879: loss=0.0006542205810546875\n",
            "step 1880: loss=0.1489114761352539\n",
            "step 1881: loss=0.006114482879638672\n",
            "step 1882: loss=0.088775634765625\n",
            "step 1883: loss=0.020968914031982422\n",
            "step 1884: loss=0.0011320114135742188\n",
            "step 1885: loss=0.01263570785522461\n",
            "step 1886: loss=0.4453902244567871\n",
            "step 1887: loss=0.03040599822998047\n",
            "step 1888: loss=0.03289365768432617\n",
            "step 1889: loss=0.18955421447753906\n",
            "step 1890: loss=0.13664793968200684\n",
            "step 1891: loss=0.004312038421630859\n",
            "step 1892: loss=2.384185791015625e-05\n",
            "step 1893: loss=0.07896852493286133\n",
            "step 1894: loss=0.07677531242370605\n",
            "step 1895: loss=0.0010290145874023438\n",
            "step 1896: loss=0.0012936592102050781\n",
            "step 1897: loss=0.19965672492980957\n",
            "step 1898: loss=0.0755147933959961\n",
            "step 1899: loss=0.0032405853271484375\n",
            "step 1900: loss=0.005822181701660156\n",
            "step 1901: loss=0.08955764770507812\n",
            "step 1902: loss=0.0026230812072753906\n",
            "step 1903: loss=0.20334601402282715\n",
            "step 1904: loss=0.008885383605957031\n",
            "step 1905: loss=0.25542783737182617\n",
            "step 1906: loss=0.03086256980895996\n",
            "step 1907: loss=0.2506754398345947\n",
            "step 1908: loss=0.0007014274597167969\n",
            "step 1909: loss=0.0051727294921875\n",
            "step 1910: loss=0.11113548278808594\n",
            "step 1911: loss=0.04044389724731445\n",
            "step 1912: loss=0.0004215240478515625\n",
            "step 1913: loss=0.019912242889404297\n",
            "step 1914: loss=1.4262325763702393\n",
            "step 1915: loss=0.027172088623046875\n",
            "step 1916: loss=0.09154462814331055\n",
            "step 1917: loss=0.010875701904296875\n",
            "step 1918: loss=0.10902118682861328\n",
            "step 1919: loss=0.11564230918884277\n",
            "step 1920: loss=0.06446361541748047\n",
            "step 1921: loss=0.08886265754699707\n",
            "step 1922: loss=0.1756899356842041\n",
            "step 1923: loss=0.023624897003173828\n",
            "step 1924: loss=0.006579399108886719\n",
            "step 1925: loss=0.006858348846435547\n",
            "step 1926: loss=0.00543212890625\n",
            "step 1927: loss=0.0240020751953125\n",
            "step 1928: loss=0.6833474636077881\n",
            "step 1929: loss=0.0577082633972168\n",
            "step 1930: loss=0.017292022705078125\n",
            "step 1931: loss=0.040328025817871094\n",
            "step 1932: loss=0.005287647247314453\n",
            "step 1933: loss=0.012613296508789062\n",
            "step 1934: loss=0.004623889923095703\n",
            "step 1935: loss=0.0011153221130371094\n",
            "step 1936: loss=0.5298681259155273\n",
            "step 1937: loss=0.00030517578125\n",
            "step 1938: loss=0.1638965606689453\n",
            "step 1939: loss=0.34738266468048096\n",
            "step 1940: loss=0.013697147369384766\n",
            "step 1941: loss=0.0030884742736816406\n",
            "step 1942: loss=0.13126540184020996\n",
            "step 1943: loss=0.0009889602661132812\n",
            "step 1944: loss=0.000640869140625\n",
            "step 1945: loss=2.8810253143310547\n",
            "step 1946: loss=0.001873016357421875\n",
            "step 1947: loss=0.09017419815063477\n",
            "step 1948: loss=0.0250396728515625\n",
            "step 1949: loss=0.15851640701293945\n",
            "step 1950: loss=0.03221940994262695\n",
            "step 1951: loss=0.031409263610839844\n",
            "step 1952: loss=0.0022249221801757812\n",
            "step 1953: loss=0.0010695457458496094\n",
            "step 1954: loss=0.07157421112060547\n",
            "step 1955: loss=0.00015926361083984375\n",
            "step 1956: loss=0.019190311431884766\n",
            "step 1957: loss=0.0012798309326171875\n",
            "step 1958: loss=0.17901945114135742\n",
            "step 1959: loss=0.09918451309204102\n",
            "step 1960: loss=0.12117528915405273\n",
            "step 1961: loss=0.26049208641052246\n",
            "step 1962: loss=0.019834041595458984\n",
            "step 1963: loss=0.000431060791015625\n",
            "step 1964: loss=0.04895210266113281\n",
            "step 1965: loss=0.29978060722351074\n",
            "step 1966: loss=0.20575571060180664\n",
            "step 1967: loss=0.025646209716796875\n",
            "step 1968: loss=0.1696157455444336\n",
            "step 1969: loss=0.008367061614990234\n",
            "step 1970: loss=0.2836923599243164\n",
            "step 1971: loss=0.0077667236328125\n",
            "step 1972: loss=0.0913543701171875\n",
            "step 1973: loss=0.03208160400390625\n",
            "step 1974: loss=0.25241875648498535\n",
            "step 1975: loss=0.6923378705978394\n",
            "step 1976: loss=0.0060482025146484375\n",
            "step 1977: loss=0.04904794692993164\n",
            "step 1978: loss=0.5755470991134644\n",
            "step 1979: loss=1.2452046871185303\n",
            "step 1980: loss=0.019805908203125\n",
            "step 1981: loss=0.1442852020263672\n",
            "step 1982: loss=0.1641380786895752\n",
            "step 1983: loss=0.22363948822021484\n",
            "step 1984: loss=0.021692276000976562\n",
            "step 1985: loss=0.0656898021697998\n",
            "step 1986: loss=0.028370380401611328\n",
            "step 1987: loss=0.016373634338378906\n",
            "step 1988: loss=0.04315376281738281\n",
            "step 1989: loss=0.00011348724365234375\n",
            "step 1990: loss=0.03131103515625\n",
            "step 1991: loss=0.02651691436767578\n",
            "step 1992: loss=0.13248801231384277\n",
            "step 1993: loss=0.0017499923706054688\n",
            "step 1994: loss=0.18218255043029785\n",
            "step 1995: loss=0.0590970516204834\n",
            "step 1996: loss=0.0019855499267578125\n",
            "step 1997: loss=0.0009751319885253906\n",
            "step 1998: loss=0.1408083438873291\n",
            "step 1999: loss=0.04166698455810547\n",
            "step 2000: loss=0.0007457733154296875\n",
            "Mean loss        0.37612873\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
          ]
        }
      ],
      "source": [
        "extra_steps = 1000\n",
        "key = jax.random.key(1)\n",
        "params = train_mlp(\n",
        "    train_data=train_data, \n",
        "    optimizer=optimizer, \n",
        "    training_state=restored_state,\n",
        "    )\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "092763fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=0.32265305519104004\n",
            "step 2: loss=0.7677340507507324\n",
            "step 3: loss=1.3151179552078247\n",
            "step 4: loss=0.6902498006820679\n",
            "step 5: loss=1.5896849632263184\n",
            "step 6: loss=0.20079827308654785\n",
            "step 7: loss=1.7467846870422363\n",
            "step 8: loss=0.7341939210891724\n",
            "step 9: loss=0.4832557439804077\n",
            "step 10: loss=2.7541565895080566\n",
            "step 11: loss=1.8300225734710693\n",
            "step 12: loss=4.892587661743164\n",
            "step 13: loss=0.03459310531616211\n",
            "step 14: loss=0.21892571449279785\n",
            "step 15: loss=6.246583938598633\n",
            "step 16: loss=3.886425018310547\n",
            "step 17: loss=0.005482196807861328\n",
            "step 18: loss=6.574459075927734\n",
            "step 19: loss=5.217857837677002\n",
            "step 20: loss=0.5348055362701416\n",
            "step 21: loss=2.4435982704162598\n",
            "step 22: loss=0.31761860847473145\n",
            "step 23: loss=0.20634746551513672\n",
            "step 24: loss=0.8876932859420776\n",
            "step 25: loss=0.6408635377883911\n",
            "step 26: loss=2.7429752349853516\n",
            "step 27: loss=4.988874912261963\n",
            "step 28: loss=0.7707905769348145\n",
            "step 29: loss=1.6248918771743774\n",
            "step 30: loss=1.6446428298950195\n",
            "step 31: loss=1.1385078430175781\n",
            "step 32: loss=1.1511971950531006\n",
            "step 33: loss=0.08532357215881348\n",
            "step 34: loss=0.5106781721115112\n",
            "step 35: loss=0.780877947807312\n",
            "step 36: loss=1.1551780700683594\n",
            "step 37: loss=0.18123197555541992\n",
            "step 38: loss=0.4434173107147217\n",
            "step 39: loss=3.417400598526001\n",
            "step 40: loss=0.10829424858093262\n",
            "step 41: loss=1.8061606884002686\n",
            "step 42: loss=4.005282402038574\n",
            "step 43: loss=0.1848764419555664\n",
            "step 44: loss=0.027591228485107422\n",
            "step 45: loss=1.5411931276321411\n",
            "step 46: loss=0.07511544227600098\n",
            "step 47: loss=2.55422043800354\n",
            "step 48: loss=2.4517924785614014\n",
            "step 49: loss=3.208172082901001\n",
            "step 50: loss=1.0272892713546753\n",
            "step 51: loss=0.34876132011413574\n",
            "step 52: loss=0.04001593589782715\n",
            "step 53: loss=5.630038261413574\n",
            "step 54: loss=0.13921570777893066\n",
            "step 55: loss=2.162700891494751\n",
            "step 56: loss=2.1467905044555664\n",
            "step 57: loss=0.565130352973938\n",
            "step 58: loss=0.20205998420715332\n",
            "step 59: loss=3.4275217056274414\n",
            "step 60: loss=0.2542855739593506\n",
            "step 61: loss=0.11789226531982422\n",
            "step 62: loss=0.4770071506500244\n",
            "step 63: loss=4.0579514503479\n",
            "step 64: loss=2.473933219909668\n",
            "step 65: loss=2.2945334911346436\n",
            "step 66: loss=0.004748821258544922\n",
            "step 67: loss=0.3415790796279907\n",
            "step 68: loss=0.008674860000610352\n",
            "step 69: loss=2.3101894855499268\n",
            "step 70: loss=0.7741418480873108\n",
            "step 71: loss=2.4873478412628174\n",
            "step 72: loss=2.179384469985962\n",
            "step 73: loss=0.11858105659484863\n",
            "step 74: loss=0.1509084701538086\n",
            "step 75: loss=0.015418291091918945\n",
            "step 76: loss=0.13529586791992188\n",
            "step 77: loss=2.7880239486694336\n",
            "step 78: loss=1.5176429748535156\n",
            "step 79: loss=0.743831992149353\n",
            "step 80: loss=0.08384871482849121\n",
            "step 81: loss=0.01779007911682129\n",
            "step 82: loss=0.22254681587219238\n",
            "step 83: loss=0.8793148994445801\n",
            "step 84: loss=0.5102661848068237\n",
            "step 85: loss=2.1784350872039795\n",
            "step 86: loss=0.2070540189743042\n",
            "step 87: loss=2.5184919834136963\n",
            "step 88: loss=0.10661184787750244\n",
            "step 89: loss=0.7768118381500244\n",
            "step 90: loss=0.7450364232063293\n",
            "step 91: loss=1.2006276845932007\n",
            "step 92: loss=0.27801084518432617\n",
            "step 93: loss=0.013895511627197266\n",
            "step 94: loss=1.7204831838607788\n",
            "step 95: loss=0.45591306686401367\n",
            "step 96: loss=1.7404078245162964\n",
            "step 97: loss=0.1535780429840088\n",
            "step 98: loss=1.05086088180542\n",
            "step 99: loss=0.7338215112686157\n",
            "step 100: loss=0.08263230323791504\n",
            "step 101: loss=1.5180065631866455\n",
            "step 102: loss=0.2216876745223999\n",
            "step 103: loss=1.6553356647491455\n",
            "step 104: loss=0.22328102588653564\n",
            "step 105: loss=0.6831310987472534\n",
            "step 106: loss=0.7622140049934387\n",
            "step 107: loss=0.7724780440330505\n",
            "step 108: loss=0.15826797485351562\n",
            "step 109: loss=0.7267024517059326\n",
            "step 110: loss=0.26287055015563965\n",
            "step 111: loss=0.6731970310211182\n",
            "step 112: loss=0.0007724761962890625\n",
            "step 113: loss=1.088413119316101\n",
            "step 114: loss=2.5593771934509277\n",
            "step 115: loss=0.3875555992126465\n",
            "step 116: loss=0.788495659828186\n",
            "step 117: loss=0.4652823209762573\n",
            "step 118: loss=0.016700029373168945\n",
            "step 119: loss=0.765770435333252\n",
            "step 120: loss=3.3574399948120117\n",
            "step 121: loss=1.613936424255371\n",
            "step 122: loss=2.8536741733551025\n",
            "step 123: loss=2.245337963104248\n",
            "step 124: loss=0.9435293674468994\n",
            "step 125: loss=0.16562044620513916\n",
            "step 126: loss=2.1027278900146484\n",
            "step 127: loss=0.07229161262512207\n",
            "step 128: loss=0.03443169593811035\n",
            "step 129: loss=0.09934830665588379\n",
            "step 130: loss=3.1200809478759766\n",
            "step 131: loss=0.08365917205810547\n",
            "step 132: loss=2.095539093017578\n",
            "step 133: loss=1.3606301546096802\n",
            "step 134: loss=0.04126381874084473\n",
            "step 135: loss=0.010730266571044922\n",
            "step 136: loss=0.027404069900512695\n",
            "step 137: loss=1.3282530307769775\n",
            "step 138: loss=0.020724773406982422\n",
            "step 139: loss=1.531835913658142\n",
            "step 140: loss=2.4742608070373535\n",
            "step 141: loss=0.0015177726745605469\n",
            "step 142: loss=1.0962364673614502\n",
            "step 143: loss=0.003307342529296875\n",
            "step 144: loss=1.5893136262893677\n",
            "step 145: loss=0.00022840499877929688\n",
            "step 146: loss=0.5872585773468018\n",
            "step 147: loss=0.8825898766517639\n",
            "step 148: loss=1.859703779220581\n",
            "step 149: loss=1.003840684890747\n",
            "step 150: loss=0.3936443328857422\n",
            "step 151: loss=0.26290345191955566\n",
            "step 152: loss=0.05683112144470215\n",
            "step 153: loss=1.0200072526931763\n",
            "step 154: loss=0.13823771476745605\n",
            "step 155: loss=1.6554133892059326\n",
            "step 156: loss=0.291939377784729\n",
            "step 157: loss=0.6884844303131104\n",
            "step 158: loss=3.8795664310455322\n",
            "step 159: loss=1.1649198532104492\n",
            "step 160: loss=0.5219359397888184\n",
            "step 161: loss=0.0643010139465332\n",
            "step 162: loss=0.13992834091186523\n",
            "step 163: loss=0.24103593826293945\n",
            "step 164: loss=0.5164413452148438\n",
            "step 165: loss=1.109811544418335\n",
            "step 166: loss=0.22077226638793945\n",
            "step 167: loss=1.6926841735839844\n",
            "step 168: loss=0.8362424373626709\n",
            "step 169: loss=0.6212509870529175\n",
            "step 170: loss=0.3304656744003296\n",
            "step 171: loss=0.03861188888549805\n",
            "step 172: loss=0.42426276206970215\n",
            "step 173: loss=1.4199190139770508\n",
            "step 174: loss=0.5431970357894897\n",
            "step 175: loss=1.1953392028808594\n",
            "step 176: loss=0.5589585304260254\n",
            "step 177: loss=0.30111610889434814\n",
            "step 178: loss=0.21876180171966553\n",
            "step 179: loss=0.05226778984069824\n",
            "step 180: loss=7.684690475463867\n",
            "step 181: loss=0.0629955530166626\n",
            "step 182: loss=0.026456832885742188\n",
            "step 183: loss=0.2521706819534302\n",
            "step 184: loss=1.133542537689209\n",
            "step 185: loss=0.0409083366394043\n",
            "step 186: loss=0.014989376068115234\n",
            "step 187: loss=1.9517629146575928\n",
            "step 188: loss=0.8857301473617554\n",
            "step 189: loss=0.18917810916900635\n",
            "step 190: loss=0.08302497863769531\n",
            "step 191: loss=0.006691932678222656\n",
            "step 192: loss=0.1600027084350586\n",
            "step 193: loss=0.04651451110839844\n",
            "step 194: loss=0.2521944046020508\n",
            "step 195: loss=0.004048347473144531\n",
            "step 196: loss=1.9263970851898193\n",
            "step 197: loss=0.3538475036621094\n",
            "step 198: loss=0.006560325622558594\n",
            "step 199: loss=4.006539821624756\n",
            "step 200: loss=0.00344085693359375\n",
            "step 201: loss=0.17727303504943848\n",
            "step 202: loss=2.0410079956054688\n",
            "step 203: loss=0.00588226318359375\n",
            "step 204: loss=0.009943962097167969\n",
            "step 205: loss=0.37145400047302246\n",
            "step 206: loss=0.3545961380004883\n",
            "step 207: loss=0.1705164909362793\n",
            "step 208: loss=0.7923063039779663\n",
            "step 209: loss=0.03950691223144531\n",
            "step 210: loss=0.06638145446777344\n",
            "step 211: loss=0.003098011016845703\n",
            "step 212: loss=0.7260702252388\n",
            "step 213: loss=2.109348773956299\n",
            "step 214: loss=3.948868989944458\n",
            "step 215: loss=0.03412032127380371\n",
            "step 216: loss=0.8013749122619629\n",
            "step 217: loss=1.1390293836593628\n",
            "step 218: loss=0.037386417388916016\n",
            "step 219: loss=0.009066581726074219\n",
            "step 220: loss=0.5269873738288879\n",
            "step 221: loss=1.0643603801727295\n",
            "step 222: loss=7.402938365936279\n",
            "step 223: loss=0.46387386322021484\n",
            "step 224: loss=0.17483091354370117\n",
            "step 225: loss=0.15937304496765137\n",
            "step 226: loss=0.7506805658340454\n",
            "step 227: loss=0.07784247398376465\n",
            "step 228: loss=0.1210477352142334\n",
            "step 229: loss=0.20534610748291016\n",
            "step 230: loss=1.8964284658432007\n",
            "step 231: loss=0.29928600788116455\n",
            "step 232: loss=0.024588346481323242\n",
            "step 233: loss=0.0014586448669433594\n",
            "step 234: loss=4.176973342895508\n",
            "step 235: loss=0.11298799514770508\n",
            "step 236: loss=0.3201591968536377\n",
            "step 237: loss=0.000431060791015625\n",
            "step 238: loss=0.08165979385375977\n",
            "step 239: loss=0.0720055103302002\n",
            "step 240: loss=0.06725811958312988\n",
            "step 241: loss=0.005335807800292969\n",
            "step 242: loss=0.05253028869628906\n",
            "step 243: loss=0.1551206111907959\n",
            "step 244: loss=0.03819417953491211\n",
            "step 245: loss=0.21362042427062988\n",
            "step 246: loss=0.8917489051818848\n",
            "step 247: loss=0.08256673812866211\n",
            "step 248: loss=4.838139057159424\n",
            "step 249: loss=0.17571258544921875\n",
            "step 250: loss=0.0005054473876953125\n",
            "step 251: loss=0.858322262763977\n",
            "step 252: loss=0.002238750457763672\n",
            "step 253: loss=0.33848685026168823\n",
            "step 254: loss=0.000408172607421875\n",
            "step 255: loss=0.00022172927856445312\n",
            "step 256: loss=0.06491231918334961\n",
            "step 257: loss=3.8026950359344482\n",
            "step 258: loss=0.5659790635108948\n",
            "step 259: loss=0.9608080387115479\n",
            "step 260: loss=0.029682159423828125\n",
            "step 261: loss=0.6717880964279175\n",
            "step 262: loss=1.2014391422271729\n",
            "step 263: loss=0.0009298324584960938\n",
            "step 264: loss=1.6889126300811768\n",
            "step 265: loss=1.1638314723968506\n",
            "step 266: loss=1.0330109596252441\n",
            "step 267: loss=0.7020214796066284\n",
            "step 268: loss=0.5142396092414856\n",
            "step 269: loss=0.00021028518676757812\n",
            "step 270: loss=0.8425743579864502\n",
            "step 271: loss=0.0003185272216796875\n",
            "step 272: loss=0.1912475824356079\n",
            "step 273: loss=0.6074347496032715\n",
            "step 274: loss=0.08104896545410156\n",
            "step 275: loss=0.7471480369567871\n",
            "step 276: loss=0.0176239013671875\n",
            "step 277: loss=0.30341410636901855\n",
            "step 278: loss=0.07025551795959473\n",
            "step 279: loss=0.15443706512451172\n",
            "step 280: loss=0.23258626461029053\n",
            "step 281: loss=0.004883289337158203\n",
            "step 282: loss=0.012855291366577148\n",
            "step 283: loss=1.2209012508392334\n",
            "step 284: loss=3.055433511734009\n",
            "step 285: loss=0.04236268997192383\n",
            "step 286: loss=0.08934330940246582\n",
            "step 287: loss=0.14063358306884766\n",
            "step 288: loss=0.012409687042236328\n",
            "step 289: loss=3.2886176109313965\n",
            "step 290: loss=0.023764610290527344\n",
            "step 291: loss=0.12459135055541992\n",
            "step 292: loss=5.072704315185547\n",
            "step 293: loss=0.0003032684326171875\n",
            "step 294: loss=0.0003199577331542969\n",
            "step 295: loss=0.41116583347320557\n",
            "step 296: loss=0.10678219795227051\n",
            "step 297: loss=0.017345905303955078\n",
            "step 298: loss=0.006708621978759766\n",
            "step 299: loss=4.57763671875e-05\n",
            "step 300: loss=0.021449804306030273\n",
            "step 301: loss=0.0001316070556640625\n",
            "step 302: loss=3.949216365814209\n",
            "step 303: loss=0.003194093704223633\n",
            "step 304: loss=1.154097080230713\n",
            "step 305: loss=0.5428704619407654\n",
            "step 306: loss=0.18551528453826904\n",
            "step 307: loss=1.8749196529388428\n",
            "step 308: loss=0.012996673583984375\n",
            "step 309: loss=1.2959485054016113\n",
            "step 310: loss=0.5560295581817627\n",
            "step 311: loss=1.8291726112365723\n",
            "step 312: loss=0.08615541458129883\n",
            "step 313: loss=0.0003256797790527344\n",
            "step 314: loss=0.29215192794799805\n",
            "step 315: loss=0.03787589073181152\n",
            "step 316: loss=0.5251855850219727\n",
            "step 317: loss=0.17471742630004883\n",
            "step 318: loss=0.803424596786499\n",
            "step 319: loss=0.06957745552062988\n",
            "step 320: loss=0.32988643646240234\n",
            "step 321: loss=1.7445917129516602\n",
            "step 322: loss=0.49298715591430664\n",
            "step 323: loss=0.00360107421875\n",
            "step 324: loss=0.1549227237701416\n",
            "step 325: loss=0.3078655004501343\n",
            "step 326: loss=2.5684375762939453\n",
            "step 327: loss=0.04657173156738281\n",
            "step 328: loss=0.6138942837715149\n",
            "step 329: loss=0.00041484832763671875\n",
            "step 330: loss=0.7314515113830566\n",
            "step 331: loss=6.4849853515625e-05\n",
            "step 332: loss=0.171295166015625\n",
            "step 333: loss=0.009427785873413086\n",
            "step 334: loss=0.0026459693908691406\n",
            "step 335: loss=0.5200076103210449\n",
            "step 336: loss=0.1210179328918457\n",
            "step 337: loss=0.1932663917541504\n",
            "step 338: loss=0.453906774520874\n",
            "step 339: loss=0.0375826358795166\n",
            "step 340: loss=0.007558345794677734\n",
            "step 341: loss=0.202500581741333\n",
            "step 342: loss=0.25803208351135254\n",
            "step 343: loss=0.00266265869140625\n",
            "step 344: loss=2.507458209991455\n",
            "step 345: loss=0.28925204277038574\n",
            "step 346: loss=0.07098007202148438\n",
            "step 347: loss=0.25934338569641113\n",
            "step 348: loss=0.0011501312255859375\n",
            "step 349: loss=0.3014342784881592\n",
            "step 350: loss=6.866455078125e-05\n",
            "step 351: loss=0.014784693717956543\n",
            "step 352: loss=3.0805153846740723\n",
            "step 353: loss=12.092905044555664\n",
            "step 354: loss=0.38045060634613037\n",
            "step 355: loss=0.004249095916748047\n",
            "step 356: loss=3.814697265625e-06\n",
            "step 357: loss=3.748836040496826\n",
            "step 358: loss=0.2553062438964844\n",
            "step 359: loss=0.014626264572143555\n",
            "step 360: loss=0.018168926239013672\n",
            "step 361: loss=0.0002727508544921875\n",
            "step 362: loss=7.62939453125e-06\n",
            "step 363: loss=2.632500410079956\n",
            "step 364: loss=0.9040087461471558\n",
            "step 365: loss=0.0024590492248535156\n",
            "step 366: loss=0.38548266887664795\n",
            "step 367: loss=0.3065621852874756\n",
            "step 368: loss=2.86102294921875e-06\n",
            "step 369: loss=0.028721332550048828\n",
            "step 370: loss=1.6159119606018066\n",
            "step 371: loss=0.32643795013427734\n",
            "step 372: loss=1.621246337890625e-05\n",
            "step 373: loss=5.245208740234375e-05\n",
            "step 374: loss=0.07358026504516602\n",
            "step 375: loss=0.019392013549804688\n",
            "step 376: loss=0.48038971424102783\n",
            "step 377: loss=1.983400821685791\n",
            "step 378: loss=0.05308675765991211\n",
            "step 379: loss=2.6464600563049316\n",
            "step 380: loss=5.91278076171875e-05\n",
            "step 381: loss=8.7738037109375e-05\n",
            "step 382: loss=1.44523286819458\n",
            "step 383: loss=0.09285855293273926\n",
            "step 384: loss=0.00015020370483398438\n",
            "step 385: loss=0.845956027507782\n",
            "step 386: loss=0.005894660949707031\n",
            "step 387: loss=0.03981900215148926\n",
            "step 388: loss=0.07858848571777344\n",
            "step 389: loss=0.00925302505493164\n",
            "step 390: loss=0.23601293563842773\n",
            "step 391: loss=0.14435720443725586\n",
            "step 392: loss=0.9261021614074707\n",
            "step 393: loss=2.9047539234161377\n",
            "step 394: loss=0.6411183476448059\n",
            "step 395: loss=0.0032625198364257812\n",
            "step 396: loss=0.3712114095687866\n",
            "step 397: loss=0.5558885931968689\n",
            "step 398: loss=0.24861681461334229\n",
            "step 399: loss=0.3766656517982483\n",
            "step 400: loss=0.0004787445068359375\n",
            "step 401: loss=0.548876166343689\n",
            "step 402: loss=0.008235692977905273\n",
            "step 403: loss=0.0\n",
            "step 404: loss=0.014981746673583984\n",
            "step 405: loss=0.17705893516540527\n",
            "step 406: loss=0.13042497634887695\n",
            "step 407: loss=0.06410336494445801\n",
            "step 408: loss=0.04739952087402344\n",
            "step 409: loss=2.131865978240967\n",
            "step 410: loss=0.0003981590270996094\n",
            "step 411: loss=0.2541177272796631\n",
            "step 412: loss=0.006911754608154297\n",
            "step 413: loss=0.4556429386138916\n",
            "step 414: loss=0.5143939256668091\n",
            "step 415: loss=0.0007839202880859375\n",
            "step 416: loss=2.880568742752075\n",
            "step 417: loss=0.36188292503356934\n",
            "step 418: loss=0.014311790466308594\n",
            "step 419: loss=0.7310397028923035\n",
            "step 420: loss=0.021846771240234375\n",
            "step 421: loss=0.1183401346206665\n",
            "step 422: loss=0.3014969825744629\n",
            "step 423: loss=0.05230975151062012\n",
            "step 424: loss=0.07787609100341797\n",
            "step 425: loss=0.6174843311309814\n",
            "step 426: loss=0.34184789657592773\n",
            "step 427: loss=2.0489163398742676\n",
            "step 428: loss=0.04196906089782715\n",
            "step 429: loss=0.006031990051269531\n",
            "step 430: loss=0.000308990478515625\n",
            "step 431: loss=0.0991511344909668\n",
            "step 432: loss=0.966881275177002\n",
            "step 433: loss=0.03653979301452637\n",
            "step 434: loss=0.05861091613769531\n",
            "step 435: loss=2.86102294921875e-05\n",
            "step 436: loss=0.489473819732666\n",
            "step 437: loss=0.0001583099365234375\n",
            "step 438: loss=0.0024547576904296875\n",
            "step 439: loss=0.39988982677459717\n",
            "step 440: loss=0.0028562545776367188\n",
            "step 441: loss=0.0016698837280273438\n",
            "step 442: loss=0.09814143180847168\n",
            "step 443: loss=1.1457037925720215\n",
            "step 444: loss=1.0738518238067627\n",
            "step 445: loss=0.36549949645996094\n",
            "step 446: loss=1.9167814254760742\n",
            "step 447: loss=0.0006799697875976562\n",
            "step 448: loss=0.04466652870178223\n",
            "step 449: loss=0.3128929138183594\n",
            "step 450: loss=0.8619799613952637\n",
            "step 451: loss=0.0\n",
            "step 452: loss=0.07321929931640625\n",
            "step 453: loss=3.528594970703125e-05\n",
            "step 454: loss=7.581179141998291\n",
            "step 455: loss=2.1552703380584717\n",
            "step 456: loss=0.4741290807723999\n",
            "step 457: loss=0.19655442237854004\n",
            "step 458: loss=0.30273276567459106\n",
            "step 459: loss=0.028781652450561523\n",
            "step 460: loss=0.13044285774230957\n",
            "step 461: loss=3.537463665008545\n",
            "step 462: loss=0.04993104934692383\n",
            "step 463: loss=0.37506961822509766\n",
            "step 464: loss=0.061324357986450195\n",
            "step 465: loss=5.0580668449401855\n",
            "step 466: loss=2.1341159343719482\n",
            "step 467: loss=2.1960487365722656\n",
            "step 468: loss=0.11863040924072266\n",
            "step 469: loss=0.005974292755126953\n",
            "step 470: loss=0.45353591442108154\n",
            "step 471: loss=2.937084197998047\n",
            "step 472: loss=0.9321002960205078\n",
            "step 473: loss=0.15945672988891602\n",
            "step 474: loss=0.08726048469543457\n",
            "step 475: loss=0.006619453430175781\n",
            "step 476: loss=0.029901742935180664\n",
            "step 477: loss=0.6068131923675537\n",
            "step 478: loss=0.0002884864807128906\n",
            "step 479: loss=0.0007834434509277344\n",
            "step 480: loss=0.4513535499572754\n",
            "step 481: loss=0.5640511512756348\n",
            "step 482: loss=0.004827976226806641\n",
            "step 483: loss=0.0021944046020507812\n",
            "step 484: loss=1.0420289039611816\n",
            "step 485: loss=1.9060800075531006\n",
            "step 486: loss=0.0\n",
            "step 487: loss=2.765655517578125e-05\n",
            "step 488: loss=2.86102294921875e-06\n",
            "step 489: loss=0.0422205924987793\n",
            "step 490: loss=0.6254687309265137\n",
            "step 491: loss=0.0\n",
            "step 492: loss=0.010772943496704102\n",
            "step 493: loss=0.818950891494751\n",
            "step 494: loss=0.08899402618408203\n",
            "step 495: loss=0.020428180694580078\n",
            "step 496: loss=0.10305237770080566\n",
            "step 497: loss=4.100799560546875e-05\n",
            "step 498: loss=0.0010018348693847656\n",
            "step 499: loss=2.345212697982788\n",
            "step 500: loss=0.9736366271972656\n",
            "step 501: loss=0.0010628700256347656\n",
            "step 502: loss=0.43415725231170654\n",
            "step 503: loss=0.002132415771484375\n",
            "step 504: loss=0.01867961883544922\n",
            "step 505: loss=0.025866031646728516\n",
            "step 506: loss=0.2119976282119751\n",
            "step 507: loss=0.8820350170135498\n",
            "step 508: loss=0.02311849594116211\n",
            "step 509: loss=0.0013871192932128906\n",
            "step 510: loss=5.4836273193359375e-05\n",
            "step 511: loss=0.8871090412139893\n",
            "step 512: loss=0.3320283889770508\n",
            "step 513: loss=0.03793144226074219\n",
            "step 514: loss=0.23874425888061523\n",
            "step 515: loss=0.0009474754333496094\n",
            "step 516: loss=0.0018267631530761719\n",
            "step 517: loss=1.2269971370697021\n",
            "step 518: loss=0.7296001315116882\n",
            "step 519: loss=2.6226043701171875e-05\n",
            "step 520: loss=0.48151957988739014\n",
            "step 521: loss=0.00591278076171875\n",
            "step 522: loss=0.004025459289550781\n",
            "step 523: loss=0.0006747245788574219\n",
            "step 524: loss=0.26104021072387695\n",
            "step 525: loss=0.02453470230102539\n",
            "step 526: loss=4.242903232574463\n",
            "step 527: loss=4.38690185546875e-05\n",
            "step 528: loss=4.4345855712890625e-05\n",
            "step 529: loss=1.219091773033142\n",
            "step 530: loss=0.013295650482177734\n",
            "step 531: loss=0.13933086395263672\n",
            "step 532: loss=0.003219127655029297\n",
            "step 533: loss=0.299074649810791\n",
            "step 534: loss=0.0024003982543945312\n",
            "step 535: loss=0.030065059661865234\n",
            "step 536: loss=0.40778589248657227\n",
            "step 537: loss=0.0035400390625\n",
            "step 538: loss=2.0618159770965576\n",
            "step 539: loss=0.9247198104858398\n",
            "step 540: loss=1.0234405994415283\n",
            "step 541: loss=9.5367431640625e-07\n",
            "step 542: loss=0.12253141403198242\n",
            "step 543: loss=0.4628567695617676\n",
            "step 544: loss=0.11419796943664551\n",
            "step 545: loss=0.6519138216972351\n",
            "step 546: loss=0.4321986436843872\n",
            "step 547: loss=0.0775449275970459\n",
            "step 548: loss=0.07050490379333496\n",
            "step 549: loss=0.0722970962524414\n",
            "step 550: loss=0.022182226181030273\n",
            "step 551: loss=0.7015266418457031\n",
            "step 552: loss=0.406868577003479\n",
            "step 553: loss=0.0003204345703125\n",
            "step 554: loss=2.955003499984741\n",
            "step 555: loss=0.5865879058837891\n",
            "step 556: loss=2.018953323364258\n",
            "step 557: loss=0.09763932228088379\n",
            "step 558: loss=6.67572021484375e-06\n",
            "step 559: loss=9.5367431640625e-07\n",
            "step 560: loss=0.04217982292175293\n",
            "step 561: loss=0.15691566467285156\n",
            "step 562: loss=0.8645279407501221\n",
            "step 563: loss=0.04625892639160156\n",
            "step 564: loss=0.1427147388458252\n",
            "step 565: loss=0.0018582344055175781\n",
            "step 566: loss=1.4060028791427612\n",
            "step 567: loss=0.0018420219421386719\n",
            "step 568: loss=0.1398511528968811\n",
            "step 569: loss=0.018711090087890625\n",
            "step 570: loss=0.0006761550903320312\n",
            "step 571: loss=0.0005822181701660156\n",
            "step 572: loss=0.0029573440551757812\n",
            "step 573: loss=0.00048732757568359375\n",
            "step 574: loss=0.9600019454956055\n",
            "step 575: loss=0.20406532287597656\n",
            "step 576: loss=0.009447097778320312\n",
            "step 577: loss=0.221785306930542\n",
            "step 578: loss=0.0006113052368164062\n",
            "step 579: loss=0.0\n",
            "step 580: loss=1.1537532806396484\n",
            "step 581: loss=4.100799560546875e-05\n",
            "step 582: loss=8.61931037902832\n",
            "step 583: loss=0.1169276237487793\n",
            "step 584: loss=0.3266434669494629\n",
            "step 585: loss=0.006010532379150391\n",
            "step 586: loss=1.3207292556762695\n",
            "step 587: loss=1.1580173969268799\n",
            "step 588: loss=1.5413885116577148\n",
            "step 589: loss=2.826756000518799\n",
            "step 590: loss=0.5321207046508789\n",
            "step 591: loss=0.23201441764831543\n",
            "step 592: loss=1.4742538928985596\n",
            "step 593: loss=0.24504661560058594\n",
            "step 594: loss=0.01492929458618164\n",
            "step 595: loss=1.6003649234771729\n",
            "step 596: loss=3.735039472579956\n",
            "step 597: loss=0.22011852264404297\n",
            "step 598: loss=0.011488676071166992\n",
            "step 599: loss=0.38115406036376953\n",
            "step 600: loss=0.06879162788391113\n",
            "step 601: loss=0.2625579833984375\n",
            "step 602: loss=0.42401325702667236\n",
            "step 603: loss=0.35512256622314453\n",
            "step 604: loss=0.1918928623199463\n",
            "step 605: loss=1.0253198146820068\n",
            "step 606: loss=0.035587310791015625\n",
            "step 607: loss=0.06663012504577637\n",
            "step 608: loss=0.42356348037719727\n",
            "step 609: loss=0.3826179504394531\n",
            "step 610: loss=0.5140845775604248\n",
            "step 611: loss=0.9962185621261597\n",
            "step 612: loss=0.30737268924713135\n",
            "step 613: loss=0.08875179290771484\n",
            "step 614: loss=0.10289883613586426\n",
            "step 615: loss=0.21781301498413086\n",
            "step 616: loss=0.13038301467895508\n",
            "step 617: loss=0.0\n",
            "step 618: loss=0.36626505851745605\n",
            "step 619: loss=0.20466995239257812\n",
            "step 620: loss=0.15040946006774902\n",
            "step 621: loss=2.709679365158081\n",
            "step 622: loss=0.012763023376464844\n",
            "step 623: loss=0.0004172325134277344\n",
            "step 624: loss=3.0633952617645264\n",
            "step 625: loss=0.11918854713439941\n",
            "step 626: loss=0.14096355438232422\n",
            "step 627: loss=0.08594751358032227\n",
            "step 628: loss=0.0\n",
            "step 629: loss=0.25449109077453613\n",
            "step 630: loss=0.0003795623779296875\n",
            "step 631: loss=0.04947376251220703\n",
            "step 632: loss=0.03761768341064453\n",
            "step 633: loss=4.234920501708984\n",
            "step 634: loss=0.0\n",
            "step 635: loss=0.06968080997467041\n",
            "step 636: loss=0.00011539459228515625\n",
            "step 637: loss=1.299339771270752\n",
            "step 638: loss=0.021266460418701172\n",
            "step 639: loss=0.00012683868408203125\n",
            "step 640: loss=0.13708281517028809\n",
            "step 641: loss=0.40067416429519653\n",
            "step 642: loss=0.03491783142089844\n",
            "step 643: loss=0.5247679352760315\n",
            "step 644: loss=0.022177934646606445\n",
            "step 645: loss=0.007782459259033203\n",
            "step 646: loss=0.0002760887145996094\n",
            "step 647: loss=0.0716397762298584\n",
            "step 648: loss=0.0011105537414550781\n",
            "step 649: loss=0.007695674896240234\n",
            "step 650: loss=0.0011582374572753906\n",
            "step 651: loss=1.3923300504684448\n",
            "step 652: loss=2.4355671405792236\n",
            "step 653: loss=0.0715017318725586\n",
            "step 654: loss=0.1533358097076416\n",
            "step 655: loss=0.08940243721008301\n",
            "step 656: loss=0.38901185989379883\n",
            "step 657: loss=0.03058338165283203\n",
            "step 658: loss=3.9936742782592773\n",
            "step 659: loss=0.30168843269348145\n",
            "step 660: loss=3.7204065322875977\n",
            "step 661: loss=0.00202178955078125\n",
            "step 662: loss=0.013043880462646484\n",
            "step 663: loss=1.6091176271438599\n",
            "step 664: loss=0.008925437927246094\n",
            "step 665: loss=0.0020503997802734375\n",
            "step 666: loss=0.0010809898376464844\n",
            "step 667: loss=0.0006780624389648438\n",
            "step 668: loss=2.0165164470672607\n",
            "step 669: loss=0.33581167459487915\n",
            "step 670: loss=0.25439882278442383\n",
            "step 671: loss=4.5892791748046875\n",
            "step 672: loss=0.28838229179382324\n",
            "step 673: loss=0.8850886225700378\n",
            "step 674: loss=0.0008249282836914062\n",
            "step 675: loss=0.15271210670471191\n",
            "step 676: loss=0.03641366958618164\n",
            "step 677: loss=0.08814787864685059\n",
            "step 678: loss=0.18788409233093262\n",
            "step 679: loss=0.03513216972351074\n",
            "step 680: loss=0.00017070770263671875\n",
            "step 681: loss=1.8320462703704834\n",
            "step 682: loss=9.632110595703125e-05\n",
            "step 683: loss=0.21492409706115723\n",
            "step 684: loss=0.04860574007034302\n",
            "step 685: loss=0.0008306503295898438\n",
            "step 686: loss=0.03287816047668457\n",
            "step 687: loss=0.3313310146331787\n",
            "step 688: loss=0.01659536361694336\n",
            "step 689: loss=0.9519222974777222\n",
            "step 690: loss=0.11244010925292969\n",
            "step 691: loss=0.01481771469116211\n",
            "step 692: loss=0.19759273529052734\n",
            "step 693: loss=0.0020389556884765625\n",
            "step 694: loss=0.012326240539550781\n",
            "step 695: loss=0.002613067626953125\n",
            "step 696: loss=0.8077821135520935\n",
            "step 697: loss=0.44576776027679443\n",
            "step 698: loss=1.1771466732025146\n",
            "step 699: loss=0.04657602310180664\n",
            "step 700: loss=0.30260610580444336\n",
            "step 701: loss=0.018115997314453125\n",
            "step 702: loss=2.288818359375e-05\n",
            "step 703: loss=0.01145172119140625\n",
            "step 704: loss=0.062308311462402344\n",
            "step 705: loss=0.05321168899536133\n",
            "step 706: loss=0.6133580207824707\n",
            "step 707: loss=0.9748408794403076\n",
            "step 708: loss=0.12584805488586426\n",
            "step 709: loss=0.051878929138183594\n",
            "step 710: loss=0.9117200374603271\n",
            "step 711: loss=3.700160503387451\n",
            "step 712: loss=0.11659550666809082\n",
            "step 713: loss=0.026853561401367188\n",
            "step 714: loss=0.05312848091125488\n",
            "step 715: loss=0.005021572113037109\n",
            "step 716: loss=0.09736204147338867\n",
            "step 717: loss=0.10871326923370361\n",
            "step 718: loss=0.008442878723144531\n",
            "step 719: loss=0.021784067153930664\n",
            "step 720: loss=0.00036334991455078125\n",
            "step 721: loss=0.027135848999023438\n",
            "step 722: loss=0.01212167739868164\n",
            "step 723: loss=0.007629871368408203\n",
            "step 724: loss=0.010610580444335938\n",
            "step 725: loss=6.191129684448242\n",
            "step 726: loss=0.429141640663147\n",
            "step 727: loss=0.01994800567626953\n",
            "step 728: loss=0.059060096740722656\n",
            "step 729: loss=1.1157556772232056\n",
            "step 730: loss=0.00021076202392578125\n",
            "step 731: loss=0.11978816986083984\n",
            "step 732: loss=0.16045665740966797\n",
            "step 733: loss=0.07729029655456543\n",
            "step 734: loss=2.0503997802734375e-05\n",
            "step 735: loss=1.796342372894287\n",
            "step 736: loss=4.673004150390625e-05\n",
            "step 737: loss=0.24309563636779785\n",
            "step 738: loss=0.1712707281112671\n",
            "step 739: loss=0.0\n",
            "step 740: loss=0.0009741783142089844\n",
            "step 741: loss=1.8369255065917969\n",
            "step 742: loss=3.297203540802002\n",
            "step 743: loss=0.09610557556152344\n",
            "step 744: loss=2.956390380859375e-05\n",
            "step 745: loss=0.03134942054748535\n",
            "step 746: loss=4.57763671875e-05\n",
            "step 747: loss=0.20643126964569092\n",
            "step 748: loss=0.008591890335083008\n",
            "step 749: loss=0.00665283203125\n",
            "step 750: loss=0.22176039218902588\n",
            "step 751: loss=2.293247699737549\n",
            "step 752: loss=0.0359799861907959\n",
            "step 753: loss=0.1016683578491211\n",
            "step 754: loss=0.4563870429992676\n",
            "step 755: loss=0.07222104072570801\n",
            "step 756: loss=2.9474287033081055\n",
            "step 757: loss=0.12914657592773438\n",
            "step 758: loss=0.7884641885757446\n",
            "step 759: loss=0.013956069946289062\n",
            "step 760: loss=0.5695130825042725\n",
            "step 761: loss=0.000335693359375\n",
            "step 762: loss=0.04605531692504883\n",
            "step 763: loss=0.9289627075195312\n",
            "step 764: loss=0.46455371379852295\n",
            "step 765: loss=0.027606964111328125\n",
            "step 766: loss=0.4059494733810425\n",
            "step 767: loss=0.11852765083312988\n",
            "step 768: loss=3.4203758239746094\n",
            "step 769: loss=0.0\n",
            "step 770: loss=0.017937183380126953\n",
            "step 771: loss=0.021827220916748047\n",
            "step 772: loss=0.7009478807449341\n",
            "step 773: loss=0.01783466339111328\n",
            "step 774: loss=0.26182103157043457\n",
            "step 775: loss=0.0025734901428222656\n",
            "step 776: loss=0.20601153373718262\n",
            "step 777: loss=0.020930767059326172\n",
            "step 778: loss=0.023335933685302734\n",
            "step 779: loss=0.012871265411376953\n",
            "step 780: loss=0.0019140243530273438\n",
            "step 781: loss=2.86102294921875e-06\n",
            "step 782: loss=0.00032901763916015625\n",
            "step 783: loss=0.014923572540283203\n",
            "step 784: loss=0.020122051239013672\n",
            "step 785: loss=0.05718874931335449\n",
            "step 786: loss=0.005372524261474609\n",
            "step 787: loss=0.009514331817626953\n",
            "step 788: loss=0.05858421325683594\n",
            "step 789: loss=0.02961254119873047\n",
            "step 790: loss=0.8438476324081421\n",
            "step 791: loss=0.180189847946167\n",
            "step 792: loss=0.013162612915039062\n",
            "step 793: loss=0.0004863739013671875\n",
            "step 794: loss=0.9618614912033081\n",
            "step 795: loss=0.00044345855712890625\n",
            "step 796: loss=0.002399444580078125\n",
            "step 797: loss=0.0026006698608398438\n",
            "step 798: loss=2.977027177810669\n",
            "step 799: loss=0.7542277574539185\n",
            "step 800: loss=0.15921902656555176\n",
            "step 801: loss=0.12540102005004883\n",
            "step 802: loss=0.0008139610290527344\n",
            "step 803: loss=0.005527019500732422\n",
            "step 804: loss=0.15577006340026855\n",
            "step 805: loss=0.009115219116210938\n",
            "step 806: loss=4.9591064453125e-05\n",
            "step 807: loss=0.0\n",
            "step 808: loss=0.009009838104248047\n",
            "step 809: loss=1.209647536277771\n",
            "step 810: loss=5.435943603515625e-05\n",
            "step 811: loss=2.585820198059082\n",
            "step 812: loss=0.272760272026062\n",
            "step 813: loss=0.14165639877319336\n",
            "step 814: loss=1.0237202644348145\n",
            "step 815: loss=0.0157318115234375\n",
            "step 816: loss=0.5518057346343994\n",
            "step 817: loss=0.29705238342285156\n",
            "step 818: loss=3.083925485610962\n",
            "step 819: loss=0.004309177398681641\n",
            "step 820: loss=0.2696821689605713\n",
            "step 821: loss=0.07562422752380371\n",
            "step 822: loss=0.02168560028076172\n",
            "step 823: loss=0.0035576820373535156\n",
            "step 824: loss=0.09384003281593323\n",
            "step 825: loss=2.2987751960754395\n",
            "step 826: loss=8.0108642578125e-05\n",
            "step 827: loss=2.0314722061157227\n",
            "step 828: loss=0.005656242370605469\n",
            "step 829: loss=0.17470598220825195\n",
            "step 830: loss=0.011296272277832031\n",
            "step 831: loss=0.4446837902069092\n",
            "step 832: loss=7.724761962890625e-05\n",
            "step 833: loss=0.09629392623901367\n",
            "step 834: loss=0.5773712396621704\n",
            "step 835: loss=0.283435583114624\n",
            "step 836: loss=0.0007152557373046875\n",
            "step 837: loss=0.0009503364562988281\n",
            "step 838: loss=0.0015888214111328125\n",
            "step 839: loss=0.0008783340454101562\n",
            "step 840: loss=2.86102294921875e-06\n",
            "step 841: loss=0.0017485618591308594\n",
            "step 842: loss=7.904542446136475\n",
            "step 843: loss=0.03731274604797363\n",
            "step 844: loss=0.029038667678833008\n",
            "step 845: loss=0.0008749961853027344\n",
            "step 846: loss=0.0502161979675293\n",
            "step 847: loss=0.008754968643188477\n",
            "step 848: loss=0.0002589225769042969\n",
            "step 849: loss=0.00012493133544921875\n",
            "step 850: loss=0.024086475372314453\n",
            "step 851: loss=0.43675661087036133\n",
            "step 852: loss=0.2410721778869629\n",
            "step 853: loss=0.4412233829498291\n",
            "step 854: loss=0.00013828277587890625\n",
            "step 855: loss=2.543863534927368\n",
            "step 856: loss=0.0002269744873046875\n",
            "step 857: loss=0.8094520568847656\n",
            "step 858: loss=0.7041930556297302\n",
            "step 859: loss=0.3394489288330078\n",
            "step 860: loss=0.055242061614990234\n",
            "step 861: loss=0.057474613189697266\n",
            "step 862: loss=5.7220458984375e-06\n",
            "step 863: loss=0.009898662567138672\n",
            "step 864: loss=0.0037937164306640625\n",
            "step 865: loss=0.2079172134399414\n",
            "step 866: loss=0.013944149017333984\n",
            "step 867: loss=0.12455368041992188\n",
            "step 868: loss=0.01570916175842285\n",
            "step 869: loss=2.436939239501953\n",
            "step 870: loss=7.152557373046875e-05\n",
            "step 871: loss=0.3015404939651489\n",
            "step 872: loss=0.20589661598205566\n",
            "step 873: loss=0.05428504943847656\n",
            "step 874: loss=0.7940071821212769\n",
            "step 875: loss=0.3275197744369507\n",
            "step 876: loss=0.2335115671157837\n",
            "step 877: loss=0.026417255401611328\n",
            "step 878: loss=0.0019626617431640625\n",
            "step 879: loss=1.4781951904296875e-05\n",
            "step 880: loss=0.013292789459228516\n",
            "step 881: loss=6.733707427978516\n",
            "step 882: loss=2.191803455352783\n",
            "step 883: loss=0.005655765533447266\n",
            "step 884: loss=0.029038429260253906\n",
            "step 885: loss=0.0004487037658691406\n",
            "step 886: loss=0.5562822818756104\n",
            "step 887: loss=0.0029821395874023438\n",
            "step 888: loss=0.13514983654022217\n",
            "step 889: loss=2.049121379852295\n",
            "step 890: loss=0.005017876625061035\n",
            "step 891: loss=3.0040740966796875e-05\n",
            "step 892: loss=1.239776611328125e-05\n",
            "step 893: loss=0.0014615058898925781\n",
            "step 894: loss=0.0055217742919921875\n",
            "step 895: loss=1.52587890625e-05\n",
            "step 896: loss=0.00019216537475585938\n",
            "step 897: loss=0.16714346408843994\n",
            "step 898: loss=0.31490984559059143\n",
            "step 899: loss=0.0031995773315429688\n",
            "step 900: loss=8.678436279296875e-05\n",
            "step 901: loss=0.0014834403991699219\n",
            "step 902: loss=1.52587890625e-05\n",
            "step 903: loss=0.15224814414978027\n",
            "step 904: loss=1.33514404296875e-05\n",
            "step 905: loss=0.0037207603454589844\n",
            "step 906: loss=0.00011157989501953125\n",
            "step 907: loss=0.19027912616729736\n",
            "step 908: loss=1.239776611328125e-05\n",
            "step 909: loss=0.0035429000854492188\n",
            "step 910: loss=0.34459590911865234\n",
            "step 911: loss=0.034879207611083984\n",
            "step 912: loss=0.1951587200164795\n",
            "step 913: loss=0.00466156005859375\n",
            "step 914: loss=7.650437355041504\n",
            "step 915: loss=1.1444091796875e-05\n",
            "step 916: loss=0.1217796802520752\n",
            "step 917: loss=0.8490414619445801\n",
            "step 918: loss=1.0185141563415527\n",
            "step 919: loss=3.5250778198242188\n",
            "step 920: loss=0.07166624069213867\n",
            "step 921: loss=0.013340950012207031\n",
            "step 922: loss=0.11171722412109375\n",
            "step 923: loss=0.308103084564209\n",
            "step 924: loss=0.0\n",
            "step 925: loss=0.0005254745483398438\n",
            "step 926: loss=0.018581390380859375\n",
            "step 927: loss=1.1677638292312622\n",
            "step 928: loss=0.6191295385360718\n",
            "step 929: loss=0.02457714080810547\n",
            "step 930: loss=0.3781716823577881\n",
            "step 931: loss=0.08062028884887695\n",
            "step 932: loss=0.07790970802307129\n",
            "step 933: loss=0.0020275115966796875\n",
            "step 934: loss=0.020385265350341797\n",
            "step 935: loss=0.0007767677307128906\n",
            "step 936: loss=0.6321337223052979\n",
            "step 937: loss=0.003490924835205078\n",
            "step 938: loss=0.12849187850952148\n",
            "step 939: loss=0.304118275642395\n",
            "step 940: loss=0.008532047271728516\n",
            "step 941: loss=0.0063877105712890625\n",
            "step 942: loss=0.2776373624801636\n",
            "step 943: loss=0.0018358230590820312\n",
            "step 944: loss=0.0002841949462890625\n",
            "step 945: loss=0.16218328475952148\n",
            "step 946: loss=15.213188171386719\n",
            "step 947: loss=0.022892475128173828\n",
            "step 948: loss=0.0\n",
            "step 949: loss=0.7444643974304199\n",
            "step 950: loss=0.146498441696167\n",
            "step 951: loss=0.029528141021728516\n",
            "step 952: loss=0.001125335693359375\n",
            "step 953: loss=0.6779463291168213\n",
            "step 954: loss=0.006968975067138672\n",
            "step 955: loss=1.9073486328125e-06\n",
            "step 956: loss=0.0003266334533691406\n",
            "step 957: loss=0.0\n",
            "step 958: loss=3.0286471843719482\n",
            "step 959: loss=0.05248546600341797\n",
            "step 960: loss=1.2659327983856201\n",
            "step 961: loss=1.6601563692092896\n",
            "step 962: loss=6.818771362304688e-05\n",
            "step 963: loss=0.00022983551025390625\n",
            "step 964: loss=0.004395961761474609\n",
            "step 965: loss=0.07254290580749512\n",
            "step 966: loss=0.6479612588882446\n",
            "step 967: loss=0.0390925407409668\n",
            "step 968: loss=3.366708755493164\n",
            "step 969: loss=0.09890580177307129\n",
            "step 970: loss=0.7086915969848633\n",
            "step 971: loss=0.0026459693908691406\n",
            "step 972: loss=0.14348721504211426\n",
            "step 973: loss=0.05600595474243164\n",
            "step 974: loss=0.7622897624969482\n",
            "step 975: loss=1.787164330482483\n",
            "step 976: loss=0.0013651847839355469\n",
            "step 977: loss=0.004380226135253906\n",
            "step 978: loss=0.38682281970977783\n",
            "step 979: loss=0.628813624382019\n",
            "step 980: loss=0.002354145050048828\n",
            "step 981: loss=0.3364085555076599\n",
            "step 982: loss=0.13457202911376953\n",
            "step 983: loss=2.497775077819824\n",
            "step 984: loss=0.0004563331604003906\n",
            "step 985: loss=1.0501189231872559\n",
            "step 986: loss=0.0025887489318847656\n",
            "step 987: loss=0.060457706451416016\n",
            "step 988: loss=0.0989084243774414\n",
            "step 989: loss=0.0012769699096679688\n",
            "step 990: loss=0.775801420211792\n",
            "step 991: loss=0.05121445655822754\n",
            "step 992: loss=0.4233372211456299\n",
            "step 993: loss=2.86102294921875e-06\n",
            "step 994: loss=0.6508564949035645\n",
            "step 995: loss=0.32206737995147705\n",
            "step 996: loss=9.5367431640625e-07\n",
            "step 997: loss=9.5367431640625e-07\n",
            "step 998: loss=0.1253260374069214\n",
            "step 999: loss=0.05963420867919922\n",
            "step 1000: loss=0.00016021728515625\n",
            "Mean loss        0.9412665\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 4 0 8 3 1]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-2\n",
        "optimizer = optax.adam(learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d1775729",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=0.0\n",
            "step 2: loss=0.021732807159423828\n",
            "step 3: loss=0.007058143615722656\n",
            "step 4: loss=1.9073486328125e-06\n",
            "step 5: loss=3.5937793254852295\n",
            "step 6: loss=0.033194541931152344\n",
            "step 7: loss=7.7817277908325195\n",
            "step 8: loss=6.1789631843566895\n",
            "step 9: loss=0.11160469055175781\n",
            "step 10: loss=15.611993789672852\n",
            "step 11: loss=1.6699985265731812\n",
            "step 12: loss=0.4520533084869385\n",
            "step 13: loss=0.01483774185180664\n",
            "step 14: loss=0.605947732925415\n",
            "step 15: loss=8.520060539245605\n",
            "step 16: loss=4.101454734802246\n",
            "step 17: loss=0.00015354156494140625\n",
            "step 18: loss=9.17702865600586\n",
            "step 19: loss=2.7513885498046875\n",
            "step 20: loss=0.17457973957061768\n",
            "step 21: loss=1.8984501361846924\n",
            "step 22: loss=0.6763033866882324\n",
            "step 23: loss=0.31062984466552734\n",
            "step 24: loss=2.33121395111084\n",
            "step 25: loss=1.7108426094055176\n",
            "step 26: loss=1.2369067668914795\n",
            "step 27: loss=2.4545674324035645\n",
            "step 28: loss=1.2753641605377197\n",
            "step 29: loss=0.6617397665977478\n",
            "step 30: loss=0.47773122787475586\n",
            "step 31: loss=2.5408034324645996\n",
            "step 32: loss=7.580662727355957\n",
            "step 33: loss=3.6657981872558594\n",
            "step 34: loss=1.3374736309051514\n",
            "step 35: loss=0.06870365142822266\n",
            "step 36: loss=1.2814165353775024\n",
            "step 37: loss=1.7750471830368042\n",
            "step 38: loss=2.5930984020233154\n",
            "step 39: loss=1.227713942527771\n",
            "step 40: loss=2.2356960773468018\n",
            "step 41: loss=2.6646997928619385\n",
            "step 42: loss=2.8871865272521973\n",
            "step 43: loss=2.607180118560791\n",
            "step 44: loss=1.6303577423095703\n",
            "step 45: loss=2.2892110347747803\n",
            "step 46: loss=0.6163055896759033\n",
            "step 47: loss=2.3377912044525146\n",
            "step 48: loss=2.6005072593688965\n",
            "step 49: loss=2.761793375015259\n",
            "step 50: loss=2.3556253910064697\n",
            "step 51: loss=2.3214292526245117\n",
            "step 52: loss=2.5302932262420654\n",
            "step 53: loss=4.058741569519043\n",
            "step 54: loss=2.2306065559387207\n",
            "step 55: loss=1.7703965902328491\n",
            "step 56: loss=2.3378453254699707\n",
            "step 57: loss=2.560159206390381\n",
            "step 58: loss=2.176805019378662\n",
            "step 59: loss=2.2868871688842773\n",
            "step 60: loss=2.1109726428985596\n",
            "step 61: loss=2.054516315460205\n",
            "step 62: loss=2.477454423904419\n",
            "step 63: loss=3.655982732772827\n",
            "step 64: loss=2.810016393661499\n",
            "step 65: loss=2.157376766204834\n",
            "step 66: loss=0.1933891773223877\n",
            "step 67: loss=1.833339810371399\n",
            "step 68: loss=0.265041708946228\n",
            "step 69: loss=3.33835506439209\n",
            "step 70: loss=2.204862117767334\n",
            "step 71: loss=3.177638292312622\n",
            "step 72: loss=2.7258076667785645\n",
            "step 73: loss=1.6854248046875\n",
            "step 74: loss=2.03922438621521\n",
            "step 75: loss=0.029169082641601562\n",
            "step 76: loss=1.6147966384887695\n",
            "step 77: loss=2.7785253524780273\n",
            "step 78: loss=2.1870923042297363\n",
            "step 79: loss=2.5536673069000244\n",
            "step 80: loss=1.5344648361206055\n",
            "step 81: loss=0.000514984130859375\n",
            "step 82: loss=1.4753280878067017\n",
            "step 83: loss=2.6166629791259766\n",
            "step 84: loss=2.024071216583252\n",
            "step 85: loss=2.5172972679138184\n",
            "step 86: loss=0.10814499855041504\n",
            "step 87: loss=2.586803674697876\n",
            "step 88: loss=2.8238766193389893\n",
            "step 89: loss=2.68784499168396\n",
            "step 90: loss=2.8579909801483154\n",
            "step 91: loss=2.6406426429748535\n",
            "step 92: loss=1.4190950393676758\n",
            "step 93: loss=1.398573875427246\n",
            "step 94: loss=2.5707623958587646\n",
            "step 95: loss=2.5287132263183594\n",
            "step 96: loss=2.4619483947753906\n",
            "step 97: loss=2.7983782291412354\n",
            "step 98: loss=2.558352470397949\n",
            "step 99: loss=1.37849760055542\n",
            "step 100: loss=2.711251974105835\n",
            "step 101: loss=2.477630615234375\n",
            "step 102: loss=2.4960598945617676\n",
            "step 103: loss=2.401211738586426\n",
            "step 104: loss=1.3945977687835693\n",
            "step 105: loss=1.3761682510375977\n",
            "step 106: loss=2.2757365703582764\n",
            "step 107: loss=2.4381844997406006\n",
            "step 108: loss=1.3433458805084229\n",
            "step 109: loss=2.3799238204956055\n",
            "step 110: loss=0.12437677383422852\n",
            "step 111: loss=2.391810655593872\n",
            "step 112: loss=5.340576171875e-05\n",
            "step 113: loss=2.5794882774353027\n",
            "step 114: loss=2.8222224712371826\n",
            "step 115: loss=2.31433367729187\n",
            "step 116: loss=2.0395994186401367\n",
            "step 117: loss=2.5218682289123535\n",
            "step 118: loss=1.3767638206481934\n",
            "step 119: loss=1.9576857089996338\n",
            "step 120: loss=2.6380553245544434\n",
            "step 121: loss=2.3939483165740967\n",
            "step 122: loss=3.2018637657165527\n",
            "step 123: loss=2.8156986236572266\n",
            "step 124: loss=1.8281716108322144\n",
            "step 125: loss=1.4899756908416748\n",
            "step 126: loss=2.2124667167663574\n",
            "step 127: loss=2.7261886596679688\n",
            "step 128: loss=2.6842498779296875\n",
            "step 129: loss=0.004815101623535156\n",
            "step 130: loss=1.543753743171692\n",
            "step 131: loss=3.0780415534973145\n",
            "step 132: loss=2.6196954250335693\n",
            "step 133: loss=1.7428058385849\n",
            "step 134: loss=3.064441680908203\n",
            "step 135: loss=0.0\n",
            "step 136: loss=2.163559675216675\n",
            "step 137: loss=1.5927643775939941\n",
            "step 138: loss=0.044171810150146484\n",
            "step 139: loss=2.2758617401123047\n",
            "step 140: loss=2.6430673599243164\n",
            "step 141: loss=2.857025146484375\n",
            "step 142: loss=2.5811924934387207\n",
            "step 143: loss=2.7508397102355957\n",
            "step 144: loss=1.7831612825393677\n",
            "step 145: loss=2.4993982315063477\n",
            "step 146: loss=2.821634292602539\n",
            "step 147: loss=2.225860118865967\n",
            "step 148: loss=1.7761833667755127\n",
            "step 149: loss=1.7402331829071045\n",
            "step 150: loss=2.2413859367370605\n",
            "step 151: loss=2.749595880508423\n",
            "step 152: loss=3.019594669342041\n",
            "step 153: loss=2.6725478172302246\n",
            "step 154: loss=2.6218602657318115\n",
            "step 155: loss=2.226591110229492\n",
            "step 156: loss=2.188432455062866\n",
            "step 157: loss=2.47189998626709\n",
            "step 158: loss=2.0460476875305176\n",
            "step 159: loss=2.446336269378662\n",
            "step 160: loss=2.4064266681671143\n",
            "step 161: loss=2.410130262374878\n",
            "step 162: loss=2.070686101913452\n",
            "step 163: loss=2.308899402618408\n",
            "step 164: loss=2.038663625717163\n",
            "step 165: loss=2.1669397354125977\n",
            "step 166: loss=1.906247615814209\n",
            "step 167: loss=2.1174209117889404\n",
            "step 168: loss=2.6901466846466064\n",
            "step 169: loss=1.9224841594696045\n",
            "step 170: loss=2.0153491497039795\n",
            "step 171: loss=1.9855419397354126\n",
            "step 172: loss=2.357429265975952\n",
            "step 173: loss=2.330686569213867\n",
            "step 174: loss=1.914656400680542\n",
            "step 175: loss=2.7942392826080322\n",
            "step 176: loss=1.868660807609558\n",
            "step 177: loss=1.8159958124160767\n",
            "step 178: loss=1.8749545812606812\n",
            "step 179: loss=1.717259168624878\n",
            "step 180: loss=2.001551628112793\n",
            "step 181: loss=0.002814769744873047\n",
            "step 182: loss=0.07105565071105957\n",
            "step 183: loss=1.8141984939575195\n",
            "step 184: loss=2.823997974395752\n",
            "step 185: loss=1.5111145973205566\n",
            "step 186: loss=1.4604157209396362\n",
            "step 187: loss=2.427281379699707\n",
            "step 188: loss=2.8350892066955566\n",
            "step 189: loss=1.7669692039489746\n",
            "step 190: loss=2.607553005218506\n",
            "step 191: loss=0.013873815536499023\n",
            "step 192: loss=2.10292649269104\n",
            "step 193: loss=2.4178593158721924\n",
            "step 194: loss=2.361445188522339\n",
            "step 195: loss=2.0697667598724365\n",
            "step 196: loss=1.7286999225616455\n",
            "step 197: loss=2.6447577476501465\n",
            "step 198: loss=0.0002589225769042969\n",
            "step 199: loss=1.9906203746795654\n",
            "step 200: loss=2.1385767459869385\n",
            "step 201: loss=3.1604509353637695\n",
            "step 202: loss=2.8486745357513428\n",
            "step 203: loss=0.05466103553771973\n",
            "step 204: loss=0.04019045829772949\n",
            "step 205: loss=1.7266936302185059\n",
            "step 206: loss=2.7279350757598877\n",
            "step 207: loss=2.591310977935791\n",
            "step 208: loss=2.5365867614746094\n",
            "step 209: loss=3.0149409770965576\n",
            "step 210: loss=1.6992095708847046\n",
            "step 211: loss=3.814697265625e-06\n",
            "step 212: loss=2.4997260570526123\n",
            "step 213: loss=1.910009741783142\n",
            "step 214: loss=1.8399258852005005\n",
            "step 215: loss=1.9373656511306763\n",
            "step 216: loss=1.8754819631576538\n",
            "step 217: loss=2.8170325756073\n",
            "step 218: loss=2.0153067111968994\n",
            "step 219: loss=2.280017375946045\n",
            "step 220: loss=2.011687755584717\n",
            "step 221: loss=1.8289296627044678\n",
            "step 222: loss=3.0477066040039062\n",
            "step 223: loss=1.6798676252365112\n",
            "step 224: loss=2.6393790245056152\n",
            "step 225: loss=1.6133829355239868\n",
            "step 226: loss=2.046485185623169\n",
            "step 227: loss=2.114856243133545\n",
            "step 228: loss=2.087606906890869\n",
            "step 229: loss=2.0125162601470947\n",
            "step 230: loss=2.1822359561920166\n",
            "step 231: loss=1.9514564275741577\n",
            "step 232: loss=1.89128839969635\n",
            "step 233: loss=1.5512293577194214\n",
            "step 234: loss=2.125469207763672\n",
            "step 235: loss=1.7525596618652344\n",
            "step 236: loss=2.6744837760925293\n",
            "step 237: loss=2.521174430847168\n",
            "step 238: loss=1.6160361766815186\n",
            "step 239: loss=0.08112430572509766\n",
            "step 240: loss=2.365511417388916\n",
            "step 241: loss=2.6458663940429688\n",
            "step 242: loss=1.452568769454956\n",
            "step 243: loss=2.354836940765381\n",
            "step 244: loss=0.002125263214111328\n",
            "step 245: loss=2.080885410308838\n",
            "step 246: loss=2.2797935009002686\n",
            "step 247: loss=2.5341882705688477\n",
            "step 248: loss=3.3784806728363037\n",
            "step 249: loss=2.0115702152252197\n",
            "step 250: loss=0.0\n",
            "step 251: loss=2.3037593364715576\n",
            "step 252: loss=3.2761878967285156\n",
            "step 253: loss=1.8833497762680054\n",
            "step 254: loss=2.353729009628296\n",
            "step 255: loss=2.2880067825317383\n",
            "step 256: loss=2.2689995765686035\n",
            "step 257: loss=3.310469150543213\n",
            "step 258: loss=0.0\n",
            "step 259: loss=1.726922631263733\n",
            "step 260: loss=2.0470478534698486\n",
            "step 261: loss=2.656020402908325\n",
            "step 262: loss=3.1807916164398193\n",
            "step 263: loss=0.00012636184692382812\n",
            "step 264: loss=2.1940572261810303\n",
            "step 265: loss=1.7962346076965332\n",
            "step 266: loss=2.1477911472320557\n",
            "step 267: loss=1.8006260395050049\n",
            "step 268: loss=1.7698111534118652\n",
            "step 269: loss=2.584540843963623\n",
            "step 270: loss=2.039144277572632\n",
            "step 271: loss=2.64872407913208\n",
            "step 272: loss=2.9132254123687744\n",
            "step 273: loss=2.6056532859802246\n",
            "step 274: loss=3.0204615592956543\n",
            "step 275: loss=1.9058310985565186\n",
            "step 276: loss=2.348039150238037\n",
            "step 277: loss=1.7156904935836792\n",
            "step 278: loss=2.4652740955352783\n",
            "step 279: loss=2.459885835647583\n",
            "step 280: loss=2.3972227573394775\n",
            "step 281: loss=2.0883517265319824\n",
            "step 282: loss=2.060187816619873\n",
            "step 283: loss=2.6858105659484863\n",
            "step 284: loss=2.3442413806915283\n",
            "step 285: loss=2.287332534790039\n",
            "step 286: loss=2.223799467086792\n",
            "step 287: loss=2.5556414127349854\n",
            "step 288: loss=1.9778820276260376\n",
            "step 289: loss=2.1355199813842773\n",
            "step 290: loss=2.262935161590576\n",
            "step 291: loss=2.232788562774658\n",
            "step 292: loss=2.175743579864502\n",
            "step 293: loss=1.9147958755493164\n",
            "step 294: loss=1.9934982061386108\n",
            "step 295: loss=2.9994194507598877\n",
            "step 296: loss=2.1213395595550537\n",
            "step 297: loss=2.3662874698638916\n",
            "step 298: loss=2.92411208152771\n",
            "step 299: loss=1.855588674545288\n",
            "step 300: loss=1.9073486328125e-06\n",
            "step 301: loss=1.8971415758132935\n",
            "step 302: loss=4.192553520202637\n",
            "step 303: loss=0.0017542839050292969\n",
            "step 304: loss=2.508835554122925\n",
            "step 305: loss=2.4693048000335693\n",
            "step 306: loss=2.231956958770752\n",
            "step 307: loss=2.2404732704162598\n",
            "step 308: loss=0.0\n",
            "step 309: loss=2.156362533569336\n",
            "step 310: loss=2.1856625080108643\n",
            "step 311: loss=2.5153660774230957\n",
            "step 312: loss=2.300621509552002\n",
            "step 313: loss=3.24249267578125e-05\n",
            "step 314: loss=2.670084238052368\n",
            "step 315: loss=2.7047736644744873\n",
            "step 316: loss=2.2166941165924072\n",
            "step 317: loss=2.6411566734313965\n",
            "step 318: loss=2.5627236366271973\n",
            "step 319: loss=2.0803444385528564\n",
            "step 320: loss=2.480414867401123\n",
            "step 321: loss=2.182408332824707\n",
            "step 322: loss=2.1447553634643555\n",
            "step 323: loss=4.041223526000977\n",
            "step 324: loss=2.3969547748565674\n",
            "step 325: loss=2.057851552963257\n",
            "step 326: loss=2.0188584327697754\n",
            "step 327: loss=1.95533287525177\n",
            "step 328: loss=2.3083016872406006\n",
            "step 329: loss=3.8842456340789795\n",
            "step 330: loss=2.2675013542175293\n",
            "step 331: loss=3.7418484687805176\n",
            "step 332: loss=2.257434606552124\n",
            "step 333: loss=3.545970916748047\n",
            "step 334: loss=1.7274208068847656\n",
            "step 335: loss=2.6973302364349365\n",
            "step 336: loss=2.3545937538146973\n",
            "step 337: loss=2.2855007648468018\n",
            "step 338: loss=2.2541329860687256\n",
            "step 339: loss=2.31988787651062\n",
            "step 340: loss=2.264472484588623\n",
            "step 341: loss=2.4567317962646484\n",
            "step 342: loss=2.172957181930542\n",
            "step 343: loss=2.9013314247131348\n",
            "step 344: loss=2.253732442855835\n",
            "step 345: loss=2.211561441421509\n",
            "step 346: loss=2.6983890533447266\n",
            "step 347: loss=2.0480332374572754\n",
            "step 348: loss=1.864542007446289\n",
            "step 349: loss=2.373607635498047\n",
            "step 350: loss=2.428265333175659\n",
            "step 351: loss=2.687056064605713\n",
            "step 352: loss=2.25849986076355\n",
            "step 353: loss=2.201662302017212\n",
            "step 354: loss=2.068345308303833\n",
            "step 355: loss=1.9674683809280396\n",
            "step 356: loss=1.870376706123352\n",
            "step 357: loss=2.404202938079834\n",
            "step 358: loss=2.648759365081787\n",
            "step 359: loss=2.35774564743042\n",
            "step 360: loss=2.773656129837036\n",
            "step 361: loss=1.5274090766906738\n",
            "step 362: loss=1.9891808032989502\n",
            "step 363: loss=2.3106188774108887\n",
            "step 364: loss=2.3444273471832275\n",
            "step 365: loss=1.3628616333007812\n",
            "step 366: loss=2.3068528175354004\n",
            "step 367: loss=2.809511661529541\n",
            "step 368: loss=1.2638102769851685\n",
            "step 369: loss=2.5850229263305664\n",
            "step 370: loss=2.359959363937378\n",
            "step 371: loss=2.7469189167022705\n",
            "step 372: loss=1.1835582256317139\n",
            "step 373: loss=2.8130218982696533\n",
            "step 374: loss=3.0727782249450684\n",
            "step 375: loss=2.1626977920532227\n",
            "step 376: loss=2.283196210861206\n",
            "step 377: loss=2.981457233428955\n",
            "step 378: loss=2.4296140670776367\n",
            "step 379: loss=2.3783187866210938\n",
            "step 380: loss=2.6979851722717285\n",
            "step 381: loss=2.1204118728637695\n",
            "step 382: loss=2.2313108444213867\n",
            "step 383: loss=3.374798536300659\n",
            "step 384: loss=2.040297508239746\n",
            "step 385: loss=1.530458927154541\n",
            "step 386: loss=3.272949695587158\n",
            "step 387: loss=3.1873745918273926\n",
            "step 388: loss=2.160482883453369\n",
            "step 389: loss=2.5138497352600098\n",
            "step 390: loss=2.5582644939422607\n",
            "step 391: loss=2.6687817573547363\n",
            "step 392: loss=2.796954393386841\n",
            "step 393: loss=2.385939836502075\n",
            "step 394: loss=1.9955345392227173\n",
            "step 395: loss=2.7481260299682617\n",
            "step 396: loss=2.116798162460327\n",
            "step 397: loss=2.071486711502075\n",
            "step 398: loss=2.7149996757507324\n",
            "step 399: loss=2.6501822471618652\n",
            "step 400: loss=2.1503193378448486\n",
            "step 401: loss=2.6598763465881348\n",
            "step 402: loss=2.477358818054199\n",
            "step 403: loss=2.55131196975708\n",
            "step 404: loss=1.9224944114685059\n",
            "step 405: loss=2.0526998043060303\n",
            "step 406: loss=2.242374897003174\n",
            "step 407: loss=1.8504607677459717\n",
            "step 408: loss=1.792586088180542\n",
            "step 409: loss=2.641585350036621\n",
            "step 410: loss=2.477858304977417\n",
            "step 411: loss=2.3842825889587402\n",
            "step 412: loss=2.4591927528381348\n",
            "step 413: loss=2.1219637393951416\n",
            "step 414: loss=2.442497968673706\n",
            "step 415: loss=2.608266830444336\n",
            "step 416: loss=2.7591075897216797\n",
            "step 417: loss=2.7137718200683594\n",
            "step 418: loss=1.640397310256958\n",
            "step 419: loss=2.0987696647644043\n",
            "step 420: loss=2.29475474357605\n",
            "step 421: loss=2.4990170001983643\n",
            "step 422: loss=2.4635610580444336\n",
            "step 423: loss=1.6339044570922852\n",
            "step 424: loss=2.5646936893463135\n",
            "step 425: loss=2.526874303817749\n",
            "step 426: loss=1.60093092918396\n",
            "step 427: loss=2.0758957862854004\n",
            "step 428: loss=2.4295554161071777\n",
            "step 429: loss=2.4414591789245605\n",
            "step 430: loss=2.522603988647461\n",
            "step 431: loss=2.3692104816436768\n",
            "step 432: loss=2.390864610671997\n",
            "step 433: loss=2.253563642501831\n",
            "step 434: loss=2.667337656021118\n",
            "step 435: loss=2.2031233310699463\n",
            "step 436: loss=1.6796817779541016\n",
            "step 437: loss=2.418605089187622\n",
            "step 438: loss=2.618652105331421\n",
            "step 439: loss=1.6849517822265625\n",
            "step 440: loss=2.332244396209717\n",
            "step 441: loss=2.9654111862182617\n",
            "step 442: loss=2.236990451812744\n",
            "step 443: loss=2.2071831226348877\n",
            "step 444: loss=2.103907346725464\n",
            "step 445: loss=2.393406629562378\n",
            "step 446: loss=2.217850685119629\n",
            "step 447: loss=2.9018197059631348\n",
            "step 448: loss=1.7706161737442017\n",
            "step 449: loss=2.3416056632995605\n",
            "step 450: loss=3.059528350830078\n",
            "step 451: loss=2.1394243240356445\n",
            "step 452: loss=2.0995430946350098\n",
            "step 453: loss=2.0348312854766846\n",
            "step 454: loss=2.106936454772949\n",
            "step 455: loss=2.878329277038574\n",
            "step 456: loss=1.8755507469177246\n",
            "step 457: loss=2.069614887237549\n",
            "step 458: loss=1.856884479522705\n",
            "step 459: loss=2.1126761436462402\n",
            "step 460: loss=2.691082239151001\n",
            "step 461: loss=2.0785324573516846\n",
            "step 462: loss=2.024641513824463\n",
            "step 463: loss=1.8095166683197021\n",
            "step 464: loss=2.827162027359009\n",
            "step 465: loss=2.871772289276123\n",
            "step 466: loss=1.9010802507400513\n",
            "step 467: loss=2.650782585144043\n",
            "step 468: loss=2.062274694442749\n",
            "step 469: loss=2.526944637298584\n",
            "step 470: loss=3.124748945236206\n",
            "step 471: loss=2.7539665699005127\n",
            "step 472: loss=1.898468017578125\n",
            "step 473: loss=1.8658064603805542\n",
            "step 474: loss=2.5605669021606445\n",
            "step 475: loss=1.8189787864685059\n",
            "step 476: loss=2.533757209777832\n",
            "step 477: loss=1.951403021812439\n",
            "step 478: loss=2.457374334335327\n",
            "step 479: loss=1.9021015167236328\n",
            "step 480: loss=1.9491270780563354\n",
            "step 481: loss=1.9092251062393188\n",
            "step 482: loss=2.5627803802490234\n",
            "step 483: loss=3.0249247550964355\n",
            "step 484: loss=2.254857063293457\n",
            "step 485: loss=2.6140496730804443\n",
            "step 486: loss=1.9154245853424072\n",
            "step 487: loss=2.2489004135131836\n",
            "step 488: loss=1.8717665672302246\n",
            "step 489: loss=1.8003228902816772\n",
            "step 490: loss=2.4982547760009766\n",
            "step 491: loss=1.7873330116271973\n",
            "step 492: loss=2.8884875774383545\n",
            "step 493: loss=2.538922071456909\n",
            "step 494: loss=3.060920238494873\n",
            "step 495: loss=2.766990900039673\n",
            "step 496: loss=2.898599147796631\n",
            "step 497: loss=1.6602809429168701\n",
            "step 498: loss=2.1878817081451416\n",
            "step 499: loss=2.9658732414245605\n",
            "step 500: loss=2.451681613922119\n",
            "step 501: loss=2.50390887260437\n",
            "step 502: loss=2.4082672595977783\n",
            "step 503: loss=2.371093511581421\n",
            "step 504: loss=2.4680142402648926\n",
            "step 505: loss=2.207348108291626\n",
            "step 506: loss=2.4150352478027344\n",
            "step 507: loss=2.188851833343506\n",
            "step 508: loss=1.9690704345703125\n",
            "step 509: loss=2.7582108974456787\n",
            "step 510: loss=2.6914327144622803\n",
            "step 511: loss=2.2975854873657227\n",
            "step 512: loss=1.886751651763916\n",
            "step 513: loss=2.511808395385742\n",
            "step 514: loss=2.3323237895965576\n",
            "step 515: loss=2.3559093475341797\n",
            "step 516: loss=2.24338960647583\n",
            "step 517: loss=2.2357699871063232\n",
            "step 518: loss=2.3248989582061768\n",
            "step 519: loss=1.692624568939209\n",
            "step 520: loss=2.8869967460632324\n",
            "step 521: loss=2.3630828857421875\n",
            "step 522: loss=2.8324546813964844\n",
            "step 523: loss=1.797412633895874\n",
            "step 524: loss=2.287531852722168\n",
            "step 525: loss=2.2370898723602295\n",
            "step 526: loss=2.6122872829437256\n",
            "step 527: loss=2.216017484664917\n",
            "step 528: loss=1.7993619441986084\n",
            "step 529: loss=2.798090934753418\n",
            "step 530: loss=1.619469165802002\n",
            "step 531: loss=2.596433401107788\n",
            "step 532: loss=2.3808159828186035\n",
            "step 533: loss=3.283832550048828\n",
            "step 534: loss=2.1478257179260254\n",
            "step 535: loss=3.209545612335205\n",
            "step 536: loss=2.4887821674346924\n",
            "step 537: loss=1.584149956703186\n",
            "step 538: loss=2.2920780181884766\n",
            "step 539: loss=2.379324436187744\n",
            "step 540: loss=2.9947710037231445\n",
            "step 541: loss=2.064624547958374\n",
            "step 542: loss=2.2374074459075928\n",
            "step 543: loss=2.2093987464904785\n",
            "step 544: loss=2.6896934509277344\n",
            "step 545: loss=2.562865734100342\n",
            "step 546: loss=2.5243005752563477\n",
            "step 547: loss=3.0707035064697266\n",
            "step 548: loss=2.0091917514801025\n",
            "step 549: loss=3.000962257385254\n",
            "step 550: loss=2.658322334289551\n",
            "step 551: loss=1.8749645948410034\n",
            "step 552: loss=2.2069811820983887\n",
            "step 553: loss=2.282076597213745\n",
            "step 554: loss=2.2477426528930664\n",
            "step 555: loss=2.3673129081726074\n",
            "step 556: loss=2.3148393630981445\n",
            "step 557: loss=2.241117477416992\n",
            "step 558: loss=2.6506452560424805\n",
            "step 559: loss=2.2259435653686523\n",
            "step 560: loss=1.726098895072937\n",
            "step 561: loss=2.6838808059692383\n",
            "step 562: loss=2.3826982975006104\n",
            "step 563: loss=1.686293125152588\n",
            "step 564: loss=2.52620792388916\n",
            "step 565: loss=2.193325996398926\n",
            "step 566: loss=2.621276617050171\n",
            "step 567: loss=2.125551223754883\n",
            "step 568: loss=2.0525600910186768\n",
            "step 569: loss=2.466777801513672\n",
            "step 570: loss=2.861032485961914\n",
            "step 571: loss=2.811610221862793\n",
            "step 572: loss=1.8316001892089844\n",
            "step 573: loss=2.4136385917663574\n",
            "step 574: loss=2.690620183944702\n",
            "step 575: loss=2.4916367530822754\n",
            "step 576: loss=2.46234393119812\n",
            "step 577: loss=2.653618812561035\n",
            "step 578: loss=2.846851348876953\n",
            "step 579: loss=2.283970355987549\n",
            "step 580: loss=2.5778160095214844\n",
            "step 581: loss=2.5734550952911377\n",
            "step 582: loss=2.033644676208496\n",
            "step 583: loss=2.4436967372894287\n",
            "step 584: loss=2.0221011638641357\n",
            "step 585: loss=2.445549249649048\n",
            "step 586: loss=2.357069253921509\n",
            "step 587: loss=2.325467348098755\n",
            "step 588: loss=1.9664615392684937\n",
            "step 589: loss=2.5221104621887207\n",
            "step 590: loss=2.4371156692504883\n",
            "step 591: loss=2.4495837688446045\n",
            "step 592: loss=2.809019088745117\n",
            "step 593: loss=2.216571569442749\n",
            "step 594: loss=2.5775842666625977\n",
            "step 595: loss=2.367260694503784\n",
            "step 596: loss=2.4228243827819824\n",
            "step 597: loss=2.708253860473633\n",
            "step 598: loss=2.2342686653137207\n",
            "step 599: loss=2.1051387786865234\n",
            "step 600: loss=2.051515817642212\n",
            "step 601: loss=2.005506992340088\n",
            "step 602: loss=1.976914644241333\n",
            "step 603: loss=1.9192625284194946\n",
            "step 604: loss=2.2741987705230713\n",
            "step 605: loss=2.5640246868133545\n",
            "step 606: loss=2.2094786167144775\n",
            "step 607: loss=1.7655681371688843\n",
            "step 608: loss=2.5086796283721924\n",
            "step 609: loss=2.3989341259002686\n",
            "step 610: loss=2.470870018005371\n",
            "step 611: loss=1.9581187963485718\n",
            "step 612: loss=2.6029715538024902\n",
            "step 613: loss=1.9188135862350464\n",
            "step 614: loss=2.375105381011963\n",
            "step 615: loss=2.673272132873535\n",
            "step 616: loss=1.8344693183898926\n",
            "step 617: loss=2.4735729694366455\n",
            "step 618: loss=2.9227075576782227\n",
            "step 619: loss=2.601637363433838\n",
            "step 620: loss=2.8680527210235596\n",
            "step 621: loss=2.750136613845825\n",
            "step 622: loss=2.760077476501465\n",
            "step 623: loss=2.456650733947754\n",
            "step 624: loss=2.635274648666382\n",
            "step 625: loss=2.229710102081299\n",
            "step 626: loss=2.5304970741271973\n",
            "step 627: loss=1.758838415145874\n",
            "step 628: loss=2.4079010486602783\n",
            "step 629: loss=2.1566646099090576\n",
            "step 630: loss=2.704578161239624\n",
            "step 631: loss=2.279012680053711\n",
            "step 632: loss=2.2432620525360107\n",
            "step 633: loss=2.50034761428833\n",
            "step 634: loss=2.6152102947235107\n",
            "step 635: loss=2.755998134613037\n",
            "step 636: loss=2.5153427124023438\n",
            "step 637: loss=2.3339571952819824\n",
            "step 638: loss=2.309783935546875\n",
            "step 639: loss=2.3423290252685547\n",
            "step 640: loss=2.5000076293945312\n",
            "step 641: loss=2.1122477054595947\n",
            "step 642: loss=2.166735887527466\n",
            "step 643: loss=2.1199700832366943\n",
            "step 644: loss=2.074465274810791\n",
            "step 645: loss=2.0806941986083984\n",
            "step 646: loss=2.5440194606781006\n",
            "step 647: loss=2.4385101795196533\n",
            "step 648: loss=2.1791906356811523\n",
            "step 649: loss=2.3050944805145264\n",
            "step 650: loss=2.8269851207733154\n",
            "step 651: loss=2.884178638458252\n",
            "step 652: loss=1.9879136085510254\n",
            "step 653: loss=1.937824010848999\n",
            "step 654: loss=2.827770471572876\n",
            "step 655: loss=2.4148051738739014\n",
            "step 656: loss=2.3607215881347656\n",
            "step 657: loss=2.106201171875\n",
            "step 658: loss=1.9861000776290894\n",
            "step 659: loss=2.0714499950408936\n",
            "step 660: loss=1.9481884241104126\n",
            "step 661: loss=2.4290735721588135\n",
            "step 662: loss=2.4058799743652344\n",
            "step 663: loss=2.3598883152008057\n",
            "step 664: loss=2.350245237350464\n",
            "step 665: loss=2.3665618896484375\n",
            "step 666: loss=1.8934234380722046\n",
            "step 667: loss=1.8712066411972046\n",
            "step 668: loss=2.2044436931610107\n",
            "step 669: loss=2.8850512504577637\n",
            "step 670: loss=2.296321153640747\n",
            "step 671: loss=2.0860350131988525\n",
            "step 672: loss=2.2306840419769287\n",
            "step 673: loss=2.164177656173706\n",
            "step 674: loss=2.688096284866333\n",
            "step 675: loss=2.7763352394104004\n",
            "step 676: loss=1.9899935722351074\n",
            "step 677: loss=2.225436210632324\n",
            "step 678: loss=2.3181285858154297\n",
            "step 679: loss=1.8210216760635376\n",
            "step 680: loss=2.008143424987793\n",
            "step 681: loss=2.197258234024048\n",
            "step 682: loss=2.170909881591797\n",
            "step 683: loss=2.2557735443115234\n",
            "step 684: loss=3.137401580810547\n",
            "step 685: loss=2.897404193878174\n",
            "step 686: loss=2.01969313621521\n",
            "step 687: loss=2.3206636905670166\n",
            "step 688: loss=2.7755227088928223\n",
            "step 689: loss=2.255751132965088\n",
            "step 690: loss=2.2114686965942383\n",
            "step 691: loss=2.6988778114318848\n",
            "step 692: loss=2.147263526916504\n",
            "step 693: loss=2.080416679382324\n",
            "step 694: loss=1.9948620796203613\n",
            "step 695: loss=2.176727771759033\n",
            "step 696: loss=2.09790301322937\n",
            "step 697: loss=2.1391477584838867\n",
            "step 698: loss=1.8827745914459229\n",
            "step 699: loss=2.848010301589966\n",
            "step 700: loss=2.0891661643981934\n",
            "step 701: loss=3.123166561126709\n",
            "step 702: loss=2.0372824668884277\n",
            "step 703: loss=2.02809476852417\n",
            "step 704: loss=1.6865804195404053\n",
            "step 705: loss=2.2516367435455322\n",
            "step 706: loss=2.6335573196411133\n",
            "step 707: loss=1.9368773698806763\n",
            "step 708: loss=2.2163896560668945\n",
            "step 709: loss=3.1224889755249023\n",
            "step 710: loss=2.609941244125366\n",
            "step 711: loss=2.1747162342071533\n",
            "step 712: loss=3.0469765663146973\n",
            "step 713: loss=2.1493568420410156\n",
            "step 714: loss=1.7386999130249023\n",
            "step 715: loss=3.0281639099121094\n",
            "step 716: loss=2.883162498474121\n",
            "step 717: loss=2.9371163845062256\n",
            "step 718: loss=1.7309739589691162\n",
            "step 719: loss=2.80397891998291\n",
            "step 720: loss=1.6866511106491089\n",
            "step 721: loss=1.9735374450683594\n",
            "step 722: loss=2.6356143951416016\n",
            "step 723: loss=2.5481433868408203\n",
            "step 724: loss=2.254169464111328\n",
            "step 725: loss=2.9612419605255127\n",
            "step 726: loss=1.9842915534973145\n",
            "step 727: loss=1.9425053596496582\n",
            "step 728: loss=1.8789762258529663\n",
            "step 729: loss=2.240078926086426\n",
            "step 730: loss=3.021658420562744\n",
            "step 731: loss=2.1771240234375\n",
            "step 732: loss=2.3247807025909424\n",
            "step 733: loss=1.8196877241134644\n",
            "step 734: loss=1.8007091283798218\n",
            "step 735: loss=2.2185044288635254\n",
            "step 736: loss=2.3220367431640625\n",
            "step 737: loss=2.4636054039001465\n",
            "step 738: loss=2.413667917251587\n",
            "step 739: loss=1.7327954769134521\n",
            "step 740: loss=1.6979758739471436\n",
            "step 741: loss=2.9912562370300293\n",
            "step 742: loss=1.6229894161224365\n",
            "step 743: loss=1.8488349914550781\n",
            "step 744: loss=1.5336363315582275\n",
            "step 745: loss=1.8343586921691895\n",
            "step 746: loss=3.026932716369629\n",
            "step 747: loss=2.920630931854248\n",
            "step 748: loss=2.851318359375\n",
            "step 749: loss=2.2960121631622314\n",
            "step 750: loss=2.305487632751465\n",
            "step 751: loss=2.41092848777771\n",
            "step 752: loss=3.156630039215088\n",
            "step 753: loss=2.3597564697265625\n",
            "step 754: loss=2.296945095062256\n",
            "step 755: loss=2.2489349842071533\n",
            "step 756: loss=3.0600132942199707\n",
            "step 757: loss=2.2851290702819824\n",
            "step 758: loss=2.465679168701172\n",
            "step 759: loss=3.009960174560547\n",
            "step 760: loss=2.975863456726074\n",
            "step 761: loss=2.440443992614746\n",
            "step 762: loss=2.394619941711426\n",
            "step 763: loss=2.849440097808838\n",
            "step 764: loss=2.9782421588897705\n",
            "step 765: loss=2.0069611072540283\n",
            "step 766: loss=2.1759393215179443\n",
            "step 767: loss=2.1274547576904297\n",
            "step 768: loss=2.2416064739227295\n",
            "step 769: loss=2.051609992980957\n",
            "step 770: loss=2.8667960166931152\n",
            "step 771: loss=2.7796058654785156\n",
            "step 772: loss=1.9498274326324463\n",
            "step 773: loss=1.907271146774292\n",
            "step 774: loss=2.2015457153320312\n",
            "step 775: loss=2.6640267372131348\n",
            "step 776: loss=2.1354448795318604\n",
            "step 777: loss=1.7911410331726074\n",
            "step 778: loss=2.038367748260498\n",
            "step 779: loss=2.0537021160125732\n",
            "step 780: loss=1.9189090728759766\n",
            "step 781: loss=2.352097272872925\n",
            "step 782: loss=2.73466420173645\n",
            "step 783: loss=2.4804656505584717\n",
            "step 784: loss=1.7195073366165161\n",
            "step 785: loss=2.4964001178741455\n",
            "step 786: loss=2.6462271213531494\n",
            "step 787: loss=1.6790680885314941\n",
            "step 788: loss=2.4061331748962402\n",
            "step 789: loss=2.4785842895507812\n",
            "step 790: loss=2.8874361515045166\n",
            "step 791: loss=2.408928155899048\n",
            "step 792: loss=2.856551170349121\n",
            "step 793: loss=2.4651124477386475\n",
            "step 794: loss=2.836892604827881\n",
            "step 795: loss=2.2396438121795654\n",
            "step 796: loss=2.393333673477173\n",
            "step 797: loss=2.335814952850342\n",
            "step 798: loss=2.917409896850586\n",
            "step 799: loss=1.7263388633728027\n",
            "step 800: loss=2.693992853164673\n",
            "step 801: loss=2.4763824939727783\n",
            "step 802: loss=0.00116729736328125\n",
            "step 803: loss=2.119325876235962\n",
            "step 804: loss=2.462939739227295\n",
            "step 805: loss=2.435828924179077\n",
            "step 806: loss=0.00011444091796875\n",
            "step 807: loss=2.4036693572998047\n",
            "step 808: loss=2.3323192596435547\n",
            "step 809: loss=2.86004638671875\n",
            "step 810: loss=0.0\n",
            "step 811: loss=2.223119020462036\n",
            "step 812: loss=8.131641387939453\n",
            "step 813: loss=0.0\n",
            "step 814: loss=2.070364475250244\n",
            "step 815: loss=2.21044921875\n",
            "step 816: loss=2.6131129264831543\n",
            "step 817: loss=0.20123767852783203\n",
            "step 818: loss=2.440242290496826\n",
            "step 819: loss=1.9672431945800781\n",
            "step 820: loss=2.4227402210235596\n",
            "step 821: loss=2.0739071369171143\n",
            "step 822: loss=2.2876696586608887\n",
            "step 823: loss=2.1056289672851562\n",
            "step 824: loss=2.0039584636688232\n",
            "step 825: loss=3.059359550476074\n",
            "step 826: loss=2.065335273742676\n",
            "step 827: loss=2.6378173828125\n",
            "step 828: loss=2.3604588508605957\n",
            "step 829: loss=2.1247146129608154\n",
            "step 830: loss=2.420316457748413\n",
            "step 831: loss=2.3525712490081787\n",
            "step 832: loss=2.5704455375671387\n",
            "step 833: loss=2.2123985290527344\n",
            "step 834: loss=2.945969581604004\n",
            "step 835: loss=2.351858139038086\n",
            "step 836: loss=1.941652536392212\n",
            "step 837: loss=2.2451865673065186\n",
            "step 838: loss=1.9354561567306519\n",
            "step 839: loss=1.9113922119140625\n",
            "step 840: loss=2.8041863441467285\n",
            "step 841: loss=1.7878113985061646\n",
            "step 842: loss=2.4560422897338867\n",
            "step 843: loss=2.4183309078216553\n",
            "step 844: loss=2.428185224533081\n",
            "step 845: loss=2.32889986038208\n",
            "step 846: loss=2.347452163696289\n",
            "step 847: loss=2.381387710571289\n",
            "step 848: loss=2.6200146675109863\n",
            "step 849: loss=2.744673013687134\n",
            "step 850: loss=2.5162761211395264\n",
            "step 851: loss=2.10432767868042\n",
            "step 852: loss=2.3757762908935547\n",
            "step 853: loss=2.935946464538574\n",
            "step 854: loss=2.5316162109375\n",
            "step 855: loss=2.495148181915283\n",
            "step 856: loss=2.4315552711486816\n",
            "step 857: loss=2.2631986141204834\n",
            "step 858: loss=2.21051025390625\n",
            "step 859: loss=2.0296730995178223\n",
            "step 860: loss=1.9528510570526123\n",
            "step 861: loss=2.4665634632110596\n",
            "step 862: loss=1.987592101097107\n",
            "step 863: loss=2.1962778568267822\n",
            "step 864: loss=2.2637593746185303\n",
            "step 865: loss=1.9525563716888428\n",
            "step 866: loss=2.906764030456543\n",
            "step 867: loss=2.454118490219116\n",
            "step 868: loss=1.703061580657959\n",
            "step 869: loss=2.862419843673706\n",
            "step 870: loss=2.3681640625\n",
            "step 871: loss=2.7749109268188477\n",
            "step 872: loss=3.0203185081481934\n",
            "step 873: loss=2.377793550491333\n",
            "step 874: loss=2.3370633125305176\n",
            "step 875: loss=2.2724030017852783\n",
            "step 876: loss=2.928386688232422\n",
            "step 877: loss=2.369084358215332\n",
            "step 878: loss=2.3113021850585938\n",
            "step 879: loss=3.057217597961426\n",
            "step 880: loss=2.7761049270629883\n",
            "step 881: loss=1.8065228462219238\n",
            "step 882: loss=2.236966609954834\n",
            "step 883: loss=2.1111345291137695\n",
            "step 884: loss=2.9650158882141113\n",
            "step 885: loss=1.9991835355758667\n",
            "step 886: loss=2.5643506050109863\n",
            "step 887: loss=2.8440678119659424\n",
            "step 888: loss=2.7584404945373535\n",
            "step 889: loss=2.2468223571777344\n",
            "step 890: loss=1.9735445976257324\n",
            "step 891: loss=2.5295753479003906\n",
            "step 892: loss=2.4103164672851562\n",
            "step 893: loss=1.9637342691421509\n",
            "step 894: loss=1.7756059169769287\n",
            "step 895: loss=2.777315378189087\n",
            "step 896: loss=2.4124491214752197\n",
            "step 897: loss=2.363562822341919\n",
            "step 898: loss=2.7304153442382812\n",
            "step 899: loss=1.724329948425293\n",
            "step 900: loss=2.5066781044006348\n",
            "step 901: loss=2.307114362716675\n",
            "step 902: loss=1.666579008102417\n",
            "step 903: loss=2.017904758453369\n",
            "step 904: loss=2.5984115600585938\n",
            "step 905: loss=2.4395689964294434\n",
            "step 906: loss=2.500723361968994\n",
            "step 907: loss=2.289588212966919\n",
            "step 908: loss=2.341440200805664\n",
            "step 909: loss=2.4522652626037598\n",
            "step 910: loss=2.0434610843658447\n",
            "step 911: loss=11.173596382141113\n",
            "step 912: loss=2.623532295227051\n",
            "step 913: loss=2.089109420776367\n",
            "step 914: loss=1.9951262474060059\n",
            "step 915: loss=2.2713446617126465\n",
            "step 916: loss=2.4112234115600586\n",
            "step 917: loss=3.248404026031494\n",
            "step 918: loss=2.8142876625061035\n",
            "step 919: loss=2.5873055458068848\n",
            "step 920: loss=2.4187893867492676\n",
            "step 921: loss=2.3777620792388916\n",
            "step 922: loss=2.267307758331299\n",
            "step 923: loss=2.3030030727386475\n",
            "step 924: loss=2.2903921604156494\n",
            "step 925: loss=2.205355405807495\n",
            "step 926: loss=1.8529279232025146\n",
            "step 927: loss=2.4803333282470703\n",
            "step 928: loss=2.197161912918091\n",
            "step 929: loss=2.1335225105285645\n",
            "step 930: loss=2.238537073135376\n",
            "step 931: loss=2.1496574878692627\n",
            "step 932: loss=2.1188602447509766\n",
            "step 933: loss=2.295938730239868\n",
            "step 934: loss=2.304058074951172\n",
            "step 935: loss=1.9185147285461426\n",
            "step 936: loss=2.8971166610717773\n",
            "step 937: loss=1.9830782413482666\n",
            "step 938: loss=2.8428831100463867\n",
            "step 939: loss=2.774096727371216\n",
            "step 940: loss=1.9650591611862183\n",
            "step 941: loss=2.270132064819336\n",
            "step 942: loss=1.789460301399231\n",
            "step 943: loss=1.9087673425674438\n",
            "step 944: loss=2.338059663772583\n",
            "step 945: loss=3.3395187854766846\n",
            "step 946: loss=2.28542423248291\n",
            "step 947: loss=3.267141819000244\n",
            "step 948: loss=2.3742856979370117\n",
            "step 949: loss=2.440078020095825\n",
            "step 950: loss=2.3733928203582764\n",
            "step 951: loss=2.28555965423584\n",
            "step 952: loss=2.331580877304077\n",
            "step 953: loss=2.3607163429260254\n",
            "step 954: loss=2.258847951889038\n",
            "step 955: loss=2.134711503982544\n",
            "step 956: loss=2.344968557357788\n",
            "step 957: loss=1.9831026792526245\n",
            "step 958: loss=2.7800188064575195\n",
            "step 959: loss=2.739513397216797\n",
            "step 960: loss=2.969251871109009\n",
            "step 961: loss=2.0790092945098877\n",
            "step 962: loss=2.3159286975860596\n",
            "step 963: loss=2.033416271209717\n",
            "step 964: loss=2.2474937438964844\n",
            "step 965: loss=2.571489095687866\n",
            "step 966: loss=2.1264798641204834\n",
            "step 967: loss=1.9659028053283691\n",
            "step 968: loss=2.147050380706787\n",
            "step 969: loss=2.0319550037384033\n",
            "step 970: loss=3.1746811866760254\n",
            "step 971: loss=3.1227259635925293\n",
            "step 972: loss=2.105175495147705\n",
            "step 973: loss=1.9598941802978516\n",
            "step 974: loss=2.3961567878723145\n",
            "step 975: loss=2.311633586883545\n",
            "step 976: loss=2.9196853637695312\n",
            "step 977: loss=2.132205009460449\n",
            "step 978: loss=2.2813618183135986\n",
            "step 979: loss=2.7506484985351562\n",
            "step 980: loss=2.8064208030700684\n",
            "step 981: loss=1.9534919261932373\n",
            "step 982: loss=2.1900393962860107\n",
            "step 983: loss=2.1190848350524902\n",
            "step 984: loss=2.4793338775634766\n",
            "step 985: loss=2.088001251220703\n",
            "step 986: loss=2.3418800830841064\n",
            "step 987: loss=2.7616536617279053\n",
            "step 988: loss=2.4232287406921387\n",
            "step 989: loss=2.109419345855713\n",
            "step 990: loss=2.3932247161865234\n",
            "step 991: loss=2.030672550201416\n",
            "step 992: loss=2.442863702774048\n",
            "step 993: loss=3.07527232170105\n",
            "step 994: loss=2.0584349632263184\n",
            "step 995: loss=2.0057828426361084\n",
            "step 996: loss=3.00581955909729\n",
            "step 997: loss=2.0014851093292236\n",
            "step 998: loss=2.665393114089966\n",
            "step 999: loss=2.5900321006774902\n",
            "step 1000: loss=1.8374029397964478\n",
            "Mean loss        2.3661168\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [3 3 3 3 3 3 3 3 3 3]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "optimizer = optax.adam(learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b6626627",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-2 # for all subsequent models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1f508a69",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=0.32265353202819824\n",
            "step 2: loss=0.7677361965179443\n",
            "step 3: loss=1.3151150941848755\n",
            "step 4: loss=0.6902555227279663\n",
            "step 5: loss=1.5896856784820557\n",
            "step 6: loss=0.20080161094665527\n",
            "step 7: loss=1.746786117553711\n",
            "step 8: loss=0.7341992855072021\n",
            "step 9: loss=0.48326241970062256\n",
            "step 10: loss=2.7541470527648926\n",
            "step 11: loss=1.8300195932388306\n",
            "step 12: loss=4.892520427703857\n",
            "step 13: loss=0.034595489501953125\n",
            "step 14: loss=0.2189323902130127\n",
            "step 15: loss=6.246511936187744\n",
            "step 16: loss=3.8863754272460938\n",
            "step 17: loss=0.005483150482177734\n",
            "step 18: loss=6.574321269989014\n",
            "step 19: loss=5.217706203460693\n",
            "step 20: loss=0.5348190069198608\n",
            "step 21: loss=2.4435720443725586\n",
            "step 22: loss=0.3176398277282715\n",
            "step 23: loss=0.2063615322113037\n",
            "step 24: loss=0.8877159953117371\n",
            "step 25: loss=0.6408861875534058\n",
            "step 26: loss=2.742943525314331\n",
            "step 27: loss=4.988760471343994\n",
            "step 28: loss=0.7708103656768799\n",
            "step 29: loss=1.6248962879180908\n",
            "step 30: loss=1.644683837890625\n",
            "step 31: loss=1.1385538578033447\n",
            "step 32: loss=1.1512335538864136\n",
            "step 33: loss=0.08533644676208496\n",
            "step 34: loss=0.5107002258300781\n",
            "step 35: loss=0.7809004783630371\n",
            "step 36: loss=1.1551954746246338\n",
            "step 37: loss=0.1812453269958496\n",
            "step 38: loss=0.44345152378082275\n",
            "step 39: loss=3.4173107147216797\n",
            "step 40: loss=0.10831332206726074\n",
            "step 41: loss=1.806178331375122\n",
            "step 42: loss=4.005232810974121\n",
            "step 43: loss=0.18488669395446777\n",
            "step 44: loss=0.027593135833740234\n",
            "step 45: loss=1.5412343740463257\n",
            "step 46: loss=0.07512927055358887\n",
            "step 47: loss=2.5542240142822266\n",
            "step 48: loss=2.451758861541748\n",
            "step 49: loss=3.2079880237579346\n",
            "step 50: loss=1.0273089408874512\n",
            "step 51: loss=0.3488011360168457\n",
            "step 52: loss=0.04002642631530762\n",
            "step 53: loss=5.629854202270508\n",
            "step 54: loss=0.13922357559204102\n",
            "step 55: loss=2.162645101547241\n",
            "step 56: loss=2.1468398571014404\n",
            "step 57: loss=0.5651768445968628\n",
            "step 58: loss=0.20208525657653809\n",
            "step 59: loss=3.4273359775543213\n",
            "step 60: loss=0.2543294429779053\n",
            "step 61: loss=0.11790108680725098\n",
            "step 62: loss=0.47708308696746826\n",
            "step 63: loss=4.057819366455078\n",
            "step 64: loss=2.4739906787872314\n",
            "step 65: loss=2.2944488525390625\n",
            "step 66: loss=0.0047512054443359375\n",
            "step 67: loss=0.3416111469268799\n",
            "step 68: loss=0.008679389953613281\n",
            "step 69: loss=2.3101720809936523\n",
            "step 70: loss=0.7742013335227966\n",
            "step 71: loss=2.487290859222412\n",
            "step 72: loss=2.1794896125793457\n",
            "step 73: loss=0.11859607696533203\n",
            "step 74: loss=0.1509566307067871\n",
            "step 75: loss=0.015425682067871094\n",
            "step 76: loss=0.13530468940734863\n",
            "step 77: loss=2.7879390716552734\n",
            "step 78: loss=1.5177329778671265\n",
            "step 79: loss=0.7439005374908447\n",
            "step 80: loss=0.08385396003723145\n",
            "step 81: loss=0.017795562744140625\n",
            "step 82: loss=0.2225627899169922\n",
            "step 83: loss=0.8793734312057495\n",
            "step 84: loss=0.5103082656860352\n",
            "step 85: loss=2.1783640384674072\n",
            "step 86: loss=0.20706677436828613\n",
            "step 87: loss=2.518519639968872\n",
            "step 88: loss=0.10665023326873779\n",
            "step 89: loss=0.7768527269363403\n",
            "step 90: loss=0.7451890707015991\n",
            "step 91: loss=1.2006385326385498\n",
            "step 92: loss=0.27802956104278564\n",
            "step 93: loss=0.013898849487304688\n",
            "step 94: loss=1.7205288410186768\n",
            "step 95: loss=0.4559425115585327\n",
            "step 96: loss=1.7405130863189697\n",
            "step 97: loss=0.15360808372497559\n",
            "step 98: loss=1.0509374141693115\n",
            "step 99: loss=0.7338051199913025\n",
            "step 100: loss=0.08263039588928223\n",
            "step 101: loss=1.5180668830871582\n",
            "step 102: loss=0.2217257022857666\n",
            "step 103: loss=1.6552200317382812\n",
            "step 104: loss=0.22332382202148438\n",
            "step 105: loss=0.6831475496292114\n",
            "step 106: loss=0.7622709274291992\n",
            "step 107: loss=0.7726247310638428\n",
            "step 108: loss=0.15828287601470947\n",
            "step 109: loss=0.726866602897644\n",
            "step 110: loss=0.2629443407058716\n",
            "step 111: loss=0.6732463836669922\n",
            "step 112: loss=0.0007729530334472656\n",
            "step 113: loss=1.0883034467697144\n",
            "step 114: loss=2.5593514442443848\n",
            "step 115: loss=0.3875765800476074\n",
            "step 116: loss=0.7885996699333191\n",
            "step 117: loss=0.4653335511684418\n",
            "step 118: loss=0.016709566116333008\n",
            "step 119: loss=0.7658334970474243\n",
            "step 120: loss=3.3576200008392334\n",
            "step 121: loss=1.613950490951538\n",
            "step 122: loss=2.853659152984619\n",
            "step 123: loss=2.245157241821289\n",
            "step 124: loss=0.9436036348342896\n",
            "step 125: loss=0.1656639575958252\n",
            "step 126: loss=2.1030101776123047\n",
            "step 127: loss=0.07231473922729492\n",
            "step 128: loss=0.03444623947143555\n",
            "step 129: loss=0.09939348697662354\n",
            "step 130: loss=3.1196091175079346\n",
            "step 131: loss=0.08367300033569336\n",
            "step 132: loss=2.096038818359375\n",
            "step 133: loss=1.3605695962905884\n",
            "step 134: loss=0.041274309158325195\n",
            "step 135: loss=0.010736942291259766\n",
            "step 136: loss=0.0274200439453125\n",
            "step 137: loss=1.3276501893997192\n",
            "step 138: loss=0.02072286605834961\n",
            "step 139: loss=1.532369613647461\n",
            "step 140: loss=2.4742841720581055\n",
            "step 141: loss=0.0015196800231933594\n",
            "step 142: loss=1.096373438835144\n",
            "step 143: loss=0.0033092498779296875\n",
            "step 144: loss=1.58939790725708\n",
            "step 145: loss=0.00022840499877929688\n",
            "step 146: loss=0.587228000164032\n",
            "step 147: loss=0.8827263116836548\n",
            "step 148: loss=1.8595784902572632\n",
            "step 149: loss=1.0038392543792725\n",
            "step 150: loss=0.3937070369720459\n",
            "step 151: loss=0.2630171775817871\n",
            "step 152: loss=0.0568389892578125\n",
            "step 153: loss=1.020188331604004\n",
            "step 154: loss=0.1382920742034912\n",
            "step 155: loss=1.655358076095581\n",
            "step 156: loss=0.29212141036987305\n",
            "step 157: loss=0.6886098384857178\n",
            "step 158: loss=3.8796944618225098\n",
            "step 159: loss=1.1654655933380127\n",
            "step 160: loss=0.5220543146133423\n",
            "step 161: loss=0.06435871124267578\n",
            "step 162: loss=0.14003539085388184\n",
            "step 163: loss=0.24125194549560547\n",
            "step 164: loss=0.5165549516677856\n",
            "step 165: loss=1.1098735332489014\n",
            "step 166: loss=0.22089672088623047\n",
            "step 167: loss=1.6918665170669556\n",
            "step 168: loss=0.8362998366355896\n",
            "step 169: loss=0.6213744878768921\n",
            "step 170: loss=0.33026957511901855\n",
            "step 171: loss=0.03860807418823242\n",
            "step 172: loss=0.4243044853210449\n",
            "step 173: loss=1.420479416847229\n",
            "step 174: loss=0.5432281494140625\n",
            "step 175: loss=1.1958606243133545\n",
            "step 176: loss=0.5589032173156738\n",
            "step 177: loss=0.30118656158447266\n",
            "step 178: loss=0.2188817262649536\n",
            "step 179: loss=0.05227303504943848\n",
            "step 180: loss=7.683701038360596\n",
            "step 181: loss=0.06298255920410156\n",
            "step 182: loss=0.02646493911743164\n",
            "step 183: loss=0.252327561378479\n",
            "step 184: loss=1.1339046955108643\n",
            "step 185: loss=0.04089927673339844\n",
            "step 186: loss=0.014993667602539062\n",
            "step 187: loss=1.951836347579956\n",
            "step 188: loss=0.8857433795928955\n",
            "step 189: loss=0.18923187255859375\n",
            "step 190: loss=0.08303499221801758\n",
            "step 191: loss=0.006696462631225586\n",
            "step 192: loss=0.16001415252685547\n",
            "step 193: loss=0.046497344970703125\n",
            "step 194: loss=0.25188159942626953\n",
            "step 195: loss=0.0040454864501953125\n",
            "step 196: loss=1.9257593154907227\n",
            "step 197: loss=0.35396236181259155\n",
            "step 198: loss=0.006565570831298828\n",
            "step 199: loss=4.005255222320557\n",
            "step 200: loss=0.0034499168395996094\n",
            "step 201: loss=0.17750918865203857\n",
            "step 202: loss=2.0410399436950684\n",
            "step 203: loss=0.005888462066650391\n",
            "step 204: loss=0.009962081909179688\n",
            "step 205: loss=0.37105298042297363\n",
            "step 206: loss=0.35458290576934814\n",
            "step 207: loss=0.17059040069580078\n",
            "step 208: loss=0.7920452356338501\n",
            "step 209: loss=0.03953719139099121\n",
            "step 210: loss=0.06644916534423828\n",
            "step 211: loss=0.0030989646911621094\n",
            "step 212: loss=0.7253940105438232\n",
            "step 213: loss=2.109890937805176\n",
            "step 214: loss=3.948280096054077\n",
            "step 215: loss=0.03413677215576172\n",
            "step 216: loss=0.801572322845459\n",
            "step 217: loss=1.1378155946731567\n",
            "step 218: loss=0.037384033203125\n",
            "step 219: loss=0.009069442749023438\n",
            "step 220: loss=0.5270925164222717\n",
            "step 221: loss=1.0650709867477417\n",
            "step 222: loss=7.401341438293457\n",
            "step 223: loss=0.46386396884918213\n",
            "step 224: loss=0.17491745948791504\n",
            "step 225: loss=0.15935277938842773\n",
            "step 226: loss=0.750916600227356\n",
            "step 227: loss=0.07780170440673828\n",
            "step 228: loss=0.12094569206237793\n",
            "step 229: loss=0.20540761947631836\n",
            "step 230: loss=1.897470474243164\n",
            "step 231: loss=0.2994046211242676\n",
            "step 232: loss=0.024597883224487305\n",
            "step 233: loss=0.0014605522155761719\n",
            "step 234: loss=4.177861213684082\n",
            "step 235: loss=0.11293911933898926\n",
            "step 236: loss=0.3203963041305542\n",
            "step 237: loss=0.0004315376281738281\n",
            "step 238: loss=0.08163070678710938\n",
            "step 239: loss=0.07216691970825195\n",
            "step 240: loss=0.06731986999511719\n",
            "step 241: loss=0.005340576171875\n",
            "step 242: loss=0.05252695083618164\n",
            "step 243: loss=0.15529990196228027\n",
            "step 244: loss=0.0381779670715332\n",
            "step 245: loss=0.21390914916992188\n",
            "step 246: loss=0.8916691541671753\n",
            "step 247: loss=0.08250689506530762\n",
            "step 248: loss=4.838135719299316\n",
            "step 249: loss=0.17591917514801025\n",
            "step 250: loss=0.0005049705505371094\n",
            "step 251: loss=0.8584518432617188\n",
            "step 252: loss=0.002243518829345703\n",
            "step 253: loss=0.33853763341903687\n",
            "step 254: loss=0.000408172607421875\n",
            "step 255: loss=0.00022220611572265625\n",
            "step 256: loss=0.06497049331665039\n",
            "step 257: loss=3.8022379875183105\n",
            "step 258: loss=0.5645577907562256\n",
            "step 259: loss=0.960322916507721\n",
            "step 260: loss=0.02972698211669922\n",
            "step 261: loss=0.6721219420433044\n",
            "step 262: loss=1.2018390893936157\n",
            "step 263: loss=0.0009274482727050781\n",
            "step 264: loss=1.6904597282409668\n",
            "step 265: loss=1.1635427474975586\n",
            "step 266: loss=1.0335867404937744\n",
            "step 267: loss=0.7021582126617432\n",
            "step 268: loss=0.5141974091529846\n",
            "step 269: loss=0.00021076202392578125\n",
            "step 270: loss=0.8425770998001099\n",
            "step 271: loss=0.0003185272216796875\n",
            "step 272: loss=0.1913689374923706\n",
            "step 273: loss=0.607884407043457\n",
            "step 274: loss=0.08121252059936523\n",
            "step 275: loss=0.7478201389312744\n",
            "step 276: loss=0.017640352249145508\n",
            "step 277: loss=0.3041074275970459\n",
            "step 278: loss=0.07037806510925293\n",
            "step 279: loss=0.15466535091400146\n",
            "step 280: loss=0.2326505184173584\n",
            "step 281: loss=0.0048885345458984375\n",
            "step 282: loss=0.01286935806274414\n",
            "step 283: loss=1.2199950218200684\n",
            "step 284: loss=3.0553770065307617\n",
            "step 285: loss=0.042328596115112305\n",
            "step 286: loss=0.08937621116638184\n",
            "step 287: loss=0.14047908782958984\n",
            "step 288: loss=0.012386322021484375\n",
            "step 289: loss=3.2871956825256348\n",
            "step 290: loss=0.02381300926208496\n",
            "step 291: loss=0.12465691566467285\n",
            "step 292: loss=5.0738983154296875\n",
            "step 293: loss=0.0003037452697753906\n",
            "step 294: loss=0.0003199577331542969\n",
            "step 295: loss=0.4114079475402832\n",
            "step 296: loss=0.10671758651733398\n",
            "step 297: loss=0.01735067367553711\n",
            "step 298: loss=0.0067195892333984375\n",
            "step 299: loss=4.57763671875e-05\n",
            "step 300: loss=0.02154219150543213\n",
            "step 301: loss=0.0001316070556640625\n",
            "step 302: loss=3.949228048324585\n",
            "step 303: loss=0.003207683563232422\n",
            "step 304: loss=1.1534712314605713\n",
            "step 305: loss=0.5426097512245178\n",
            "step 306: loss=0.18557894229888916\n",
            "step 307: loss=1.875374674797058\n",
            "step 308: loss=0.013012886047363281\n",
            "step 309: loss=1.295936107635498\n",
            "step 310: loss=0.5563555955886841\n",
            "step 311: loss=1.8282849788665771\n",
            "step 312: loss=0.08618056774139404\n",
            "step 313: loss=0.0003261566162109375\n",
            "step 314: loss=0.29242920875549316\n",
            "step 315: loss=0.03786349296569824\n",
            "step 316: loss=0.5255357623100281\n",
            "step 317: loss=0.1747748851776123\n",
            "step 318: loss=0.8038355112075806\n",
            "step 319: loss=0.06973791122436523\n",
            "step 320: loss=0.3295741081237793\n",
            "step 321: loss=1.7440779209136963\n",
            "step 322: loss=0.49291276931762695\n",
            "step 323: loss=0.0035965442657470703\n",
            "step 324: loss=0.15467143058776855\n",
            "step 325: loss=0.30799591541290283\n",
            "step 326: loss=2.5678951740264893\n",
            "step 327: loss=0.04656839370727539\n",
            "step 328: loss=0.6120253801345825\n",
            "step 329: loss=0.0004143714904785156\n",
            "step 330: loss=0.7317354679107666\n",
            "step 331: loss=6.4849853515625e-05\n",
            "step 332: loss=0.17156267166137695\n",
            "step 333: loss=0.009419441223144531\n",
            "step 334: loss=0.0026421546936035156\n",
            "step 335: loss=0.5201983451843262\n",
            "step 336: loss=0.12111687660217285\n",
            "step 337: loss=0.19329023361206055\n",
            "step 338: loss=0.4536595344543457\n",
            "step 339: loss=0.03757452964782715\n",
            "step 340: loss=0.007574319839477539\n",
            "step 341: loss=0.20283639430999756\n",
            "step 342: loss=0.2585725784301758\n",
            "step 343: loss=0.002673625946044922\n",
            "step 344: loss=2.5032799243927\n",
            "step 345: loss=0.2889130115509033\n",
            "step 346: loss=0.07102608680725098\n",
            "step 347: loss=0.2594940662384033\n",
            "step 348: loss=0.00115203857421875\n",
            "step 349: loss=0.3011915683746338\n",
            "step 350: loss=6.914138793945312e-05\n",
            "step 351: loss=0.014842987060546875\n",
            "step 352: loss=3.0785984992980957\n",
            "step 353: loss=12.086795806884766\n",
            "step 354: loss=0.3800543546676636\n",
            "step 355: loss=0.004242420196533203\n",
            "step 356: loss=3.814697265625e-06\n",
            "step 357: loss=3.7478935718536377\n",
            "step 358: loss=0.2555088996887207\n",
            "step 359: loss=0.01464390754699707\n",
            "step 360: loss=0.018193721771240234\n",
            "step 361: loss=0.00027179718017578125\n",
            "step 362: loss=7.62939453125e-06\n",
            "step 363: loss=2.6358022689819336\n",
            "step 364: loss=0.9038869738578796\n",
            "step 365: loss=0.002448558807373047\n",
            "step 366: loss=0.385647177696228\n",
            "step 367: loss=0.306560754776001\n",
            "step 368: loss=2.86102294921875e-06\n",
            "step 369: loss=0.028749942779541016\n",
            "step 370: loss=1.6154565811157227\n",
            "step 371: loss=0.32637524604797363\n",
            "step 372: loss=1.621246337890625e-05\n",
            "step 373: loss=5.245208740234375e-05\n",
            "step 374: loss=0.07365357875823975\n",
            "step 375: loss=0.019390106201171875\n",
            "step 376: loss=0.4800921678543091\n",
            "step 377: loss=1.9815953969955444\n",
            "step 378: loss=0.053194522857666016\n",
            "step 379: loss=2.6462576389312744\n",
            "step 380: loss=5.91278076171875e-05\n",
            "step 381: loss=8.7738037109375e-05\n",
            "step 382: loss=1.4447065591812134\n",
            "step 383: loss=0.09317278861999512\n",
            "step 384: loss=0.00015020370483398438\n",
            "step 385: loss=0.8477288484573364\n",
            "step 386: loss=0.005916595458984375\n",
            "step 387: loss=0.03995966911315918\n",
            "step 388: loss=0.07850384712219238\n",
            "step 389: loss=0.009350776672363281\n",
            "step 390: loss=0.2361316680908203\n",
            "step 391: loss=0.1444687843322754\n",
            "step 392: loss=0.9264535903930664\n",
            "step 393: loss=2.910033702850342\n",
            "step 394: loss=0.6414859890937805\n",
            "step 395: loss=0.003253459930419922\n",
            "step 396: loss=0.37188220024108887\n",
            "step 397: loss=0.5570233464241028\n",
            "step 398: loss=0.24906635284423828\n",
            "step 399: loss=0.3767280578613281\n",
            "step 400: loss=0.0004782676696777344\n",
            "step 401: loss=0.5489275455474854\n",
            "step 402: loss=0.008211135864257812\n",
            "step 403: loss=0.0\n",
            "step 404: loss=0.014952659606933594\n",
            "step 405: loss=0.1774153709411621\n",
            "step 406: loss=0.13045740127563477\n",
            "step 407: loss=0.06400132179260254\n",
            "step 408: loss=0.04745984077453613\n",
            "step 409: loss=2.131531000137329\n",
            "step 410: loss=0.0003986358642578125\n",
            "step 411: loss=0.2538273334503174\n",
            "step 412: loss=0.006916522979736328\n",
            "step 413: loss=0.4559413194656372\n",
            "step 414: loss=0.5141700506210327\n",
            "step 415: loss=0.0007843971252441406\n",
            "step 416: loss=2.8805787563323975\n",
            "step 417: loss=0.3612031936645508\n",
            "step 418: loss=0.014259576797485352\n",
            "step 419: loss=0.7309237718582153\n",
            "step 420: loss=0.021861553192138672\n",
            "step 421: loss=0.11862730979919434\n",
            "step 422: loss=0.3005180358886719\n",
            "step 423: loss=0.05219674110412598\n",
            "step 424: loss=0.07789897918701172\n",
            "step 425: loss=0.61839759349823\n",
            "step 426: loss=0.34236741065979004\n",
            "step 427: loss=2.04842472076416\n",
            "step 428: loss=0.0419921875\n",
            "step 429: loss=0.006047725677490234\n",
            "step 430: loss=0.0003094673156738281\n",
            "step 431: loss=0.09932446479797363\n",
            "step 432: loss=0.96659255027771\n",
            "step 433: loss=0.036533355712890625\n",
            "step 434: loss=0.05873823165893555\n",
            "step 435: loss=2.86102294921875e-05\n",
            "step 436: loss=0.4893862009048462\n",
            "step 437: loss=0.0001583099365234375\n",
            "step 438: loss=0.002471923828125\n",
            "step 439: loss=0.40089356899261475\n",
            "step 440: loss=0.002857208251953125\n",
            "step 441: loss=0.0016713142395019531\n",
            "step 442: loss=0.0978161096572876\n",
            "step 443: loss=1.1452834606170654\n",
            "step 444: loss=1.0741101503372192\n",
            "step 445: loss=0.365409255027771\n",
            "step 446: loss=1.9127360582351685\n",
            "step 447: loss=0.0006847381591796875\n",
            "step 448: loss=0.04475688934326172\n",
            "step 449: loss=0.3132532835006714\n",
            "step 450: loss=0.860146164894104\n",
            "step 451: loss=0.0\n",
            "step 452: loss=0.073272705078125\n",
            "step 453: loss=3.528594970703125e-05\n",
            "step 454: loss=7.578391075134277\n",
            "step 455: loss=2.154956340789795\n",
            "step 456: loss=0.4746134281158447\n",
            "step 457: loss=0.19623756408691406\n",
            "step 458: loss=0.302389919757843\n",
            "step 459: loss=0.028781414031982422\n",
            "step 460: loss=0.13115382194519043\n",
            "step 461: loss=3.535740613937378\n",
            "step 462: loss=0.049870967864990234\n",
            "step 463: loss=0.3754899501800537\n",
            "step 464: loss=0.06133532524108887\n",
            "step 465: loss=5.055114269256592\n",
            "step 466: loss=2.113246440887451\n",
            "step 467: loss=2.186758041381836\n",
            "step 468: loss=0.11871933937072754\n",
            "step 469: loss=0.0059909820556640625\n",
            "step 470: loss=0.45180392265319824\n",
            "step 471: loss=2.9364514350891113\n",
            "step 472: loss=0.9227668046951294\n",
            "step 473: loss=0.160292387008667\n",
            "step 474: loss=0.08689546585083008\n",
            "step 475: loss=0.006661415100097656\n",
            "step 476: loss=0.03014516830444336\n",
            "step 477: loss=0.6065902709960938\n",
            "step 478: loss=0.00028705596923828125\n",
            "step 479: loss=0.0007677078247070312\n",
            "step 480: loss=0.45163655281066895\n",
            "step 481: loss=0.5646555423736572\n",
            "step 482: loss=0.0047986507415771484\n",
            "step 483: loss=0.0021982192993164062\n",
            "step 484: loss=1.0458555221557617\n",
            "step 485: loss=1.9045675992965698\n",
            "step 486: loss=0.0\n",
            "step 487: loss=2.86102294921875e-05\n",
            "step 488: loss=2.86102294921875e-06\n",
            "step 489: loss=0.04232454299926758\n",
            "step 490: loss=0.6244235038757324\n",
            "step 491: loss=0.0\n",
            "step 492: loss=0.010768890380859375\n",
            "step 493: loss=0.8142609596252441\n",
            "step 494: loss=0.08916425704956055\n",
            "step 495: loss=0.02032470703125\n",
            "step 496: loss=0.10290408134460449\n",
            "step 497: loss=4.00543212890625e-05\n",
            "step 498: loss=0.0010089874267578125\n",
            "step 499: loss=2.3498404026031494\n",
            "step 500: loss=0.9742305278778076\n",
            "step 501: loss=0.0010728836059570312\n",
            "step 502: loss=0.4342411756515503\n",
            "step 503: loss=0.0021448135375976562\n",
            "step 504: loss=0.01860189437866211\n",
            "step 505: loss=0.025838375091552734\n",
            "step 506: loss=0.2113633155822754\n",
            "step 507: loss=0.8807037472724915\n",
            "step 508: loss=0.023178577423095703\n",
            "step 509: loss=0.0013837814331054688\n",
            "step 510: loss=5.4836273193359375e-05\n",
            "step 511: loss=0.8866256475448608\n",
            "step 512: loss=0.33205974102020264\n",
            "step 513: loss=0.03796648979187012\n",
            "step 514: loss=0.23883867263793945\n",
            "step 515: loss=0.0009427070617675781\n",
            "step 516: loss=0.0018229484558105469\n",
            "step 517: loss=1.2260942459106445\n",
            "step 518: loss=0.7298150658607483\n",
            "step 519: loss=2.6702880859375e-05\n",
            "step 520: loss=0.4805949926376343\n",
            "step 521: loss=0.005927085876464844\n",
            "step 522: loss=0.0039920806884765625\n",
            "step 523: loss=0.0006723403930664062\n",
            "step 524: loss=0.26099443435668945\n",
            "step 525: loss=0.024256229400634766\n",
            "step 526: loss=4.241933822631836\n",
            "step 527: loss=4.291534423828125e-05\n",
            "step 528: loss=4.4345855712890625e-05\n",
            "step 529: loss=1.2163028717041016\n",
            "step 530: loss=0.013246297836303711\n",
            "step 531: loss=0.13908052444458008\n",
            "step 532: loss=0.00324249267578125\n",
            "step 533: loss=0.2986764907836914\n",
            "step 534: loss=0.0023717880249023438\n",
            "step 535: loss=0.030161380767822266\n",
            "step 536: loss=0.4065842628479004\n",
            "step 537: loss=0.0035238265991210938\n",
            "step 538: loss=2.0653510093688965\n",
            "step 539: loss=0.9242643713951111\n",
            "step 540: loss=1.0168535709381104\n",
            "step 541: loss=9.5367431640625e-07\n",
            "step 542: loss=0.12304878234863281\n",
            "step 543: loss=0.4614980220794678\n",
            "step 544: loss=0.11388826370239258\n",
            "step 545: loss=0.6514334678649902\n",
            "step 546: loss=0.4323842525482178\n",
            "step 547: loss=0.07764077186584473\n",
            "step 548: loss=0.07050323486328125\n",
            "step 549: loss=0.07230520248413086\n",
            "step 550: loss=0.0221555233001709\n",
            "step 551: loss=0.7021660804748535\n",
            "step 552: loss=0.40671491622924805\n",
            "step 553: loss=0.0003204345703125\n",
            "step 554: loss=2.957624912261963\n",
            "step 555: loss=0.5868644714355469\n",
            "step 556: loss=2.0177736282348633\n",
            "step 557: loss=0.09784746170043945\n",
            "step 558: loss=6.67572021484375e-06\n",
            "step 559: loss=9.5367431640625e-07\n",
            "step 560: loss=0.04224801063537598\n",
            "step 561: loss=0.15607118606567383\n",
            "step 562: loss=0.8630045652389526\n",
            "step 563: loss=0.04617786407470703\n",
            "step 564: loss=0.14325189590454102\n",
            "step 565: loss=0.0018618106842041016\n",
            "step 566: loss=1.4064244031906128\n",
            "step 567: loss=0.0018486976623535156\n",
            "step 568: loss=0.13969182968139648\n",
            "step 569: loss=0.018780231475830078\n",
            "step 570: loss=0.0006818771362304688\n",
            "step 571: loss=0.0005865097045898438\n",
            "step 572: loss=0.002959012985229492\n",
            "step 573: loss=0.00048828125\n",
            "step 574: loss=0.9589760303497314\n",
            "step 575: loss=0.20408987998962402\n",
            "step 576: loss=0.009453773498535156\n",
            "step 577: loss=0.22179794311523438\n",
            "step 578: loss=0.0006165504455566406\n",
            "step 579: loss=0.0\n",
            "step 580: loss=1.154198408126831\n",
            "step 581: loss=4.1484832763671875e-05\n",
            "step 582: loss=8.618797302246094\n",
            "step 583: loss=0.11798453330993652\n",
            "step 584: loss=0.32603919506073\n",
            "step 585: loss=0.006068229675292969\n",
            "step 586: loss=1.320252537727356\n",
            "step 587: loss=1.1563626527786255\n",
            "step 588: loss=1.5408351421356201\n",
            "step 589: loss=2.8252980709075928\n",
            "step 590: loss=0.5323317050933838\n",
            "step 591: loss=0.23235082626342773\n",
            "step 592: loss=1.4742919206619263\n",
            "step 593: loss=0.2450258731842041\n",
            "step 594: loss=0.014989852905273438\n",
            "step 595: loss=1.6002821922302246\n",
            "step 596: loss=3.7283854484558105\n",
            "step 597: loss=0.22040343284606934\n",
            "step 598: loss=0.011552810668945312\n",
            "step 599: loss=0.3823707103729248\n",
            "step 600: loss=0.06885457038879395\n",
            "step 601: loss=0.2625606060028076\n",
            "step 602: loss=0.4238154888153076\n",
            "step 603: loss=0.356487512588501\n",
            "step 604: loss=0.1910794973373413\n",
            "step 605: loss=1.0253950357437134\n",
            "step 606: loss=0.035448551177978516\n",
            "step 607: loss=0.0662541389465332\n",
            "step 608: loss=0.4234049320220947\n",
            "step 609: loss=0.38295817375183105\n",
            "step 610: loss=0.5096626281738281\n",
            "step 611: loss=0.9986616373062134\n",
            "step 612: loss=0.3077183961868286\n",
            "step 613: loss=0.08851981163024902\n",
            "step 614: loss=0.10343742370605469\n",
            "step 615: loss=0.21922707557678223\n",
            "step 616: loss=0.13033342361450195\n",
            "step 617: loss=0.0\n",
            "step 618: loss=0.36725103855133057\n",
            "step 619: loss=0.2065262794494629\n",
            "step 620: loss=0.1508791446685791\n",
            "step 621: loss=2.713796615600586\n",
            "step 622: loss=0.012843608856201172\n",
            "step 623: loss=0.0004124641418457031\n",
            "step 624: loss=3.0626142024993896\n",
            "step 625: loss=0.11932182312011719\n",
            "step 626: loss=0.14122843742370605\n",
            "step 627: loss=0.08586478233337402\n",
            "step 628: loss=0.0\n",
            "step 629: loss=0.2543797492980957\n",
            "step 630: loss=0.00038051605224609375\n",
            "step 631: loss=0.04945969581604004\n",
            "step 632: loss=0.037545204162597656\n",
            "step 633: loss=4.2690534591674805\n",
            "step 634: loss=0.0\n",
            "step 635: loss=0.06963992118835449\n",
            "step 636: loss=0.00011539459228515625\n",
            "step 637: loss=1.2984988689422607\n",
            "step 638: loss=0.021070480346679688\n",
            "step 639: loss=0.000125885009765625\n",
            "step 640: loss=0.13863348960876465\n",
            "step 641: loss=0.39957553148269653\n",
            "step 642: loss=0.034924983978271484\n",
            "step 643: loss=0.5273580551147461\n",
            "step 644: loss=0.022338151931762695\n",
            "step 645: loss=0.0077190399169921875\n",
            "step 646: loss=0.0002770423889160156\n",
            "step 647: loss=0.07212328910827637\n",
            "step 648: loss=0.0011014938354492188\n",
            "step 649: loss=0.007819175720214844\n",
            "step 650: loss=0.00115966796875\n",
            "step 651: loss=1.4007136821746826\n",
            "step 652: loss=2.4345920085906982\n",
            "step 653: loss=0.07117938995361328\n",
            "step 654: loss=0.152024507522583\n",
            "step 655: loss=0.08870697021484375\n",
            "step 656: loss=0.3868415355682373\n",
            "step 657: loss=0.030848026275634766\n",
            "step 658: loss=3.981494903564453\n",
            "step 659: loss=0.3024861812591553\n",
            "step 660: loss=3.7122976779937744\n",
            "step 661: loss=0.0020356178283691406\n",
            "step 662: loss=0.013081550598144531\n",
            "step 663: loss=1.6066772937774658\n",
            "step 664: loss=0.008979082107543945\n",
            "step 665: loss=0.0020570755004882812\n",
            "step 666: loss=0.0010972023010253906\n",
            "step 667: loss=0.000659942626953125\n",
            "step 668: loss=2.019679069519043\n",
            "step 669: loss=0.3365902304649353\n",
            "step 670: loss=0.2550210952758789\n",
            "step 671: loss=4.617800235748291\n",
            "step 672: loss=0.2889091968536377\n",
            "step 673: loss=0.8889949321746826\n",
            "step 674: loss=0.000823974609375\n",
            "step 675: loss=0.15426421165466309\n",
            "step 676: loss=0.036524057388305664\n",
            "step 677: loss=0.08731460571289062\n",
            "step 678: loss=0.1879720687866211\n",
            "step 679: loss=0.0350644588470459\n",
            "step 680: loss=0.00016689300537109375\n",
            "step 681: loss=1.8256093263626099\n",
            "step 682: loss=9.822845458984375e-05\n",
            "step 683: loss=0.21468019485473633\n",
            "step 684: loss=0.032248616218566895\n",
            "step 685: loss=0.0008411407470703125\n",
            "step 686: loss=0.03297901153564453\n",
            "step 687: loss=0.32915496826171875\n",
            "step 688: loss=0.016516685485839844\n",
            "step 689: loss=0.9532333612442017\n",
            "step 690: loss=0.11373090744018555\n",
            "step 691: loss=0.014770030975341797\n",
            "step 692: loss=0.1986027956008911\n",
            "step 693: loss=0.002079486846923828\n",
            "step 694: loss=0.012534618377685547\n",
            "step 695: loss=0.0026731491088867188\n",
            "step 696: loss=0.8083550930023193\n",
            "step 697: loss=0.4416372776031494\n",
            "step 698: loss=1.1680574417114258\n",
            "step 699: loss=0.04668116569519043\n",
            "step 700: loss=0.3047165870666504\n",
            "step 701: loss=0.00895071029663086\n",
            "step 702: loss=2.384185791015625e-05\n",
            "step 703: loss=0.01160430908203125\n",
            "step 704: loss=0.0609288215637207\n",
            "step 705: loss=0.05363893508911133\n",
            "step 706: loss=0.607996940612793\n",
            "step 707: loss=0.9707705974578857\n",
            "step 708: loss=0.12684988975524902\n",
            "step 709: loss=0.05230522155761719\n",
            "step 710: loss=1.3818752765655518\n",
            "step 711: loss=3.678192615509033\n",
            "step 712: loss=0.11706781387329102\n",
            "step 713: loss=0.02717137336730957\n",
            "step 714: loss=0.049896240234375\n",
            "step 715: loss=0.0024819374084472656\n",
            "step 716: loss=0.09686899185180664\n",
            "step 717: loss=0.026456117630004883\n",
            "step 718: loss=0.007931232452392578\n",
            "step 719: loss=0.005489349365234375\n",
            "step 720: loss=0.0003132820129394531\n",
            "step 721: loss=0.02823495864868164\n",
            "step 722: loss=0.012425899505615234\n",
            "step 723: loss=0.0038580894470214844\n",
            "step 724: loss=0.010411262512207031\n",
            "step 725: loss=6.244894027709961\n",
            "step 726: loss=0.41932213306427\n",
            "step 727: loss=0.02390289306640625\n",
            "step 728: loss=0.05993342399597168\n",
            "step 729: loss=1.137274146080017\n",
            "step 730: loss=0.0002040863037109375\n",
            "step 731: loss=0.11639285087585449\n",
            "step 732: loss=0.16224980354309082\n",
            "step 733: loss=0.07465505599975586\n",
            "step 734: loss=1.9550323486328125e-05\n",
            "step 735: loss=2.0121335983276367\n",
            "step 736: loss=4.1961669921875e-05\n",
            "step 737: loss=0.22056126594543457\n",
            "step 738: loss=0.17699050903320312\n",
            "step 739: loss=0.0\n",
            "step 740: loss=0.000972747802734375\n",
            "step 741: loss=1.8952304124832153\n",
            "step 742: loss=2.746640920639038\n",
            "step 743: loss=0.09477591514587402\n",
            "step 744: loss=2.86102294921875e-05\n",
            "step 745: loss=0.03348493576049805\n",
            "step 746: loss=2.47955322265625e-05\n",
            "step 747: loss=0.22501862049102783\n",
            "step 748: loss=0.008269786834716797\n",
            "step 749: loss=0.006746768951416016\n",
            "step 750: loss=0.21331393718719482\n",
            "step 751: loss=2.2871322631835938\n",
            "step 752: loss=0.03307318687438965\n",
            "step 753: loss=0.11149859428405762\n",
            "step 754: loss=0.4392533302307129\n",
            "step 755: loss=0.03747677803039551\n",
            "step 756: loss=3.162141799926758\n",
            "step 757: loss=0.16058349609375\n",
            "step 758: loss=0.7159360647201538\n",
            "step 759: loss=0.01501321792602539\n",
            "step 760: loss=0.6492156982421875\n",
            "step 761: loss=0.0003871917724609375\n",
            "step 762: loss=0.051429033279418945\n",
            "step 763: loss=1.033636450767517\n",
            "step 764: loss=0.3501342535018921\n",
            "step 765: loss=0.02764749526977539\n",
            "step 766: loss=0.7127386927604675\n",
            "step 767: loss=0.04774141311645508\n",
            "step 768: loss=3.4349770545959473\n",
            "step 769: loss=0.0\n",
            "step 770: loss=0.020919322967529297\n",
            "step 771: loss=0.022438526153564453\n",
            "step 772: loss=0.7133641242980957\n",
            "step 773: loss=0.017412185668945312\n",
            "step 774: loss=0.14902615547180176\n",
            "step 775: loss=0.003330230712890625\n",
            "step 776: loss=0.23011159896850586\n",
            "step 777: loss=0.022404193878173828\n",
            "step 778: loss=0.033196449279785156\n",
            "step 779: loss=0.015970706939697266\n",
            "step 780: loss=0.0032405853271484375\n",
            "step 781: loss=2.86102294921875e-06\n",
            "step 782: loss=0.000347137451171875\n",
            "step 783: loss=0.010954856872558594\n",
            "step 784: loss=0.012445449829101562\n",
            "step 785: loss=0.05112314224243164\n",
            "step 786: loss=0.007018089294433594\n",
            "step 787: loss=0.018446922302246094\n",
            "step 788: loss=0.0483241081237793\n",
            "step 789: loss=0.014584064483642578\n",
            "step 790: loss=0.7889449596405029\n",
            "step 791: loss=0.23250746726989746\n",
            "step 792: loss=0.04334306716918945\n",
            "step 793: loss=0.0008573532104492188\n",
            "step 794: loss=0.9619614481925964\n",
            "step 795: loss=0.00034618377685546875\n",
            "step 796: loss=0.003024578094482422\n",
            "step 797: loss=0.003124237060546875\n",
            "step 798: loss=2.8231334686279297\n",
            "step 799: loss=0.6481636762619019\n",
            "step 800: loss=0.1400747299194336\n",
            "step 801: loss=0.07184505462646484\n",
            "step 802: loss=0.0009989738464355469\n",
            "step 803: loss=0.006951808929443359\n",
            "step 804: loss=0.22261404991149902\n",
            "step 805: loss=0.0024023056030273438\n",
            "step 806: loss=7.2479248046875e-05\n",
            "step 807: loss=0.0\n",
            "step 808: loss=0.005190849304199219\n",
            "step 809: loss=0.9975419044494629\n",
            "step 810: loss=6.580352783203125e-05\n",
            "step 811: loss=2.5435214042663574\n",
            "step 812: loss=0.27237725257873535\n",
            "step 813: loss=0.12061619758605957\n",
            "step 814: loss=0.9106348752975464\n",
            "step 815: loss=0.015834331512451172\n",
            "step 816: loss=0.5051039457321167\n",
            "step 817: loss=0.33333349227905273\n",
            "step 818: loss=3.5948104858398438\n",
            "step 819: loss=0.004091739654541016\n",
            "step 820: loss=0.26546812057495117\n",
            "step 821: loss=0.07645177841186523\n",
            "step 822: loss=0.04256772994995117\n",
            "step 823: loss=0.002716064453125\n",
            "step 824: loss=0.11609315872192383\n",
            "step 825: loss=2.4214842319488525\n",
            "step 826: loss=4.863739013671875e-05\n",
            "step 827: loss=1.7128987312316895\n",
            "step 828: loss=0.0009756088256835938\n",
            "step 829: loss=0.10599517822265625\n",
            "step 830: loss=0.004246711730957031\n",
            "step 831: loss=0.3280372619628906\n",
            "step 832: loss=0.000141143798828125\n",
            "step 833: loss=0.01462411880493164\n",
            "step 834: loss=0.9338372945785522\n",
            "step 835: loss=0.22400689125061035\n",
            "step 836: loss=0.0029959678649902344\n",
            "step 837: loss=0.0010943412780761719\n",
            "step 838: loss=0.0015420913696289062\n",
            "step 839: loss=0.0022859573364257812\n",
            "step 840: loss=1.621246337890625e-05\n",
            "step 841: loss=0.003119945526123047\n",
            "step 842: loss=6.305079460144043\n",
            "step 843: loss=0.005166053771972656\n",
            "step 844: loss=0.019012451171875\n",
            "step 845: loss=6.198883056640625e-05\n",
            "step 846: loss=0.04728126525878906\n",
            "step 847: loss=0.0031905174255371094\n",
            "step 848: loss=0.00025272369384765625\n",
            "step 849: loss=0.00037097930908203125\n",
            "step 850: loss=0.03383636474609375\n",
            "step 851: loss=0.11502671241760254\n",
            "step 852: loss=0.16956686973571777\n",
            "step 853: loss=0.4942450523376465\n",
            "step 854: loss=5.435943603515625e-05\n",
            "step 855: loss=3.0406928062438965\n",
            "step 856: loss=0.00028896331787109375\n",
            "step 857: loss=0.6474759578704834\n",
            "step 858: loss=2.8732500076293945\n",
            "step 859: loss=0.5234791040420532\n",
            "step 860: loss=0.0675351619720459\n",
            "step 861: loss=0.02125835418701172\n",
            "step 862: loss=0.0\n",
            "step 863: loss=0.023676156997680664\n",
            "step 864: loss=0.021146297454833984\n",
            "step 865: loss=0.0006322860717773438\n",
            "step 866: loss=0.008186817169189453\n",
            "step 867: loss=0.12165379524230957\n",
            "step 868: loss=0.04052281379699707\n",
            "step 869: loss=2.4929490089416504\n",
            "step 870: loss=0.00018787384033203125\n",
            "step 871: loss=0.222997784614563\n",
            "step 872: loss=0.10728311538696289\n",
            "step 873: loss=0.039269208908081055\n",
            "step 874: loss=0.9810992479324341\n",
            "step 875: loss=0.4807114601135254\n",
            "step 876: loss=0.8026491403579712\n",
            "step 877: loss=0.03318977355957031\n",
            "step 878: loss=0.0033431053161621094\n",
            "step 879: loss=0.0025200843811035156\n",
            "step 880: loss=0.14512109756469727\n",
            "step 881: loss=0.02129197120666504\n",
            "step 882: loss=0.033204078674316406\n",
            "step 883: loss=0.006505727767944336\n",
            "step 884: loss=0.025543212890625\n",
            "step 885: loss=0.0013241767883300781\n",
            "step 886: loss=1.338795781135559\n",
            "step 887: loss=0.1381983757019043\n",
            "step 888: loss=1.3210296630859375\n",
            "step 889: loss=1.6180400848388672\n",
            "step 890: loss=0.09690141677856445\n",
            "step 891: loss=0.0002779960632324219\n",
            "step 892: loss=9.5367431640625e-07\n",
            "step 893: loss=0.44170117378234863\n",
            "step 894: loss=0.004294395446777344\n",
            "step 895: loss=3.62396240234375e-05\n",
            "step 896: loss=0.0010209083557128906\n",
            "step 897: loss=0.47489118576049805\n",
            "step 898: loss=0.5672416687011719\n",
            "step 899: loss=0.0009517669677734375\n",
            "step 900: loss=0.0031113624572753906\n",
            "step 901: loss=0.05000662803649902\n",
            "step 902: loss=1.239776611328125e-05\n",
            "step 903: loss=0.1499800682067871\n",
            "step 904: loss=0.0010480880737304688\n",
            "step 905: loss=0.1077580451965332\n",
            "step 906: loss=0.00020170211791992188\n",
            "step 907: loss=0.7162883877754211\n",
            "step 908: loss=0.004637241363525391\n",
            "step 909: loss=0.05552530288696289\n",
            "step 910: loss=0.018813610076904297\n",
            "step 911: loss=0.15681767463684082\n",
            "step 912: loss=0.0028743743896484375\n",
            "step 913: loss=0.024193763732910156\n",
            "step 914: loss=5.537679195404053\n",
            "step 915: loss=9.441375732421875e-05\n",
            "step 916: loss=0.22968435287475586\n",
            "step 917: loss=1.7680273056030273\n",
            "step 918: loss=0.5831551551818848\n",
            "step 919: loss=0.9334617853164673\n",
            "step 920: loss=0.08303499221801758\n",
            "step 921: loss=0.046778202056884766\n",
            "step 922: loss=0.819887638092041\n",
            "step 923: loss=0.08802413940429688\n",
            "step 924: loss=0.0\n",
            "step 925: loss=0.0012335777282714844\n",
            "step 926: loss=0.005395412445068359\n",
            "step 927: loss=0.16192054748535156\n",
            "step 928: loss=0.557651162147522\n",
            "step 929: loss=0.04935169219970703\n",
            "step 930: loss=0.2836599349975586\n",
            "step 931: loss=0.1014167070388794\n",
            "step 932: loss=0.13187456130981445\n",
            "step 933: loss=0.004682064056396484\n",
            "step 934: loss=0.018306255340576172\n",
            "step 935: loss=0.0002532005310058594\n",
            "step 936: loss=0.473768949508667\n",
            "step 937: loss=0.0014162063598632812\n",
            "step 938: loss=0.23521685600280762\n",
            "step 939: loss=0.6272199153900146\n",
            "step 940: loss=0.0009245872497558594\n",
            "step 941: loss=0.0020751953125\n",
            "step 942: loss=0.10157093405723572\n",
            "step 943: loss=0.0004858970642089844\n",
            "step 944: loss=0.00021791458129882812\n",
            "step 945: loss=0.12719249725341797\n",
            "step 946: loss=8.025716781616211\n",
            "step 947: loss=0.015633583068847656\n",
            "step 948: loss=5.7220458984375e-06\n",
            "step 949: loss=1.0354394912719727\n",
            "step 950: loss=0.12440371513366699\n",
            "step 951: loss=0.025871753692626953\n",
            "step 952: loss=0.0031418800354003906\n",
            "step 953: loss=2.7614359855651855\n",
            "step 954: loss=0.3145637512207031\n",
            "step 955: loss=1.9073486328125e-06\n",
            "step 956: loss=0.0002961158752441406\n",
            "step 957: loss=1.9073486328125e-06\n",
            "step 958: loss=3.804166316986084\n",
            "step 959: loss=0.1464707851409912\n",
            "step 960: loss=2.4591071605682373\n",
            "step 961: loss=1.6737509965896606\n",
            "step 962: loss=0.00013494491577148438\n",
            "step 963: loss=0.10182619094848633\n",
            "step 964: loss=7.200241088867188e-05\n",
            "step 965: loss=0.2720479965209961\n",
            "step 966: loss=0.807050347328186\n",
            "step 967: loss=0.1557779312133789\n",
            "step 968: loss=0.16544032096862793\n",
            "step 969: loss=0.362612247467041\n",
            "step 970: loss=0.21241044998168945\n",
            "step 971: loss=0.0015053749084472656\n",
            "step 972: loss=0.48368024826049805\n",
            "step 973: loss=0.6051385998725891\n",
            "step 974: loss=0.31362366676330566\n",
            "step 975: loss=0.8090308904647827\n",
            "step 976: loss=0.0002994537353515625\n",
            "step 977: loss=0.0005540847778320312\n",
            "step 978: loss=0.31195664405822754\n",
            "step 979: loss=0.4971933364868164\n",
            "step 980: loss=0.00138092041015625\n",
            "step 981: loss=0.25642824172973633\n",
            "step 982: loss=0.012373924255371094\n",
            "step 983: loss=0.48532605171203613\n",
            "step 984: loss=0.00019931793212890625\n",
            "step 985: loss=0.6169703006744385\n",
            "step 986: loss=0.0010499954223632812\n",
            "step 987: loss=3.814697265625e-06\n",
            "step 988: loss=0.5913219451904297\n",
            "step 989: loss=0.0034475326538085938\n",
            "step 990: loss=0.19599390029907227\n",
            "step 991: loss=0.07637786865234375\n",
            "step 992: loss=0.0705721378326416\n",
            "step 993: loss=0.0002155303955078125\n",
            "step 994: loss=1.0466413497924805\n",
            "step 995: loss=0.9066888689994812\n",
            "step 996: loss=0.00015926361083984375\n",
            "step 997: loss=9.5367431640625e-07\n",
            "step 998: loss=0.12610793113708496\n",
            "step 999: loss=0.1231851577758789\n",
            "step 1000: loss=0.0014848709106445312\n",
            "Mean loss        0.6587795\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 8 3 1]\n"
          ]
        }
      ],
      "source": [
        "optimizer = optax.adamw(learning_rate, weight_decay=1e-4)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8017b6a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=0.32265353202819824\n",
            "step 2: loss=0.7677361965179443\n",
            "step 3: loss=1.3151150941848755\n",
            "step 4: loss=0.6902559995651245\n",
            "step 5: loss=1.5896861553192139\n",
            "step 6: loss=0.20080137252807617\n",
            "step 7: loss=1.7467864751815796\n",
            "step 8: loss=0.7341995239257812\n",
            "step 9: loss=0.48326265811920166\n",
            "step 10: loss=2.754148483276367\n",
            "step 11: loss=1.8300219774246216\n",
            "step 12: loss=4.892520904541016\n",
            "step 13: loss=0.034595489501953125\n",
            "step 14: loss=0.2189321517944336\n",
            "step 15: loss=6.246511459350586\n",
            "step 16: loss=3.886373996734619\n",
            "step 17: loss=0.005483150482177734\n",
            "step 18: loss=6.574323654174805\n",
            "step 19: loss=5.217709541320801\n",
            "step 20: loss=0.5348189473152161\n",
            "step 21: loss=2.443570613861084\n",
            "step 22: loss=0.3176398277282715\n",
            "step 23: loss=0.2063612937927246\n",
            "step 24: loss=0.8877174854278564\n",
            "step 25: loss=0.6408871412277222\n",
            "step 26: loss=2.742940664291382\n",
            "step 27: loss=4.988762855529785\n",
            "step 28: loss=0.7708112001419067\n",
            "step 29: loss=1.6248937845230103\n",
            "step 30: loss=1.6446845531463623\n",
            "step 31: loss=1.1385552883148193\n",
            "step 32: loss=1.1512348651885986\n",
            "step 33: loss=0.08533644676208496\n",
            "step 34: loss=0.5107001066207886\n",
            "step 35: loss=0.7808982133865356\n",
            "step 36: loss=1.1551976203918457\n",
            "step 37: loss=0.1812453269958496\n",
            "step 38: loss=0.44345223903656006\n",
            "step 39: loss=3.41731595993042\n",
            "step 40: loss=0.10831308364868164\n",
            "step 41: loss=1.8061790466308594\n",
            "step 42: loss=4.005237102508545\n",
            "step 43: loss=0.18488645553588867\n",
            "step 44: loss=0.027593135833740234\n",
            "step 45: loss=1.5412310361862183\n",
            "step 46: loss=0.07512915134429932\n",
            "step 47: loss=2.554224967956543\n",
            "step 48: loss=2.4517619609832764\n",
            "step 49: loss=3.207988977432251\n",
            "step 50: loss=1.0273098945617676\n",
            "step 51: loss=0.3488011360168457\n",
            "step 52: loss=0.040026187896728516\n",
            "step 53: loss=5.629855155944824\n",
            "step 54: loss=0.1392230987548828\n",
            "step 55: loss=2.1626482009887695\n",
            "step 56: loss=2.146838903427124\n",
            "step 57: loss=0.5651766061782837\n",
            "step 58: loss=0.20208501815795898\n",
            "step 59: loss=3.4273324012756348\n",
            "step 60: loss=0.25432801246643066\n",
            "step 61: loss=0.11790204048156738\n",
            "step 62: loss=0.47708380222320557\n",
            "step 63: loss=4.057825088500977\n",
            "step 64: loss=2.473989486694336\n",
            "step 65: loss=2.2944486141204834\n",
            "step 66: loss=0.0047512054443359375\n",
            "step 67: loss=0.3416098356246948\n",
            "step 68: loss=0.008679389953613281\n",
            "step 69: loss=2.310180187225342\n",
            "step 70: loss=0.7741975784301758\n",
            "step 71: loss=2.4872939586639404\n",
            "step 72: loss=2.1794891357421875\n",
            "step 73: loss=0.11859536170959473\n",
            "step 74: loss=0.1509559154510498\n",
            "step 75: loss=0.015425682067871094\n",
            "step 76: loss=0.13530445098876953\n",
            "step 77: loss=2.78794264793396\n",
            "step 78: loss=1.5177351236343384\n",
            "step 79: loss=0.7439028024673462\n",
            "step 80: loss=0.08385419845581055\n",
            "step 81: loss=0.017795801162719727\n",
            "step 82: loss=0.22256255149841309\n",
            "step 83: loss=0.8793761730194092\n",
            "step 84: loss=0.5103070735931396\n",
            "step 85: loss=2.178370475769043\n",
            "step 86: loss=0.20706629753112793\n",
            "step 87: loss=2.5185208320617676\n",
            "step 88: loss=0.10664987564086914\n",
            "step 89: loss=0.7768559455871582\n",
            "step 90: loss=0.7451890707015991\n",
            "step 91: loss=1.2006421089172363\n",
            "step 92: loss=0.27802860736846924\n",
            "step 93: loss=0.013898849487304688\n",
            "step 94: loss=1.7205289602279663\n",
            "step 95: loss=0.45594263076782227\n",
            "step 96: loss=1.7405085563659668\n",
            "step 97: loss=0.15360784530639648\n",
            "step 98: loss=1.0509381294250488\n",
            "step 99: loss=0.7338082194328308\n",
            "step 100: loss=0.08263015747070312\n",
            "step 101: loss=1.5180706977844238\n",
            "step 102: loss=0.22172343730926514\n",
            "step 103: loss=1.6552200317382812\n",
            "step 104: loss=0.22332489490509033\n",
            "step 105: loss=0.6831488609313965\n",
            "step 106: loss=0.7622657418251038\n",
            "step 107: loss=0.7726213932037354\n",
            "step 108: loss=0.15828311443328857\n",
            "step 109: loss=0.7268622517585754\n",
            "step 110: loss=0.26294422149658203\n",
            "step 111: loss=0.6732480525970459\n",
            "step 112: loss=0.0007729530334472656\n",
            "step 113: loss=1.0882985591888428\n",
            "step 114: loss=2.5593531131744385\n",
            "step 115: loss=0.38757896423339844\n",
            "step 116: loss=0.7885910868644714\n",
            "step 117: loss=0.4653339684009552\n",
            "step 118: loss=0.01670980453491211\n",
            "step 119: loss=0.7658326625823975\n",
            "step 120: loss=3.3576154708862305\n",
            "step 121: loss=1.613948106765747\n",
            "step 122: loss=2.8536770343780518\n",
            "step 123: loss=2.245163917541504\n",
            "step 124: loss=0.9436007738113403\n",
            "step 125: loss=0.1656639575958252\n",
            "step 126: loss=2.1030073165893555\n",
            "step 127: loss=0.07231497764587402\n",
            "step 128: loss=0.03444623947143555\n",
            "step 129: loss=0.09939420223236084\n",
            "step 130: loss=3.1196115016937256\n",
            "step 131: loss=0.08367347717285156\n",
            "step 132: loss=2.0960330963134766\n",
            "step 133: loss=1.360571026802063\n",
            "step 134: loss=0.0412747859954834\n",
            "step 135: loss=0.010736942291259766\n",
            "step 136: loss=0.027419567108154297\n",
            "step 137: loss=1.327653169631958\n",
            "step 138: loss=0.020722627639770508\n",
            "step 139: loss=1.53236985206604\n",
            "step 140: loss=2.474282741546631\n",
            "step 141: loss=0.0015196800231933594\n",
            "step 142: loss=1.0963696241378784\n",
            "step 143: loss=0.0033092498779296875\n",
            "step 144: loss=1.5894001722335815\n",
            "step 145: loss=0.00022840499877929688\n",
            "step 146: loss=0.5872328877449036\n",
            "step 147: loss=0.8827279210090637\n",
            "step 148: loss=1.8595818281173706\n",
            "step 149: loss=1.0038464069366455\n",
            "step 150: loss=0.39370787143707275\n",
            "step 151: loss=0.2630171775817871\n",
            "step 152: loss=0.0568385124206543\n",
            "step 153: loss=1.0201878547668457\n",
            "step 154: loss=0.1382920742034912\n",
            "step 155: loss=1.6553680896759033\n",
            "step 156: loss=0.2921198606491089\n",
            "step 157: loss=0.6886123418807983\n",
            "step 158: loss=3.879697799682617\n",
            "step 159: loss=1.1654353141784668\n",
            "step 160: loss=0.5220597386360168\n",
            "step 161: loss=0.06435775756835938\n",
            "step 162: loss=0.14003610610961914\n",
            "step 163: loss=0.24124741554260254\n",
            "step 164: loss=0.5165512561798096\n",
            "step 165: loss=1.1098742485046387\n",
            "step 166: loss=0.22089600563049316\n",
            "step 167: loss=1.6918854713439941\n",
            "step 168: loss=0.8363021612167358\n",
            "step 169: loss=0.6213746070861816\n",
            "step 170: loss=0.330275297164917\n",
            "step 171: loss=0.03860902786254883\n",
            "step 172: loss=0.42430007457733154\n",
            "step 173: loss=1.4204695224761963\n",
            "step 174: loss=0.5432294607162476\n",
            "step 175: loss=1.195847988128662\n",
            "step 176: loss=0.5588985681533813\n",
            "step 177: loss=0.30118799209594727\n",
            "step 178: loss=0.2188800573348999\n",
            "step 179: loss=0.05227351188659668\n",
            "step 180: loss=7.683727741241455\n",
            "step 181: loss=0.06298255920410156\n",
            "step 182: loss=0.02646493911743164\n",
            "step 183: loss=0.25232452154159546\n",
            "step 184: loss=1.1338999271392822\n",
            "step 185: loss=0.04089999198913574\n",
            "step 186: loss=0.014993667602539062\n",
            "step 187: loss=1.9518489837646484\n",
            "step 188: loss=0.8857477307319641\n",
            "step 189: loss=0.18923187255859375\n",
            "step 190: loss=0.08303499221801758\n",
            "step 191: loss=0.006696462631225586\n",
            "step 192: loss=0.16001462936401367\n",
            "step 193: loss=0.04649782180786133\n",
            "step 194: loss=0.2518782615661621\n",
            "step 195: loss=0.0040454864501953125\n",
            "step 196: loss=1.925736904144287\n",
            "step 197: loss=0.353962779045105\n",
            "step 198: loss=0.00656580924987793\n",
            "step 199: loss=4.005247116088867\n",
            "step 200: loss=0.0034499168395996094\n",
            "step 201: loss=0.17750608921051025\n",
            "step 202: loss=2.0410399436950684\n",
            "step 203: loss=0.005888938903808594\n",
            "step 204: loss=0.00996255874633789\n",
            "step 205: loss=0.37105488777160645\n",
            "step 206: loss=0.35456782579421997\n",
            "step 207: loss=0.17059087753295898\n",
            "step 208: loss=0.7920424938201904\n",
            "step 209: loss=0.03953742980957031\n",
            "step 210: loss=0.06645035743713379\n",
            "step 211: loss=0.0030989646911621094\n",
            "step 212: loss=0.7253737449645996\n",
            "step 213: loss=2.1099436283111572\n",
            "step 214: loss=3.948305130004883\n",
            "step 215: loss=0.03413724899291992\n",
            "step 216: loss=0.8015700578689575\n",
            "step 217: loss=1.1378507614135742\n",
            "step 218: loss=0.0373845100402832\n",
            "step 219: loss=0.009069442749023438\n",
            "step 220: loss=0.5271031856536865\n",
            "step 221: loss=1.065048336982727\n",
            "step 222: loss=7.40135383605957\n",
            "step 223: loss=0.46386706829071045\n",
            "step 224: loss=0.17491412162780762\n",
            "step 225: loss=0.15935158729553223\n",
            "step 226: loss=0.7509163618087769\n",
            "step 227: loss=0.07780194282531738\n",
            "step 228: loss=0.12094664573669434\n",
            "step 229: loss=0.20540380477905273\n",
            "step 230: loss=1.8975226879119873\n",
            "step 231: loss=0.29940366744995117\n",
            "step 232: loss=0.02459728717803955\n",
            "step 233: loss=0.0014605522155761719\n",
            "step 234: loss=4.177887916564941\n",
            "step 235: loss=0.11293554306030273\n",
            "step 236: loss=0.32040607929229736\n",
            "step 237: loss=0.0004315376281738281\n",
            "step 238: loss=0.08162736892700195\n",
            "step 239: loss=0.07216644287109375\n",
            "step 240: loss=0.06731915473937988\n",
            "step 241: loss=0.005340576171875\n",
            "step 242: loss=0.05252695083618164\n",
            "step 243: loss=0.15529489517211914\n",
            "step 244: loss=0.0381779670715332\n",
            "step 245: loss=0.21390938758850098\n",
            "step 246: loss=0.8916970491409302\n",
            "step 247: loss=0.08251023292541504\n",
            "step 248: loss=4.838141441345215\n",
            "step 249: loss=0.17592167854309082\n",
            "step 250: loss=0.0005049705505371094\n",
            "step 251: loss=0.8584268093109131\n",
            "step 252: loss=0.002243518829345703\n",
            "step 253: loss=0.3385353684425354\n",
            "step 254: loss=0.000408172607421875\n",
            "step 255: loss=0.00022220611572265625\n",
            "step 256: loss=0.06496715545654297\n",
            "step 257: loss=3.8022594451904297\n",
            "step 258: loss=0.5645700097084045\n",
            "step 259: loss=0.9603403806686401\n",
            "step 260: loss=0.029726266860961914\n",
            "step 261: loss=0.6721160411834717\n",
            "step 262: loss=1.2018390893936157\n",
            "step 263: loss=0.0009274482727050781\n",
            "step 264: loss=1.6904218196868896\n",
            "step 265: loss=1.1635504961013794\n",
            "step 266: loss=1.033583641052246\n",
            "step 267: loss=0.7021710872650146\n",
            "step 268: loss=0.5142069458961487\n",
            "step 269: loss=0.00021076202392578125\n",
            "step 270: loss=0.8425696492195129\n",
            "step 271: loss=0.0003185272216796875\n",
            "step 272: loss=0.19136810302734375\n",
            "step 273: loss=0.6078743934631348\n",
            "step 274: loss=0.08120870590209961\n",
            "step 275: loss=0.747827410697937\n",
            "step 276: loss=0.017641305923461914\n",
            "step 277: loss=0.3041130304336548\n",
            "step 278: loss=0.07038092613220215\n",
            "step 279: loss=0.1546717882156372\n",
            "step 280: loss=0.2326543927192688\n",
            "step 281: loss=0.004887580871582031\n",
            "step 282: loss=0.012868165969848633\n",
            "step 283: loss=1.2199844121932983\n",
            "step 284: loss=3.0553863048553467\n",
            "step 285: loss=0.04233050346374512\n",
            "step 286: loss=0.08937788009643555\n",
            "step 287: loss=0.14047694206237793\n",
            "step 288: loss=0.012384891510009766\n",
            "step 289: loss=3.2872185707092285\n",
            "step 290: loss=0.023813962936401367\n",
            "step 291: loss=0.12465476989746094\n",
            "step 292: loss=5.07390022277832\n",
            "step 293: loss=0.0003037452697753906\n",
            "step 294: loss=0.0003199577331542969\n",
            "step 295: loss=0.4114037752151489\n",
            "step 296: loss=0.10671734809875488\n",
            "step 297: loss=0.01735067367553711\n",
            "step 298: loss=0.006719112396240234\n",
            "step 299: loss=4.57763671875e-05\n",
            "step 300: loss=0.02154064178466797\n",
            "step 301: loss=0.0001316070556640625\n",
            "step 302: loss=3.9492483139038086\n",
            "step 303: loss=0.003207683563232422\n",
            "step 304: loss=1.1535091400146484\n",
            "step 305: loss=0.542633056640625\n",
            "step 306: loss=0.18557560443878174\n",
            "step 307: loss=1.8754061460494995\n",
            "step 308: loss=0.013012409210205078\n",
            "step 309: loss=1.295968770980835\n",
            "step 310: loss=0.5563477277755737\n",
            "step 311: loss=1.8282326459884644\n",
            "step 312: loss=0.08617830276489258\n",
            "step 313: loss=0.0003261566162109375\n",
            "step 314: loss=0.29241108894348145\n",
            "step 315: loss=0.03786349296569824\n",
            "step 316: loss=0.5255333781242371\n",
            "step 317: loss=0.17476695775985718\n",
            "step 318: loss=0.8038290143013\n",
            "step 319: loss=0.06973600387573242\n",
            "step 320: loss=0.32958555221557617\n",
            "step 321: loss=1.7440454959869385\n",
            "step 322: loss=0.4929006099700928\n",
            "step 323: loss=0.003596782684326172\n",
            "step 324: loss=0.15468049049377441\n",
            "step 325: loss=0.30800652503967285\n",
            "step 326: loss=2.5679173469543457\n",
            "step 327: loss=0.04656982421875\n",
            "step 328: loss=0.6119879484176636\n",
            "step 329: loss=0.0004143714904785156\n",
            "step 330: loss=0.7317526340484619\n",
            "step 331: loss=6.4849853515625e-05\n",
            "step 332: loss=0.17156767845153809\n",
            "step 333: loss=0.009419679641723633\n",
            "step 334: loss=0.0026426315307617188\n",
            "step 335: loss=0.5202206373214722\n",
            "step 336: loss=0.12111759185791016\n",
            "step 337: loss=0.19329833984375\n",
            "step 338: loss=0.4536569118499756\n",
            "step 339: loss=0.03757333755493164\n",
            "step 340: loss=0.007573843002319336\n",
            "step 341: loss=0.20282578468322754\n",
            "step 342: loss=0.25856709480285645\n",
            "step 343: loss=0.002673625946044922\n",
            "step 344: loss=2.503208875656128\n",
            "step 345: loss=0.2889082431793213\n",
            "step 346: loss=0.07102656364440918\n",
            "step 347: loss=0.25949764251708984\n",
            "step 348: loss=0.00115203857421875\n",
            "step 349: loss=0.30120158195495605\n",
            "step 350: loss=6.914138793945312e-05\n",
            "step 351: loss=0.014844059944152832\n",
            "step 352: loss=3.0785622596740723\n",
            "step 353: loss=12.08682632446289\n",
            "step 354: loss=0.3800380229949951\n",
            "step 355: loss=0.004242897033691406\n",
            "step 356: loss=3.814697265625e-06\n",
            "step 357: loss=3.7479214668273926\n",
            "step 358: loss=0.25550365447998047\n",
            "step 359: loss=0.01464390754699707\n",
            "step 360: loss=0.018192768096923828\n",
            "step 361: loss=0.00027179718017578125\n",
            "step 362: loss=7.62939453125e-06\n",
            "step 363: loss=2.6357474327087402\n",
            "step 364: loss=0.9038617014884949\n",
            "step 365: loss=0.002448558807373047\n",
            "step 366: loss=0.3856492042541504\n",
            "step 367: loss=0.30655956268310547\n",
            "step 368: loss=2.86102294921875e-06\n",
            "step 369: loss=0.028751373291015625\n",
            "step 370: loss=1.6154536008834839\n",
            "step 371: loss=0.32639002799987793\n",
            "step 372: loss=1.621246337890625e-05\n",
            "step 373: loss=5.245208740234375e-05\n",
            "step 374: loss=0.07365119457244873\n",
            "step 375: loss=0.019389629364013672\n",
            "step 376: loss=0.4800848960876465\n",
            "step 377: loss=1.9815808534622192\n",
            "step 378: loss=0.05319070816040039\n",
            "step 379: loss=2.646247386932373\n",
            "step 380: loss=5.91278076171875e-05\n",
            "step 381: loss=8.7738037109375e-05\n",
            "step 382: loss=1.4447195529937744\n",
            "step 383: loss=0.0931631326675415\n",
            "step 384: loss=0.00015020370483398438\n",
            "step 385: loss=0.8477308750152588\n",
            "step 386: loss=0.005915641784667969\n",
            "step 387: loss=0.03995561599731445\n",
            "step 388: loss=0.07850384712219238\n",
            "step 389: loss=0.009351730346679688\n",
            "step 390: loss=0.23613667488098145\n",
            "step 391: loss=0.1444687843322754\n",
            "step 392: loss=0.9264271855354309\n",
            "step 393: loss=2.9100217819213867\n",
            "step 394: loss=0.6414291262626648\n",
            "step 395: loss=0.003254413604736328\n",
            "step 396: loss=0.37188100814819336\n",
            "step 397: loss=0.5570312142372131\n",
            "step 398: loss=0.249068021774292\n",
            "step 399: loss=0.3767368793487549\n",
            "step 400: loss=0.00047779083251953125\n",
            "step 401: loss=0.5489314794540405\n",
            "step 402: loss=0.008211135864257812\n",
            "step 403: loss=0.0\n",
            "step 404: loss=0.014952659606933594\n",
            "step 405: loss=0.17740583419799805\n",
            "step 406: loss=0.13045763969421387\n",
            "step 407: loss=0.06400322914123535\n",
            "step 408: loss=0.04746079444885254\n",
            "step 409: loss=2.1315200328826904\n",
            "step 410: loss=0.0003986358642578125\n",
            "step 411: loss=0.2538149356842041\n",
            "step 412: loss=0.006916522979736328\n",
            "step 413: loss=0.4559333324432373\n",
            "step 414: loss=0.5141557455062866\n",
            "step 415: loss=0.0007843971252441406\n",
            "step 416: loss=2.8805699348449707\n",
            "step 417: loss=0.3611927032470703\n",
            "step 418: loss=0.014260053634643555\n",
            "step 419: loss=0.7309393882751465\n",
            "step 420: loss=0.021860122680664062\n",
            "step 421: loss=0.11862552165985107\n",
            "step 422: loss=0.3005349636077881\n",
            "step 423: loss=0.052201032638549805\n",
            "step 424: loss=0.07789897918701172\n",
            "step 425: loss=0.6183964610099792\n",
            "step 426: loss=0.34236741065979004\n",
            "step 427: loss=2.048393726348877\n",
            "step 428: loss=0.0419921875\n",
            "step 429: loss=0.006049156188964844\n",
            "step 430: loss=0.0003094673156738281\n",
            "step 431: loss=0.09932804107666016\n",
            "step 432: loss=0.9665517807006836\n",
            "step 433: loss=0.03653407096862793\n",
            "step 434: loss=0.05873990058898926\n",
            "step 435: loss=2.86102294921875e-05\n",
            "step 436: loss=0.4893817901611328\n",
            "step 437: loss=0.0001583099365234375\n",
            "step 438: loss=0.002471923828125\n",
            "step 439: loss=0.4008873701095581\n",
            "step 440: loss=0.002857208251953125\n",
            "step 441: loss=0.0016713142395019531\n",
            "step 442: loss=0.09781372547149658\n",
            "step 443: loss=1.1453019380569458\n",
            "step 444: loss=1.0740913152694702\n",
            "step 445: loss=0.36539196968078613\n",
            "step 446: loss=1.912704348564148\n",
            "step 447: loss=0.0006847381591796875\n",
            "step 448: loss=0.044756174087524414\n",
            "step 449: loss=0.3132387399673462\n",
            "step 450: loss=0.8601288795471191\n",
            "step 451: loss=0.0\n",
            "step 452: loss=0.07326102256774902\n",
            "step 453: loss=3.528594970703125e-05\n",
            "step 454: loss=7.578174114227295\n",
            "step 455: loss=2.1549315452575684\n",
            "step 456: loss=0.4745934009552002\n",
            "step 457: loss=0.19621825218200684\n",
            "step 458: loss=0.3023858666419983\n",
            "step 459: loss=0.028782129287719727\n",
            "step 460: loss=0.13114619255065918\n",
            "step 461: loss=3.5358164310455322\n",
            "step 462: loss=0.04987144470214844\n",
            "step 463: loss=0.37550175189971924\n",
            "step 464: loss=0.061316728591918945\n",
            "step 465: loss=5.055030822753906\n",
            "step 466: loss=2.1137704849243164\n",
            "step 467: loss=2.186795711517334\n",
            "step 468: loss=0.11872291564941406\n",
            "step 469: loss=0.0059909820556640625\n",
            "step 470: loss=0.45182693004608154\n",
            "step 471: loss=2.936472177505493\n",
            "step 472: loss=0.9229355454444885\n",
            "step 473: loss=0.16029810905456543\n",
            "step 474: loss=0.08690404891967773\n",
            "step 475: loss=0.006660938262939453\n",
            "step 476: loss=0.030141115188598633\n",
            "step 477: loss=0.6065821647644043\n",
            "step 478: loss=0.00028705596923828125\n",
            "step 479: loss=0.0007681846618652344\n",
            "step 480: loss=0.45162951946258545\n",
            "step 481: loss=0.5646929740905762\n",
            "step 482: loss=0.0047991275787353516\n",
            "step 483: loss=0.0021982192993164062\n",
            "step 484: loss=1.0457749366760254\n",
            "step 485: loss=1.9045690298080444\n",
            "step 486: loss=0.0\n",
            "step 487: loss=2.86102294921875e-05\n",
            "step 488: loss=2.86102294921875e-06\n",
            "step 489: loss=0.042319297790527344\n",
            "step 490: loss=0.624491810798645\n",
            "step 491: loss=0.0\n",
            "step 492: loss=0.010770797729492188\n",
            "step 493: loss=0.8143465518951416\n",
            "step 494: loss=0.08912944793701172\n",
            "step 495: loss=0.02032756805419922\n",
            "step 496: loss=0.10290718078613281\n",
            "step 497: loss=4.00543212890625e-05\n",
            "step 498: loss=0.0010094642639160156\n",
            "step 499: loss=2.3496599197387695\n",
            "step 500: loss=0.9742568731307983\n",
            "step 501: loss=0.0010728836059570312\n",
            "step 502: loss=0.43423712253570557\n",
            "step 503: loss=0.0021448135375976562\n",
            "step 504: loss=0.0186007022857666\n",
            "step 505: loss=0.025837421417236328\n",
            "step 506: loss=0.21135175228118896\n",
            "step 507: loss=0.8806896209716797\n",
            "step 508: loss=0.0231778621673584\n",
            "step 509: loss=0.0013833045959472656\n",
            "step 510: loss=5.4836273193359375e-05\n",
            "step 511: loss=0.8866664171218872\n",
            "step 512: loss=0.3320409059524536\n",
            "step 513: loss=0.03795933723449707\n",
            "step 514: loss=0.23882699012756348\n",
            "step 515: loss=0.0009431838989257812\n",
            "step 516: loss=0.0018243789672851562\n",
            "step 517: loss=1.2261697053909302\n",
            "step 518: loss=0.7298409938812256\n",
            "step 519: loss=2.6702880859375e-05\n",
            "step 520: loss=0.4806029796600342\n",
            "step 521: loss=0.005927562713623047\n",
            "step 522: loss=0.003992557525634766\n",
            "step 523: loss=0.0006728172302246094\n",
            "step 524: loss=0.2610177993774414\n",
            "step 525: loss=0.024268627166748047\n",
            "step 526: loss=4.241772651672363\n",
            "step 527: loss=4.291534423828125e-05\n",
            "step 528: loss=4.4345855712890625e-05\n",
            "step 529: loss=1.2163276672363281\n",
            "step 530: loss=0.013242483139038086\n",
            "step 531: loss=0.13908910751342773\n",
            "step 532: loss=0.003242015838623047\n",
            "step 533: loss=0.2986736297607422\n",
            "step 534: loss=0.00237274169921875\n",
            "step 535: loss=0.030156612396240234\n",
            "step 536: loss=0.40659379959106445\n",
            "step 537: loss=0.0035219192504882812\n",
            "step 538: loss=2.065150737762451\n",
            "step 539: loss=0.9241777658462524\n",
            "step 540: loss=1.0168293714523315\n",
            "step 541: loss=9.5367431640625e-07\n",
            "step 542: loss=0.1230154037475586\n",
            "step 543: loss=0.46150803565979004\n",
            "step 544: loss=0.11389446258544922\n",
            "step 545: loss=0.6514673233032227\n",
            "step 546: loss=0.4323982000350952\n",
            "step 547: loss=0.0776512622833252\n",
            "step 548: loss=0.07050275802612305\n",
            "step 549: loss=0.07230567932128906\n",
            "step 550: loss=0.022155046463012695\n",
            "step 551: loss=0.7021396160125732\n",
            "step 552: loss=0.4067016839981079\n",
            "step 553: loss=0.0003204345703125\n",
            "step 554: loss=2.9574482440948486\n",
            "step 555: loss=0.5868667364120483\n",
            "step 556: loss=2.0178582668304443\n",
            "step 557: loss=0.09785151481628418\n",
            "step 558: loss=6.67572021484375e-06\n",
            "step 559: loss=9.5367431640625e-07\n",
            "step 560: loss=0.0422513484954834\n",
            "step 561: loss=0.15607404708862305\n",
            "step 562: loss=0.8629575967788696\n",
            "step 563: loss=0.046181678771972656\n",
            "step 564: loss=0.14323902130126953\n",
            "step 565: loss=0.001861572265625\n",
            "step 566: loss=1.4064178466796875\n",
            "step 567: loss=0.0018486976623535156\n",
            "step 568: loss=0.13968932628631592\n",
            "step 569: loss=0.018782615661621094\n",
            "step 570: loss=0.0006814002990722656\n",
            "step 571: loss=0.0005865097045898438\n",
            "step 572: loss=0.0029578208923339844\n",
            "step 573: loss=0.00048828125\n",
            "step 574: loss=0.9590053558349609\n",
            "step 575: loss=0.20408415794372559\n",
            "step 576: loss=0.009456157684326172\n",
            "step 577: loss=0.22182035446166992\n",
            "step 578: loss=0.0006170272827148438\n",
            "step 579: loss=0.0\n",
            "step 580: loss=1.1540865898132324\n",
            "step 581: loss=4.100799560546875e-05\n",
            "step 582: loss=8.618965148925781\n",
            "step 583: loss=0.11800146102905273\n",
            "step 584: loss=0.3261057138442993\n",
            "step 585: loss=0.006064414978027344\n",
            "step 586: loss=1.320351004600525\n",
            "step 587: loss=1.1564998626708984\n",
            "step 588: loss=1.5408616065979004\n",
            "step 589: loss=2.8252999782562256\n",
            "step 590: loss=0.5323293209075928\n",
            "step 591: loss=0.23233914375305176\n",
            "step 592: loss=1.4742640256881714\n",
            "step 593: loss=0.24505925178527832\n",
            "step 594: loss=0.014989376068115234\n",
            "step 595: loss=1.600351333618164\n",
            "step 596: loss=3.7285213470458984\n",
            "step 597: loss=0.22040629386901855\n",
            "step 598: loss=0.01154637336730957\n",
            "step 599: loss=0.38244104385375977\n",
            "step 600: loss=0.06886601448059082\n",
            "step 601: loss=0.26257824897766113\n",
            "step 602: loss=0.42383790016174316\n",
            "step 603: loss=0.35649943351745605\n",
            "step 604: loss=0.1910794973373413\n",
            "step 605: loss=1.0254179239273071\n",
            "step 606: loss=0.0354461669921875\n",
            "step 607: loss=0.0662386417388916\n",
            "step 608: loss=0.42342495918273926\n",
            "step 609: loss=0.38297271728515625\n",
            "step 610: loss=0.5099790096282959\n",
            "step 611: loss=0.9986231327056885\n",
            "step 612: loss=0.30766475200653076\n",
            "step 613: loss=0.08852434158325195\n",
            "step 614: loss=0.10343050956726074\n",
            "step 615: loss=0.21927404403686523\n",
            "step 616: loss=0.13034415245056152\n",
            "step 617: loss=0.0\n",
            "step 618: loss=0.3672417402267456\n",
            "step 619: loss=0.20646905899047852\n",
            "step 620: loss=0.15087628364562988\n",
            "step 621: loss=2.7137346267700195\n",
            "step 622: loss=0.012846946716308594\n",
            "step 623: loss=0.00041294097900390625\n",
            "step 624: loss=3.0628349781036377\n",
            "step 625: loss=0.11929845809936523\n",
            "step 626: loss=0.14122796058654785\n",
            "step 627: loss=0.08585619926452637\n",
            "step 628: loss=0.0\n",
            "step 629: loss=0.2543833255767822\n",
            "step 630: loss=0.00038051605224609375\n",
            "step 631: loss=0.04947829246520996\n",
            "step 632: loss=0.037550926208496094\n",
            "step 633: loss=4.268228530883789\n",
            "step 634: loss=0.0\n",
            "step 635: loss=0.06965303421020508\n",
            "step 636: loss=0.00011539459228515625\n",
            "step 637: loss=1.2985060214996338\n",
            "step 638: loss=0.021075725555419922\n",
            "step 639: loss=0.000125885009765625\n",
            "step 640: loss=0.13861584663391113\n",
            "step 641: loss=0.3995513916015625\n",
            "step 642: loss=0.03494119644165039\n",
            "step 643: loss=0.5273630023002625\n",
            "step 644: loss=0.022318124771118164\n",
            "step 645: loss=0.007720947265625\n",
            "step 646: loss=0.0002770423889160156\n",
            "step 647: loss=0.07208657264709473\n",
            "step 648: loss=0.0011019706726074219\n",
            "step 649: loss=0.007816314697265625\n",
            "step 650: loss=0.00115966796875\n",
            "step 651: loss=1.4007774591445923\n",
            "step 652: loss=2.4345500469207764\n",
            "step 653: loss=0.07119035720825195\n",
            "step 654: loss=0.15207290649414062\n",
            "step 655: loss=0.08873319625854492\n",
            "step 656: loss=0.38689351081848145\n",
            "step 657: loss=0.030845165252685547\n",
            "step 658: loss=3.9822185039520264\n",
            "step 659: loss=0.3024892807006836\n",
            "step 660: loss=3.7127878665924072\n",
            "step 661: loss=0.0020351409912109375\n",
            "step 662: loss=0.013080358505249023\n",
            "step 663: loss=1.6068084239959717\n",
            "step 664: loss=0.008979558944702148\n",
            "step 665: loss=0.002056598663330078\n",
            "step 666: loss=0.0010967254638671875\n",
            "step 667: loss=0.0006608963012695312\n",
            "step 668: loss=2.0197343826293945\n",
            "step 669: loss=0.33664339780807495\n",
            "step 670: loss=0.25503993034362793\n",
            "step 671: loss=4.6165361404418945\n",
            "step 672: loss=0.2889378070831299\n",
            "step 673: loss=0.8888764977455139\n",
            "step 674: loss=0.000823974609375\n",
            "step 675: loss=0.15418171882629395\n",
            "step 676: loss=0.03651595115661621\n",
            "step 677: loss=0.08733487129211426\n",
            "step 678: loss=0.1879713535308838\n",
            "step 679: loss=0.03506755828857422\n",
            "step 680: loss=0.00016689300537109375\n",
            "step 681: loss=1.8258718252182007\n",
            "step 682: loss=9.822845458984375e-05\n",
            "step 683: loss=0.2146773338317871\n",
            "step 684: loss=0.032250404357910156\n",
            "step 685: loss=0.0008411407470703125\n",
            "step 686: loss=0.032976388931274414\n",
            "step 687: loss=0.32922446727752686\n",
            "step 688: loss=0.01651763916015625\n",
            "step 689: loss=0.9530888199806213\n",
            "step 690: loss=0.11372041702270508\n",
            "step 691: loss=0.014770984649658203\n",
            "step 692: loss=0.1985480785369873\n",
            "step 693: loss=0.002078533172607422\n",
            "step 694: loss=0.01252889633178711\n",
            "step 695: loss=0.0026717185974121094\n",
            "step 696: loss=0.8083721995353699\n",
            "step 697: loss=0.4415851831436157\n",
            "step 698: loss=1.168351650238037\n",
            "step 699: loss=0.04668307304382324\n",
            "step 700: loss=0.30454063415527344\n",
            "step 701: loss=0.008948802947998047\n",
            "step 702: loss=2.384185791015625e-05\n",
            "step 703: loss=0.01160430908203125\n",
            "step 704: loss=0.06095743179321289\n",
            "step 705: loss=0.05363965034484863\n",
            "step 706: loss=0.6081218719482422\n",
            "step 707: loss=0.9707538485527039\n",
            "step 708: loss=0.1268622875213623\n",
            "step 709: loss=0.052323341369628906\n",
            "step 710: loss=1.3820064067840576\n",
            "step 711: loss=3.678420066833496\n",
            "step 712: loss=0.1170804500579834\n",
            "step 713: loss=0.0271761417388916\n",
            "step 714: loss=0.04988884925842285\n",
            "step 715: loss=0.0024814605712890625\n",
            "step 716: loss=0.096893310546875\n",
            "step 717: loss=0.026457548141479492\n",
            "step 718: loss=0.007933616638183594\n",
            "step 719: loss=0.005488872528076172\n",
            "step 720: loss=0.0003132820129394531\n",
            "step 721: loss=0.028234481811523438\n",
            "step 722: loss=0.012423515319824219\n",
            "step 723: loss=0.003856658935546875\n",
            "step 724: loss=0.010411739349365234\n",
            "step 725: loss=6.244528770446777\n",
            "step 726: loss=0.4190746545791626\n",
            "step 727: loss=0.023894309997558594\n",
            "step 728: loss=0.059925079345703125\n",
            "step 729: loss=1.1369565725326538\n",
            "step 730: loss=0.0002040863037109375\n",
            "step 731: loss=0.11639761924743652\n",
            "step 732: loss=0.1622321605682373\n",
            "step 733: loss=0.07464981079101562\n",
            "step 734: loss=1.9550323486328125e-05\n",
            "step 735: loss=2.011380434036255\n",
            "step 736: loss=4.1961669921875e-05\n",
            "step 737: loss=0.22059011459350586\n",
            "step 738: loss=0.17694377899169922\n",
            "step 739: loss=0.0\n",
            "step 740: loss=0.0009737014770507812\n",
            "step 741: loss=1.8953258991241455\n",
            "step 742: loss=2.746763229370117\n",
            "step 743: loss=0.09476518630981445\n",
            "step 744: loss=2.86102294921875e-05\n",
            "step 745: loss=0.03348064422607422\n",
            "step 746: loss=2.47955322265625e-05\n",
            "step 747: loss=0.225030779838562\n",
            "step 748: loss=0.008271217346191406\n",
            "step 749: loss=0.006745815277099609\n",
            "step 750: loss=0.21326994895935059\n",
            "step 751: loss=2.2888832092285156\n",
            "step 752: loss=0.03307771682739258\n",
            "step 753: loss=0.11158275604248047\n",
            "step 754: loss=0.43932127952575684\n",
            "step 755: loss=0.0374910831451416\n",
            "step 756: loss=3.1628124713897705\n",
            "step 757: loss=0.16072368621826172\n",
            "step 758: loss=0.7160812616348267\n",
            "step 759: loss=0.01501607894897461\n",
            "step 760: loss=0.6495115756988525\n",
            "step 761: loss=0.00038623809814453125\n",
            "step 762: loss=0.051390886306762695\n",
            "step 763: loss=1.0338019132614136\n",
            "step 764: loss=0.350339412689209\n",
            "step 765: loss=0.027638912200927734\n",
            "step 766: loss=0.7128168940544128\n",
            "step 767: loss=0.04774045944213867\n",
            "step 768: loss=3.4355740547180176\n",
            "step 769: loss=0.0\n",
            "step 770: loss=0.020926475524902344\n",
            "step 771: loss=0.02243518829345703\n",
            "step 772: loss=0.7133201956748962\n",
            "step 773: loss=0.017409801483154297\n",
            "step 774: loss=0.14902567863464355\n",
            "step 775: loss=0.003330707550048828\n",
            "step 776: loss=0.22999167442321777\n",
            "step 777: loss=0.022410869598388672\n",
            "step 778: loss=0.033197879791259766\n",
            "step 779: loss=0.015965938568115234\n",
            "step 780: loss=0.003239154815673828\n",
            "step 781: loss=2.86102294921875e-06\n",
            "step 782: loss=0.0003466606140136719\n",
            "step 783: loss=0.010939598083496094\n",
            "step 784: loss=0.012454032897949219\n",
            "step 785: loss=0.051111698150634766\n",
            "step 786: loss=0.007010459899902344\n",
            "step 787: loss=0.018442153930664062\n",
            "step 788: loss=0.04834628105163574\n",
            "step 789: loss=0.014559745788574219\n",
            "step 790: loss=0.7888869047164917\n",
            "step 791: loss=0.23250651359558105\n",
            "step 792: loss=0.04334878921508789\n",
            "step 793: loss=0.0008554458618164062\n",
            "step 794: loss=0.9621310234069824\n",
            "step 795: loss=0.00034618377685546875\n",
            "step 796: loss=0.0030226707458496094\n",
            "step 797: loss=0.0031256675720214844\n",
            "step 798: loss=2.822838306427002\n",
            "step 799: loss=0.64838707447052\n",
            "step 800: loss=0.14017415046691895\n",
            "step 801: loss=0.0718374252319336\n",
            "step 802: loss=0.0009975433349609375\n",
            "step 803: loss=0.006953716278076172\n",
            "step 804: loss=0.22263145446777344\n",
            "step 805: loss=0.00240325927734375\n",
            "step 806: loss=7.2479248046875e-05\n",
            "step 807: loss=0.0\n",
            "step 808: loss=0.005190849304199219\n",
            "step 809: loss=0.9984101057052612\n",
            "step 810: loss=6.580352783203125e-05\n",
            "step 811: loss=2.5438284873962402\n",
            "step 812: loss=0.27243244647979736\n",
            "step 813: loss=0.12060308456420898\n",
            "step 814: loss=0.9106175899505615\n",
            "step 815: loss=0.015821456909179688\n",
            "step 816: loss=0.5052783489227295\n",
            "step 817: loss=0.3334888219833374\n",
            "step 818: loss=3.595881223678589\n",
            "step 819: loss=0.004091739654541016\n",
            "step 820: loss=0.265578031539917\n",
            "step 821: loss=0.07645940780639648\n",
            "step 822: loss=0.04256916046142578\n",
            "step 823: loss=0.0027174949645996094\n",
            "step 824: loss=0.11615389585494995\n",
            "step 825: loss=2.422677516937256\n",
            "step 826: loss=4.863739013671875e-05\n",
            "step 827: loss=1.712720274925232\n",
            "step 828: loss=0.0009756088256835938\n",
            "step 829: loss=0.10599613189697266\n",
            "step 830: loss=0.004247188568115234\n",
            "step 831: loss=0.32804298400878906\n",
            "step 832: loss=0.00014162063598632812\n",
            "step 833: loss=0.014614105224609375\n",
            "step 834: loss=0.934762716293335\n",
            "step 835: loss=0.2241199016571045\n",
            "step 836: loss=0.002994537353515625\n",
            "step 837: loss=0.0010938644409179688\n",
            "step 838: loss=0.0015430450439453125\n",
            "step 839: loss=0.0022821426391601562\n",
            "step 840: loss=1.621246337890625e-05\n",
            "step 841: loss=0.003120899200439453\n",
            "step 842: loss=6.303908348083496\n",
            "step 843: loss=0.005168437957763672\n",
            "step 844: loss=0.019023895263671875\n",
            "step 845: loss=6.198883056640625e-05\n",
            "step 846: loss=0.04726696014404297\n",
            "step 847: loss=0.0031909942626953125\n",
            "step 848: loss=0.00025272369384765625\n",
            "step 849: loss=0.00037097930908203125\n",
            "step 850: loss=0.03382253646850586\n",
            "step 851: loss=0.11497092247009277\n",
            "step 852: loss=0.16957449913024902\n",
            "step 853: loss=0.49427151679992676\n",
            "step 854: loss=5.435943603515625e-05\n",
            "step 855: loss=3.041072130203247\n",
            "step 856: loss=0.00028896331787109375\n",
            "step 857: loss=0.6473691463470459\n",
            "step 858: loss=2.8723654747009277\n",
            "step 859: loss=0.5234723091125488\n",
            "step 860: loss=0.06753802299499512\n",
            "step 861: loss=0.021260976791381836\n",
            "step 862: loss=0.0\n",
            "step 863: loss=0.023668289184570312\n",
            "step 864: loss=0.02113199234008789\n",
            "step 865: loss=0.0006322860717773438\n",
            "step 866: loss=0.008189678192138672\n",
            "step 867: loss=0.12167787551879883\n",
            "step 868: loss=0.04052329063415527\n",
            "step 869: loss=2.492642402648926\n",
            "step 870: loss=0.0001888275146484375\n",
            "step 871: loss=0.22299611568450928\n",
            "step 872: loss=0.10732412338256836\n",
            "step 873: loss=0.03928184509277344\n",
            "step 874: loss=0.9805281162261963\n",
            "step 875: loss=0.4805511236190796\n",
            "step 876: loss=0.8027201890945435\n",
            "step 877: loss=0.033196210861206055\n",
            "step 878: loss=0.0033431053161621094\n",
            "step 879: loss=0.0025186538696289062\n",
            "step 880: loss=0.14515137672424316\n",
            "step 881: loss=0.021310091018676758\n",
            "step 882: loss=0.03321695327758789\n",
            "step 883: loss=0.006505489349365234\n",
            "step 884: loss=0.025542736053466797\n",
            "step 885: loss=0.0013241767883300781\n",
            "step 886: loss=1.338914155960083\n",
            "step 887: loss=0.13813495635986328\n",
            "step 888: loss=1.3211994171142578\n",
            "step 889: loss=1.6180028915405273\n",
            "step 890: loss=0.0968858003616333\n",
            "step 891: loss=0.0002779960632324219\n",
            "step 892: loss=9.5367431640625e-07\n",
            "step 893: loss=0.44179725646972656\n",
            "step 894: loss=0.0042934417724609375\n",
            "step 895: loss=3.62396240234375e-05\n",
            "step 896: loss=0.0010213851928710938\n",
            "step 897: loss=0.47501277923583984\n",
            "step 898: loss=0.5673651695251465\n",
            "step 899: loss=0.0009512901306152344\n",
            "step 900: loss=0.0031137466430664062\n",
            "step 901: loss=0.049977779388427734\n",
            "step 902: loss=1.239776611328125e-05\n",
            "step 903: loss=0.14996886253356934\n",
            "step 904: loss=0.0010485649108886719\n",
            "step 905: loss=0.10764098167419434\n",
            "step 906: loss=0.00020170211791992188\n",
            "step 907: loss=0.7161453366279602\n",
            "step 908: loss=0.004637241363525391\n",
            "step 909: loss=0.05553007125854492\n",
            "step 910: loss=0.018809795379638672\n",
            "step 911: loss=0.15685153007507324\n",
            "step 912: loss=0.0028738975524902344\n",
            "step 913: loss=0.02420330047607422\n",
            "step 914: loss=5.537294387817383\n",
            "step 915: loss=9.441375732421875e-05\n",
            "step 916: loss=0.22970914840698242\n",
            "step 917: loss=1.7678396701812744\n",
            "step 918: loss=0.5832853317260742\n",
            "step 919: loss=0.9335628747940063\n",
            "step 920: loss=0.08302164077758789\n",
            "step 921: loss=0.04675483703613281\n",
            "step 922: loss=0.8199524879455566\n",
            "step 923: loss=0.08801698684692383\n",
            "step 924: loss=0.0\n",
            "step 925: loss=0.0012335777282714844\n",
            "step 926: loss=0.005401611328125\n",
            "step 927: loss=0.16196227073669434\n",
            "step 928: loss=0.5575300455093384\n",
            "step 929: loss=0.049349308013916016\n",
            "step 930: loss=0.28379130363464355\n",
            "step 931: loss=0.10143256187438965\n",
            "step 932: loss=0.13189196586608887\n",
            "step 933: loss=0.004683017730712891\n",
            "step 934: loss=0.01829242706298828\n",
            "step 935: loss=0.0002532005310058594\n",
            "step 936: loss=0.47372007369995117\n",
            "step 937: loss=0.0014209747314453125\n",
            "step 938: loss=0.23517894744873047\n",
            "step 939: loss=0.6270471215248108\n",
            "step 940: loss=0.0009245872497558594\n",
            "step 941: loss=0.0020742416381835938\n",
            "step 942: loss=0.10157772898674011\n",
            "step 943: loss=0.0004868507385253906\n",
            "step 944: loss=0.000217437744140625\n",
            "step 945: loss=0.1272754669189453\n",
            "step 946: loss=8.022424697875977\n",
            "step 947: loss=0.01564645767211914\n",
            "step 948: loss=5.7220458984375e-06\n",
            "step 949: loss=1.0353565216064453\n",
            "step 950: loss=0.1244058609008789\n",
            "step 951: loss=0.025864362716674805\n",
            "step 952: loss=0.0031480789184570312\n",
            "step 953: loss=2.7638301849365234\n",
            "step 954: loss=0.31478404998779297\n",
            "step 955: loss=1.9073486328125e-06\n",
            "step 956: loss=0.0002956390380859375\n",
            "step 957: loss=1.9073486328125e-06\n",
            "step 958: loss=3.804717540740967\n",
            "step 959: loss=0.146406888961792\n",
            "step 960: loss=2.458515167236328\n",
            "step 961: loss=1.6739331483840942\n",
            "step 962: loss=0.00013494491577148438\n",
            "step 963: loss=0.10185050964355469\n",
            "step 964: loss=7.200241088867188e-05\n",
            "step 965: loss=0.2721977233886719\n",
            "step 966: loss=0.8069049119949341\n",
            "step 967: loss=0.15566682815551758\n",
            "step 968: loss=0.1653527021408081\n",
            "step 969: loss=0.3627464771270752\n",
            "step 970: loss=0.21231865882873535\n",
            "step 971: loss=0.0015063285827636719\n",
            "step 972: loss=0.484058141708374\n",
            "step 973: loss=0.6047828197479248\n",
            "step 974: loss=0.31355762481689453\n",
            "step 975: loss=0.8087024092674255\n",
            "step 976: loss=0.0002999305725097656\n",
            "step 977: loss=0.0005540847778320312\n",
            "step 978: loss=0.31179094314575195\n",
            "step 979: loss=0.4971165657043457\n",
            "step 980: loss=0.0013852119445800781\n",
            "step 981: loss=0.2564563751220703\n",
            "step 982: loss=0.01238393783569336\n",
            "step 983: loss=0.485332727432251\n",
            "step 984: loss=0.00019931793212890625\n",
            "step 985: loss=0.6167352199554443\n",
            "step 986: loss=0.001049041748046875\n",
            "step 987: loss=3.814697265625e-06\n",
            "step 988: loss=0.5907893180847168\n",
            "step 989: loss=0.0034475326538085938\n",
            "step 990: loss=0.19600462913513184\n",
            "step 991: loss=0.07634830474853516\n",
            "step 992: loss=0.07055139541625977\n",
            "step 993: loss=0.0002155303955078125\n",
            "step 994: loss=1.0463757514953613\n",
            "step 995: loss=0.9066271781921387\n",
            "step 996: loss=0.00015926361083984375\n",
            "step 997: loss=9.5367431640625e-07\n",
            "step 998: loss=0.12614011764526367\n",
            "step 999: loss=0.12311649322509766\n",
            "step 1000: loss=0.0014834403991699219\n",
            "Mean loss        0.6586794\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 8 3 1]\n"
          ]
        }
      ],
      "source": [
        "mask_fn = lambda p: jax.tree_util.tree_map(lambda x: x.ndim != 1, p) # mask biases\n",
        "optimizer = optax.adamw(learning_rate, weight_decay=1e-4, mask=mask_fn)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f5964dce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=2.2421863079071045\n",
            "step 2: loss=1.4559381008148193\n",
            "step 3: loss=3.085052490234375\n",
            "step 4: loss=1.6123230457305908\n",
            "step 5: loss=1.9464938640594482\n",
            "step 6: loss=1.5765169858932495\n",
            "step 7: loss=2.2348220348358154\n",
            "step 8: loss=2.330615520477295\n",
            "step 9: loss=1.399373173713684\n",
            "step 10: loss=2.4562790393829346\n",
            "step 11: loss=2.742587089538574\n",
            "step 12: loss=3.5112454891204834\n",
            "step 13: loss=1.9610660076141357\n",
            "step 14: loss=2.111886978149414\n",
            "step 15: loss=3.3601794242858887\n",
            "step 16: loss=3.1095786094665527\n",
            "step 17: loss=1.3849537372589111\n",
            "step 18: loss=2.7661705017089844\n",
            "step 19: loss=2.647594928741455\n",
            "step 20: loss=1.561249017715454\n",
            "step 21: loss=2.3637077808380127\n",
            "step 22: loss=2.654891014099121\n",
            "step 23: loss=2.297776699066162\n",
            "step 24: loss=2.9272146224975586\n",
            "step 25: loss=1.6235723495483398\n",
            "step 26: loss=2.1043612957000732\n",
            "step 27: loss=2.7969167232513428\n",
            "step 28: loss=2.523972749710083\n",
            "step 29: loss=2.4751412868499756\n",
            "step 30: loss=2.1431922912597656\n",
            "step 31: loss=3.28304386138916\n",
            "step 32: loss=3.079850196838379\n",
            "step 33: loss=2.51116943359375\n",
            "step 34: loss=1.917013168334961\n",
            "step 35: loss=1.5922491550445557\n",
            "step 36: loss=2.772650957107544\n",
            "step 37: loss=3.4645769596099854\n",
            "step 38: loss=2.724334239959717\n",
            "step 39: loss=2.73052978515625\n",
            "step 40: loss=2.610581159591675\n",
            "step 41: loss=1.2705742120742798\n",
            "step 42: loss=2.1976189613342285\n",
            "step 43: loss=2.778535842895508\n",
            "step 44: loss=2.896061420440674\n",
            "step 45: loss=2.950147867202759\n",
            "step 46: loss=1.4428362846374512\n",
            "step 47: loss=1.3264340162277222\n",
            "step 48: loss=2.266848564147949\n",
            "step 49: loss=2.60626482963562\n",
            "step 50: loss=2.0107462406158447\n",
            "step 51: loss=1.5741785764694214\n",
            "step 52: loss=2.4198460578918457\n",
            "step 53: loss=2.075371503829956\n",
            "step 54: loss=1.0753508806228638\n",
            "step 55: loss=2.038837432861328\n",
            "step 56: loss=3.021540641784668\n",
            "step 57: loss=3.052746295928955\n",
            "step 58: loss=2.160950183868408\n",
            "step 59: loss=3.0264482498168945\n",
            "step 60: loss=1.8717128038406372\n",
            "step 61: loss=1.780563473701477\n",
            "step 62: loss=1.9731054306030273\n",
            "step 63: loss=2.891355276107788\n",
            "step 64: loss=2.5201401710510254\n",
            "step 65: loss=1.9541761875152588\n",
            "step 66: loss=1.1066423654556274\n",
            "step 67: loss=1.7638072967529297\n",
            "step 68: loss=1.2348321676254272\n",
            "step 69: loss=2.8145816326141357\n",
            "step 70: loss=2.4608142375946045\n",
            "step 71: loss=2.7380001544952393\n",
            "step 72: loss=3.831146717071533\n",
            "step 73: loss=1.4136945009231567\n",
            "step 74: loss=1.4678406715393066\n",
            "step 75: loss=1.679764747619629\n",
            "step 76: loss=1.0130945444107056\n",
            "step 77: loss=1.8389990329742432\n",
            "step 78: loss=3.4174962043762207\n",
            "step 79: loss=2.4600918292999268\n",
            "step 80: loss=1.8802964687347412\n",
            "step 81: loss=1.153184175491333\n",
            "step 82: loss=1.934180498123169\n",
            "step 83: loss=2.552190065383911\n",
            "step 84: loss=1.2676652669906616\n",
            "step 85: loss=2.5031745433807373\n",
            "step 86: loss=1.1603964567184448\n",
            "step 87: loss=2.739018678665161\n",
            "step 88: loss=2.3474783897399902\n",
            "step 89: loss=1.8958055973052979\n",
            "step 90: loss=2.0915753841400146\n",
            "step 91: loss=1.9891102313995361\n",
            "step 92: loss=2.080958366394043\n",
            "step 93: loss=1.58925199508667\n",
            "step 94: loss=2.0787572860717773\n",
            "step 95: loss=1.4311790466308594\n",
            "step 96: loss=2.547168254852295\n",
            "step 97: loss=3.257232666015625\n",
            "step 98: loss=2.455991506576538\n",
            "step 99: loss=2.0813260078430176\n",
            "step 100: loss=2.715155601501465\n",
            "step 101: loss=2.3847599029541016\n",
            "step 102: loss=2.9120025634765625\n",
            "step 103: loss=3.106847047805786\n",
            "step 104: loss=2.4792892932891846\n",
            "step 105: loss=1.8238879442214966\n",
            "step 106: loss=3.3787105083465576\n",
            "step 107: loss=1.7758305072784424\n",
            "step 108: loss=2.276481866836548\n",
            "step 109: loss=2.6397409439086914\n",
            "step 110: loss=2.772592544555664\n",
            "step 111: loss=1.4192333221435547\n",
            "step 112: loss=1.3957408666610718\n",
            "step 113: loss=2.5470950603485107\n",
            "step 114: loss=2.7266929149627686\n",
            "step 115: loss=1.1549280881881714\n",
            "step 116: loss=2.7441513538360596\n",
            "step 117: loss=2.2453272342681885\n",
            "step 118: loss=2.367126703262329\n",
            "step 119: loss=2.4574942588806152\n",
            "step 120: loss=1.3329459428787231\n",
            "step 121: loss=3.481229782104492\n",
            "step 122: loss=2.365586280822754\n",
            "step 123: loss=3.2223756313323975\n",
            "step 124: loss=2.597825765609741\n",
            "step 125: loss=1.3528517484664917\n",
            "step 126: loss=1.7805711030960083\n",
            "step 127: loss=2.8108835220336914\n",
            "step 128: loss=2.620694398880005\n",
            "step 129: loss=1.3520594835281372\n",
            "step 130: loss=1.136674165725708\n",
            "step 131: loss=2.4415061473846436\n",
            "step 132: loss=2.615053176879883\n",
            "step 133: loss=2.455946445465088\n",
            "step 134: loss=1.1859095096588135\n",
            "step 135: loss=2.512007713317871\n",
            "step 136: loss=1.0291022062301636\n",
            "step 137: loss=1.741264820098877\n",
            "step 138: loss=1.6689473390579224\n",
            "step 139: loss=2.278477668762207\n",
            "step 140: loss=3.0842785835266113\n",
            "step 141: loss=2.2265453338623047\n",
            "step 142: loss=2.258383274078369\n",
            "step 143: loss=2.350719451904297\n",
            "step 144: loss=2.4916012287139893\n",
            "step 145: loss=2.662445306777954\n",
            "step 146: loss=2.700526237487793\n",
            "step 147: loss=2.824399709701538\n",
            "step 148: loss=2.708142042160034\n",
            "step 149: loss=2.7048418521881104\n",
            "step 150: loss=1.4532983303070068\n",
            "step 151: loss=3.530831813812256\n",
            "step 152: loss=1.6740576028823853\n",
            "step 153: loss=2.01247501373291\n",
            "step 154: loss=2.52925181388855\n",
            "step 155: loss=2.2701938152313232\n",
            "step 156: loss=2.678058624267578\n",
            "step 157: loss=2.9550743103027344\n",
            "step 158: loss=3.062917709350586\n",
            "step 159: loss=2.657618522644043\n",
            "step 160: loss=3.186411142349243\n",
            "step 161: loss=1.3771408796310425\n",
            "step 162: loss=2.1002025604248047\n",
            "step 163: loss=2.451606035232544\n",
            "step 164: loss=1.5803146362304688\n",
            "step 165: loss=3.2255733013153076\n",
            "step 166: loss=2.9819931983947754\n",
            "step 167: loss=3.3450639247894287\n",
            "step 168: loss=2.6375277042388916\n",
            "step 169: loss=2.5869600772857666\n",
            "step 170: loss=2.216160774230957\n",
            "step 171: loss=1.5904948711395264\n",
            "step 172: loss=1.8049285411834717\n",
            "step 173: loss=2.810737371444702\n",
            "step 174: loss=2.2000811100006104\n",
            "step 175: loss=2.2271218299865723\n",
            "step 176: loss=2.230067014694214\n",
            "step 177: loss=2.650635004043579\n",
            "step 178: loss=2.6906728744506836\n",
            "step 179: loss=2.600717782974243\n",
            "step 180: loss=2.9840781688690186\n",
            "step 181: loss=1.7284671068191528\n",
            "step 182: loss=1.3906135559082031\n",
            "step 183: loss=2.7317943572998047\n",
            "step 184: loss=2.381579637527466\n",
            "step 185: loss=1.7398152351379395\n",
            "step 186: loss=2.720386505126953\n",
            "step 187: loss=1.1779066324234009\n",
            "step 188: loss=3.8678994178771973\n",
            "step 189: loss=3.279266595840454\n",
            "step 190: loss=1.401140570640564\n",
            "step 191: loss=1.4532971382141113\n",
            "step 192: loss=1.3859878778457642\n",
            "step 193: loss=1.709574580192566\n",
            "step 194: loss=2.0112972259521484\n",
            "step 195: loss=2.119262218475342\n",
            "step 196: loss=3.34930682182312\n",
            "step 197: loss=2.5211305618286133\n",
            "step 198: loss=1.4698550701141357\n",
            "step 199: loss=1.743014931678772\n",
            "step 200: loss=2.492750883102417\n",
            "step 201: loss=2.4340362548828125\n",
            "step 202: loss=2.405163049697876\n",
            "step 203: loss=1.3755238056182861\n",
            "step 204: loss=1.0597456693649292\n",
            "step 205: loss=2.1209497451782227\n",
            "step 206: loss=2.6568353176116943\n",
            "step 207: loss=2.247775077819824\n",
            "step 208: loss=3.140488386154175\n",
            "step 209: loss=2.5085954666137695\n",
            "step 210: loss=2.333026170730591\n",
            "step 211: loss=1.449198603630066\n",
            "step 212: loss=2.6405861377716064\n",
            "step 213: loss=2.4194111824035645\n",
            "step 214: loss=2.635425329208374\n",
            "step 215: loss=2.322136402130127\n",
            "step 216: loss=2.4113245010375977\n",
            "step 217: loss=2.3974480628967285\n",
            "step 218: loss=1.5360066890716553\n",
            "step 219: loss=3.453566312789917\n",
            "step 220: loss=2.8999879360198975\n",
            "step 221: loss=2.080258846282959\n",
            "step 222: loss=3.0170044898986816\n",
            "step 223: loss=2.6805808544158936\n",
            "step 224: loss=2.016932487487793\n",
            "step 225: loss=2.489001750946045\n",
            "step 226: loss=2.3519392013549805\n",
            "step 227: loss=1.928572654724121\n",
            "step 228: loss=1.7042627334594727\n",
            "step 229: loss=2.636045455932617\n",
            "step 230: loss=3.575512170791626\n",
            "step 231: loss=3.1082940101623535\n",
            "step 232: loss=1.9610052108764648\n",
            "step 233: loss=2.0984692573547363\n",
            "step 234: loss=2.2733094692230225\n",
            "step 235: loss=2.357238292694092\n",
            "step 236: loss=1.9906686544418335\n",
            "step 237: loss=2.5017452239990234\n",
            "step 238: loss=2.556908130645752\n",
            "step 239: loss=1.623496174812317\n",
            "step 240: loss=2.377209186553955\n",
            "step 241: loss=3.049227237701416\n",
            "step 242: loss=2.520096778869629\n",
            "step 243: loss=2.281975746154785\n",
            "step 244: loss=1.5147318840026855\n",
            "step 245: loss=2.9609010219573975\n",
            "step 246: loss=3.4314589500427246\n",
            "step 247: loss=2.4515790939331055\n",
            "step 248: loss=1.6569795608520508\n",
            "step 249: loss=3.0099151134490967\n",
            "step 250: loss=1.4864219427108765\n",
            "step 251: loss=2.5706684589385986\n",
            "step 252: loss=1.5256322622299194\n",
            "step 253: loss=2.9779341220855713\n",
            "step 254: loss=2.241046905517578\n",
            "step 255: loss=2.0960018634796143\n",
            "step 256: loss=2.06215238571167\n",
            "step 257: loss=2.9494731426239014\n",
            "step 258: loss=1.3359276056289673\n",
            "step 259: loss=2.569751024246216\n",
            "step 260: loss=2.295391082763672\n",
            "step 261: loss=2.052069664001465\n",
            "step 262: loss=2.504887342453003\n",
            "step 263: loss=1.4462952613830566\n",
            "step 264: loss=2.7616963386535645\n",
            "step 265: loss=2.445884943008423\n",
            "step 266: loss=2.6785802841186523\n",
            "step 267: loss=2.2980663776397705\n",
            "step 268: loss=2.9745073318481445\n",
            "step 269: loss=2.5043692588806152\n",
            "step 270: loss=2.8068342208862305\n",
            "step 271: loss=2.2660984992980957\n",
            "step 272: loss=3.0564918518066406\n",
            "step 273: loss=2.0938472747802734\n",
            "step 274: loss=1.457321047782898\n",
            "step 275: loss=2.1097354888916016\n",
            "step 276: loss=1.6676883697509766\n",
            "step 277: loss=2.095802068710327\n",
            "step 278: loss=1.5458675622940063\n",
            "step 279: loss=2.711874485015869\n",
            "step 280: loss=2.1740217208862305\n",
            "step 281: loss=2.582977771759033\n",
            "step 282: loss=2.505199670791626\n",
            "step 283: loss=3.118259906768799\n",
            "step 284: loss=1.7285661697387695\n",
            "step 285: loss=2.0947000980377197\n",
            "step 286: loss=2.030956745147705\n",
            "step 287: loss=2.6244659423828125\n",
            "step 288: loss=2.8873836994171143\n",
            "step 289: loss=3.022646903991699\n",
            "step 290: loss=2.252254009246826\n",
            "step 291: loss=2.4670934677124023\n",
            "step 292: loss=1.927098035812378\n",
            "step 293: loss=2.6480014324188232\n",
            "step 294: loss=1.2941874265670776\n",
            "step 295: loss=1.717361569404602\n",
            "step 296: loss=1.9842861890792847\n",
            "step 297: loss=2.7423245906829834\n",
            "step 298: loss=1.6281484365463257\n",
            "step 299: loss=2.5050926208496094\n",
            "step 300: loss=1.4747322797775269\n",
            "step 301: loss=2.162879705429077\n",
            "step 302: loss=1.6113450527191162\n",
            "step 303: loss=2.282771110534668\n",
            "step 304: loss=2.6650285720825195\n",
            "step 305: loss=2.798597812652588\n",
            "step 306: loss=2.4593026638031006\n",
            "step 307: loss=2.8083066940307617\n",
            "step 308: loss=1.3145582675933838\n",
            "step 309: loss=2.726593494415283\n",
            "step 310: loss=2.276571035385132\n",
            "step 311: loss=3.406991720199585\n",
            "step 312: loss=2.114511728286743\n",
            "step 313: loss=1.8025357723236084\n",
            "step 314: loss=1.9327529668807983\n",
            "step 315: loss=2.1802945137023926\n",
            "step 316: loss=3.272582530975342\n",
            "step 317: loss=2.1028871536254883\n",
            "step 318: loss=1.8016530275344849\n",
            "step 319: loss=2.0556640625\n",
            "step 320: loss=3.56372332572937\n",
            "step 321: loss=1.716363787651062\n",
            "step 322: loss=2.5309460163116455\n",
            "step 323: loss=1.7472472190856934\n",
            "step 324: loss=3.145803451538086\n",
            "step 325: loss=2.496901035308838\n",
            "step 326: loss=3.6929612159729004\n",
            "step 327: loss=3.7405648231506348\n",
            "step 328: loss=2.275761127471924\n",
            "step 329: loss=1.248457431793213\n",
            "step 330: loss=1.7137837409973145\n",
            "step 331: loss=1.455980658531189\n",
            "step 332: loss=2.3813962936401367\n",
            "step 333: loss=1.26932692527771\n",
            "step 334: loss=2.6489100456237793\n",
            "step 335: loss=2.6979336738586426\n",
            "step 336: loss=2.5788965225219727\n",
            "step 337: loss=2.8272478580474854\n",
            "step 338: loss=2.521517276763916\n",
            "step 339: loss=2.9806485176086426\n",
            "step 340: loss=2.6410176753997803\n",
            "step 341: loss=2.3069653511047363\n",
            "step 342: loss=2.6048991680145264\n",
            "step 343: loss=1.4534624814987183\n",
            "step 344: loss=3.3154408931732178\n",
            "step 345: loss=2.5195465087890625\n",
            "step 346: loss=1.2152187824249268\n",
            "step 347: loss=2.688023567199707\n",
            "step 348: loss=2.8830959796905518\n",
            "step 349: loss=3.438002347946167\n",
            "step 350: loss=1.1998072862625122\n",
            "step 351: loss=2.552180767059326\n",
            "step 352: loss=1.6405454874038696\n",
            "step 353: loss=3.405515432357788\n",
            "step 354: loss=1.4653406143188477\n",
            "step 355: loss=2.895552635192871\n",
            "step 356: loss=1.16762113571167\n",
            "step 357: loss=2.439291000366211\n",
            "step 358: loss=1.7089513540267944\n",
            "step 359: loss=2.860565423965454\n",
            "step 360: loss=2.280653715133667\n",
            "step 361: loss=1.5576579570770264\n",
            "step 362: loss=2.909472703933716\n",
            "step 363: loss=2.8627028465270996\n",
            "step 364: loss=2.007302761077881\n",
            "step 365: loss=1.4308630228042603\n",
            "step 366: loss=2.390198230743408\n",
            "step 367: loss=2.3181631565093994\n",
            "step 368: loss=1.1475110054016113\n",
            "step 369: loss=3.9979605674743652\n",
            "step 370: loss=2.18925404548645\n",
            "step 371: loss=1.2176271677017212\n",
            "step 372: loss=1.1979050636291504\n",
            "step 373: loss=1.764173984527588\n",
            "step 374: loss=1.929783821105957\n",
            "step 375: loss=2.964144229888916\n",
            "step 376: loss=2.6962311267852783\n",
            "step 377: loss=2.816523551940918\n",
            "step 378: loss=1.8724676370620728\n",
            "step 379: loss=2.15863037109375\n",
            "step 380: loss=2.099510908126831\n",
            "step 381: loss=2.5043649673461914\n",
            "step 382: loss=2.3786587715148926\n",
            "step 383: loss=1.841060757637024\n",
            "step 384: loss=2.7666282653808594\n",
            "step 385: loss=1.3600820302963257\n",
            "step 386: loss=1.403924584388733\n",
            "step 387: loss=1.446884036064148\n",
            "step 388: loss=3.6861302852630615\n",
            "step 389: loss=2.115628957748413\n",
            "step 390: loss=1.223962426185608\n",
            "step 391: loss=1.9569189548492432\n",
            "step 392: loss=2.83998966217041\n",
            "step 393: loss=1.8476351499557495\n",
            "step 394: loss=2.1155624389648438\n",
            "step 395: loss=1.1164451837539673\n",
            "step 396: loss=2.904618501663208\n",
            "step 397: loss=2.4271256923675537\n",
            "step 398: loss=3.071345329284668\n",
            "step 399: loss=2.712083101272583\n",
            "step 400: loss=1.372778296470642\n",
            "step 401: loss=2.2051196098327637\n",
            "step 402: loss=1.5540297031402588\n",
            "step 403: loss=3.274181604385376\n",
            "step 404: loss=2.3523027896881104\n",
            "step 405: loss=3.4439868927001953\n",
            "step 406: loss=1.3103771209716797\n",
            "step 407: loss=2.666656970977783\n",
            "step 408: loss=2.5837631225585938\n",
            "step 409: loss=1.861356496810913\n",
            "step 410: loss=2.0630931854248047\n",
            "step 411: loss=2.922231435775757\n",
            "step 412: loss=2.3938801288604736\n",
            "step 413: loss=2.895819902420044\n",
            "step 414: loss=1.4767391681671143\n",
            "step 415: loss=2.333731174468994\n",
            "step 416: loss=2.678530216217041\n",
            "step 417: loss=2.4035122394561768\n",
            "step 418: loss=2.792379856109619\n",
            "step 419: loss=2.844214916229248\n",
            "step 420: loss=3.1012074947357178\n",
            "step 421: loss=2.679537534713745\n",
            "step 422: loss=1.812907099723816\n",
            "step 423: loss=3.2403218746185303\n",
            "step 424: loss=2.1334879398345947\n",
            "step 425: loss=3.089066505432129\n",
            "step 426: loss=2.382899045944214\n",
            "step 427: loss=2.2178118228912354\n",
            "step 428: loss=2.1295583248138428\n",
            "step 429: loss=1.7410566806793213\n",
            "step 430: loss=2.371309995651245\n",
            "step 431: loss=1.9165136814117432\n",
            "step 432: loss=1.767698049545288\n",
            "step 433: loss=2.541945695877075\n",
            "step 434: loss=2.635652780532837\n",
            "step 435: loss=1.665094017982483\n",
            "step 436: loss=2.3261101245880127\n",
            "step 437: loss=3.0691118240356445\n",
            "step 438: loss=2.6385862827301025\n",
            "step 439: loss=3.3200104236602783\n",
            "step 440: loss=2.0778887271881104\n",
            "step 441: loss=1.6173781156539917\n",
            "step 442: loss=2.4767699241638184\n",
            "step 443: loss=2.446138620376587\n",
            "step 444: loss=2.453357219696045\n",
            "step 445: loss=1.6275691986083984\n",
            "step 446: loss=2.5135350227355957\n",
            "step 447: loss=1.4003257751464844\n",
            "step 448: loss=2.6144862174987793\n",
            "step 449: loss=2.7347235679626465\n",
            "step 450: loss=2.2358462810516357\n",
            "step 451: loss=1.7955106496810913\n",
            "step 452: loss=1.979914665222168\n",
            "step 453: loss=1.8940356969833374\n",
            "step 454: loss=3.139139175415039\n",
            "step 455: loss=2.8762106895446777\n",
            "step 456: loss=2.756274938583374\n",
            "step 457: loss=2.6641523838043213\n",
            "step 458: loss=2.4930741786956787\n",
            "step 459: loss=2.3517494201660156\n",
            "step 460: loss=1.2830902338027954\n",
            "step 461: loss=2.6999826431274414\n",
            "step 462: loss=2.360557794570923\n",
            "step 463: loss=3.3369007110595703\n",
            "step 464: loss=1.3529398441314697\n",
            "step 465: loss=2.766545534133911\n",
            "step 466: loss=1.2156707048416138\n",
            "step 467: loss=2.7181529998779297\n",
            "step 468: loss=2.283658027648926\n",
            "step 469: loss=3.478766441345215\n",
            "step 470: loss=1.4929096698760986\n",
            "step 471: loss=3.7128334045410156\n",
            "step 472: loss=1.9892874956130981\n",
            "step 473: loss=2.792391300201416\n",
            "step 474: loss=1.528887391090393\n",
            "step 475: loss=3.535614490509033\n",
            "step 476: loss=2.0069570541381836\n",
            "step 477: loss=2.143526077270508\n",
            "step 478: loss=1.9365113973617554\n",
            "step 479: loss=1.6307952404022217\n",
            "step 480: loss=3.315739393234253\n",
            "step 481: loss=2.494529962539673\n",
            "step 482: loss=2.5866596698760986\n",
            "step 483: loss=1.9929033517837524\n",
            "step 484: loss=1.7227325439453125\n",
            "step 485: loss=3.191467523574829\n",
            "step 486: loss=1.37134850025177\n",
            "step 487: loss=2.353402614593506\n",
            "step 488: loss=1.423935055732727\n",
            "step 489: loss=3.382782459259033\n",
            "step 490: loss=1.4203131198883057\n",
            "step 491: loss=1.5314136743545532\n",
            "step 492: loss=2.2492756843566895\n",
            "step 493: loss=3.3359954357147217\n",
            "step 494: loss=1.2814562320709229\n",
            "step 495: loss=3.019207000732422\n",
            "step 496: loss=2.5454535484313965\n",
            "step 497: loss=1.4651685953140259\n",
            "step 498: loss=1.6142497062683105\n",
            "step 499: loss=2.0432722568511963\n",
            "step 500: loss=2.6065611839294434\n",
            "step 501: loss=1.479080319404602\n",
            "step 502: loss=2.806997776031494\n",
            "step 503: loss=2.080308675765991\n",
            "step 504: loss=1.9291778802871704\n",
            "step 505: loss=2.713534355163574\n",
            "step 506: loss=2.4723968505859375\n",
            "step 507: loss=2.8816466331481934\n",
            "step 508: loss=2.554898262023926\n",
            "step 509: loss=1.3496325016021729\n",
            "step 510: loss=1.574747085571289\n",
            "step 511: loss=2.723172903060913\n",
            "step 512: loss=1.3614509105682373\n",
            "step 513: loss=1.593804955482483\n",
            "step 514: loss=1.8970308303833008\n",
            "step 515: loss=1.1430705785751343\n",
            "step 516: loss=1.61757230758667\n",
            "step 517: loss=2.7827415466308594\n",
            "step 518: loss=2.425929069519043\n",
            "step 519: loss=3.520458698272705\n",
            "step 520: loss=3.1432151794433594\n",
            "step 521: loss=2.211343765258789\n",
            "step 522: loss=3.381690263748169\n",
            "step 523: loss=1.3768458366394043\n",
            "step 524: loss=1.4063377380371094\n",
            "step 525: loss=1.2131940126419067\n",
            "step 526: loss=2.5118367671966553\n",
            "step 527: loss=1.418584942817688\n",
            "step 528: loss=2.8096446990966797\n",
            "step 529: loss=3.487062454223633\n",
            "step 530: loss=1.5972273349761963\n",
            "step 531: loss=2.2954530715942383\n",
            "step 532: loss=1.2655690908432007\n",
            "step 533: loss=3.100360870361328\n",
            "step 534: loss=1.6952085494995117\n",
            "step 535: loss=3.620802402496338\n",
            "step 536: loss=2.6640658378601074\n",
            "step 537: loss=1.5224168300628662\n",
            "step 538: loss=2.780470371246338\n",
            "step 539: loss=2.9127721786499023\n",
            "step 540: loss=2.7082607746124268\n",
            "step 541: loss=2.3238275051116943\n",
            "step 542: loss=1.1740760803222656\n",
            "step 543: loss=3.1295509338378906\n",
            "step 544: loss=2.845734119415283\n",
            "step 545: loss=2.2165534496307373\n",
            "step 546: loss=2.223253011703491\n",
            "step 547: loss=2.417135000228882\n",
            "step 548: loss=3.0446505546569824\n",
            "step 549: loss=3.4671411514282227\n",
            "step 550: loss=2.6605606079101562\n",
            "step 551: loss=3.0523834228515625\n",
            "step 552: loss=1.8818199634552002\n",
            "step 553: loss=1.507523536682129\n",
            "step 554: loss=1.9413862228393555\n",
            "step 555: loss=2.5440762042999268\n",
            "step 556: loss=1.9007201194763184\n",
            "step 557: loss=2.524850368499756\n",
            "step 558: loss=2.34330677986145\n",
            "step 559: loss=2.7639153003692627\n",
            "step 560: loss=2.845360040664673\n",
            "step 561: loss=3.1496694087982178\n",
            "step 562: loss=1.2812215089797974\n",
            "step 563: loss=1.965979814529419\n",
            "step 564: loss=2.2268362045288086\n",
            "step 565: loss=2.5769004821777344\n",
            "step 566: loss=3.7618019580841064\n",
            "step 567: loss=2.5611460208892822\n",
            "step 568: loss=2.4708263874053955\n",
            "step 569: loss=1.417123556137085\n",
            "step 570: loss=2.473639488220215\n",
            "step 571: loss=2.2695376873016357\n",
            "step 572: loss=2.584618091583252\n",
            "step 573: loss=1.5738215446472168\n",
            "step 574: loss=1.8628971576690674\n",
            "step 575: loss=2.9368233680725098\n",
            "step 576: loss=1.8124208450317383\n",
            "step 577: loss=1.8217229843139648\n",
            "step 578: loss=1.663659691810608\n",
            "step 579: loss=1.295406460762024\n",
            "step 580: loss=2.822096347808838\n",
            "step 581: loss=1.8343294858932495\n",
            "step 582: loss=2.195168972015381\n",
            "step 583: loss=1.7162911891937256\n",
            "step 584: loss=2.140528917312622\n",
            "step 585: loss=2.4911277294158936\n",
            "step 586: loss=2.7707560062408447\n",
            "step 587: loss=2.565255880355835\n",
            "step 588: loss=3.0481855869293213\n",
            "step 589: loss=2.8351082801818848\n",
            "step 590: loss=3.6792619228363037\n",
            "step 591: loss=2.7251696586608887\n",
            "step 592: loss=1.531751275062561\n",
            "step 593: loss=2.2892208099365234\n",
            "step 594: loss=1.360618233680725\n",
            "step 595: loss=4.170351028442383\n",
            "step 596: loss=1.6905732154846191\n",
            "step 597: loss=2.3966033458709717\n",
            "step 598: loss=2.5275402069091797\n",
            "step 599: loss=3.005890369415283\n",
            "step 600: loss=2.3826003074645996\n",
            "step 601: loss=2.2016963958740234\n",
            "step 602: loss=2.2998063564300537\n",
            "step 603: loss=2.6834847927093506\n",
            "step 604: loss=3.43703556060791\n",
            "step 605: loss=1.6343393325805664\n",
            "step 606: loss=3.243525505065918\n",
            "step 607: loss=2.8548831939697266\n",
            "step 608: loss=2.389930486679077\n",
            "step 609: loss=3.2453670501708984\n",
            "step 610: loss=2.2160186767578125\n",
            "step 611: loss=2.5093390941619873\n",
            "step 612: loss=3.754777431488037\n",
            "step 613: loss=3.2545266151428223\n",
            "step 614: loss=1.5355310440063477\n",
            "step 615: loss=1.7712321281433105\n",
            "step 616: loss=2.7397007942199707\n",
            "step 617: loss=2.290513753890991\n",
            "step 618: loss=1.934370517730713\n",
            "step 619: loss=2.3496766090393066\n",
            "step 620: loss=1.7980859279632568\n",
            "step 621: loss=1.4754672050476074\n",
            "step 622: loss=1.2631163597106934\n",
            "step 623: loss=2.0920987129211426\n",
            "step 624: loss=2.70458722114563\n",
            "step 625: loss=2.924757957458496\n",
            "step 626: loss=1.4453372955322266\n",
            "step 627: loss=3.64792537689209\n",
            "step 628: loss=2.2652640342712402\n",
            "step 629: loss=2.6496243476867676\n",
            "step 630: loss=1.4664955139160156\n",
            "step 631: loss=1.5993678569793701\n",
            "step 632: loss=2.129849910736084\n",
            "step 633: loss=2.80721116065979\n",
            "step 634: loss=1.5084824562072754\n",
            "step 635: loss=2.6098480224609375\n",
            "step 636: loss=1.640735387802124\n",
            "step 637: loss=2.3439533710479736\n",
            "step 638: loss=2.8184051513671875\n",
            "step 639: loss=1.9015414714813232\n",
            "step 640: loss=3.505195379257202\n",
            "step 641: loss=2.4596457481384277\n",
            "step 642: loss=2.2326929569244385\n",
            "step 643: loss=2.25039005279541\n",
            "step 644: loss=2.249326467514038\n",
            "step 645: loss=1.4279896020889282\n",
            "step 646: loss=2.3752264976501465\n",
            "step 647: loss=3.819251537322998\n",
            "step 648: loss=1.6101845502853394\n",
            "step 649: loss=2.726173162460327\n",
            "step 650: loss=2.315988063812256\n",
            "step 651: loss=2.41989803314209\n",
            "step 652: loss=2.3225488662719727\n",
            "step 653: loss=1.6190717220306396\n",
            "step 654: loss=2.6476261615753174\n",
            "step 655: loss=2.3507959842681885\n",
            "step 656: loss=2.602318286895752\n",
            "step 657: loss=1.565073847770691\n",
            "step 658: loss=2.9454355239868164\n",
            "step 659: loss=2.178337335586548\n",
            "step 660: loss=2.2002623081207275\n",
            "step 661: loss=3.580019950866699\n",
            "step 662: loss=2.88759183883667\n",
            "step 663: loss=2.6755547523498535\n",
            "step 664: loss=1.8928972482681274\n",
            "step 665: loss=2.48433780670166\n",
            "step 666: loss=1.7599374055862427\n",
            "step 667: loss=1.2485493421554565\n",
            "step 668: loss=2.932452440261841\n",
            "step 669: loss=2.6465249061584473\n",
            "step 670: loss=2.496339797973633\n",
            "step 671: loss=2.4565510749816895\n",
            "step 672: loss=2.679865598678589\n",
            "step 673: loss=2.305738687515259\n",
            "step 674: loss=1.5638383626937866\n",
            "step 675: loss=2.695335626602173\n",
            "step 676: loss=2.2831621170043945\n",
            "step 677: loss=2.2132647037506104\n",
            "step 678: loss=3.1620991230010986\n",
            "step 679: loss=2.6812431812286377\n",
            "step 680: loss=1.495452880859375\n",
            "step 681: loss=2.6098484992980957\n",
            "step 682: loss=2.021700382232666\n",
            "step 683: loss=2.092708110809326\n",
            "step 684: loss=3.509326934814453\n",
            "step 685: loss=2.7441341876983643\n",
            "step 686: loss=2.9591064453125\n",
            "step 687: loss=2.549801826477051\n",
            "step 688: loss=1.2113507986068726\n",
            "step 689: loss=3.128737449645996\n",
            "step 690: loss=1.5257008075714111\n",
            "step 691: loss=1.6061241626739502\n",
            "step 692: loss=1.9910728931427002\n",
            "step 693: loss=1.680955171585083\n",
            "step 694: loss=1.550674319267273\n",
            "step 695: loss=1.6425549983978271\n",
            "step 696: loss=2.3761587142944336\n",
            "step 697: loss=2.1227879524230957\n",
            "step 698: loss=2.374452829360962\n",
            "step 699: loss=2.2167129516601562\n",
            "step 700: loss=2.334932804107666\n",
            "step 701: loss=2.167573928833008\n",
            "step 702: loss=1.44703209400177\n",
            "step 703: loss=2.574751615524292\n",
            "step 704: loss=1.1000486612319946\n",
            "step 705: loss=2.623506784439087\n",
            "step 706: loss=2.3448455333709717\n",
            "step 707: loss=2.641730546951294\n",
            "step 708: loss=2.850374221801758\n",
            "step 709: loss=2.4035885334014893\n",
            "step 710: loss=1.6549372673034668\n",
            "step 711: loss=2.7574315071105957\n",
            "step 712: loss=2.811325788497925\n",
            "step 713: loss=2.6798343658447266\n",
            "step 714: loss=1.9846456050872803\n",
            "step 715: loss=2.2443151473999023\n",
            "step 716: loss=1.943162202835083\n",
            "step 717: loss=3.269174098968506\n",
            "step 718: loss=2.8795289993286133\n",
            "step 719: loss=1.9196670055389404\n",
            "step 720: loss=1.9940757751464844\n",
            "step 721: loss=2.6669533252716064\n",
            "step 722: loss=2.7927064895629883\n",
            "step 723: loss=2.3474957942962646\n",
            "step 724: loss=3.5755367279052734\n",
            "step 725: loss=2.7029309272766113\n",
            "step 726: loss=2.119117259979248\n",
            "step 727: loss=1.3571001291275024\n",
            "step 728: loss=2.5353775024414062\n",
            "step 729: loss=3.024970769882202\n",
            "step 730: loss=1.5199124813079834\n",
            "step 731: loss=2.7596421241760254\n",
            "step 732: loss=2.6212852001190186\n",
            "step 733: loss=1.7236120700836182\n",
            "step 734: loss=1.2225500345230103\n",
            "step 735: loss=2.1291236877441406\n",
            "step 736: loss=2.0214664936065674\n",
            "step 737: loss=2.879042625427246\n",
            "step 738: loss=2.6710457801818848\n",
            "step 739: loss=2.0260143280029297\n",
            "step 740: loss=2.198016405105591\n",
            "step 741: loss=2.43125057220459\n",
            "step 742: loss=2.413334846496582\n",
            "step 743: loss=2.949221134185791\n",
            "step 744: loss=2.176680564880371\n",
            "step 745: loss=2.2014682292938232\n",
            "step 746: loss=2.173898935317993\n",
            "step 747: loss=2.819784641265869\n",
            "step 748: loss=2.758587598800659\n",
            "step 749: loss=2.3081295490264893\n",
            "step 750: loss=3.1063785552978516\n",
            "step 751: loss=3.447319269180298\n",
            "step 752: loss=1.3664271831512451\n",
            "step 753: loss=2.314589738845825\n",
            "step 754: loss=2.399427890777588\n",
            "step 755: loss=2.7140588760375977\n",
            "step 756: loss=2.6138463020324707\n",
            "step 757: loss=2.8753204345703125\n",
            "step 758: loss=2.2743804454803467\n",
            "step 759: loss=1.4174675941467285\n",
            "step 760: loss=2.8722457885742188\n",
            "step 761: loss=2.1775622367858887\n",
            "step 762: loss=2.678496837615967\n",
            "step 763: loss=2.9239606857299805\n",
            "step 764: loss=1.7838311195373535\n",
            "step 765: loss=2.4178497791290283\n",
            "step 766: loss=2.9219415187835693\n",
            "step 767: loss=2.046964168548584\n",
            "step 768: loss=3.6102209091186523\n",
            "step 769: loss=2.3411900997161865\n",
            "step 770: loss=2.1161930561065674\n",
            "step 771: loss=1.7968926429748535\n",
            "step 772: loss=3.1634347438812256\n",
            "step 773: loss=3.075334310531616\n",
            "step 774: loss=2.2097275257110596\n",
            "step 775: loss=1.0598238706588745\n",
            "step 776: loss=3.897688865661621\n",
            "step 777: loss=2.621487855911255\n",
            "step 778: loss=3.0920252799987793\n",
            "step 779: loss=1.8525879383087158\n",
            "step 780: loss=2.8117387294769287\n",
            "step 781: loss=1.4782568216323853\n",
            "step 782: loss=2.0426113605499268\n",
            "step 783: loss=2.3653247356414795\n",
            "step 784: loss=2.4146924018859863\n",
            "step 785: loss=1.379044532775879\n",
            "step 786: loss=1.788468360900879\n",
            "step 787: loss=2.831237554550171\n",
            "step 788: loss=2.1625523567199707\n",
            "step 789: loss=3.1349234580993652\n",
            "step 790: loss=2.578227996826172\n",
            "step 791: loss=2.4254508018493652\n",
            "step 792: loss=2.7070276737213135\n",
            "step 793: loss=2.1344687938690186\n",
            "step 794: loss=2.078580617904663\n",
            "step 795: loss=2.394829273223877\n",
            "step 796: loss=1.3201344013214111\n",
            "step 797: loss=1.7640334367752075\n",
            "step 798: loss=2.1216535568237305\n",
            "step 799: loss=3.0591228008270264\n",
            "step 800: loss=2.5462801456451416\n",
            "step 801: loss=2.1617133617401123\n",
            "step 802: loss=2.016702175140381\n",
            "step 803: loss=1.5508596897125244\n",
            "step 804: loss=1.5301145315170288\n",
            "step 805: loss=2.0135250091552734\n",
            "step 806: loss=1.3868016004562378\n",
            "step 807: loss=2.26713228225708\n",
            "step 808: loss=2.3278968334198\n",
            "step 809: loss=1.985727071762085\n",
            "step 810: loss=1.3245265483856201\n",
            "step 811: loss=1.6601743698120117\n",
            "step 812: loss=2.5737197399139404\n",
            "step 813: loss=1.9246803522109985\n",
            "step 814: loss=2.8885390758514404\n",
            "step 815: loss=1.7356810569763184\n",
            "step 816: loss=3.334552764892578\n",
            "step 817: loss=2.3281660079956055\n",
            "step 818: loss=1.8482463359832764\n",
            "step 819: loss=1.2458761930465698\n",
            "step 820: loss=1.8804347515106201\n",
            "step 821: loss=1.8886088132858276\n",
            "step 822: loss=3.3523402214050293\n",
            "step 823: loss=3.1204349994659424\n",
            "step 824: loss=2.9650797843933105\n",
            "step 825: loss=2.1562280654907227\n",
            "step 826: loss=3.1428608894348145\n",
            "step 827: loss=1.785048246383667\n",
            "step 828: loss=2.308678150177002\n",
            "step 829: loss=2.183675765991211\n",
            "step 830: loss=1.6731538772583008\n",
            "step 831: loss=2.11592960357666\n",
            "step 832: loss=2.2769827842712402\n",
            "step 833: loss=1.76935875415802\n",
            "step 834: loss=2.503162145614624\n",
            "step 835: loss=2.232351541519165\n",
            "step 836: loss=2.105062961578369\n",
            "step 837: loss=1.584532380104065\n",
            "step 838: loss=1.484620213508606\n",
            "step 839: loss=2.2268199920654297\n",
            "step 840: loss=2.2262566089630127\n",
            "step 841: loss=1.8311495780944824\n",
            "step 842: loss=2.083104372024536\n",
            "step 843: loss=2.264904499053955\n",
            "step 844: loss=3.5743813514709473\n",
            "step 845: loss=2.5358734130859375\n",
            "step 846: loss=1.4305412769317627\n",
            "step 847: loss=2.7285280227661133\n",
            "step 848: loss=2.0668156147003174\n",
            "step 849: loss=2.779129981994629\n",
            "step 850: loss=2.2931904792785645\n",
            "step 851: loss=2.6882340908050537\n",
            "step 852: loss=2.2102537155151367\n",
            "step 853: loss=2.7215235233306885\n",
            "step 854: loss=3.4909229278564453\n",
            "step 855: loss=2.8081820011138916\n",
            "step 856: loss=4.024091720581055\n",
            "step 857: loss=2.981457233428955\n",
            "step 858: loss=2.922752857208252\n",
            "step 859: loss=2.0271570682525635\n",
            "step 860: loss=3.192188262939453\n",
            "step 861: loss=1.0022385120391846\n",
            "step 862: loss=2.0809061527252197\n",
            "step 863: loss=2.2970118522644043\n",
            "step 864: loss=2.5457775592803955\n",
            "step 865: loss=1.9834542274475098\n",
            "step 866: loss=2.0241243839263916\n",
            "step 867: loss=1.8505768775939941\n",
            "step 868: loss=2.842897415161133\n",
            "step 869: loss=1.6856493949890137\n",
            "step 870: loss=1.6916557550430298\n",
            "step 871: loss=1.2438843250274658\n",
            "step 872: loss=2.5950565338134766\n",
            "step 873: loss=2.223424196243286\n",
            "step 874: loss=2.481534719467163\n",
            "step 875: loss=2.530003786087036\n",
            "step 876: loss=3.076472759246826\n",
            "step 877: loss=1.317960262298584\n",
            "step 878: loss=1.7283875942230225\n",
            "step 879: loss=2.6510746479034424\n",
            "step 880: loss=2.212265968322754\n",
            "step 881: loss=2.378565788269043\n",
            "step 882: loss=3.158229351043701\n",
            "step 883: loss=1.4144649505615234\n",
            "step 884: loss=2.6878604888916016\n",
            "step 885: loss=1.6356935501098633\n",
            "step 886: loss=2.7919962406158447\n",
            "step 887: loss=2.7722795009613037\n",
            "step 888: loss=2.7426199913024902\n",
            "step 889: loss=4.037431716918945\n",
            "step 890: loss=2.2298786640167236\n",
            "step 891: loss=2.554527521133423\n",
            "step 892: loss=1.842656135559082\n",
            "step 893: loss=1.920966625213623\n",
            "step 894: loss=1.7163323163986206\n",
            "step 895: loss=2.450195074081421\n",
            "step 896: loss=2.5979063510894775\n",
            "step 897: loss=2.057121992111206\n",
            "step 898: loss=1.8970675468444824\n",
            "step 899: loss=0.9963932037353516\n",
            "step 900: loss=2.00344181060791\n",
            "step 901: loss=2.5597963333129883\n",
            "step 902: loss=1.340796947479248\n",
            "step 903: loss=2.1988890171051025\n",
            "step 904: loss=2.4157848358154297\n",
            "step 905: loss=3.12302565574646\n",
            "step 906: loss=2.466625928878784\n",
            "step 907: loss=2.4708781242370605\n",
            "step 908: loss=2.617004632949829\n",
            "step 909: loss=2.707047700881958\n",
            "step 910: loss=2.008483648300171\n",
            "step 911: loss=2.6421470642089844\n",
            "step 912: loss=3.3586654663085938\n",
            "step 913: loss=1.3489688634872437\n",
            "step 914: loss=1.956845760345459\n",
            "step 915: loss=3.0731050968170166\n",
            "step 916: loss=2.194636821746826\n",
            "step 917: loss=1.4363572597503662\n",
            "step 918: loss=3.198756694793701\n",
            "step 919: loss=2.207362413406372\n",
            "step 920: loss=1.9283784627914429\n",
            "step 921: loss=1.2888509035110474\n",
            "step 922: loss=3.4281973838806152\n",
            "step 923: loss=2.2186055183410645\n",
            "step 924: loss=2.022179126739502\n",
            "step 925: loss=1.5552706718444824\n",
            "step 926: loss=1.85262930393219\n",
            "step 927: loss=2.4584686756134033\n",
            "step 928: loss=2.762091636657715\n",
            "step 929: loss=2.315365791320801\n",
            "step 930: loss=1.751189947128296\n",
            "step 931: loss=1.9830150604248047\n",
            "step 932: loss=2.5045082569122314\n",
            "step 933: loss=1.9394564628601074\n",
            "step 934: loss=3.4933741092681885\n",
            "step 935: loss=2.500746011734009\n",
            "step 936: loss=2.8871634006500244\n",
            "step 937: loss=1.4493745565414429\n",
            "step 938: loss=2.5450491905212402\n",
            "step 939: loss=2.493314027786255\n",
            "step 940: loss=2.538501262664795\n",
            "step 941: loss=2.3186869621276855\n",
            "step 942: loss=1.9653781652450562\n",
            "step 943: loss=2.137615442276001\n",
            "step 944: loss=3.5132484436035156\n",
            "step 945: loss=1.704443335533142\n",
            "step 946: loss=3.0681047439575195\n",
            "step 947: loss=1.6371575593948364\n",
            "step 948: loss=1.6108871698379517\n",
            "step 949: loss=2.5912954807281494\n",
            "step 950: loss=2.2177464962005615\n",
            "step 951: loss=1.9986492395401\n",
            "step 952: loss=2.0294570922851562\n",
            "step 953: loss=2.515213966369629\n",
            "step 954: loss=1.5794419050216675\n",
            "step 955: loss=3.413130044937134\n",
            "step 956: loss=2.253105878829956\n",
            "step 957: loss=2.554239511489868\n",
            "step 958: loss=2.990266799926758\n",
            "step 959: loss=2.355329751968384\n",
            "step 960: loss=1.6812915802001953\n",
            "step 961: loss=1.743807315826416\n",
            "step 962: loss=2.1946165561676025\n",
            "step 963: loss=2.939544439315796\n",
            "step 964: loss=2.5611138343811035\n",
            "step 965: loss=2.5738492012023926\n",
            "step 966: loss=3.370220184326172\n",
            "step 967: loss=2.3166706562042236\n",
            "step 968: loss=2.2047595977783203\n",
            "step 969: loss=2.4156906604766846\n",
            "step 970: loss=1.1211817264556885\n",
            "step 971: loss=1.2492706775665283\n",
            "step 972: loss=3.076132297515869\n",
            "step 973: loss=2.128228187561035\n",
            "step 974: loss=2.930509567260742\n",
            "step 975: loss=3.1567530632019043\n",
            "step 976: loss=1.1918926239013672\n",
            "step 977: loss=2.56075119972229\n",
            "step 978: loss=2.3704686164855957\n",
            "step 979: loss=2.1774184703826904\n",
            "step 980: loss=2.9620285034179688\n",
            "step 981: loss=2.840249538421631\n",
            "step 982: loss=1.1691899299621582\n",
            "step 983: loss=3.3349695205688477\n",
            "step 984: loss=1.4377344846725464\n",
            "step 985: loss=2.8290247917175293\n",
            "step 986: loss=1.4145793914794922\n",
            "step 987: loss=2.834582567214966\n",
            "step 988: loss=1.2110522985458374\n",
            "step 989: loss=2.6331002712249756\n",
            "step 990: loss=3.0765039920806885\n",
            "step 991: loss=2.732036590576172\n",
            "step 992: loss=2.7122790813446045\n",
            "step 993: loss=1.6272233724594116\n",
            "step 994: loss=3.421574592590332\n",
            "step 995: loss=2.9440228939056396\n",
            "step 996: loss=1.4241355657577515\n",
            "step 997: loss=1.6645926237106323\n",
            "step 998: loss=1.6388745307922363\n",
            "step 999: loss=1.8581149578094482\n",
            "step 1000: loss=2.997143268585205\n",
            "Mean loss        2.2755096\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 1 4 1 7 1 5 4 1 1]\n"
          ]
        }
      ],
      "source": [
        "lr = optax.cosine_decay_schedule(init_value=1e-3, decay_steps=20)\n",
        "optimizer = optax.adamw(lr, weight_decay=1e-4, mask=mask_fn)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dd672339",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=2.430816650390625\n",
            "step 2: loss=1.5101913213729858\n",
            "step 3: loss=3.2343151569366455\n",
            "step 4: loss=1.6777729988098145\n",
            "step 5: loss=2.034066677093506\n",
            "step 6: loss=1.636694073677063\n",
            "step 7: loss=2.13456130027771\n",
            "step 8: loss=2.250941276550293\n",
            "step 9: loss=1.3820940256118774\n",
            "step 10: loss=2.4763283729553223\n",
            "step 11: loss=2.7123100757598877\n",
            "step 12: loss=3.4853312969207764\n",
            "step 13: loss=1.7098143100738525\n",
            "step 14: loss=1.984086275100708\n",
            "step 15: loss=3.205305576324463\n",
            "step 16: loss=3.001132011413574\n",
            "step 17: loss=1.0761189460754395\n",
            "step 18: loss=2.6243045330047607\n",
            "step 19: loss=2.5216028690338135\n",
            "step 20: loss=1.3572767972946167\n",
            "step 21: loss=1.971611738204956\n",
            "step 22: loss=2.129270076751709\n",
            "step 23: loss=1.6986463069915771\n",
            "step 24: loss=2.4057188034057617\n",
            "step 25: loss=1.6564723253250122\n",
            "step 26: loss=2.4194247722625732\n",
            "step 27: loss=2.6503312587738037\n",
            "step 28: loss=2.3030922412872314\n",
            "step 29: loss=2.2875442504882812\n",
            "step 30: loss=1.3227665424346924\n",
            "step 31: loss=1.9616154432296753\n",
            "step 32: loss=2.0181641578674316\n",
            "step 33: loss=1.5724323987960815\n",
            "step 34: loss=1.2824522256851196\n",
            "step 35: loss=1.7038906812667847\n",
            "step 36: loss=1.9637864828109741\n",
            "step 37: loss=2.7566580772399902\n",
            "step 38: loss=1.3124538660049438\n",
            "step 39: loss=2.606201648712158\n",
            "step 40: loss=1.8863856792449951\n",
            "step 41: loss=0.6819286346435547\n",
            "step 42: loss=2.295436382293701\n",
            "step 43: loss=2.0027084350585938\n",
            "step 44: loss=2.007981777191162\n",
            "step 45: loss=2.383359432220459\n",
            "step 46: loss=0.7342274188995361\n",
            "step 47: loss=1.890496015548706\n",
            "step 48: loss=1.6937888860702515\n",
            "step 49: loss=3.0425069332122803\n",
            "step 50: loss=1.3755284547805786\n",
            "step 51: loss=1.0622296333312988\n",
            "step 52: loss=1.1108752489089966\n",
            "step 53: loss=3.169288158416748\n",
            "step 54: loss=1.2115046977996826\n",
            "step 55: loss=1.2280232906341553\n",
            "step 56: loss=2.6092538833618164\n",
            "step 57: loss=2.665940284729004\n",
            "step 58: loss=1.124311089515686\n",
            "step 59: loss=2.565053939819336\n",
            "step 60: loss=0.9089148044586182\n",
            "step 61: loss=0.6878364086151123\n",
            "step 62: loss=0.8675327301025391\n",
            "step 63: loss=4.203286170959473\n",
            "step 64: loss=2.7563278675079346\n",
            "step 65: loss=2.5254228115081787\n",
            "step 66: loss=0.19264483451843262\n",
            "step 67: loss=0.700934648513794\n",
            "step 68: loss=0.2755255699157715\n",
            "step 69: loss=3.7137718200683594\n",
            "step 70: loss=1.044548511505127\n",
            "step 71: loss=3.424377679824829\n",
            "step 72: loss=3.167172431945801\n",
            "step 73: loss=0.3443877696990967\n",
            "step 74: loss=0.6741211414337158\n",
            "step 75: loss=0.40306878089904785\n",
            "step 76: loss=0.04297971725463867\n",
            "step 77: loss=2.602733850479126\n",
            "step 78: loss=1.9870998859405518\n",
            "step 79: loss=1.1122770309448242\n",
            "step 80: loss=0.13094282150268555\n",
            "step 81: loss=0.10486650466918945\n",
            "step 82: loss=0.5019204616546631\n",
            "step 83: loss=1.1483359336853027\n",
            "step 84: loss=0.4780435562133789\n",
            "step 85: loss=2.7337918281555176\n",
            "step 86: loss=0.19182300567626953\n",
            "step 87: loss=2.761962413787842\n",
            "step 88: loss=0.16903257369995117\n",
            "step 89: loss=0.4720478057861328\n",
            "step 90: loss=1.15058434009552\n",
            "step 91: loss=0.8823702335357666\n",
            "step 92: loss=0.832001805305481\n",
            "step 93: loss=0.04198122024536133\n",
            "step 94: loss=2.076871871948242\n",
            "step 95: loss=0.4427204132080078\n",
            "step 96: loss=2.1423211097717285\n",
            "step 97: loss=0.9466767311096191\n",
            "step 98: loss=0.8943672180175781\n",
            "step 99: loss=0.6860325336456299\n",
            "step 100: loss=0.3266263008117676\n",
            "step 101: loss=1.4713616371154785\n",
            "step 102: loss=0.7329425811767578\n",
            "step 103: loss=2.8202481269836426\n",
            "step 104: loss=0.7853033542633057\n",
            "step 105: loss=0.4279038906097412\n",
            "step 106: loss=2.15079665184021\n",
            "step 107: loss=1.6970298290252686\n",
            "step 108: loss=0.40688371658325195\n",
            "step 109: loss=1.4725710153579712\n",
            "step 110: loss=0.7395162582397461\n",
            "step 111: loss=0.3150827884674072\n",
            "step 112: loss=0.12813568115234375\n",
            "step 113: loss=0.6697661876678467\n",
            "step 114: loss=1.4820427894592285\n",
            "step 115: loss=0.05836915969848633\n",
            "step 116: loss=0.7087745666503906\n",
            "step 117: loss=0.7028727531433105\n",
            "step 118: loss=0.5073907375335693\n",
            "step 119: loss=1.0387604236602783\n",
            "step 120: loss=1.9149961471557617\n",
            "step 121: loss=2.9595119953155518\n",
            "step 122: loss=0.8235645294189453\n",
            "step 123: loss=2.4234461784362793\n",
            "step 124: loss=0.40535688400268555\n",
            "step 125: loss=0.08510112762451172\n",
            "step 126: loss=1.4432271718978882\n",
            "step 127: loss=1.4649779796600342\n",
            "step 128: loss=0.38213157653808594\n",
            "step 129: loss=0.11646699905395508\n",
            "step 130: loss=0.34304046630859375\n",
            "step 131: loss=0.567082405090332\n",
            "step 132: loss=2.074583053588867\n",
            "step 133: loss=0.2864189147949219\n",
            "step 134: loss=0.11138677597045898\n",
            "step 135: loss=0.25486040115356445\n",
            "step 136: loss=0.3455190658569336\n",
            "step 137: loss=0.47538280487060547\n",
            "step 138: loss=0.14723777770996094\n",
            "step 139: loss=0.24801111221313477\n",
            "step 140: loss=1.323771595954895\n",
            "step 141: loss=0.16408061981201172\n",
            "step 142: loss=2.4303994178771973\n",
            "step 143: loss=0.08760452270507812\n",
            "step 144: loss=1.7610883712768555\n",
            "step 145: loss=0.03139305114746094\n",
            "step 146: loss=1.2534289360046387\n",
            "step 147: loss=1.4303553104400635\n",
            "step 148: loss=1.2394180297851562\n",
            "step 149: loss=0.26124000549316406\n",
            "step 150: loss=0.08202362060546875\n",
            "step 151: loss=1.3903378248214722\n",
            "step 152: loss=0.14835500717163086\n",
            "step 153: loss=0.6335842609405518\n",
            "step 154: loss=0.3799571990966797\n",
            "step 155: loss=0.5997271537780762\n",
            "step 156: loss=0.30876588821411133\n",
            "step 157: loss=0.7495439052581787\n",
            "step 158: loss=2.5246529579162598\n",
            "step 159: loss=0.17488574981689453\n",
            "step 160: loss=0.6876530647277832\n",
            "step 161: loss=0.11330652236938477\n",
            "step 162: loss=0.10636091232299805\n",
            "step 163: loss=0.3595294952392578\n",
            "step 164: loss=0.46627378463745117\n",
            "step 165: loss=1.063812494277954\n",
            "step 166: loss=0.10440397262573242\n",
            "step 167: loss=2.0270068645477295\n",
            "step 168: loss=2.1881322860717773\n",
            "step 169: loss=0.22011470794677734\n",
            "step 170: loss=0.40134096145629883\n",
            "step 171: loss=0.0993051528930664\n",
            "step 172: loss=0.21466636657714844\n",
            "step 173: loss=0.009157657623291016\n",
            "step 174: loss=0.30261754989624023\n",
            "step 175: loss=1.2077977657318115\n",
            "step 176: loss=0.22098922729492188\n",
            "step 177: loss=0.6972386837005615\n",
            "step 178: loss=0.43668222427368164\n",
            "step 179: loss=0.07398176193237305\n",
            "step 180: loss=6.407944679260254\n",
            "step 181: loss=0.04528045654296875\n",
            "step 182: loss=0.013095378875732422\n",
            "step 183: loss=0.8023326396942139\n",
            "step 184: loss=1.6341822147369385\n",
            "step 185: loss=0.6475319862365723\n",
            "step 186: loss=0.2610964775085449\n",
            "step 187: loss=0.6413688659667969\n",
            "step 188: loss=1.9672443866729736\n",
            "step 189: loss=0.18943500518798828\n",
            "step 190: loss=0.19222545623779297\n",
            "step 191: loss=0.01690387725830078\n",
            "step 192: loss=0.014261245727539062\n",
            "step 193: loss=0.02270793914794922\n",
            "step 194: loss=0.16132450103759766\n",
            "step 195: loss=0.003996849060058594\n",
            "step 196: loss=1.0043296813964844\n",
            "step 197: loss=0.9871828556060791\n",
            "step 198: loss=0.03763628005981445\n",
            "step 199: loss=4.82365608215332\n",
            "step 200: loss=2.4402215480804443\n",
            "step 201: loss=0.007051944732666016\n",
            "step 202: loss=1.7774583101272583\n",
            "step 203: loss=0.05864429473876953\n",
            "step 204: loss=0.013776302337646484\n",
            "step 205: loss=0.08152008056640625\n",
            "step 206: loss=0.2081308364868164\n",
            "step 207: loss=0.06054067611694336\n",
            "step 208: loss=0.6546964645385742\n",
            "step 209: loss=0.015395641326904297\n",
            "step 210: loss=0.1451559066772461\n",
            "step 211: loss=0.11124897003173828\n",
            "step 212: loss=1.2685301303863525\n",
            "step 213: loss=1.2463016510009766\n",
            "step 214: loss=1.8063714504241943\n",
            "step 215: loss=0.005324363708496094\n",
            "step 216: loss=0.3416175842285156\n",
            "step 217: loss=0.24512290954589844\n",
            "step 218: loss=0.014131546020507812\n",
            "step 219: loss=0.2784104347229004\n",
            "step 220: loss=0.1787886619567871\n",
            "step 221: loss=0.3405642509460449\n",
            "step 222: loss=2.5907583236694336\n",
            "step 223: loss=0.6824519634246826\n",
            "step 224: loss=0.23347759246826172\n",
            "step 225: loss=0.21041488647460938\n",
            "step 226: loss=0.4104282855987549\n",
            "step 227: loss=0.06784963607788086\n",
            "step 228: loss=0.05543708801269531\n",
            "step 229: loss=0.26860570907592773\n",
            "step 230: loss=0.3541254997253418\n",
            "step 231: loss=0.655205249786377\n",
            "step 232: loss=0.11911201477050781\n",
            "step 233: loss=0.00014591217041015625\n",
            "step 234: loss=2.5747179985046387\n",
            "step 235: loss=0.07790565490722656\n",
            "step 236: loss=0.5849583148956299\n",
            "step 237: loss=0.01661968231201172\n",
            "step 238: loss=0.6021947860717773\n",
            "step 239: loss=0.14755773544311523\n",
            "step 240: loss=0.11648035049438477\n",
            "step 241: loss=0.1120610237121582\n",
            "step 242: loss=0.1345353126525879\n",
            "step 243: loss=1.6484403610229492\n",
            "step 244: loss=0.11362123489379883\n",
            "step 245: loss=0.08615255355834961\n",
            "step 246: loss=0.8325786590576172\n",
            "step 247: loss=0.41167163848876953\n",
            "step 248: loss=6.056793689727783\n",
            "step 249: loss=0.008564949035644531\n",
            "step 250: loss=0.004367828369140625\n",
            "step 251: loss=3.2801270484924316\n",
            "step 252: loss=0.04525947570800781\n",
            "step 253: loss=0.12401723861694336\n",
            "step 254: loss=0.020478248596191406\n",
            "step 255: loss=0.011260986328125\n",
            "step 256: loss=0.29513978958129883\n",
            "step 257: loss=2.8279666900634766\n",
            "step 258: loss=0.7695465087890625\n",
            "step 259: loss=0.3043556213378906\n",
            "step 260: loss=0.30303287506103516\n",
            "step 261: loss=0.6711359024047852\n",
            "step 262: loss=3.3361692428588867\n",
            "step 263: loss=0.01773834228515625\n",
            "step 264: loss=0.1204228401184082\n",
            "step 265: loss=0.06874227523803711\n",
            "step 266: loss=0.3681826591491699\n",
            "step 267: loss=0.061634063720703125\n",
            "step 268: loss=0.10282230377197266\n",
            "step 269: loss=0.0012683868408203125\n",
            "step 270: loss=0.489285945892334\n",
            "step 271: loss=0.0012636184692382812\n",
            "step 272: loss=0.1784195899963379\n",
            "step 273: loss=0.12223005294799805\n",
            "step 274: loss=0.008587837219238281\n",
            "step 275: loss=1.1145339012145996\n",
            "step 276: loss=0.005671977996826172\n",
            "step 277: loss=0.07019519805908203\n",
            "step 278: loss=0.20012712478637695\n",
            "step 279: loss=0.08238601684570312\n",
            "step 280: loss=0.06807231903076172\n",
            "step 281: loss=0.004179954528808594\n",
            "step 282: loss=0.004910469055175781\n",
            "step 283: loss=3.1206231117248535\n",
            "step 284: loss=3.1687474250793457\n",
            "step 285: loss=0.6413402557373047\n",
            "step 286: loss=0.41305971145629883\n",
            "step 287: loss=0.0372314453125\n",
            "step 288: loss=0.0009832382202148438\n",
            "step 289: loss=1.920060396194458\n",
            "step 290: loss=0.06899070739746094\n",
            "step 291: loss=0.1879119873046875\n",
            "step 292: loss=0.9943466186523438\n",
            "step 293: loss=0.0006084442138671875\n",
            "step 294: loss=0.00041103363037109375\n",
            "step 295: loss=0.09590339660644531\n",
            "step 296: loss=0.24095916748046875\n",
            "step 297: loss=0.1582484245300293\n",
            "step 298: loss=0.00020313262939453125\n",
            "step 299: loss=0.00022125244140625\n",
            "step 300: loss=0.0002155303955078125\n",
            "step 301: loss=0.0039043426513671875\n",
            "step 302: loss=4.238811492919922\n",
            "step 303: loss=0.04584026336669922\n",
            "step 304: loss=0.03665018081665039\n",
            "step 305: loss=0.07413053512573242\n",
            "step 306: loss=0.35274457931518555\n",
            "step 307: loss=0.40444231033325195\n",
            "step 308: loss=0.0002689361572265625\n",
            "step 309: loss=0.06497049331665039\n",
            "step 310: loss=0.23946094512939453\n",
            "step 311: loss=0.6939427852630615\n",
            "step 312: loss=0.5034165382385254\n",
            "step 313: loss=0.0006380081176757812\n",
            "step 314: loss=0.013569831848144531\n",
            "step 315: loss=0.0058383941650390625\n",
            "step 316: loss=0.7530632019042969\n",
            "step 317: loss=0.048696041107177734\n",
            "step 318: loss=0.19825124740600586\n",
            "step 319: loss=0.11652421951293945\n",
            "step 320: loss=0.015484809875488281\n",
            "step 321: loss=0.7882394790649414\n",
            "step 322: loss=0.025994300842285156\n",
            "step 323: loss=0.00043487548828125\n",
            "step 324: loss=0.007741451263427734\n",
            "step 325: loss=0.04334449768066406\n",
            "step 326: loss=1.8152194023132324\n",
            "step 327: loss=0.0748910903930664\n",
            "step 328: loss=7.2733259201049805\n",
            "step 329: loss=8.58306884765625e-06\n",
            "step 330: loss=0.31049633026123047\n",
            "step 331: loss=1.33514404296875e-05\n",
            "step 332: loss=0.012402534484863281\n",
            "step 333: loss=0.00025844573974609375\n",
            "step 334: loss=0.026798248291015625\n",
            "step 335: loss=0.2419581413269043\n",
            "step 336: loss=0.1682109832763672\n",
            "step 337: loss=0.22863483428955078\n",
            "step 338: loss=0.02817392349243164\n",
            "step 339: loss=0.08152627944946289\n",
            "step 340: loss=0.023679733276367188\n",
            "step 341: loss=4.348959922790527\n",
            "step 342: loss=0.6528491973876953\n",
            "step 343: loss=0.00010013580322265625\n",
            "step 344: loss=0.9601335525512695\n",
            "step 345: loss=0.013706207275390625\n",
            "step 346: loss=0.0009527206420898438\n",
            "step 347: loss=0.32802915573120117\n",
            "step 348: loss=0.030027389526367188\n",
            "step 349: loss=0.014661788940429688\n",
            "step 350: loss=4.76837158203125e-06\n",
            "step 351: loss=0.010106086730957031\n",
            "step 352: loss=0.1157083511352539\n",
            "step 353: loss=2.108105182647705\n",
            "step 354: loss=0.033129215240478516\n",
            "step 355: loss=2.7838921546936035\n",
            "step 356: loss=0.0004673004150390625\n",
            "step 357: loss=0.031528472900390625\n",
            "step 358: loss=0.5952281951904297\n",
            "step 359: loss=0.054457664489746094\n",
            "step 360: loss=0.00045680999755859375\n",
            "step 361: loss=0.11643505096435547\n",
            "step 362: loss=0.015880584716796875\n",
            "step 363: loss=2.78996205329895\n",
            "step 364: loss=0.0052947998046875\n",
            "step 365: loss=1.8068511486053467\n",
            "step 366: loss=0.9828100204467773\n",
            "step 367: loss=0.024313926696777344\n",
            "step 368: loss=6.961822509765625e-05\n",
            "step 369: loss=0.0029516220092773438\n",
            "step 370: loss=1.45013427734375\n",
            "step 371: loss=0.08695316314697266\n",
            "step 372: loss=0.0041599273681640625\n",
            "step 373: loss=0.0053005218505859375\n",
            "step 374: loss=0.053905487060546875\n",
            "step 375: loss=0.06104278564453125\n",
            "step 376: loss=0.3504014015197754\n",
            "step 377: loss=1.194603443145752\n",
            "step 378: loss=0.5086808204650879\n",
            "step 379: loss=2.693997859954834\n",
            "step 380: loss=0.003055572509765625\n",
            "step 381: loss=0.00047206878662109375\n",
            "step 382: loss=0.19910001754760742\n",
            "step 383: loss=0.02656698226928711\n",
            "step 384: loss=5.7220458984375e-06\n",
            "step 385: loss=0.08241081237792969\n",
            "step 386: loss=0.06575775146484375\n",
            "step 387: loss=0.2118377685546875\n",
            "step 388: loss=0.0017843246459960938\n",
            "step 389: loss=0.008601188659667969\n",
            "step 390: loss=0.5536222457885742\n",
            "step 391: loss=0.036644935607910156\n",
            "step 392: loss=0.21162939071655273\n",
            "step 393: loss=1.6224966049194336\n",
            "step 394: loss=5.356033802032471\n",
            "step 395: loss=0.06775474548339844\n",
            "step 396: loss=0.00402069091796875\n",
            "step 397: loss=0.0026988983154296875\n",
            "step 398: loss=1.158644437789917\n",
            "step 399: loss=0.22422075271606445\n",
            "step 400: loss=0.0036258697509765625\n",
            "step 401: loss=0.03632831573486328\n",
            "step 402: loss=0.13716888427734375\n",
            "step 403: loss=6.29425048828125e-05\n",
            "step 404: loss=0.00083160400390625\n",
            "step 405: loss=0.009955406188964844\n",
            "step 406: loss=0.177978515625\n",
            "step 407: loss=0.015786170959472656\n",
            "step 408: loss=0.000217437744140625\n",
            "step 409: loss=6.794549465179443\n",
            "step 410: loss=0.0017833709716796875\n",
            "step 411: loss=1.8245465755462646\n",
            "step 412: loss=0.0006656646728515625\n",
            "step 413: loss=0.3454475402832031\n",
            "step 414: loss=0.001800537109375\n",
            "step 415: loss=9.5367431640625e-05\n",
            "step 416: loss=3.4716978073120117\n",
            "step 417: loss=0.15312623977661133\n",
            "step 418: loss=0.0031538009643554688\n",
            "step 419: loss=2.09041428565979\n",
            "step 420: loss=0.056540489196777344\n",
            "step 421: loss=0.023670673370361328\n",
            "step 422: loss=0.15976905822753906\n",
            "step 423: loss=0.0040111541748046875\n",
            "step 424: loss=0.0023679733276367188\n",
            "step 425: loss=1.1924023628234863\n",
            "step 426: loss=0.4451909065246582\n",
            "step 427: loss=0.0683889389038086\n",
            "step 428: loss=0.28870630264282227\n",
            "step 429: loss=1.1532440185546875\n",
            "step 430: loss=0.0012264251708984375\n",
            "step 431: loss=0.31137990951538086\n",
            "step 432: loss=0.012101173400878906\n",
            "step 433: loss=0.049210548400878906\n",
            "step 434: loss=1.0427517890930176\n",
            "step 435: loss=0.016333580017089844\n",
            "step 436: loss=0.3576669692993164\n",
            "step 437: loss=0.0026445388793945312\n",
            "step 438: loss=5.14984130859375e-05\n",
            "step 439: loss=0.7507786750793457\n",
            "step 440: loss=0.024170875549316406\n",
            "step 441: loss=0.001567840576171875\n",
            "step 442: loss=0.029513835906982422\n",
            "step 443: loss=5.519654273986816\n",
            "step 444: loss=0.8343808650970459\n",
            "step 445: loss=0.013663291931152344\n",
            "step 446: loss=0.1986103057861328\n",
            "step 447: loss=0.001781463623046875\n",
            "step 448: loss=0.05141735076904297\n",
            "step 449: loss=0.37174463272094727\n",
            "step 450: loss=3.6205577850341797\n",
            "step 451: loss=3.814697265625e-06\n",
            "step 452: loss=3.600924491882324\n",
            "step 453: loss=0.0003986358642578125\n",
            "step 454: loss=1.7776689529418945\n",
            "step 455: loss=0.47640514373779297\n",
            "step 456: loss=5.38460636138916\n",
            "step 457: loss=0.6379804611206055\n",
            "step 458: loss=2.8752448558807373\n",
            "step 459: loss=0.018686294555664062\n",
            "step 460: loss=0.0003871917724609375\n",
            "step 461: loss=4.7113447189331055\n",
            "step 462: loss=0.01566600799560547\n",
            "step 463: loss=0.05087709426879883\n",
            "step 464: loss=0.00012969970703125\n",
            "step 465: loss=0.09293842315673828\n",
            "step 466: loss=1.1444091796875e-05\n",
            "step 467: loss=2.1898598670959473\n",
            "step 468: loss=0.11582469940185547\n",
            "step 469: loss=0.008595466613769531\n",
            "step 470: loss=0.0775156021118164\n",
            "step 471: loss=3.3813962936401367\n",
            "step 472: loss=0.19826126098632812\n",
            "step 473: loss=0.036701202392578125\n",
            "step 474: loss=0.1555323600769043\n",
            "step 475: loss=0.012220382690429688\n",
            "step 476: loss=0.19070911407470703\n",
            "step 477: loss=0.012537002563476562\n",
            "step 478: loss=2.193450927734375e-05\n",
            "step 479: loss=0.021169662475585938\n",
            "step 480: loss=0.014632225036621094\n",
            "step 481: loss=0.14523983001708984\n",
            "step 482: loss=0.11488485336303711\n",
            "step 483: loss=0.8604354858398438\n",
            "step 484: loss=0.09349918365478516\n",
            "step 485: loss=3.8527305126190186\n",
            "step 486: loss=0.00011348724365234375\n",
            "step 487: loss=1.33514404296875e-05\n",
            "step 488: loss=0.0001277923583984375\n",
            "step 489: loss=0.0019359588623046875\n",
            "step 490: loss=0.0651092529296875\n",
            "step 491: loss=5.7220458984375e-06\n",
            "step 492: loss=0.0011148452758789062\n",
            "step 493: loss=0.010511398315429688\n",
            "step 494: loss=0.5067214965820312\n",
            "step 495: loss=0.05121278762817383\n",
            "step 496: loss=0.8067708015441895\n",
            "step 497: loss=0.0012035369873046875\n",
            "step 498: loss=0.0005636215209960938\n",
            "step 499: loss=3.9908530712127686\n",
            "step 500: loss=0.03806257247924805\n",
            "step 501: loss=0.00041866302490234375\n",
            "step 502: loss=0.01632213592529297\n",
            "step 503: loss=0.009531021118164062\n",
            "step 504: loss=0.051606178283691406\n",
            "step 505: loss=0.0046844482421875\n",
            "step 506: loss=0.21791982650756836\n",
            "step 507: loss=2.6733314990997314\n",
            "step 508: loss=0.0023946762084960938\n",
            "step 509: loss=0.011342048645019531\n",
            "step 510: loss=0.00948953628540039\n",
            "step 511: loss=0.25917816162109375\n",
            "step 512: loss=0.2266983985900879\n",
            "step 513: loss=0.5069470405578613\n",
            "step 514: loss=0.07859659194946289\n",
            "step 515: loss=0.004550933837890625\n",
            "step 516: loss=0.028441429138183594\n",
            "step 517: loss=1.0769801139831543\n",
            "step 518: loss=1.0893759727478027\n",
            "step 519: loss=0.0207977294921875\n",
            "step 520: loss=0.1847233772277832\n",
            "step 521: loss=0.0011358261108398438\n",
            "step 522: loss=0.002544403076171875\n",
            "step 523: loss=0.0017910003662109375\n",
            "step 524: loss=0.0006885528564453125\n",
            "step 525: loss=0.0002288818359375\n",
            "step 526: loss=7.210210800170898\n",
            "step 527: loss=1.33514404296875e-05\n",
            "step 528: loss=0.0\n",
            "step 529: loss=0.009985923767089844\n",
            "step 530: loss=0.0012655258178710938\n",
            "step 531: loss=0.013316154479980469\n",
            "step 532: loss=1.430511474609375e-05\n",
            "step 533: loss=0.3438591957092285\n",
            "step 534: loss=0.000774383544921875\n",
            "step 535: loss=0.0616302490234375\n",
            "step 536: loss=0.0041599273681640625\n",
            "step 537: loss=0.0010232925415039062\n",
            "step 538: loss=0.9538817405700684\n",
            "step 539: loss=3.0246241092681885\n",
            "step 540: loss=6.5522918701171875\n",
            "step 541: loss=2.86102294921875e-06\n",
            "step 542: loss=0.0041561126708984375\n",
            "step 543: loss=0.41371726989746094\n",
            "step 544: loss=0.010771751403808594\n",
            "step 545: loss=1.3530247211456299\n",
            "step 546: loss=0.4567112922668457\n",
            "step 547: loss=0.014287948608398438\n",
            "step 548: loss=0.0008869171142578125\n",
            "step 549: loss=1.5145769119262695\n",
            "step 550: loss=0.19260883331298828\n",
            "step 551: loss=0.830204963684082\n",
            "step 552: loss=0.03132057189941406\n",
            "step 553: loss=0.00022411346435546875\n",
            "step 554: loss=1.4544804096221924\n",
            "step 555: loss=0.5345754623413086\n",
            "step 556: loss=0.7646408081054688\n",
            "step 557: loss=0.016007423400878906\n",
            "step 558: loss=7.62939453125e-06\n",
            "step 559: loss=1.9073486328125e-06\n",
            "step 560: loss=0.12009572982788086\n",
            "step 561: loss=0.026035308837890625\n",
            "step 562: loss=0.02651214599609375\n",
            "step 563: loss=0.06905746459960938\n",
            "step 564: loss=0.06959915161132812\n",
            "step 565: loss=8.869171142578125e-05\n",
            "step 566: loss=2.7635157108306885\n",
            "step 567: loss=0.003406524658203125\n",
            "step 568: loss=0.22663164138793945\n",
            "step 569: loss=0.012228965759277344\n",
            "step 570: loss=0.0031385421752929688\n",
            "step 571: loss=0.00177001953125\n",
            "step 572: loss=0.0035963058471679688\n",
            "step 573: loss=0.1531381607055664\n",
            "step 574: loss=0.0724935531616211\n",
            "step 575: loss=0.1255340576171875\n",
            "step 576: loss=0.02918243408203125\n",
            "step 577: loss=0.013894081115722656\n",
            "step 578: loss=0.0013551712036132812\n",
            "step 579: loss=0.012083053588867188\n",
            "step 580: loss=0.04894542694091797\n",
            "step 581: loss=0.0005502700805664062\n",
            "step 582: loss=0.2536625862121582\n",
            "step 583: loss=0.04343223571777344\n",
            "step 584: loss=0.0013904571533203125\n",
            "step 585: loss=0.00183868408203125\n",
            "step 586: loss=1.237051010131836\n",
            "step 587: loss=0.047086238861083984\n",
            "step 588: loss=1.3391871452331543\n",
            "step 589: loss=5.144454479217529\n",
            "step 590: loss=0.0805044174194336\n",
            "step 591: loss=0.001651763916015625\n",
            "step 592: loss=1.5861303806304932\n",
            "step 593: loss=0.01200103759765625\n",
            "step 594: loss=0.0012979507446289062\n",
            "step 595: loss=2.6404922008514404\n",
            "step 596: loss=0.03969430923461914\n",
            "step 597: loss=1.2851359844207764\n",
            "step 598: loss=0.9576687812805176\n",
            "step 599: loss=0.04650688171386719\n",
            "step 600: loss=0.011490821838378906\n",
            "step 601: loss=0.0011463165283203125\n",
            "step 602: loss=0.0059909820556640625\n",
            "step 603: loss=0.12888240814208984\n",
            "step 604: loss=3.909438133239746\n",
            "step 605: loss=0.6165862083435059\n",
            "step 606: loss=0.9095067977905273\n",
            "step 607: loss=0.7418718338012695\n",
            "step 608: loss=0.052657127380371094\n",
            "step 609: loss=0.5939149856567383\n",
            "step 610: loss=0.011339187622070312\n",
            "step 611: loss=1.2586374282836914\n",
            "step 612: loss=0.24498653411865234\n",
            "step 613: loss=0.007367134094238281\n",
            "step 614: loss=0.01715373992919922\n",
            "step 615: loss=3.068140983581543\n",
            "step 616: loss=0.13239002227783203\n",
            "step 617: loss=0.0\n",
            "step 618: loss=0.36594104766845703\n",
            "step 619: loss=0.11091852188110352\n",
            "step 620: loss=0.09301948547363281\n",
            "step 621: loss=2.45327091217041\n",
            "step 622: loss=0.3301215171813965\n",
            "step 623: loss=0.04981231689453125\n",
            "step 624: loss=0.10261678695678711\n",
            "step 625: loss=0.18711566925048828\n",
            "step 626: loss=2.153895854949951\n",
            "step 627: loss=0.013571739196777344\n",
            "step 628: loss=0.0\n",
            "step 629: loss=2.690868616104126\n",
            "step 630: loss=0.004370689392089844\n",
            "step 631: loss=0.16437578201293945\n",
            "step 632: loss=0.05879497528076172\n",
            "step 633: loss=0.27587366104125977\n",
            "step 634: loss=1.33514404296875e-05\n",
            "step 635: loss=0.6673083305358887\n",
            "step 636: loss=0.006646156311035156\n",
            "step 637: loss=0.1656780242919922\n",
            "step 638: loss=0.00699615478515625\n",
            "step 639: loss=0.0005855560302734375\n",
            "step 640: loss=0.6939210891723633\n",
            "step 641: loss=0.9721102714538574\n",
            "step 642: loss=0.0013914108276367188\n",
            "step 643: loss=0.4274778366088867\n",
            "step 644: loss=0.000667572021484375\n",
            "step 645: loss=0.06541252136230469\n",
            "step 646: loss=0.000904083251953125\n",
            "step 647: loss=2.147653102874756\n",
            "step 648: loss=0.005261421203613281\n",
            "step 649: loss=0.008122444152832031\n",
            "step 650: loss=0.0001850128173828125\n",
            "step 651: loss=0.08937215805053711\n",
            "step 652: loss=0.28654003143310547\n",
            "step 653: loss=0.008829116821289062\n",
            "step 654: loss=1.0461673736572266\n",
            "step 655: loss=0.8203020095825195\n",
            "step 656: loss=0.3219752311706543\n",
            "step 657: loss=0.00054168701171875\n",
            "step 658: loss=0.9956603050231934\n",
            "step 659: loss=0.004925727844238281\n",
            "step 660: loss=2.649219036102295\n",
            "step 661: loss=5.435943603515625e-05\n",
            "step 662: loss=0.0018205642700195312\n",
            "step 663: loss=0.3430781364440918\n",
            "step 664: loss=2.084014415740967\n",
            "step 665: loss=0.004302978515625\n",
            "step 666: loss=0.00034236907958984375\n",
            "step 667: loss=0.01294708251953125\n",
            "step 668: loss=2.7187068462371826\n",
            "step 669: loss=2.357236862182617\n",
            "step 670: loss=0.30407142639160156\n",
            "step 671: loss=6.170305252075195\n",
            "step 672: loss=0.6924562454223633\n",
            "step 673: loss=1.7846760749816895\n",
            "step 674: loss=0.0011348724365234375\n",
            "step 675: loss=2.196044445037842\n",
            "step 676: loss=0.029821395874023438\n",
            "step 677: loss=0.0118865966796875\n",
            "step 678: loss=0.0075359344482421875\n",
            "step 679: loss=0.05722618103027344\n",
            "step 680: loss=7.62939453125e-06\n",
            "step 681: loss=3.325324058532715\n",
            "step 682: loss=0.01614093780517578\n",
            "step 683: loss=0.024885177612304688\n",
            "step 684: loss=0.4541902542114258\n",
            "step 685: loss=0.0019197463989257812\n",
            "step 686: loss=0.0021352767944335938\n",
            "step 687: loss=0.0003681182861328125\n",
            "step 688: loss=1.9073486328125e-06\n",
            "step 689: loss=0.34659576416015625\n",
            "step 690: loss=0.2933082580566406\n",
            "step 691: loss=0.010479927062988281\n",
            "step 692: loss=0.5571396350860596\n",
            "step 693: loss=0.00035858154296875\n",
            "step 694: loss=0.0029153823852539062\n",
            "step 695: loss=0.058551788330078125\n",
            "step 696: loss=0.006093025207519531\n",
            "step 697: loss=0.6764545440673828\n",
            "step 698: loss=0.3045206069946289\n",
            "step 699: loss=0.4723691940307617\n",
            "step 700: loss=0.025836944580078125\n",
            "step 701: loss=0.0017414093017578125\n",
            "step 702: loss=3.910064697265625e-05\n",
            "step 703: loss=0.09843254089355469\n",
            "step 704: loss=0.03251934051513672\n",
            "step 705: loss=0.005131721496582031\n",
            "step 706: loss=0.00045871734619140625\n",
            "step 707: loss=0.1641550064086914\n",
            "step 708: loss=1.0705780982971191\n",
            "step 709: loss=0.004836082458496094\n",
            "step 710: loss=0.025987625122070312\n",
            "step 711: loss=0.03412151336669922\n",
            "step 712: loss=0.11502552032470703\n",
            "step 713: loss=0.008001327514648438\n",
            "step 714: loss=0.03227996826171875\n",
            "step 715: loss=0.00441741943359375\n",
            "step 716: loss=0.07857704162597656\n",
            "step 717: loss=0.03924894332885742\n",
            "step 718: loss=0.7353448867797852\n",
            "step 719: loss=7.62939453125e-06\n",
            "step 720: loss=0.0032281875610351562\n",
            "step 721: loss=0.00196075439453125\n",
            "step 722: loss=0.005803108215332031\n",
            "step 723: loss=3.814697265625e-06\n",
            "step 724: loss=0.08685302734375\n",
            "step 725: loss=8.134077072143555\n",
            "step 726: loss=3.2057952880859375\n",
            "step 727: loss=2.288818359375e-05\n",
            "step 728: loss=0.7393798828125\n",
            "step 729: loss=0.7793064117431641\n",
            "step 730: loss=0.0008382797241210938\n",
            "step 731: loss=0.06834220886230469\n",
            "step 732: loss=1.9570094347000122\n",
            "step 733: loss=0.26990795135498047\n",
            "step 734: loss=0.01724720001220703\n",
            "step 735: loss=0.3745129108428955\n",
            "step 736: loss=0.0009479522705078125\n",
            "step 737: loss=0.0018949508666992188\n",
            "step 738: loss=0.001720428466796875\n",
            "step 739: loss=0.0\n",
            "step 740: loss=0.020730972290039062\n",
            "step 741: loss=0.18381786346435547\n",
            "step 742: loss=11.700695037841797\n",
            "step 743: loss=3.8645825386047363\n",
            "step 744: loss=0.00035572052001953125\n",
            "step 745: loss=1.839472770690918\n",
            "step 746: loss=0.00022125244140625\n",
            "step 747: loss=0.8345260620117188\n",
            "step 748: loss=0.0034360885620117188\n",
            "step 749: loss=7.05718994140625e-05\n",
            "step 750: loss=3.0070953369140625\n",
            "step 751: loss=8.869171142578125e-05\n",
            "step 752: loss=0.10140228271484375\n",
            "step 753: loss=0.0049591064453125\n",
            "step 754: loss=0.0004987716674804688\n",
            "step 755: loss=0.01761627197265625\n",
            "step 756: loss=2.253580093383789\n",
            "step 757: loss=0.010903358459472656\n",
            "step 758: loss=0.10082483291625977\n",
            "step 759: loss=0.10035228729248047\n",
            "step 760: loss=0.017485618591308594\n",
            "step 761: loss=0.003604888916015625\n",
            "step 762: loss=0.2944355010986328\n",
            "step 763: loss=0.6307485103607178\n",
            "step 764: loss=2.2296950817108154\n",
            "step 765: loss=0.07599258422851562\n",
            "step 766: loss=0.33605480194091797\n",
            "step 767: loss=0.0012874603271484375\n",
            "step 768: loss=0.011927604675292969\n",
            "step 769: loss=1.9073486328125e-06\n",
            "step 770: loss=0.021536827087402344\n",
            "step 771: loss=0.05537843704223633\n",
            "step 772: loss=0.7564830780029297\n",
            "step 773: loss=0.05304527282714844\n",
            "step 774: loss=0.0641021728515625\n",
            "step 775: loss=9.5367431640625e-07\n",
            "step 776: loss=0.3005037307739258\n",
            "step 777: loss=0.007281303405761719\n",
            "step 778: loss=0.0007534027099609375\n",
            "step 779: loss=7.2479248046875e-05\n",
            "step 780: loss=0.00045299530029296875\n",
            "step 781: loss=0.0001621246337890625\n",
            "step 782: loss=0.027116775512695312\n",
            "step 783: loss=0.007917404174804688\n",
            "step 784: loss=0.007584571838378906\n",
            "step 785: loss=0.0058002471923828125\n",
            "step 786: loss=0.002895355224609375\n",
            "step 787: loss=0.002231597900390625\n",
            "step 788: loss=1.981694221496582\n",
            "step 789: loss=0.005278587341308594\n",
            "step 790: loss=0.037647247314453125\n",
            "step 791: loss=0.45624542236328125\n",
            "step 792: loss=0.0007162094116210938\n",
            "step 793: loss=0.0049343109130859375\n",
            "step 794: loss=0.7428224086761475\n",
            "step 795: loss=0.0007171630859375\n",
            "step 796: loss=4.76837158203125e-06\n",
            "step 797: loss=0.00032901763916015625\n",
            "step 798: loss=5.803304195404053\n",
            "step 799: loss=0.01948690414428711\n",
            "step 800: loss=1.291365623474121\n",
            "step 801: loss=0.008294105529785156\n",
            "step 802: loss=0.00018405914306640625\n",
            "step 803: loss=0.00043010711669921875\n",
            "step 804: loss=2.1976613998413086\n",
            "step 805: loss=0.0\n",
            "step 806: loss=6.008148193359375e-05\n",
            "step 807: loss=0.0001220703125\n",
            "step 808: loss=0.0\n",
            "step 809: loss=0.6237568855285645\n",
            "step 810: loss=0.0003108978271484375\n",
            "step 811: loss=2.4623687267303467\n",
            "step 812: loss=0.06832504272460938\n",
            "step 813: loss=0.006619453430175781\n",
            "step 814: loss=0.026712417602539062\n",
            "step 815: loss=0.150299072265625\n",
            "step 816: loss=0.04860401153564453\n",
            "step 817: loss=0.2936239242553711\n",
            "step 818: loss=2.088675022125244\n",
            "step 819: loss=0.0\n",
            "step 820: loss=0.3713665008544922\n",
            "step 821: loss=0.00011157989501953125\n",
            "step 822: loss=5.7220458984375e-05\n",
            "step 823: loss=0.000247955322265625\n",
            "step 824: loss=0.0018939971923828125\n",
            "step 825: loss=5.651551246643066\n",
            "step 826: loss=7.343292236328125e-05\n",
            "step 827: loss=5.061742305755615\n",
            "step 828: loss=0.000179290771484375\n",
            "step 829: loss=0.03612995147705078\n",
            "step 830: loss=0.25597429275512695\n",
            "step 831: loss=0.07159900665283203\n",
            "step 832: loss=0.00020885467529296875\n",
            "step 833: loss=0.13234424591064453\n",
            "step 834: loss=8.409918785095215\n",
            "step 835: loss=0.2047586441040039\n",
            "step 836: loss=0.0012569427490234375\n",
            "step 837: loss=0.0\n",
            "step 838: loss=0.004000663757324219\n",
            "step 839: loss=0.0\n",
            "step 840: loss=2.530790090560913\n",
            "step 841: loss=0.0008230209350585938\n",
            "step 842: loss=0.00020694732666015625\n",
            "step 843: loss=3.0517578125e-05\n",
            "step 844: loss=0.011150360107421875\n",
            "step 845: loss=0.0\n",
            "step 846: loss=0.0026645660400390625\n",
            "step 847: loss=0.13273143768310547\n",
            "step 848: loss=0.6009900569915771\n",
            "step 849: loss=0.0\n",
            "step 850: loss=0.03385639190673828\n",
            "step 851: loss=0.010324478149414062\n",
            "step 852: loss=2.62894868850708\n",
            "step 853: loss=0.022233963012695312\n",
            "step 854: loss=0.01743793487548828\n",
            "step 855: loss=3.2791028022766113\n",
            "step 856: loss=0.06092119216918945\n",
            "step 857: loss=0.10755062103271484\n",
            "step 858: loss=5.245208740234375e-05\n",
            "step 859: loss=0.8159189224243164\n",
            "step 860: loss=0.04806804656982422\n",
            "step 861: loss=6.29425048828125e-05\n",
            "step 862: loss=1.239776611328125e-05\n",
            "step 863: loss=0.0729975700378418\n",
            "step 864: loss=0.000396728515625\n",
            "step 865: loss=0.29297542572021484\n",
            "step 866: loss=2.288818359375e-05\n",
            "step 867: loss=0.13003969192504883\n",
            "step 868: loss=0.0074672698974609375\n",
            "step 869: loss=5.582449436187744\n",
            "step 870: loss=0.0001888275146484375\n",
            "step 871: loss=0.00977325439453125\n",
            "step 872: loss=0.0031108856201171875\n",
            "step 873: loss=0.0012578964233398438\n",
            "step 874: loss=2.5499608516693115\n",
            "step 875: loss=0.012433052062988281\n",
            "step 876: loss=0.8528828620910645\n",
            "step 877: loss=0.00617218017578125\n",
            "step 878: loss=0.0002651214599609375\n",
            "step 879: loss=7.43865966796875e-05\n",
            "step 880: loss=0.07283592224121094\n",
            "step 881: loss=0.0827932357788086\n",
            "step 882: loss=0.02257823944091797\n",
            "step 883: loss=0.004891395568847656\n",
            "step 884: loss=3.4332275390625e-05\n",
            "step 885: loss=1.1444091796875e-05\n",
            "step 886: loss=0.3730788230895996\n",
            "step 887: loss=0.17141437530517578\n",
            "step 888: loss=0.10196495056152344\n",
            "step 889: loss=0.623807430267334\n",
            "step 890: loss=0.01866626739501953\n",
            "step 891: loss=0.000682830810546875\n",
            "step 892: loss=0.0\n",
            "step 893: loss=0.0157623291015625\n",
            "step 894: loss=0.000705718994140625\n",
            "step 895: loss=0.0015382766723632812\n",
            "step 896: loss=0.0007419586181640625\n",
            "step 897: loss=0.04038572311401367\n",
            "step 898: loss=0.7001004219055176\n",
            "step 899: loss=0.0001811981201171875\n",
            "step 900: loss=0.0003910064697265625\n",
            "step 901: loss=0.0030984878540039062\n",
            "step 902: loss=2.86102294921875e-06\n",
            "step 903: loss=2.4237773418426514\n",
            "step 904: loss=2.3096442222595215\n",
            "step 905: loss=0.0286102294921875\n",
            "step 906: loss=0.02706003189086914\n",
            "step 907: loss=1.6895108222961426\n",
            "step 908: loss=2.47955322265625e-05\n",
            "step 909: loss=0.0001811981201171875\n",
            "step 910: loss=0.014546394348144531\n",
            "step 911: loss=0.004012107849121094\n",
            "step 912: loss=0.00222015380859375\n",
            "step 913: loss=0.0022706985473632812\n",
            "step 914: loss=5.210331439971924\n",
            "step 915: loss=0.007401466369628906\n",
            "step 916: loss=0.0024089813232421875\n",
            "step 917: loss=0.23400497436523438\n",
            "step 918: loss=0.5985498428344727\n",
            "step 919: loss=0.025618553161621094\n",
            "step 920: loss=0.10088443756103516\n",
            "step 921: loss=0.0035066604614257812\n",
            "step 922: loss=0.011301040649414062\n",
            "step 923: loss=5.14984130859375e-05\n",
            "step 924: loss=3.814697265625e-06\n",
            "step 925: loss=0.0003337860107421875\n",
            "step 926: loss=0.000476837158203125\n",
            "step 927: loss=0.14440250396728516\n",
            "step 928: loss=2.899588108062744\n",
            "step 929: loss=0.006424903869628906\n",
            "step 930: loss=0.00131988525390625\n",
            "step 931: loss=0.00463104248046875\n",
            "step 932: loss=0.0009069442749023438\n",
            "step 933: loss=0.001789093017578125\n",
            "step 934: loss=0.0019159317016601562\n",
            "step 935: loss=1.9073486328125e-05\n",
            "step 936: loss=0.09371042251586914\n",
            "step 937: loss=0.0008144378662109375\n",
            "step 938: loss=0.009626388549804688\n",
            "step 939: loss=1.9198107719421387\n",
            "step 940: loss=0.0781254768371582\n",
            "step 941: loss=1.9073486328125e-06\n",
            "step 942: loss=0.1515183448791504\n",
            "step 943: loss=0.0001239776611328125\n",
            "step 944: loss=6.771087646484375e-05\n",
            "step 945: loss=5.743521690368652\n",
            "step 946: loss=0.1622018814086914\n",
            "step 947: loss=0.06946277618408203\n",
            "step 948: loss=9.5367431640625e-05\n",
            "step 949: loss=0.9143581390380859\n",
            "step 950: loss=0.043241024017333984\n",
            "step 951: loss=0.0032052993774414062\n",
            "step 952: loss=9.441375732421875e-05\n",
            "step 953: loss=0.00041961669921875\n",
            "step 954: loss=0.0031414031982421875\n",
            "step 955: loss=3.814697265625e-06\n",
            "step 956: loss=5.7220458984375e-05\n",
            "step 957: loss=0.0001068115234375\n",
            "step 958: loss=0.19945049285888672\n",
            "step 959: loss=0.12284278869628906\n",
            "step 960: loss=0.3335075378417969\n",
            "step 961: loss=0.8218884468078613\n",
            "step 962: loss=2.6702880859375e-05\n",
            "step 963: loss=9.5367431640625e-06\n",
            "step 964: loss=0.0\n",
            "step 965: loss=1.2736027240753174\n",
            "step 966: loss=0.019702911376953125\n",
            "step 967: loss=0.09659767150878906\n",
            "step 968: loss=0.1679515838623047\n",
            "step 969: loss=0.00013446807861328125\n",
            "step 970: loss=3.3188230991363525\n",
            "step 971: loss=1.1409077644348145\n",
            "step 972: loss=0.00013256072998046875\n",
            "step 973: loss=0.004202842712402344\n",
            "step 974: loss=3.412497043609619\n",
            "step 975: loss=2.1781585216522217\n",
            "step 976: loss=0.488893985748291\n",
            "step 977: loss=3.814697265625e-05\n",
            "step 978: loss=2.8759284019470215\n",
            "step 979: loss=3.6702263355255127\n",
            "step 980: loss=0.9372410774230957\n",
            "step 981: loss=0.00637054443359375\n",
            "step 982: loss=0.15968036651611328\n",
            "step 983: loss=0.0011653900146484375\n",
            "step 984: loss=0.034711360931396484\n",
            "step 985: loss=6.541893482208252\n",
            "step 986: loss=0.17281293869018555\n",
            "step 987: loss=0.1320204734802246\n",
            "step 988: loss=0.0160675048828125\n",
            "step 989: loss=0.0\n",
            "step 990: loss=0.016210556030273438\n",
            "step 991: loss=0.00014781951904296875\n",
            "step 992: loss=0.021401405334472656\n",
            "step 993: loss=0.000392913818359375\n",
            "step 994: loss=1.5985784530639648\n",
            "step 995: loss=0.1483154296875\n",
            "step 996: loss=9.5367431640625e-06\n",
            "step 997: loss=1.1444091796875e-05\n",
            "step 998: loss=0.052496910095214844\n",
            "step 999: loss=0.021558761596679688\n",
            "step 1000: loss=2.86102294921875e-06\n",
            "Mean loss        0.52360946\n",
            "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "Predictions:     [2 0 4 8 7 6 0 6 5 1]\n"
          ]
        }
      ],
      "source": [
        "optimizer = optax.contrib.muon(learning_rate=learning_rate)\n",
        "params = train_mlp(train_data, optimizer)\n",
        "evaluate_mlp(test_data, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "6352f95d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: loss=1.940758228302002\n",
            "step 2: loss=2.1760780811309814\n",
            "step 3: loss=2.123516082763672\n",
            "step 4: loss=1.7909069061279297\n",
            "step 5: loss=2.7731266021728516\n",
            "step 6: loss=1.962531328201294\n",
            "step 7: loss=2.494056224822998\n",
            "step 8: loss=1.8388534784317017\n",
            "step 9: loss=1.8824307918548584\n",
            "step 10: loss=2.6979455947875977\n",
            "step 11: loss=2.197720766067505\n",
            "step 12: loss=2.1643967628479004\n",
            "step 13: loss=1.4617383480072021\n",
            "step 14: loss=1.5403313636779785\n",
            "step 15: loss=2.446437358856201\n",
            "step 16: loss=2.2722434997558594\n",
            "step 17: loss=1.3893849849700928\n",
            "step 18: loss=3.549886703491211\n",
            "step 19: loss=3.3583061695098877\n",
            "step 20: loss=1.5383902788162231\n",
            "step 21: loss=2.0145435333251953\n",
            "step 22: loss=1.8640811443328857\n",
            "step 23: loss=1.9871046543121338\n",
            "step 24: loss=2.46518611907959\n",
            "step 25: loss=2.6712143421173096\n",
            "step 26: loss=2.0123965740203857\n",
            "step 27: loss=3.0125784873962402\n",
            "step 28: loss=2.691230297088623\n",
            "step 29: loss=1.9900810718536377\n",
            "step 30: loss=1.6837618350982666\n",
            "step 31: loss=1.7050302028656006\n",
            "step 32: loss=2.1044046878814697\n",
            "step 33: loss=1.6550555229187012\n",
            "step 34: loss=2.254828691482544\n",
            "step 35: loss=1.9034671783447266\n",
            "step 36: loss=1.9907734394073486\n",
            "step 37: loss=2.0229430198669434\n",
            "step 38: loss=1.9300038814544678\n",
            "step 39: loss=2.7000436782836914\n",
            "step 40: loss=1.8529770374298096\n",
            "step 41: loss=1.8260843753814697\n",
            "step 42: loss=2.6148486137390137\n",
            "step 43: loss=1.9493474960327148\n",
            "step 44: loss=1.6372787952423096\n",
            "step 45: loss=2.0136427879333496\n",
            "step 46: loss=1.6119754314422607\n",
            "step 47: loss=2.0991406440734863\n",
            "step 48: loss=2.258373498916626\n",
            "step 49: loss=2.42681622505188\n",
            "step 50: loss=1.7030603885650635\n",
            "step 51: loss=1.8889999389648438\n",
            "step 52: loss=1.7781010866165161\n",
            "step 53: loss=3.225069284439087\n",
            "step 54: loss=1.3905025720596313\n",
            "step 55: loss=1.9320303201675415\n",
            "step 56: loss=2.2162458896636963\n",
            "step 57: loss=2.3552563190460205\n",
            "step 58: loss=1.6320664882659912\n",
            "step 59: loss=2.202133893966675\n",
            "step 60: loss=1.6967023611068726\n",
            "step 61: loss=1.246525764465332\n",
            "step 62: loss=1.6203155517578125\n",
            "step 63: loss=2.9075191020965576\n",
            "step 64: loss=2.1749255657196045\n",
            "step 65: loss=1.6314904689788818\n",
            "step 66: loss=0.9881222248077393\n",
            "step 67: loss=1.622375726699829\n",
            "step 68: loss=0.7821948528289795\n",
            "step 69: loss=2.6255650520324707\n",
            "step 70: loss=1.743849277496338\n",
            "step 71: loss=2.8109583854675293\n",
            "step 72: loss=2.7828750610351562\n",
            "step 73: loss=1.4112918376922607\n",
            "step 74: loss=1.306151032447815\n",
            "step 75: loss=0.8926953077316284\n",
            "step 76: loss=0.7406435012817383\n",
            "step 77: loss=2.758932590484619\n",
            "step 78: loss=2.4050614833831787\n",
            "step 79: loss=2.227637529373169\n",
            "step 80: loss=0.7494920492172241\n",
            "step 81: loss=0.23082280158996582\n",
            "step 82: loss=0.9866675138473511\n",
            "step 83: loss=2.122668981552124\n",
            "step 84: loss=0.6010141372680664\n",
            "step 85: loss=3.019683599472046\n",
            "step 86: loss=0.16074657440185547\n",
            "step 87: loss=1.9915804862976074\n",
            "step 88: loss=1.4108773469924927\n",
            "step 89: loss=1.5791131258010864\n",
            "step 90: loss=2.1513686180114746\n",
            "step 91: loss=1.6802482604980469\n",
            "step 92: loss=1.527822494506836\n",
            "step 93: loss=0.43253612518310547\n",
            "step 94: loss=2.0817129611968994\n",
            "step 95: loss=1.6460349559783936\n",
            "step 96: loss=2.0777554512023926\n",
            "step 97: loss=1.7677419185638428\n",
            "step 98: loss=2.6245574951171875\n",
            "step 99: loss=1.132123589515686\n",
            "step 100: loss=2.119290828704834\n",
            "step 101: loss=2.098219633102417\n",
            "step 102: loss=1.5714093446731567\n",
            "step 103: loss=2.2930195331573486\n",
            "step 104: loss=0.5091879367828369\n",
            "step 105: loss=0.4726097583770752\n",
            "step 106: loss=1.8006337881088257\n",
            "step 107: loss=2.1027603149414062\n",
            "step 108: loss=1.0504649877548218\n",
            "step 109: loss=1.6988062858581543\n",
            "step 110: loss=1.2320650815963745\n",
            "step 111: loss=1.6928997039794922\n",
            "step 112: loss=0.32294797897338867\n",
            "step 113: loss=1.959110975265503\n",
            "step 114: loss=2.2115509510040283\n",
            "step 115: loss=1.3057934045791626\n",
            "step 116: loss=1.507971167564392\n",
            "step 117: loss=2.102586030960083\n",
            "step 118: loss=0.9875845909118652\n",
            "step 119: loss=1.5989067554473877\n",
            "step 120: loss=1.7031259536743164\n",
            "step 121: loss=2.6286754608154297\n",
            "step 122: loss=1.5850002765655518\n",
            "step 123: loss=2.2829391956329346\n",
            "step 124: loss=1.293221116065979\n",
            "step 125: loss=0.8511708974838257\n",
            "step 126: loss=1.8777501583099365\n",
            "step 127: loss=2.0515291690826416\n",
            "step 128: loss=1.751558542251587\n",
            "step 129: loss=0.2879312038421631\n",
            "step 130: loss=0.9196925163269043\n",
            "step 131: loss=1.134814739227295\n",
            "step 132: loss=1.388135552406311\n",
            "step 133: loss=1.320654273033142\n",
            "step 134: loss=0.142578125\n",
            "step 135: loss=0.6125261783599854\n",
            "step 136: loss=1.151767373085022\n",
            "step 137: loss=0.8093137741088867\n",
            "step 138: loss=0.15493559837341309\n",
            "step 139: loss=1.6861603260040283\n",
            "step 140: loss=2.0366413593292236\n",
            "step 141: loss=0.7237839698791504\n",
            "step 142: loss=2.8405189514160156\n",
            "step 143: loss=0.6206004619598389\n",
            "step 144: loss=2.0611507892608643\n",
            "step 145: loss=0.9473954439163208\n",
            "step 146: loss=1.8579058647155762\n",
            "step 147: loss=2.705409049987793\n",
            "step 148: loss=1.6843841075897217\n",
            "step 149: loss=1.0625377893447876\n",
            "step 150: loss=0.9671682119369507\n",
            "step 151: loss=1.7277209758758545\n",
            "step 152: loss=0.5180060863494873\n",
            "step 153: loss=2.3091580867767334\n",
            "step 154: loss=1.3028345108032227\n",
            "step 155: loss=0.7140522003173828\n",
            "step 156: loss=1.1415538787841797\n",
            "step 157: loss=1.4793301820755005\n",
            "step 158: loss=1.6714287996292114\n",
            "step 159: loss=1.0126360654830933\n",
            "step 160: loss=2.005323886871338\n",
            "step 161: loss=1.0441279411315918\n",
            "step 162: loss=0.7576258182525635\n",
            "step 163: loss=0.9640073776245117\n",
            "step 164: loss=0.8265089988708496\n",
            "step 165: loss=1.7699790000915527\n",
            "step 166: loss=0.9343881607055664\n",
            "step 167: loss=1.606757640838623\n",
            "step 168: loss=2.594965696334839\n",
            "step 169: loss=1.0063514709472656\n",
            "step 170: loss=1.1417509317398071\n",
            "step 171: loss=0.4511137008666992\n",
            "step 172: loss=0.9041800498962402\n",
            "step 173: loss=0.5230724811553955\n",
            "step 174: loss=1.045677661895752\n",
            "step 175: loss=2.3977839946746826\n",
            "step 176: loss=1.2700635194778442\n",
            "step 177: loss=0.9969329833984375\n",
            "step 178: loss=1.160459280014038\n",
            "step 179: loss=0.4204108715057373\n",
            "step 180: loss=1.9843924045562744\n",
            "step 181: loss=0.2923877239227295\n",
            "step 182: loss=0.1150503158569336\n",
            "step 183: loss=1.0070995092391968\n",
            "step 184: loss=1.9128669500350952\n",
            "step 185: loss=0.47420787811279297\n",
            "step 186: loss=0.39101147651672363\n",
            "step 187: loss=2.231415271759033\n",
            "step 188: loss=2.5517473220825195\n",
            "step 189: loss=1.437174677848816\n",
            "step 190: loss=0.9578108787536621\n",
            "step 191: loss=0.08653736114501953\n",
            "step 192: loss=0.6862363815307617\n",
            "step 193: loss=0.3823404312133789\n",
            "step 194: loss=1.6281425952911377\n",
            "step 195: loss=0.3901338577270508\n",
            "step 196: loss=1.500989556312561\n",
            "step 197: loss=2.893779754638672\n",
            "step 198: loss=0.11595010757446289\n",
            "step 199: loss=2.770063877105713\n",
            "step 200: loss=0.5390090942382812\n",
            "step 201: loss=1.4554927349090576\n",
            "step 202: loss=1.9631141424179077\n",
            "step 203: loss=0.15400266647338867\n",
            "step 204: loss=0.10439205169677734\n",
            "step 205: loss=0.996600866317749\n",
            "step 206: loss=1.8002665042877197\n",
            "step 207: loss=0.7035231590270996\n",
            "step 208: loss=1.2461644411087036\n",
            "step 209: loss=1.3734639883041382\n",
            "step 210: loss=1.1962624788284302\n",
            "step 211: loss=0.047454833984375\n",
            "step 212: loss=3.489471912384033\n",
            "step 213: loss=1.5609102249145508\n",
            "step 214: loss=1.0745561122894287\n",
            "step 215: loss=0.06079435348510742\n",
            "step 216: loss=0.9788827896118164\n",
            "step 217: loss=0.6263360977172852\n",
            "step 218: loss=0.20669102668762207\n",
            "step 219: loss=0.9191924333572388\n",
            "step 220: loss=1.1311601400375366\n",
            "step 221: loss=1.6031124591827393\n",
            "step 222: loss=3.672095537185669\n",
            "step 223: loss=0.9712107181549072\n",
            "step 224: loss=0.8627488613128662\n",
            "step 225: loss=0.5474696159362793\n",
            "step 226: loss=1.2204034328460693\n",
            "step 227: loss=0.9344918727874756\n",
            "step 228: loss=0.4932880401611328\n",
            "step 229: loss=0.9787428379058838\n",
            "step 230: loss=0.5024271011352539\n",
            "step 231: loss=1.1683123111724854\n",
            "step 232: loss=0.7211713790893555\n",
            "step 233: loss=0.03919029235839844\n",
            "step 234: loss=0.9381096363067627\n",
            "step 235: loss=0.5780558586120605\n",
            "step 236: loss=0.9687178134918213\n",
            "step 237: loss=0.6189484596252441\n",
            "step 238: loss=0.41588544845581055\n",
            "step 239: loss=0.27318477630615234\n",
            "step 240: loss=0.9093091487884521\n",
            "step 241: loss=0.8887255191802979\n",
            "step 242: loss=0.22299766540527344\n",
            "step 243: loss=0.7276806831359863\n",
            "step 244: loss=0.2764899730682373\n",
            "step 245: loss=0.873140811920166\n",
            "step 246: loss=0.6811590194702148\n",
            "step 247: loss=0.46933579444885254\n",
            "step 248: loss=3.176146984100342\n",
            "step 249: loss=0.1948843002319336\n",
            "step 250: loss=0.022376537322998047\n",
            "step 251: loss=2.0302629470825195\n",
            "step 252: loss=0.7815532684326172\n",
            "step 253: loss=0.6058900356292725\n",
            "step 254: loss=0.32948780059814453\n",
            "step 255: loss=0.513887882232666\n",
            "step 256: loss=0.8629508018493652\n",
            "step 257: loss=2.4881768226623535\n",
            "step 258: loss=0.11145973205566406\n",
            "step 259: loss=1.424945592880249\n",
            "step 260: loss=1.1523075103759766\n",
            "step 261: loss=0.9763450622558594\n",
            "step 262: loss=3.0668561458587646\n",
            "step 263: loss=0.1405811309814453\n",
            "step 264: loss=0.4369082450866699\n",
            "step 265: loss=0.651355504989624\n",
            "step 266: loss=0.7770178318023682\n",
            "step 267: loss=0.5752403736114502\n",
            "step 268: loss=0.8714203834533691\n",
            "step 269: loss=0.2317209243774414\n",
            "step 270: loss=2.120420217514038\n",
            "step 271: loss=0.030502796173095703\n",
            "step 272: loss=2.8100504875183105\n",
            "step 273: loss=0.3450281620025635\n",
            "step 274: loss=0.4464294910430908\n",
            "step 275: loss=1.237307071685791\n",
            "step 276: loss=0.24210023880004883\n",
            "step 277: loss=0.6025030612945557\n",
            "step 278: loss=0.817582368850708\n",
            "step 279: loss=0.8477745056152344\n",
            "step 280: loss=0.6654469966888428\n",
            "step 281: loss=0.022299766540527344\n",
            "step 282: loss=0.05568695068359375\n",
            "step 283: loss=1.7557082176208496\n",
            "step 284: loss=2.3239552974700928\n",
            "step 285: loss=0.5763859748840332\n",
            "step 286: loss=0.27073144912719727\n",
            "step 287: loss=1.0109319686889648\n",
            "step 288: loss=0.11882925033569336\n",
            "step 289: loss=1.7510555982589722\n",
            "step 290: loss=0.2724437713623047\n",
            "step 291: loss=0.09409570693969727\n",
            "step 292: loss=0.36136913299560547\n",
            "step 293: loss=0.09254312515258789\n",
            "step 294: loss=0.005002021789550781\n",
            "step 295: loss=1.15247642993927\n",
            "step 296: loss=0.39642763137817383\n",
            "step 297: loss=0.6609292030334473\n",
            "step 298: loss=0.03882598876953125\n",
            "step 299: loss=0.06192445755004883\n",
            "step 300: loss=0.18926763534545898\n",
            "step 301: loss=0.016364097595214844\n",
            "step 302: loss=4.598568439483643\n",
            "step 303: loss=0.5449767112731934\n",
            "step 304: loss=0.5422558784484863\n",
            "step 305: loss=0.5100915431976318\n",
            "step 306: loss=0.38719916343688965\n",
            "step 307: loss=1.2490555047988892\n",
            "step 308: loss=0.025793075561523438\n",
            "step 309: loss=0.7196416854858398\n",
            "step 310: loss=0.616187334060669\n",
            "step 311: loss=1.2755953073501587\n",
            "step 312: loss=0.8189480304718018\n",
            "step 313: loss=0.06185770034790039\n",
            "step 314: loss=0.4077730178833008\n",
            "step 315: loss=0.28505587577819824\n",
            "step 316: loss=1.021296739578247\n",
            "step 317: loss=0.5595149993896484\n",
            "step 318: loss=1.267256736755371\n",
            "step 319: loss=0.2652921676635742\n",
            "step 320: loss=1.289210319519043\n",
            "step 321: loss=0.531684160232544\n",
            "step 322: loss=0.5415894985198975\n",
            "step 323: loss=0.012047767639160156\n",
            "step 324: loss=0.5780124664306641\n",
            "step 325: loss=0.49553704261779785\n",
            "step 326: loss=1.5517516136169434\n",
            "step 327: loss=0.5258686542510986\n",
            "step 328: loss=3.252657413482666\n",
            "step 329: loss=0.0019884109497070312\n",
            "step 330: loss=1.616234540939331\n",
            "step 331: loss=0.026736736297607422\n",
            "step 332: loss=0.16749238967895508\n",
            "step 333: loss=0.04065370559692383\n",
            "step 334: loss=0.4888019561767578\n",
            "step 335: loss=0.5071408748626709\n",
            "step 336: loss=0.34523630142211914\n",
            "step 337: loss=0.6336531639099121\n",
            "step 338: loss=0.41834449768066406\n",
            "step 339: loss=0.19454383850097656\n",
            "step 340: loss=0.08806180953979492\n",
            "step 341: loss=2.3574581146240234\n",
            "step 342: loss=0.47588229179382324\n",
            "step 343: loss=0.09826517105102539\n",
            "step 344: loss=0.7378146648406982\n",
            "step 345: loss=0.5165562629699707\n",
            "step 346: loss=0.12990188598632812\n",
            "step 347: loss=0.4433863162994385\n",
            "step 348: loss=0.19400978088378906\n",
            "step 349: loss=0.08053302764892578\n",
            "step 350: loss=0.02692890167236328\n",
            "step 351: loss=0.38886308670043945\n",
            "step 352: loss=1.523944616317749\n",
            "step 353: loss=1.557647943496704\n",
            "step 354: loss=0.7366688251495361\n",
            "step 355: loss=1.5161864757537842\n",
            "step 356: loss=0.018377304077148438\n",
            "step 357: loss=1.2819724082946777\n",
            "step 358: loss=0.48952770233154297\n",
            "step 359: loss=0.3830528259277344\n",
            "step 360: loss=0.2774205207824707\n",
            "step 361: loss=0.15614843368530273\n",
            "step 362: loss=0.026198387145996094\n",
            "step 363: loss=2.45873761177063\n",
            "step 364: loss=0.3456599712371826\n",
            "step 365: loss=0.384934663772583\n",
            "step 366: loss=1.2253036499023438\n",
            "step 367: loss=0.42652225494384766\n",
            "step 368: loss=0.0024127960205078125\n",
            "step 369: loss=0.29679250717163086\n",
            "step 370: loss=3.9339802265167236\n",
            "step 371: loss=1.1476609706878662\n",
            "step 372: loss=0.006028175354003906\n",
            "step 373: loss=0.025094985961914062\n",
            "step 374: loss=0.030088424682617188\n",
            "step 375: loss=0.2909841537475586\n",
            "step 376: loss=1.5548505783081055\n",
            "step 377: loss=0.18581438064575195\n",
            "step 378: loss=0.37858057022094727\n",
            "step 379: loss=3.816361427307129\n",
            "step 380: loss=0.1276102066040039\n",
            "step 381: loss=0.0203094482421875\n",
            "step 382: loss=0.6352777481079102\n",
            "step 383: loss=0.41879844665527344\n",
            "step 384: loss=0.0018863677978515625\n",
            "step 385: loss=0.0485844612121582\n",
            "step 386: loss=0.03948688507080078\n",
            "step 387: loss=0.19300317764282227\n",
            "step 388: loss=0.31698179244995117\n",
            "step 389: loss=0.3550095558166504\n",
            "step 390: loss=0.30381011962890625\n",
            "step 391: loss=0.5752663612365723\n",
            "step 392: loss=0.7523448467254639\n",
            "step 393: loss=0.9307587146759033\n",
            "step 394: loss=4.163150787353516\n",
            "step 395: loss=0.21659135818481445\n",
            "step 396: loss=0.5148525238037109\n",
            "step 397: loss=0.5608940124511719\n",
            "step 398: loss=1.8423759937286377\n",
            "step 399: loss=0.4655938148498535\n",
            "step 400: loss=0.04902029037475586\n",
            "step 401: loss=0.7953062057495117\n",
            "step 402: loss=0.34423065185546875\n",
            "step 403: loss=0.00025272369384765625\n",
            "step 404: loss=0.1343703269958496\n",
            "step 405: loss=0.1162419319152832\n",
            "step 406: loss=2.0450353622436523\n",
            "step 407: loss=0.22037982940673828\n",
            "step 408: loss=0.32031679153442383\n",
            "step 409: loss=4.15341854095459\n",
            "step 410: loss=0.015311241149902344\n",
            "step 411: loss=0.836982250213623\n",
            "step 412: loss=0.08687162399291992\n",
            "step 413: loss=1.182823896408081\n",
            "step 414: loss=0.6655287742614746\n",
            "step 415: loss=0.030199050903320312\n",
            "step 416: loss=1.7114145755767822\n",
            "step 417: loss=0.8987936973571777\n",
            "step 418: loss=0.1588435173034668\n",
            "step 419: loss=0.9800012111663818\n",
            "step 420: loss=0.14326953887939453\n",
            "step 421: loss=0.7330856323242188\n",
            "step 422: loss=0.5911045074462891\n",
            "step 423: loss=0.22020196914672852\n",
            "step 424: loss=0.02915668487548828\n",
            "step 425: loss=0.8084700107574463\n",
            "step 426: loss=1.3249101638793945\n",
            "step 427: loss=1.9790465831756592\n",
            "step 428: loss=0.49387168884277344\n",
            "step 429: loss=0.2029423713684082\n",
            "step 430: loss=0.010867118835449219\n",
            "step 431: loss=0.9041163921356201\n",
            "step 432: loss=0.7239291667938232\n",
            "step 433: loss=0.32804107666015625\n",
            "step 434: loss=2.6595869064331055\n",
            "step 435: loss=0.05801725387573242\n",
            "step 436: loss=0.2945094108581543\n",
            "step 437: loss=0.0058422088623046875\n",
            "step 438: loss=0.00118255615234375\n",
            "step 439: loss=0.9894213676452637\n",
            "step 440: loss=0.035610198974609375\n",
            "step 441: loss=0.07864665985107422\n",
            "step 442: loss=1.0972976684570312\n",
            "step 443: loss=2.7395873069763184\n",
            "step 444: loss=1.2684141397476196\n",
            "step 445: loss=0.8271484375\n",
            "step 446: loss=2.030148983001709\n",
            "step 447: loss=0.01706981658935547\n",
            "step 448: loss=0.14652442932128906\n",
            "step 449: loss=0.8165044784545898\n",
            "step 450: loss=2.9411308765411377\n",
            "step 451: loss=0.0037317276000976562\n",
            "step 452: loss=1.6171183586120605\n",
            "step 453: loss=0.05402040481567383\n",
            "step 454: loss=2.872264862060547\n",
            "step 455: loss=2.0467159748077393\n",
            "step 456: loss=1.5652191638946533\n",
            "step 457: loss=0.2162480354309082\n",
            "step 458: loss=2.43509578704834\n",
            "step 459: loss=0.12433147430419922\n",
            "step 460: loss=0.3025479316711426\n",
            "step 461: loss=4.552499294281006\n",
            "step 462: loss=0.032677650451660156\n",
            "step 463: loss=0.2710254192352295\n",
            "step 464: loss=0.08902263641357422\n",
            "step 465: loss=2.5139384269714355\n",
            "step 466: loss=0.0032968521118164062\n",
            "step 467: loss=2.8083159923553467\n",
            "step 468: loss=0.16899442672729492\n",
            "step 469: loss=0.6191864013671875\n",
            "step 470: loss=0.8607311248779297\n",
            "step 471: loss=1.9129371643066406\n",
            "step 472: loss=1.7333407402038574\n",
            "step 473: loss=0.3941974639892578\n",
            "step 474: loss=0.4605250358581543\n",
            "step 475: loss=0.028141021728515625\n",
            "step 476: loss=1.498698353767395\n",
            "step 477: loss=0.31674671173095703\n",
            "step 478: loss=0.00168609619140625\n",
            "step 479: loss=0.1409587860107422\n",
            "step 480: loss=0.22607660293579102\n",
            "step 481: loss=0.19871997833251953\n",
            "step 482: loss=0.45366716384887695\n",
            "step 483: loss=0.31646299362182617\n",
            "step 484: loss=0.9758138656616211\n",
            "step 485: loss=0.9316377639770508\n",
            "step 486: loss=0.0037441253662109375\n",
            "step 487: loss=0.0011358261108398438\n",
            "step 488: loss=0.045144081115722656\n",
            "step 489: loss=0.1974639892578125\n",
            "step 490: loss=0.6927015781402588\n",
            "step 491: loss=0.0006265640258789062\n",
            "step 492: loss=0.1433544158935547\n",
            "step 493: loss=0.2641420364379883\n",
            "step 494: loss=0.3066391944885254\n",
            "step 495: loss=1.760775089263916\n",
            "step 496: loss=0.8624153137207031\n",
            "step 497: loss=0.009818077087402344\n",
            "step 498: loss=0.0015277862548828125\n",
            "step 499: loss=3.7697038650512695\n",
            "step 500: loss=0.9256117343902588\n",
            "step 501: loss=0.03202962875366211\n",
            "step 502: loss=0.4472360610961914\n",
            "step 503: loss=0.25348567962646484\n",
            "step 504: loss=0.2319965362548828\n",
            "step 505: loss=0.016386985778808594\n",
            "step 506: loss=0.7594037055969238\n",
            "step 507: loss=2.1363370418548584\n",
            "step 508: loss=0.11826562881469727\n",
            "step 509: loss=0.07980871200561523\n",
            "step 510: loss=0.035796165466308594\n",
            "step 511: loss=0.22045469284057617\n",
            "step 512: loss=0.18213653564453125\n",
            "step 513: loss=0.5054140090942383\n",
            "step 514: loss=0.5509743690490723\n",
            "step 515: loss=0.019242286682128906\n",
            "step 516: loss=0.01963186264038086\n",
            "step 517: loss=1.2115912437438965\n",
            "step 518: loss=0.6631019115447998\n",
            "step 519: loss=0.08200645446777344\n",
            "step 520: loss=1.3339124917984009\n",
            "step 521: loss=0.1945514678955078\n",
            "step 522: loss=0.12208223342895508\n",
            "step 523: loss=0.018505096435546875\n",
            "step 524: loss=0.055393218994140625\n",
            "step 525: loss=0.09736061096191406\n",
            "step 526: loss=2.7697913646698\n",
            "step 527: loss=0.004085540771484375\n",
            "step 528: loss=0.004528045654296875\n",
            "step 529: loss=0.28116345405578613\n",
            "step 530: loss=0.05777549743652344\n",
            "step 531: loss=0.502511739730835\n",
            "step 532: loss=0.0038518905639648438\n",
            "step 533: loss=0.17044687271118164\n",
            "step 534: loss=0.051999568939208984\n",
            "step 535: loss=0.020951271057128906\n",
            "step 536: loss=0.3621788024902344\n",
            "step 537: loss=0.04747915267944336\n",
            "step 538: loss=0.6598091125488281\n",
            "step 539: loss=0.9023668766021729\n",
            "step 540: loss=2.9668502807617188\n",
            "step 541: loss=0.0038824081420898438\n",
            "step 542: loss=0.0684056282043457\n",
            "step 543: loss=0.49814701080322266\n",
            "step 544: loss=0.24260377883911133\n",
            "step 545: loss=0.6901845932006836\n",
            "step 546: loss=0.7440426349639893\n",
            "step 547: loss=0.17847728729248047\n",
            "step 548: loss=0.07002639770507812\n",
            "step 549: loss=0.09167671203613281\n",
            "step 550: loss=0.6276669502258301\n",
            "step 551: loss=1.7527539730072021\n",
            "step 552: loss=1.4664568901062012\n",
            "step 553: loss=0.01801156997680664\n",
            "step 554: loss=1.588718056678772\n",
            "step 555: loss=0.6365838050842285\n",
            "step 556: loss=2.359156608581543\n",
            "step 557: loss=0.2429332733154297\n",
            "step 558: loss=0.002788543701171875\n",
            "step 559: loss=0.0004901885986328125\n",
            "step 560: loss=0.14608001708984375\n",
            "step 561: loss=0.4102630615234375\n",
            "step 562: loss=0.8199503421783447\n",
            "step 563: loss=0.2971162796020508\n",
            "step 564: loss=0.2929973602294922\n",
            "step 565: loss=0.011035919189453125\n",
            "step 566: loss=2.90153169631958\n",
            "step 567: loss=0.06121063232421875\n",
            "step 568: loss=0.46771693229675293\n",
            "step 569: loss=0.2417287826538086\n",
            "step 570: loss=0.06613349914550781\n",
            "step 571: loss=0.046952247619628906\n",
            "step 572: loss=0.10270547866821289\n",
            "step 573: loss=0.10192632675170898\n",
            "step 574: loss=0.7308492660522461\n",
            "step 575: loss=0.18541479110717773\n",
            "step 576: loss=0.0673065185546875\n",
            "step 577: loss=1.1619279384613037\n",
            "step 578: loss=0.0492095947265625\n",
            "step 579: loss=0.010560035705566406\n",
            "step 580: loss=0.6665723323822021\n",
            "step 581: loss=0.06694650650024414\n",
            "step 582: loss=2.2689805030822754\n",
            "step 583: loss=0.22746658325195312\n",
            "step 584: loss=0.5360050201416016\n",
            "step 585: loss=0.14012861251831055\n",
            "step 586: loss=2.471245527267456\n",
            "step 587: loss=0.6285347938537598\n",
            "step 588: loss=0.49411869049072266\n",
            "step 589: loss=2.9138755798339844\n",
            "step 590: loss=0.5282862186431885\n",
            "step 591: loss=0.13997840881347656\n",
            "step 592: loss=2.677387237548828\n",
            "step 593: loss=0.6622674465179443\n",
            "step 594: loss=0.14083099365234375\n",
            "step 595: loss=1.5200077295303345\n",
            "step 596: loss=1.2404632568359375\n",
            "step 597: loss=0.9440689086914062\n",
            "step 598: loss=1.384364366531372\n",
            "step 599: loss=0.8591203689575195\n",
            "step 600: loss=0.14505529403686523\n",
            "step 601: loss=0.14066791534423828\n",
            "step 602: loss=0.21306133270263672\n",
            "step 603: loss=0.3978385925292969\n",
            "step 604: loss=2.1604790687561035\n",
            "step 605: loss=0.4126307964324951\n",
            "step 606: loss=1.072939157485962\n",
            "step 607: loss=0.3438701629638672\n",
            "step 608: loss=0.6456050872802734\n",
            "step 609: loss=0.12460613250732422\n",
            "step 610: loss=0.2161273956298828\n",
            "step 611: loss=1.4234001636505127\n",
            "step 612: loss=1.3905119895935059\n",
            "step 613: loss=0.44379472732543945\n",
            "step 614: loss=0.2622561454772949\n",
            "step 615: loss=1.1251068115234375\n",
            "step 616: loss=0.27696847915649414\n",
            "step 617: loss=0.03400135040283203\n",
            "step 618: loss=1.0506377220153809\n",
            "step 619: loss=0.0554356575012207\n",
            "step 620: loss=0.5775830745697021\n",
            "step 621: loss=2.9132144451141357\n",
            "step 622: loss=0.2057032585144043\n",
            "step 623: loss=0.12502622604370117\n",
            "step 624: loss=2.5381407737731934\n",
            "step 625: loss=0.5142438411712646\n",
            "step 626: loss=0.4413156509399414\n",
            "step 627: loss=0.21494150161743164\n",
            "step 628: loss=0.025819778442382812\n",
            "step 629: loss=1.5782334804534912\n",
            "step 630: loss=0.5100393295288086\n",
            "step 631: loss=0.10705423355102539\n",
            "step 632: loss=0.34012508392333984\n",
            "step 633: loss=2.570765733718872\n",
            "step 634: loss=0.02091503143310547\n",
            "step 635: loss=6.683933734893799\n",
            "step 636: loss=0.09868621826171875\n",
            "step 637: loss=0.9821426868438721\n",
            "step 638: loss=0.09847307205200195\n",
            "step 639: loss=0.23086905479431152\n",
            "step 640: loss=0.5108976364135742\n",
            "step 641: loss=0.6506447792053223\n",
            "step 642: loss=0.019531726837158203\n",
            "step 643: loss=1.9818276166915894\n",
            "step 644: loss=0.037041664123535156\n",
            "step 645: loss=0.25092124938964844\n",
            "step 646: loss=0.1673135757446289\n",
            "step 647: loss=3.2143805027008057\n",
            "step 648: loss=0.05027294158935547\n",
            "step 649: loss=0.0907602310180664\n",
            "step 650: loss=0.16423797607421875\n",
            "step 651: loss=0.8346233367919922\n",
            "step 652: loss=0.681767463684082\n",
            "step 653: loss=0.012887954711914062\n",
            "step 654: loss=0.21555805206298828\n",
            "step 655: loss=1.1414265632629395\n",
            "step 656: loss=0.4264535903930664\n",
            "step 657: loss=0.060373783111572266\n",
            "step 658: loss=0.3809828758239746\n",
            "step 659: loss=0.34478139877319336\n",
            "step 660: loss=0.5492372512817383\n",
            "step 661: loss=0.020505428314208984\n",
            "step 662: loss=0.17387819290161133\n",
            "step 663: loss=0.9730091094970703\n",
            "step 664: loss=0.41421985626220703\n",
            "step 665: loss=0.023694515228271484\n",
            "step 666: loss=0.09189701080322266\n",
            "step 667: loss=0.01885509490966797\n",
            "step 668: loss=1.4084246158599854\n",
            "step 669: loss=2.8859853744506836\n",
            "step 670: loss=0.2081911563873291\n",
            "step 671: loss=1.9136443138122559\n",
            "step 672: loss=0.5286903381347656\n",
            "step 673: loss=1.4348630905151367\n",
            "step 674: loss=0.16555047035217285\n",
            "step 675: loss=0.9397437572479248\n",
            "step 676: loss=0.03146553039550781\n",
            "step 677: loss=0.0074787139892578125\n",
            "step 678: loss=0.46175050735473633\n",
            "step 679: loss=0.09473705291748047\n",
            "step 680: loss=0.008462905883789062\n",
            "step 681: loss=1.46683931350708\n",
            "step 682: loss=0.058338165283203125\n",
            "step 683: loss=0.9928662776947021\n",
            "step 684: loss=0.0972757339477539\n",
            "step 685: loss=0.10205459594726562\n",
            "step 686: loss=0.03220033645629883\n",
            "step 687: loss=0.1822977066040039\n",
            "step 688: loss=0.18982315063476562\n",
            "step 689: loss=0.5775923728942871\n",
            "step 690: loss=0.3673434257507324\n",
            "step 691: loss=0.6741101741790771\n",
            "step 692: loss=2.0240063667297363\n",
            "step 693: loss=0.00860595703125\n",
            "step 694: loss=0.06125497817993164\n",
            "step 695: loss=0.13144350051879883\n",
            "step 696: loss=0.15802907943725586\n",
            "step 697: loss=2.6603844165802\n",
            "step 698: loss=0.852266788482666\n",
            "step 699: loss=0.5932917594909668\n",
            "step 700: loss=0.029884815216064453\n",
            "step 701: loss=0.12184667587280273\n",
            "step 702: loss=0.0046176910400390625\n",
            "step 703: loss=0.038036346435546875\n",
            "step 704: loss=0.03569364547729492\n",
            "step 705: loss=0.05467510223388672\n",
            "step 706: loss=0.17128419876098633\n",
            "step 707: loss=0.9932460784912109\n",
            "step 708: loss=0.39558982849121094\n",
            "step 709: loss=0.26425933837890625\n",
            "step 710: loss=1.4729747772216797\n",
            "step 711: loss=0.7270638942718506\n",
            "step 712: loss=2.2098217010498047\n",
            "step 713: loss=0.18172788619995117\n",
            "step 714: loss=0.12151336669921875\n",
            "step 715: loss=0.17783498764038086\n",
            "step 716: loss=1.0094122886657715\n",
            "step 717: loss=0.2937312126159668\n",
            "step 718: loss=0.27271175384521484\n",
            "step 719: loss=0.02951812744140625\n",
            "step 720: loss=0.026482105255126953\n",
            "step 721: loss=0.4348630905151367\n",
            "step 722: loss=0.04599809646606445\n",
            "step 723: loss=0.06534814834594727\n",
            "step 724: loss=0.30994606018066406\n",
            "step 725: loss=6.10931921005249\n",
            "step 726: loss=2.822141408920288\n",
            "step 727: loss=0.19489383697509766\n",
            "step 728: loss=1.1917390823364258\n",
            "step 729: loss=1.0231399536132812\n",
            "step 730: loss=0.22848963737487793\n",
            "step 731: loss=0.23949384689331055\n",
            "step 732: loss=0.35466933250427246\n",
            "step 733: loss=0.8382441997528076\n",
            "step 734: loss=0.14731597900390625\n",
            "step 735: loss=0.8544707298278809\n",
            "step 736: loss=0.0206451416015625\n",
            "step 737: loss=0.2973334789276123\n",
            "step 738: loss=0.04121255874633789\n",
            "step 739: loss=0.009181976318359375\n",
            "step 740: loss=1.2764720916748047\n",
            "step 741: loss=1.273885726928711\n",
            "step 742: loss=4.272341728210449\n",
            "step 743: loss=1.4487228393554688\n",
            "step 744: loss=0.06347942352294922\n",
            "step 745: loss=0.4986743927001953\n",
            "step 746: loss=0.007968902587890625\n",
            "step 747: loss=1.4382514953613281\n",
            "step 748: loss=0.11493587493896484\n",
            "step 749: loss=0.006710052490234375\n",
            "step 750: loss=0.43000030517578125\n",
            "step 751: loss=0.38118982315063477\n",
            "step 752: loss=0.4966104030609131\n",
            "step 753: loss=0.11653280258178711\n",
            "step 754: loss=0.46392297744750977\n",
            "step 755: loss=0.023680686950683594\n",
            "step 756: loss=0.7851626873016357\n",
            "step 757: loss=0.008550643920898438\n",
            "step 758: loss=1.274908423423767\n",
            "step 759: loss=0.08631372451782227\n",
            "step 760: loss=0.4114065170288086\n",
            "step 761: loss=0.030434608459472656\n",
            "step 762: loss=0.4647047519683838\n",
            "step 763: loss=0.25783538818359375\n",
            "step 764: loss=0.12276840209960938\n",
            "step 765: loss=0.34235191345214844\n",
            "step 766: loss=0.16598224639892578\n",
            "step 767: loss=0.08156108856201172\n",
            "step 768: loss=0.8856585025787354\n",
            "step 769: loss=0.001708984375\n",
            "step 770: loss=0.06731557846069336\n",
            "step 771: loss=0.34869885444641113\n",
            "step 772: loss=1.9329450130462646\n",
            "step 773: loss=0.0950460433959961\n",
            "step 774: loss=0.09786367416381836\n",
            "step 775: loss=0.035532474517822266\n",
            "step 776: loss=0.15671443939208984\n",
            "step 777: loss=0.0809788703918457\n",
            "step 778: loss=0.01971292495727539\n",
            "step 779: loss=0.02810382843017578\n",
            "step 780: loss=0.016853809356689453\n",
            "step 781: loss=0.0031080245971679688\n",
            "step 782: loss=0.024465560913085938\n",
            "step 783: loss=0.6460561752319336\n",
            "step 784: loss=0.018276691436767578\n",
            "step 785: loss=0.08486080169677734\n",
            "step 786: loss=0.032095909118652344\n",
            "step 787: loss=0.23444843292236328\n",
            "step 788: loss=0.6643266677856445\n",
            "step 789: loss=0.015120983123779297\n",
            "step 790: loss=0.5551629066467285\n",
            "step 791: loss=0.2002096176147461\n",
            "step 792: loss=0.6620044708251953\n",
            "step 793: loss=0.10877704620361328\n",
            "step 794: loss=0.6455426216125488\n",
            "step 795: loss=0.04246807098388672\n",
            "step 796: loss=0.02660989761352539\n",
            "step 797: loss=0.2708439826965332\n",
            "step 798: loss=2.9363293647766113\n",
            "step 799: loss=0.481719970703125\n",
            "step 800: loss=0.5913913249969482\n",
            "step 801: loss=0.1209716796875\n",
            "step 802: loss=0.004852294921875\n",
            "step 803: loss=0.11364507675170898\n",
            "step 804: loss=2.612170696258545\n",
            "step 805: loss=0.011935234069824219\n",
            "step 806: loss=0.0046215057373046875\n",
            "step 807: loss=0.041417598724365234\n",
            "step 808: loss=0.007549285888671875\n",
            "step 809: loss=0.5903716087341309\n",
            "step 810: loss=0.00115966796875\n",
            "step 811: loss=1.6003857851028442\n",
            "step 812: loss=0.2842984199523926\n",
            "step 813: loss=0.04505729675292969\n",
            "step 814: loss=0.9899144172668457\n",
            "step 815: loss=0.025193214416503906\n",
            "step 816: loss=0.6489248275756836\n",
            "step 817: loss=0.29872918128967285\n",
            "step 818: loss=0.8774468898773193\n",
            "step 819: loss=0.00337982177734375\n",
            "step 820: loss=0.18364381790161133\n",
            "step 821: loss=0.13299942016601562\n",
            "step 822: loss=0.04888343811035156\n",
            "step 823: loss=0.0027322769165039062\n",
            "step 824: loss=0.5085883140563965\n",
            "step 825: loss=3.0380735397338867\n",
            "step 826: loss=0.0025577545166015625\n",
            "step 827: loss=1.741243600845337\n",
            "step 828: loss=0.020491600036621094\n",
            "step 829: loss=0.4415397644042969\n",
            "step 830: loss=0.15727806091308594\n",
            "step 831: loss=0.6046748161315918\n",
            "step 832: loss=0.01106405258178711\n",
            "step 833: loss=0.29731130599975586\n",
            "step 834: loss=3.7791943550109863\n",
            "step 835: loss=0.3176240921020508\n",
            "step 836: loss=0.008383750915527344\n",
            "step 837: loss=0.0025529861450195312\n",
            "step 838: loss=0.0021047592163085938\n",
            "step 839: loss=0.04311656951904297\n",
            "step 840: loss=0.02485513687133789\n",
            "step 841: loss=0.0011339187622070312\n",
            "step 842: loss=0.04321432113647461\n",
            "step 843: loss=0.02466869354248047\n",
            "step 844: loss=0.21531963348388672\n",
            "step 845: loss=0.0077533721923828125\n",
            "step 846: loss=0.15342044830322266\n",
            "step 847: loss=1.3729257583618164\n",
            "step 848: loss=0.12397575378417969\n",
            "step 849: loss=0.0162353515625\n",
            "step 850: loss=0.06940364837646484\n",
            "step 851: loss=0.1914076805114746\n",
            "step 852: loss=0.3683433532714844\n",
            "step 853: loss=0.5249056816101074\n",
            "step 854: loss=0.10742568969726562\n",
            "step 855: loss=3.3022565841674805\n",
            "step 856: loss=0.11857175827026367\n",
            "step 857: loss=0.09141826629638672\n",
            "step 858: loss=0.12316131591796875\n",
            "step 859: loss=0.09176254272460938\n",
            "step 860: loss=0.511023998260498\n",
            "step 861: loss=0.023911476135253906\n",
            "step 862: loss=0.004899024963378906\n",
            "step 863: loss=0.1987757682800293\n",
            "step 864: loss=0.36321258544921875\n",
            "step 865: loss=1.4952280521392822\n",
            "step 866: loss=0.47373294830322266\n",
            "step 867: loss=0.6865699291229248\n",
            "step 868: loss=0.2302231788635254\n",
            "step 869: loss=2.550868272781372\n",
            "step 870: loss=8.58306884765625e-05\n",
            "step 871: loss=0.4573197364807129\n",
            "step 872: loss=0.5439281463623047\n",
            "step 873: loss=0.288485050201416\n",
            "step 874: loss=0.9280276298522949\n",
            "step 875: loss=0.36085987091064453\n",
            "step 876: loss=1.9587894678115845\n",
            "step 877: loss=0.4587221145629883\n",
            "step 878: loss=0.19086813926696777\n",
            "step 879: loss=0.11730003356933594\n",
            "step 880: loss=0.2756223678588867\n",
            "step 881: loss=0.43172788619995117\n",
            "step 882: loss=0.05208110809326172\n",
            "step 883: loss=0.2842135429382324\n",
            "step 884: loss=0.13697004318237305\n",
            "step 885: loss=0.036350250244140625\n",
            "step 886: loss=1.500255823135376\n",
            "step 887: loss=0.07356500625610352\n",
            "step 888: loss=0.17042112350463867\n",
            "step 889: loss=0.3357248306274414\n",
            "step 890: loss=0.006404399871826172\n",
            "step 891: loss=0.030520915985107422\n",
            "step 892: loss=0.0009679794311523438\n",
            "step 893: loss=0.046398162841796875\n",
            "step 894: loss=0.15717458724975586\n",
            "step 895: loss=0.06026315689086914\n",
            "step 896: loss=0.04476022720336914\n",
            "step 897: loss=0.6424202919006348\n",
            "step 898: loss=0.7885024547576904\n",
            "step 899: loss=0.017273426055908203\n",
            "step 900: loss=0.014695167541503906\n",
            "step 901: loss=0.030878543853759766\n",
            "step 902: loss=0.004338264465332031\n",
            "step 903: loss=1.2929409742355347\n",
            "step 904: loss=0.09101390838623047\n",
            "step 905: loss=0.08078718185424805\n",
            "step 906: loss=1.155379295349121\n",
            "step 907: loss=0.33635473251342773\n",
            "step 908: loss=0.006482124328613281\n",
            "step 909: loss=0.30418872833251953\n",
            "step 910: loss=0.06952381134033203\n",
            "step 911: loss=0.6127896308898926\n",
            "step 912: loss=0.15243005752563477\n",
            "step 913: loss=0.028969287872314453\n",
            "step 914: loss=4.3818135261535645\n",
            "step 915: loss=0.01670217514038086\n",
            "step 916: loss=0.7612361907958984\n",
            "step 917: loss=0.8915224075317383\n",
            "step 918: loss=0.9248113632202148\n",
            "step 919: loss=1.341148853302002\n",
            "step 920: loss=0.3162088394165039\n",
            "step 921: loss=0.4000864028930664\n",
            "step 922: loss=1.9502477645874023\n",
            "step 923: loss=0.08072280883789062\n",
            "step 924: loss=0.011460304260253906\n",
            "step 925: loss=0.011565685272216797\n",
            "step 926: loss=0.051601409912109375\n",
            "step 927: loss=0.1639399528503418\n",
            "step 928: loss=2.8063321113586426\n",
            "step 929: loss=0.29902219772338867\n",
            "step 930: loss=0.15179920196533203\n",
            "step 931: loss=0.5291564464569092\n",
            "step 932: loss=0.3462502956390381\n",
            "step 933: loss=0.06670761108398438\n",
            "step 934: loss=0.009406089782714844\n",
            "step 935: loss=0.02409839630126953\n",
            "step 936: loss=0.46036291122436523\n",
            "step 937: loss=0.03229331970214844\n",
            "step 938: loss=0.046222686767578125\n",
            "step 939: loss=0.3735795021057129\n",
            "step 940: loss=0.20336627960205078\n",
            "step 941: loss=0.024906158447265625\n",
            "step 942: loss=0.3062772750854492\n",
            "step 943: loss=0.019220352172851562\n",
            "step 944: loss=0.0009450912475585938\n",
            "step 945: loss=2.307795763015747\n",
            "step 946: loss=0.24263477325439453\n",
            "step 947: loss=0.03453683853149414\n",
            "step 948: loss=0.044391632080078125\n",
            "step 949: loss=0.3814692497253418\n",
            "step 950: loss=0.22207140922546387\n",
            "step 951: loss=0.14189815521240234\n",
            "step 952: loss=0.0019931793212890625\n",
            "step 953: loss=0.22443532943725586\n",
            "step 954: loss=0.08647441864013672\n",
            "step 955: loss=7.724761962890625e-05\n",
            "step 956: loss=0.20012760162353516\n",
            "step 957: loss=0.005602836608886719\n",
            "step 958: loss=0.8122437000274658\n",
            "step 959: loss=0.2339034080505371\n",
            "step 960: loss=0.4978032112121582\n",
            "step 961: loss=0.9441063404083252\n",
            "step 962: loss=0.059866905212402344\n",
            "step 963: loss=0.05092620849609375\n",
            "step 964: loss=0.0217132568359375\n",
            "step 965: loss=1.3094550371170044\n",
            "step 966: loss=0.08177995681762695\n",
            "step 967: loss=0.5009264945983887\n",
            "step 968: loss=2.086491584777832\n",
            "step 969: loss=0.11873388290405273\n",
            "step 970: loss=0.23385000228881836\n",
            "step 971: loss=0.008561134338378906\n",
            "step 972: loss=0.25334978103637695\n",
            "step 973: loss=0.1501774787902832\n",
            "step 974: loss=1.9254882335662842\n",
            "step 975: loss=1.4295926094055176\n",
            "step 976: loss=0.00786447525024414\n",
            "step 977: loss=0.11253166198730469\n",
            "step 978: loss=1.1687957048416138\n",
            "step 979: loss=0.7309257984161377\n",
            "step 980: loss=0.24778318405151367\n",
            "step 981: loss=0.5749430656433105\n",
            "step 982: loss=0.6188721656799316\n",
            "step 983: loss=0.26140594482421875\n",
            "step 984: loss=0.02825021743774414\n",
            "step 985: loss=1.3639967441558838\n",
            "step 986: loss=0.08413457870483398\n",
            "step 987: loss=0.06549072265625\n",
            "step 988: loss=0.2288646697998047\n",
            "step 989: loss=3.528594970703125e-05\n",
            "step 990: loss=0.08487510681152344\n",
            "step 991: loss=0.1023263931274414\n",
            "step 992: loss=0.6776700019836426\n",
            "step 993: loss=0.003749847412109375\n",
            "step 994: loss=0.17822027206420898\n",
            "step 995: loss=0.6524538993835449\n",
            "step 996: loss=0.00047206878662109375\n",
            "step 997: loss=0.0029249191284179688\n",
            "step 998: loss=0.32758665084838867\n",
            "step 999: loss=0.05590200424194336\n",
            "step 1000: loss=0.00032520294189453125\n",
            "[low-rank] Mean loss        0.5849002\n",
            "[low-rank] True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
            "[low-rank] Predictions:     [2 0 4 8 7 6 0 8 3 1]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "rank = 32\n",
        "layer_sizes = (784, 128, 10)\n",
        "\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "params_lowrank = train_mlp(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    use_lowrank=True,\n",
        "    rank=rank,\n",
        "    layer_sizes=layer_sizes,\n",
        ")\n",
        "evaluate_mlp(\n",
        "    test_data,\n",
        "    params_lowrank,\n",
        "    use_lowrank=True,\n",
        "    rank=rank,\n",
        "    layer_sizes=layer_sizes,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
