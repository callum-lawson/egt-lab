{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bffd5b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Key differences from JAX implementation:**  \n",
    "- <b>Network definition:</b> Use a Flax <code>nn.Module</code> (e.g., an <code>MLP</code> class) instead of lists of parameter dicts.  \n",
    "- <b>Initialization:</b> Flax handles parameter initialization with <code>model.init(...)</code>, using specified initializers within the class.  \n",
    "- <b>Forward pass:</b> Compute outputs with <code>model.apply(params, x)</code> instead of manual matrix multiplications.  \n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    \"\"\"Produces one greyscale vector per sample\"\"\"\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict: dict, subsample_size: int=0):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data = flatten_image_for_mlp(data)\n",
    "        if subsample_size > 0:\n",
    "            data = data[:subsample_size]\n",
    "        data_jax[data_type] = data\n",
    "\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9813eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    layer_sizes: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, activations):\n",
    "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
    "            activations = nn.Dense(\n",
    "                layer_size,\n",
    "                kernel_init=nn.initializers.normal(0.1),\n",
    "                bias_init=nn.initializers.normal(0.1)\n",
    "            )(activations)\n",
    "\n",
    "            if layer_number != (len(self.layer_sizes) - 1):\n",
    "                activations = nn.relu(activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_network_params(model, input_layer_size, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
    "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "244a19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_loss_batch(params, apply_fn, images, labels):\n",
    "    logits = apply_fn({\"params\": params}, images) # FORWARD PASS\n",
    "    cross_entropy_by_sample = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    cross_entropy_mean = cross_entropy_by_sample.mean()\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fa6224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def take_training_step(training_state, images, labels):\n",
    "    \"\"\"\n",
    "    Single training step \n",
    "    The model and optimiser are passed in the training state\n",
    "    returns a training state\n",
    "    \"\"\"\n",
    "    grads_by_params_fn = jax.grad(calculate_mean_loss_batch)\n",
    "    grads_by_params = grads_by_params_fn(\n",
    "        training_state.params,     # params is first â†’ grad w.r.t. params\n",
    "        training_state.apply_fn,\n",
    "        images,\n",
    "        labels,\n",
    "    )\n",
    "    return training_state.apply_gradients(grads=grads_by_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d4d625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(images, labels, n_batches):\n",
    "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
    "    n_samples = len(images)\n",
    "    assert len(images) == len(labels)\n",
    "    assert n_samples >= n_batches\n",
    "    assert n_batches > 0\n",
    "    n_samples_per_batch = n_samples // n_batches\n",
    "    start = 0\n",
    "    end = n_samples_per_batch\n",
    "    while end <= n_samples: \n",
    "        yield (images[start:end], labels[start:end])\n",
    "        start += n_samples_per_batch\n",
    "        end += n_samples_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "72c149a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(images, labels, n_steps, layer_sizes, optimizer, checkpoint_manager, key, steps_per_save=100, training_state=None, start_step=1):\n",
    "    \"\"\"\n",
    "    The training state ('state') is an instance of TrainState that holds:\n",
    "    - apply_fn: the model's apply function, used for forward passes\n",
    "    - params: the parameters of the neural network\n",
    "    - tx: the optimizers (Optax transformation) for parameter updates\n",
    "    - opt_state: the state of the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer_size = layer_sizes[0]\n",
    "    network_layer_sizes = layer_sizes[1:]\n",
    "    model = MLP(layer_sizes=network_layer_sizes)\n",
    "\n",
    "    if training_state is None:\n",
    "        params = initialise_network_params(model, input_layer_size, key)\n",
    "        apply_fn = model.apply\n",
    "        training_state = train_state.TrainState.create(\n",
    "            apply_fn=apply_fn,\n",
    "            params=params,\n",
    "            tx=optimizer,\n",
    "        )\n",
    "\n",
    "    step = start_step\n",
    "    for images_batch, labels_batch in get_batches(images=images, labels=labels, n_batches=n_steps):\n",
    "        training_state = take_training_step(training_state, images_batch, labels_batch)\n",
    "        loss = calculate_mean_loss_batch(training_state.params, training_state.apply_fn, images_batch, labels_batch)\n",
    "        print(f\"step {step}: loss={loss}\")\n",
    "        if step == 1 or step % steps_per_save == 0:\n",
    "            to_save = {\n",
    "                \"params\": training_state.params,\n",
    "                \"optimiser_state\": training_state.opt_state,\n",
    "            }\n",
    "            checkpoint_manager.save(\n",
    "                step,\n",
    "                args=ocp.args.StandardSave(to_save),\n",
    "            )\n",
    "        step += 1\n",
    "\n",
    "    return training_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "27fcb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_checkpoint_manager(experiment_name: str = \"mlp\"):\n",
    "    project_root = Path().resolve()\n",
    "    base_dir = project_root / \"checkpoints\"\n",
    "    checkpoint_dir = base_dir / experiment_name\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_manager = ocp.CheckpointManager(\n",
    "        directory=str(checkpoint_dir),\n",
    "        options=ocp.CheckpointManagerOptions(max_to_keep=3),\n",
    "    )\n",
    "    return checkpoint_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "852ef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experiment_name(layer_sizes, optimizer):\n",
    "    layer_part = \"mlp_\" + \"-\".join(str(s) for s in layer_sizes)\n",
    "    opt_name = optimizer.__class__.__name__\n",
    "    return f\"{layer_part}_{opt_name}\"\n",
    "\n",
    "\n",
    "def train_mlp(train_data, optimizer, n_steps=10**3):\n",
    "    layer_sizes = [784, 128, 10]\n",
    "    experiment_name = make_experiment_name(layer_sizes, optimizer)\n",
    "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
    "    key = jax.random.key(0)\n",
    "    final_params = run_training(\n",
    "        train_data[\"image\"], \n",
    "        train_data[\"label\"], \n",
    "        n_steps, \n",
    "        layer_sizes, \n",
    "        optimizer,\n",
    "        checkpoint_manager,\n",
    "        key,\n",
    "        )\n",
    "    return final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dca913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_sizes(params):\n",
    "    layer_sizes = []\n",
    "    for layer, layer_params in enumerate(params.values()):\n",
    "        if layer == 0:\n",
    "            layer_sizes.append(layer_params[\"kernel\"].shape[0])\n",
    "            layer_sizes.append(layer_params[\"kernel\"].shape[1])\n",
    "        else:\n",
    "            layer_sizes.append(layer_params[\"bias\"].shape[0])\n",
    "    return layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1b7d5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlp(test_data, params, n_examples=10):\n",
    "    layer_sizes = extract_layer_sizes(params)\n",
    "    model = MLP(layer_sizes=layer_sizes[1:])\n",
    "    apply_fn = model.apply\n",
    "\n",
    "    images = test_data[\"image\"]\n",
    "    labels = test_data[\"label\"]\n",
    "\n",
    "    mean_loss = calculate_mean_loss_batch(params, apply_fn, images, labels)\n",
    "    example_images = images[:n_examples]\n",
    "    example_labels = labels[:n_examples]\n",
    "    logits = apply_fn({\"params\": params}, example_images)\n",
    "    example_predictions = jnp.argmax(logits, axis=1)\n",
    "\n",
    "    print(\"Mean loss       \", mean_loss)\n",
    "    print(\"True labels:    \", example_labels)\n",
    "    print(\"Predictions:    \", example_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876ad27",
   "metadata": {},
   "source": [
    "1. Learning rate decay\n",
    "2. Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4575e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**3) \n",
    "test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b492873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.2421865463256836\n",
      "step 2: loss=1.4549814462661743\n",
      "step 3: loss=3.0767364501953125\n",
      "step 4: loss=1.6018766164779663\n",
      "step 5: loss=1.922335147857666\n",
      "step 6: loss=1.5518207550048828\n",
      "step 7: loss=2.2047572135925293\n",
      "step 8: loss=2.28387188911438\n",
      "step 9: loss=1.3409204483032227\n",
      "step 10: loss=2.344090700149536\n",
      "step 11: loss=2.6371214389801025\n",
      "step 12: loss=3.4621236324310303\n",
      "step 13: loss=1.713283658027649\n",
      "step 14: loss=1.9781702756881714\n",
      "step 15: loss=3.3416006565093994\n",
      "step 16: loss=2.9397127628326416\n",
      "step 17: loss=1.0316027402877808\n",
      "step 18: loss=2.92775821685791\n",
      "step 19: loss=2.7445106506347656\n",
      "step 20: loss=1.4158217906951904\n",
      "step 21: loss=1.9610295295715332\n",
      "step 22: loss=1.9986664056777954\n",
      "step 23: loss=1.672990083694458\n",
      "step 24: loss=2.457414388656616\n",
      "step 25: loss=1.5557224750518799\n",
      "step 26: loss=2.3051018714904785\n",
      "step 27: loss=2.603790044784546\n",
      "step 28: loss=1.988590955734253\n",
      "step 29: loss=2.3711462020874023\n",
      "step 30: loss=1.6227275133132935\n",
      "step 31: loss=1.862285852432251\n",
      "step 32: loss=2.1689186096191406\n",
      "step 33: loss=1.754330039024353\n",
      "step 34: loss=1.313774824142456\n",
      "step 35: loss=1.7799018621444702\n",
      "step 36: loss=1.6797168254852295\n",
      "step 37: loss=2.9055237770080566\n",
      "step 38: loss=1.5091187953948975\n",
      "step 39: loss=2.635629892349243\n",
      "step 40: loss=1.9036000967025757\n",
      "step 41: loss=0.8337869644165039\n",
      "step 42: loss=2.479337453842163\n",
      "step 43: loss=2.0102226734161377\n",
      "step 44: loss=1.8198356628417969\n",
      "step 45: loss=2.361449956893921\n",
      "step 46: loss=1.0358593463897705\n",
      "step 47: loss=2.0157792568206787\n",
      "step 48: loss=1.810377597808838\n",
      "step 49: loss=3.01554536819458\n",
      "step 50: loss=1.4894685745239258\n",
      "step 51: loss=1.1730490922927856\n",
      "step 52: loss=1.2065200805664062\n",
      "step 53: loss=3.256063938140869\n",
      "step 54: loss=1.3365157842636108\n",
      "step 55: loss=1.2758262157440186\n",
      "step 56: loss=2.6482057571411133\n",
      "step 57: loss=2.540787696838379\n",
      "step 58: loss=1.3128734827041626\n",
      "step 59: loss=2.506495714187622\n",
      "step 60: loss=1.0716583728790283\n",
      "step 61: loss=0.8002644777297974\n",
      "step 62: loss=1.215010404586792\n",
      "step 63: loss=3.9132957458496094\n",
      "step 64: loss=2.513690233230591\n",
      "step 65: loss=2.3943910598754883\n",
      "step 66: loss=0.41791653633117676\n",
      "step 67: loss=0.9733765125274658\n",
      "step 68: loss=0.6354427337646484\n",
      "step 69: loss=3.076798439025879\n",
      "step 70: loss=1.3095070123672485\n",
      "step 71: loss=3.296731948852539\n",
      "step 72: loss=3.5359928607940674\n",
      "step 73: loss=0.5057768821716309\n",
      "step 74: loss=1.0847030878067017\n",
      "step 75: loss=0.8231465816497803\n",
      "step 76: loss=0.12777495384216309\n",
      "step 77: loss=2.528164863586426\n",
      "step 78: loss=2.6920502185821533\n",
      "step 79: loss=1.4572802782058716\n",
      "step 80: loss=0.3264431953430176\n",
      "step 81: loss=0.3224916458129883\n",
      "step 82: loss=0.880436897277832\n",
      "step 83: loss=1.6871346235275269\n",
      "step 84: loss=0.7689778804779053\n",
      "step 85: loss=2.236729621887207\n",
      "step 86: loss=0.39814305305480957\n",
      "step 87: loss=2.505406141281128\n",
      "step 88: loss=1.016530990600586\n",
      "step 89: loss=1.1767767667770386\n",
      "step 90: loss=1.6118593215942383\n",
      "step 91: loss=1.3020222187042236\n",
      "step 92: loss=1.0679250955581665\n",
      "step 93: loss=0.22047114372253418\n",
      "step 94: loss=1.8968682289123535\n",
      "step 95: loss=0.7754878997802734\n",
      "step 96: loss=2.2688071727752686\n",
      "step 97: loss=1.6609830856323242\n",
      "step 98: loss=1.714942216873169\n",
      "step 99: loss=0.8860684633255005\n",
      "step 100: loss=1.043237566947937\n",
      "step 101: loss=1.6429299116134644\n",
      "step 102: loss=1.3760747909545898\n",
      "step 103: loss=2.3424899578094482\n",
      "step 104: loss=0.7394161224365234\n",
      "step 105: loss=0.8303945064544678\n",
      "step 106: loss=1.6889162063598633\n",
      "step 107: loss=1.849713921546936\n",
      "step 108: loss=0.8009697198867798\n",
      "step 109: loss=1.9210385084152222\n",
      "step 110: loss=1.5583198070526123\n",
      "step 111: loss=0.6788965463638306\n",
      "step 112: loss=0.4137570858001709\n",
      "step 113: loss=1.6254088878631592\n",
      "step 114: loss=1.488877534866333\n",
      "step 115: loss=0.2103595733642578\n",
      "step 116: loss=0.9437799453735352\n",
      "step 117: loss=1.3162198066711426\n",
      "step 118: loss=0.7798060178756714\n",
      "step 119: loss=0.9784126281738281\n",
      "step 120: loss=1.2214597463607788\n",
      "step 121: loss=3.1054039001464844\n",
      "step 122: loss=1.6958223581314087\n",
      "step 123: loss=2.7126426696777344\n",
      "step 124: loss=0.5880532264709473\n",
      "step 125: loss=0.2147374153137207\n",
      "step 126: loss=1.3695894479751587\n",
      "step 127: loss=1.7903401851654053\n",
      "step 128: loss=0.8010847568511963\n",
      "step 129: loss=0.47148585319519043\n",
      "step 130: loss=0.42186427116394043\n",
      "step 131: loss=0.919746994972229\n",
      "step 132: loss=2.370388984680176\n",
      "step 133: loss=0.7040364742279053\n",
      "step 134: loss=0.6195166110992432\n",
      "step 135: loss=0.8623490333557129\n",
      "step 136: loss=0.2626211643218994\n",
      "step 137: loss=0.47997236251831055\n",
      "step 138: loss=0.6141399145126343\n",
      "step 139: loss=1.0177688598632812\n",
      "step 140: loss=1.6761692762374878\n",
      "step 141: loss=0.37014293670654297\n",
      "step 142: loss=2.50685977935791\n",
      "step 143: loss=0.473976731300354\n",
      "step 144: loss=1.8222389221191406\n",
      "step 145: loss=0.10611176490783691\n",
      "step 146: loss=1.3444867134094238\n",
      "step 147: loss=1.890256643295288\n",
      "step 148: loss=2.019212007522583\n",
      "step 149: loss=0.708979606628418\n",
      "step 150: loss=0.3448455333709717\n",
      "step 151: loss=1.4451547861099243\n",
      "step 152: loss=0.887959361076355\n",
      "step 153: loss=1.6465024948120117\n",
      "step 154: loss=1.2734510898590088\n",
      "step 155: loss=0.8892346620559692\n",
      "step 156: loss=0.8326750993728638\n",
      "step 157: loss=1.0322668552398682\n",
      "step 158: loss=1.7533605098724365\n",
      "step 159: loss=0.9787423610687256\n",
      "step 160: loss=1.5738203525543213\n",
      "step 161: loss=0.48970115184783936\n",
      "step 162: loss=0.15803837776184082\n",
      "step 163: loss=0.7275893688201904\n",
      "step 164: loss=0.5339555740356445\n",
      "step 165: loss=1.5419628620147705\n",
      "step 166: loss=0.4611380100250244\n",
      "step 167: loss=2.06067156791687\n",
      "step 168: loss=2.2312281131744385\n",
      "step 169: loss=0.656808614730835\n",
      "step 170: loss=0.4211156368255615\n",
      "step 171: loss=0.1600961685180664\n",
      "step 172: loss=0.6779335737228394\n",
      "step 173: loss=0.1400458812713623\n",
      "step 174: loss=0.7043989896774292\n",
      "step 175: loss=2.0440971851348877\n",
      "step 176: loss=0.5685373544692993\n",
      "step 177: loss=0.7488729953765869\n",
      "step 178: loss=0.2943863868713379\n",
      "step 179: loss=0.2867746353149414\n",
      "step 180: loss=2.0111541748046875\n",
      "step 181: loss=0.5551998615264893\n",
      "step 182: loss=0.42223358154296875\n",
      "step 183: loss=0.4737052917480469\n",
      "step 184: loss=2.3058722019195557\n",
      "step 185: loss=0.40650367736816406\n",
      "step 186: loss=0.4796102046966553\n",
      "step 187: loss=0.6871426105499268\n",
      "step 188: loss=2.680981159210205\n",
      "step 189: loss=0.7373931407928467\n",
      "step 190: loss=0.6501474380493164\n",
      "step 191: loss=0.44893431663513184\n",
      "step 192: loss=0.12216806411743164\n",
      "step 193: loss=0.12809205055236816\n",
      "step 194: loss=0.3562130928039551\n",
      "step 195: loss=0.060632944107055664\n",
      "step 196: loss=1.4033054113388062\n",
      "step 197: loss=1.2201976776123047\n",
      "step 198: loss=0.2856619358062744\n",
      "step 199: loss=2.5699079036712646\n",
      "step 200: loss=1.3816474676132202\n",
      "step 201: loss=0.4719282388687134\n",
      "step 202: loss=2.0478811264038086\n",
      "step 203: loss=0.4015340805053711\n",
      "step 204: loss=0.14312219619750977\n",
      "step 205: loss=0.49718499183654785\n",
      "step 206: loss=0.7516837120056152\n",
      "step 207: loss=0.17197680473327637\n",
      "step 208: loss=0.5553793907165527\n",
      "step 209: loss=0.3742920160293579\n",
      "step 210: loss=0.7562179565429688\n",
      "step 211: loss=0.15678930282592773\n",
      "step 212: loss=1.6474084854125977\n",
      "step 213: loss=1.2462327480316162\n",
      "step 214: loss=1.599760890007019\n",
      "step 215: loss=0.15593624114990234\n",
      "step 216: loss=0.29805707931518555\n",
      "step 217: loss=0.29372286796569824\n",
      "step 218: loss=0.15705633163452148\n",
      "step 219: loss=0.22482776641845703\n",
      "step 220: loss=0.9453972578048706\n",
      "step 221: loss=1.1685106754302979\n",
      "step 222: loss=2.3041555881500244\n",
      "step 223: loss=0.9989070892333984\n",
      "step 224: loss=0.7327048778533936\n",
      "step 225: loss=0.37357258796691895\n",
      "step 226: loss=0.6927950382232666\n",
      "step 227: loss=0.4200795888900757\n",
      "step 228: loss=0.5350275039672852\n",
      "step 229: loss=0.604094386100769\n",
      "step 230: loss=0.5697106122970581\n",
      "step 231: loss=0.8951528668403625\n",
      "step 232: loss=0.281496524810791\n",
      "step 233: loss=0.0047473907470703125\n",
      "step 234: loss=2.366014003753662\n",
      "step 235: loss=0.27408552169799805\n",
      "step 236: loss=1.1047792434692383\n",
      "step 237: loss=0.040659427642822266\n",
      "step 238: loss=0.22864484786987305\n",
      "step 239: loss=0.3796806335449219\n",
      "step 240: loss=0.34667110443115234\n",
      "step 241: loss=0.4054981470108032\n",
      "step 242: loss=0.1246330738067627\n",
      "step 243: loss=0.5560510158538818\n",
      "step 244: loss=0.44894957542419434\n",
      "step 245: loss=0.4151649475097656\n",
      "step 246: loss=0.9324600696563721\n",
      "step 247: loss=0.44788551330566406\n",
      "step 248: loss=3.057542085647583\n",
      "step 249: loss=0.05451393127441406\n",
      "step 250: loss=0.06193351745605469\n",
      "step 251: loss=2.078672170639038\n",
      "step 252: loss=0.5296133756637573\n",
      "step 253: loss=0.9901057481765747\n",
      "step 254: loss=0.09525799751281738\n",
      "step 255: loss=0.07671642303466797\n",
      "step 256: loss=0.489521861076355\n",
      "step 257: loss=2.7890677452087402\n",
      "step 258: loss=0.3984382152557373\n",
      "step 259: loss=0.8330893516540527\n",
      "step 260: loss=0.47151756286621094\n",
      "step 261: loss=0.983100175857544\n",
      "step 262: loss=2.7335660457611084\n",
      "step 263: loss=0.32002758979797363\n",
      "step 264: loss=0.1883378028869629\n",
      "step 265: loss=0.4457817077636719\n",
      "step 266: loss=0.6293996572494507\n",
      "step 267: loss=0.46929991245269775\n",
      "step 268: loss=0.7847534418106079\n",
      "step 269: loss=0.032675743103027344\n",
      "step 270: loss=0.6592978239059448\n",
      "step 271: loss=0.03919816017150879\n",
      "step 272: loss=0.7018625736236572\n",
      "step 273: loss=0.5264933109283447\n",
      "step 274: loss=0.12643933296203613\n",
      "step 275: loss=1.092840552330017\n",
      "step 276: loss=0.03591275215148926\n",
      "step 277: loss=0.29474616050720215\n",
      "step 278: loss=0.43141674995422363\n",
      "step 279: loss=0.4353444576263428\n",
      "step 280: loss=0.35237371921539307\n",
      "step 281: loss=0.08173036575317383\n",
      "step 282: loss=0.05996131896972656\n",
      "step 283: loss=1.4846298694610596\n",
      "step 284: loss=1.8887841701507568\n",
      "step 285: loss=0.4659993648529053\n",
      "step 286: loss=0.2334280014038086\n",
      "step 287: loss=0.15755367279052734\n",
      "step 288: loss=0.07255125045776367\n",
      "step 289: loss=2.1834232807159424\n",
      "step 290: loss=0.18325090408325195\n",
      "step 291: loss=0.37158799171447754\n",
      "step 292: loss=1.1927613019943237\n",
      "step 293: loss=0.044721126556396484\n",
      "step 294: loss=0.009671211242675781\n",
      "step 295: loss=0.8239340782165527\n",
      "step 296: loss=0.2812347412109375\n",
      "step 297: loss=0.29264354705810547\n",
      "step 298: loss=0.07653141021728516\n",
      "step 299: loss=0.012074947357177734\n",
      "step 300: loss=0.05053567886352539\n",
      "step 301: loss=0.006805896759033203\n",
      "step 302: loss=2.7376489639282227\n",
      "step 303: loss=0.9433908462524414\n",
      "step 304: loss=0.3315999507904053\n",
      "step 305: loss=0.4553706645965576\n",
      "step 306: loss=0.3246622085571289\n",
      "step 307: loss=0.9534530639648438\n",
      "step 308: loss=0.044396400451660156\n",
      "step 309: loss=0.4811127185821533\n",
      "step 310: loss=0.48668503761291504\n",
      "step 311: loss=2.132352828979492\n",
      "step 312: loss=0.17094135284423828\n",
      "step 313: loss=0.12753868103027344\n",
      "step 314: loss=0.551832914352417\n",
      "step 315: loss=0.07557868957519531\n",
      "step 316: loss=0.30028343200683594\n",
      "step 317: loss=0.33416497707366943\n",
      "step 318: loss=0.3379436731338501\n",
      "step 319: loss=0.21451210975646973\n",
      "step 320: loss=0.3522576093673706\n",
      "step 321: loss=0.8380552530288696\n",
      "step 322: loss=0.17711257934570312\n",
      "step 323: loss=0.09087562561035156\n",
      "step 324: loss=0.07382750511169434\n",
      "step 325: loss=0.23125982284545898\n",
      "step 326: loss=1.4439153671264648\n",
      "step 327: loss=0.47332286834716797\n",
      "step 328: loss=2.746384859085083\n",
      "step 329: loss=0.005428791046142578\n",
      "step 330: loss=0.5077652931213379\n",
      "step 331: loss=0.06275653839111328\n",
      "step 332: loss=0.3127434253692627\n",
      "step 333: loss=0.04568958282470703\n",
      "step 334: loss=0.0166778564453125\n",
      "step 335: loss=0.9150269031524658\n",
      "step 336: loss=0.34367990493774414\n",
      "step 337: loss=1.1579874753952026\n",
      "step 338: loss=0.33712172508239746\n",
      "step 339: loss=0.47450244426727295\n",
      "step 340: loss=0.0789024829864502\n",
      "step 341: loss=1.6881626844406128\n",
      "step 342: loss=0.3500051498413086\n",
      "step 343: loss=0.08574962615966797\n",
      "step 344: loss=1.064251184463501\n",
      "step 345: loss=0.7578305006027222\n",
      "step 346: loss=0.07385015487670898\n",
      "step 347: loss=0.2304244041442871\n",
      "step 348: loss=0.23037385940551758\n",
      "step 349: loss=0.35393309593200684\n",
      "step 350: loss=0.018471717834472656\n",
      "step 351: loss=0.36508679389953613\n",
      "step 352: loss=1.012511134147644\n",
      "step 353: loss=2.5447545051574707\n",
      "step 354: loss=0.7907414436340332\n",
      "step 355: loss=0.7476091384887695\n",
      "step 356: loss=0.012488365173339844\n",
      "step 357: loss=0.3724796772003174\n",
      "step 358: loss=1.1695151329040527\n",
      "step 359: loss=0.17859244346618652\n",
      "step 360: loss=0.08150219917297363\n",
      "step 361: loss=0.5774656534194946\n",
      "step 362: loss=0.12587261199951172\n",
      "step 363: loss=3.199237823486328\n",
      "step 364: loss=0.12036800384521484\n",
      "step 365: loss=1.0232316255569458\n",
      "step 366: loss=0.5209536552429199\n",
      "step 367: loss=0.47525346279144287\n",
      "step 368: loss=0.01161336898803711\n",
      "step 369: loss=0.19076108932495117\n",
      "step 370: loss=2.11970591545105\n",
      "step 371: loss=0.198927640914917\n",
      "step 372: loss=0.015585899353027344\n",
      "step 373: loss=0.014299869537353516\n",
      "step 374: loss=0.08033037185668945\n",
      "step 375: loss=1.9676306247711182\n",
      "step 376: loss=1.01432204246521\n",
      "step 377: loss=0.5524725914001465\n",
      "step 378: loss=0.629766583442688\n",
      "step 379: loss=3.3354454040527344\n",
      "step 380: loss=0.0397944450378418\n",
      "step 381: loss=0.008146286010742188\n",
      "step 382: loss=0.39034605026245117\n",
      "step 383: loss=0.3461265563964844\n",
      "step 384: loss=0.0007467269897460938\n",
      "step 385: loss=0.06762409210205078\n",
      "step 386: loss=0.09310150146484375\n",
      "step 387: loss=0.11734533309936523\n",
      "step 388: loss=0.42160606384277344\n",
      "step 389: loss=0.04375648498535156\n",
      "step 390: loss=0.6377222537994385\n",
      "step 391: loss=0.13796329498291016\n",
      "step 392: loss=0.9367666244506836\n",
      "step 393: loss=1.7518149614334106\n",
      "step 394: loss=3.9221692085266113\n",
      "step 395: loss=0.05047798156738281\n",
      "step 396: loss=0.25234508514404297\n",
      "step 397: loss=0.1284027099609375\n",
      "step 398: loss=0.9674408435821533\n",
      "step 399: loss=0.41379332542419434\n",
      "step 400: loss=0.029466629028320312\n",
      "step 401: loss=0.5387718677520752\n",
      "step 402: loss=0.44220614433288574\n",
      "step 403: loss=0.004082679748535156\n",
      "step 404: loss=0.01567983627319336\n",
      "step 405: loss=0.015554428100585938\n",
      "step 406: loss=0.6025831699371338\n",
      "step 407: loss=0.08917903900146484\n",
      "step 408: loss=0.018439769744873047\n",
      "step 409: loss=3.631787061691284\n",
      "step 410: loss=0.054201602935791016\n",
      "step 411: loss=0.5691431760787964\n",
      "step 412: loss=0.01025390625\n",
      "step 413: loss=0.16602587699890137\n",
      "step 414: loss=0.2280120849609375\n",
      "step 415: loss=0.033725738525390625\n",
      "step 416: loss=2.8930912017822266\n",
      "step 417: loss=0.4680633544921875\n",
      "step 418: loss=0.11901974678039551\n",
      "step 419: loss=1.3566269874572754\n",
      "step 420: loss=0.05841684341430664\n",
      "step 421: loss=0.28403472900390625\n",
      "step 422: loss=0.8525378704071045\n",
      "step 423: loss=0.2811431884765625\n",
      "step 424: loss=0.020607948303222656\n",
      "step 425: loss=0.8618518114089966\n",
      "step 426: loss=1.1827762126922607\n",
      "step 427: loss=0.8799014091491699\n",
      "step 428: loss=0.19707965850830078\n",
      "step 429: loss=0.2692444324493408\n",
      "step 430: loss=0.0019240379333496094\n",
      "step 431: loss=0.5260114669799805\n",
      "step 432: loss=0.2273731231689453\n",
      "step 433: loss=0.3134944438934326\n",
      "step 434: loss=1.1230807304382324\n",
      "step 435: loss=0.05976676940917969\n",
      "step 436: loss=0.18072032928466797\n",
      "step 437: loss=0.002167224884033203\n",
      "step 438: loss=0.01470041275024414\n",
      "step 439: loss=0.9843233823776245\n",
      "step 440: loss=0.011989116668701172\n",
      "step 441: loss=0.14016485214233398\n",
      "step 442: loss=0.7073942422866821\n",
      "step 443: loss=1.4848883152008057\n",
      "step 444: loss=1.0383212566375732\n",
      "step 445: loss=0.41783320903778076\n",
      "step 446: loss=0.7536642551422119\n",
      "step 447: loss=0.07221364974975586\n",
      "step 448: loss=0.05558300018310547\n",
      "step 449: loss=0.8591423034667969\n",
      "step 450: loss=3.564875364303589\n",
      "step 451: loss=0.006774425506591797\n",
      "step 452: loss=1.669120192527771\n",
      "step 453: loss=0.12167143821716309\n",
      "step 454: loss=3.2515270709991455\n",
      "step 455: loss=1.7813029289245605\n",
      "step 456: loss=1.3433032035827637\n",
      "step 457: loss=0.18211889266967773\n",
      "step 458: loss=0.7063822746276855\n",
      "step 459: loss=0.08410120010375977\n",
      "step 460: loss=0.03919649124145508\n",
      "step 461: loss=2.979452610015869\n",
      "step 462: loss=0.0026497840881347656\n",
      "step 463: loss=0.43689000606536865\n",
      "step 464: loss=0.07270312309265137\n",
      "step 465: loss=1.1039822101593018\n",
      "step 466: loss=0.03798484802246094\n",
      "step 467: loss=1.9705100059509277\n",
      "step 468: loss=0.11281657218933105\n",
      "step 469: loss=0.687421441078186\n",
      "step 470: loss=0.13335704803466797\n",
      "step 471: loss=2.6948351860046387\n",
      "step 472: loss=1.5483877658843994\n",
      "step 473: loss=0.11220932006835938\n",
      "step 474: loss=0.44478631019592285\n",
      "step 475: loss=0.013322830200195312\n",
      "step 476: loss=0.38378405570983887\n",
      "step 477: loss=0.04301595687866211\n",
      "step 478: loss=0.026033878326416016\n",
      "step 479: loss=0.1391465663909912\n",
      "step 480: loss=0.3230702877044678\n",
      "step 481: loss=0.24133872985839844\n",
      "step 482: loss=0.29029393196105957\n",
      "step 483: loss=0.4514617919921875\n",
      "step 484: loss=0.44598913192749023\n",
      "step 485: loss=1.3443806171417236\n",
      "step 486: loss=0.010129451751708984\n",
      "step 487: loss=0.008464813232421875\n",
      "step 488: loss=0.10316658020019531\n",
      "step 489: loss=0.14998960494995117\n",
      "step 490: loss=0.24096250534057617\n",
      "step 491: loss=0.0014219284057617188\n",
      "step 492: loss=0.05595660209655762\n",
      "step 493: loss=0.04845285415649414\n",
      "step 494: loss=0.3449218273162842\n",
      "step 495: loss=0.24515247344970703\n",
      "step 496: loss=1.8468502759933472\n",
      "step 497: loss=0.025726318359375\n",
      "step 498: loss=0.10625171661376953\n",
      "step 499: loss=3.827439308166504\n",
      "step 500: loss=0.8061550855636597\n",
      "step 501: loss=0.02237081527709961\n",
      "step 502: loss=0.3995704650878906\n",
      "step 503: loss=0.20708990097045898\n",
      "step 504: loss=0.045786142349243164\n",
      "step 505: loss=0.032854557037353516\n",
      "step 506: loss=0.20964956283569336\n",
      "step 507: loss=0.9962095022201538\n",
      "step 508: loss=0.16164588928222656\n",
      "step 509: loss=0.03909635543823242\n",
      "step 510: loss=0.10247039794921875\n",
      "step 511: loss=0.33330416679382324\n",
      "step 512: loss=0.12527990341186523\n",
      "step 513: loss=0.21143198013305664\n",
      "step 514: loss=0.3745746612548828\n",
      "step 515: loss=0.027648448944091797\n",
      "step 516: loss=0.08393144607543945\n",
      "step 517: loss=0.5669987201690674\n",
      "step 518: loss=0.957893967628479\n",
      "step 519: loss=0.02363729476928711\n",
      "step 520: loss=1.48310387134552\n",
      "step 521: loss=0.13979244232177734\n",
      "step 522: loss=0.042284488677978516\n",
      "step 523: loss=0.062416791915893555\n",
      "step 524: loss=0.03997659683227539\n",
      "step 525: loss=0.14003419876098633\n",
      "step 526: loss=3.1203131675720215\n",
      "step 527: loss=0.0059146881103515625\n",
      "step 528: loss=0.00048542022705078125\n",
      "step 529: loss=0.29979002475738525\n",
      "step 530: loss=0.05205678939819336\n",
      "step 531: loss=0.16924405097961426\n",
      "step 532: loss=0.0011043548583984375\n",
      "step 533: loss=0.22463774681091309\n",
      "step 534: loss=0.05318403244018555\n",
      "step 535: loss=0.08916521072387695\n",
      "step 536: loss=0.07006692886352539\n",
      "step 537: loss=0.07175874710083008\n",
      "step 538: loss=1.605657696723938\n",
      "step 539: loss=0.8576207160949707\n",
      "step 540: loss=3.2982354164123535\n",
      "step 541: loss=0.0028433799743652344\n",
      "step 542: loss=0.19463753700256348\n",
      "step 543: loss=0.8926622271537781\n",
      "step 544: loss=0.12177920341491699\n",
      "step 545: loss=0.7661073207855225\n",
      "step 546: loss=0.662636399269104\n",
      "step 547: loss=0.07155418395996094\n",
      "step 548: loss=0.01363372802734375\n",
      "step 549: loss=0.2269115447998047\n",
      "step 550: loss=0.19848036766052246\n",
      "step 551: loss=0.945902943611145\n",
      "step 552: loss=0.6102805137634277\n",
      "step 553: loss=0.054195404052734375\n",
      "step 554: loss=2.051149606704712\n",
      "step 555: loss=0.6343873739242554\n",
      "step 556: loss=1.581680178642273\n",
      "step 557: loss=0.35944652557373047\n",
      "step 558: loss=0.005198001861572266\n",
      "step 559: loss=0.0024509429931640625\n",
      "step 560: loss=0.09252715110778809\n",
      "step 561: loss=0.15797710418701172\n",
      "step 562: loss=0.5221741199493408\n",
      "step 563: loss=0.1967637538909912\n",
      "step 564: loss=0.16375017166137695\n",
      "step 565: loss=0.019205570220947266\n",
      "step 566: loss=2.8943355083465576\n",
      "step 567: loss=0.06458353996276855\n",
      "step 568: loss=0.2374105453491211\n",
      "step 569: loss=0.3637881278991699\n",
      "step 570: loss=0.05800652503967285\n",
      "step 571: loss=0.04111528396606445\n",
      "step 572: loss=0.05983877182006836\n",
      "step 573: loss=0.3058300018310547\n",
      "step 574: loss=1.237825632095337\n",
      "step 575: loss=0.28524351119995117\n",
      "step 576: loss=0.21417760848999023\n",
      "step 577: loss=0.4051241874694824\n",
      "step 578: loss=0.02850055694580078\n",
      "step 579: loss=0.01888132095336914\n",
      "step 580: loss=0.13847827911376953\n",
      "step 581: loss=0.016454219818115234\n",
      "step 582: loss=1.398078441619873\n",
      "step 583: loss=0.5642738342285156\n",
      "step 584: loss=0.09487390518188477\n",
      "step 585: loss=0.09597134590148926\n",
      "step 586: loss=1.049293041229248\n",
      "step 587: loss=0.47356557846069336\n",
      "step 588: loss=0.8383779525756836\n",
      "step 589: loss=3.177544116973877\n",
      "step 590: loss=0.4497392177581787\n",
      "step 591: loss=0.0792703628540039\n",
      "step 592: loss=1.357460856437683\n",
      "step 593: loss=0.2543766498565674\n",
      "step 594: loss=0.07349824905395508\n",
      "step 595: loss=2.8370463848114014\n",
      "step 596: loss=0.7364479303359985\n",
      "step 597: loss=0.7231221199035645\n",
      "step 598: loss=1.374111533164978\n",
      "step 599: loss=0.4321753978729248\n",
      "step 600: loss=0.17777228355407715\n",
      "step 601: loss=0.07130742073059082\n",
      "step 602: loss=0.2371366024017334\n",
      "step 603: loss=0.30576348304748535\n",
      "step 604: loss=2.668428421020508\n",
      "step 605: loss=1.1140674352645874\n",
      "step 606: loss=0.5520972013473511\n",
      "step 607: loss=0.8259549140930176\n",
      "step 608: loss=0.44512462615966797\n",
      "step 609: loss=0.7199970483779907\n",
      "step 610: loss=0.377687931060791\n",
      "step 611: loss=2.0839905738830566\n",
      "step 612: loss=1.1282179355621338\n",
      "step 613: loss=0.2851424217224121\n",
      "step 614: loss=0.04206252098083496\n",
      "step 615: loss=1.4286022186279297\n",
      "step 616: loss=0.5027270317077637\n",
      "step 617: loss=0.012112617492675781\n",
      "step 618: loss=0.7415882349014282\n",
      "step 619: loss=0.06085538864135742\n",
      "step 620: loss=0.5705982446670532\n",
      "step 621: loss=1.8605399131774902\n",
      "step 622: loss=0.23227739334106445\n",
      "step 623: loss=0.04270195960998535\n",
      "step 624: loss=2.679222822189331\n",
      "step 625: loss=0.2715725898742676\n",
      "step 626: loss=0.9293808937072754\n",
      "step 627: loss=0.16028642654418945\n",
      "step 628: loss=0.010308265686035156\n",
      "step 629: loss=1.290609359741211\n",
      "step 630: loss=0.08072900772094727\n",
      "step 631: loss=0.27584099769592285\n",
      "step 632: loss=0.2107226848602295\n",
      "step 633: loss=0.5600347518920898\n",
      "step 634: loss=0.011282920837402344\n",
      "step 635: loss=3.11643385887146\n",
      "step 636: loss=0.023334503173828125\n",
      "step 637: loss=0.49912166595458984\n",
      "step 638: loss=0.21662330627441406\n",
      "step 639: loss=0.0618898868560791\n",
      "step 640: loss=1.093286395072937\n",
      "step 641: loss=0.2558565139770508\n",
      "step 642: loss=0.010252952575683594\n",
      "step 643: loss=1.6186238527297974\n",
      "step 644: loss=0.011042594909667969\n",
      "step 645: loss=0.30058932304382324\n",
      "step 646: loss=0.09044694900512695\n",
      "step 647: loss=1.9067965745925903\n",
      "step 648: loss=0.12402224540710449\n",
      "step 649: loss=0.05713081359863281\n",
      "step 650: loss=0.10149407386779785\n",
      "step 651: loss=0.2526705265045166\n",
      "step 652: loss=0.6070406436920166\n",
      "step 653: loss=0.004278659820556641\n",
      "step 654: loss=0.7120640277862549\n",
      "step 655: loss=0.667755126953125\n",
      "step 656: loss=0.45967531204223633\n",
      "step 657: loss=0.05305337905883789\n",
      "step 658: loss=0.6365063190460205\n",
      "step 659: loss=0.12805700302124023\n",
      "step 660: loss=0.6878716945648193\n",
      "step 661: loss=0.004768848419189453\n",
      "step 662: loss=0.17098355293273926\n",
      "step 663: loss=1.0485873222351074\n",
      "step 664: loss=0.2143080234527588\n",
      "step 665: loss=0.040230751037597656\n",
      "step 666: loss=0.016267776489257812\n",
      "step 667: loss=0.011294364929199219\n",
      "step 668: loss=1.3694889545440674\n",
      "step 669: loss=2.1131973266601562\n",
      "step 670: loss=0.31995701789855957\n",
      "step 671: loss=1.4489619731903076\n",
      "step 672: loss=0.7029204368591309\n",
      "step 673: loss=1.19868803024292\n",
      "step 674: loss=0.05759167671203613\n",
      "step 675: loss=0.8192206621170044\n",
      "step 676: loss=0.05337262153625488\n",
      "step 677: loss=0.10202956199645996\n",
      "step 678: loss=0.4015166759490967\n",
      "step 679: loss=0.11560511589050293\n",
      "step 680: loss=0.0026750564575195312\n",
      "step 681: loss=1.3169834613800049\n",
      "step 682: loss=0.007153034210205078\n",
      "step 683: loss=0.580612063407898\n",
      "step 684: loss=1.1635642051696777\n",
      "step 685: loss=0.16467809677124023\n",
      "step 686: loss=0.029392719268798828\n",
      "step 687: loss=0.089141845703125\n",
      "step 688: loss=0.034459590911865234\n",
      "step 689: loss=1.5467545986175537\n",
      "step 690: loss=0.7600469589233398\n",
      "step 691: loss=0.3672494888305664\n",
      "step 692: loss=1.7930370569229126\n",
      "step 693: loss=0.04040336608886719\n",
      "step 694: loss=0.09868192672729492\n",
      "step 695: loss=0.09852075576782227\n",
      "step 696: loss=0.05927014350891113\n",
      "step 697: loss=1.2638287544250488\n",
      "step 698: loss=0.546238899230957\n",
      "step 699: loss=0.7163470983505249\n",
      "step 700: loss=0.0858759880065918\n",
      "step 701: loss=0.03256988525390625\n",
      "step 702: loss=0.02334737777709961\n",
      "step 703: loss=0.1107792854309082\n",
      "step 704: loss=0.030237197875976562\n",
      "step 705: loss=0.1586596965789795\n",
      "step 706: loss=0.14440345764160156\n",
      "step 707: loss=0.7704955339431763\n",
      "step 708: loss=0.273378849029541\n",
      "step 709: loss=0.12483501434326172\n",
      "step 710: loss=0.6951863765716553\n",
      "step 711: loss=1.7305415868759155\n",
      "step 712: loss=0.6946097612380981\n",
      "step 713: loss=0.1503126621246338\n",
      "step 714: loss=0.09028935432434082\n",
      "step 715: loss=0.06392288208007812\n",
      "step 716: loss=0.03419208526611328\n",
      "step 717: loss=0.28113722801208496\n",
      "step 718: loss=0.3789252042770386\n",
      "step 719: loss=0.010700225830078125\n",
      "step 720: loss=0.020720958709716797\n",
      "step 721: loss=0.061791181564331055\n",
      "step 722: loss=0.13160324096679688\n",
      "step 723: loss=0.022855758666992188\n",
      "step 724: loss=0.16715764999389648\n",
      "step 725: loss=4.071563243865967\n",
      "step 726: loss=0.454559326171875\n",
      "step 727: loss=0.00247955322265625\n",
      "step 728: loss=0.4241828918457031\n",
      "step 729: loss=1.0255894660949707\n",
      "step 730: loss=0.11156606674194336\n",
      "step 731: loss=0.061654090881347656\n",
      "step 732: loss=1.2832696437835693\n",
      "step 733: loss=0.6139696836471558\n",
      "step 734: loss=0.032423973083496094\n",
      "step 735: loss=0.8118246793746948\n",
      "step 736: loss=0.009581565856933594\n",
      "step 737: loss=0.28511953353881836\n",
      "step 738: loss=0.1188817024230957\n",
      "step 739: loss=0.0021009445190429688\n",
      "step 740: loss=0.11481595039367676\n",
      "step 741: loss=0.9182499647140503\n",
      "step 742: loss=5.225607872009277\n",
      "step 743: loss=1.704210877418518\n",
      "step 744: loss=0.04639291763305664\n",
      "step 745: loss=0.5776848793029785\n",
      "step 746: loss=0.01789712905883789\n",
      "step 747: loss=1.502614974975586\n",
      "step 748: loss=0.03278923034667969\n",
      "step 749: loss=0.01644754409790039\n",
      "step 750: loss=0.8263120651245117\n",
      "step 751: loss=0.04602527618408203\n",
      "step 752: loss=0.21524906158447266\n",
      "step 753: loss=0.061154842376708984\n",
      "step 754: loss=0.3718116283416748\n",
      "step 755: loss=0.28887057304382324\n",
      "step 756: loss=1.5567007064819336\n",
      "step 757: loss=0.028926372528076172\n",
      "step 758: loss=0.18595385551452637\n",
      "step 759: loss=0.07905268669128418\n",
      "step 760: loss=0.4115278720855713\n",
      "step 761: loss=0.008421897888183594\n",
      "step 762: loss=0.4036257266998291\n",
      "step 763: loss=0.9590024948120117\n",
      "step 764: loss=0.7133641242980957\n",
      "step 765: loss=0.48048698902130127\n",
      "step 766: loss=0.3986119031906128\n",
      "step 767: loss=0.13569283485412598\n",
      "step 768: loss=0.4674079418182373\n",
      "step 769: loss=0.00102996826171875\n",
      "step 770: loss=0.16486167907714844\n",
      "step 771: loss=0.43318843841552734\n",
      "step 772: loss=1.4921143054962158\n",
      "step 773: loss=0.08220386505126953\n",
      "step 774: loss=0.038150787353515625\n",
      "step 775: loss=0.013050556182861328\n",
      "step 776: loss=0.2003769874572754\n",
      "step 777: loss=0.05414152145385742\n",
      "step 778: loss=0.011188030242919922\n",
      "step 779: loss=0.014467239379882812\n",
      "step 780: loss=0.02270984649658203\n",
      "step 781: loss=0.0022478103637695312\n",
      "step 782: loss=0.07770586013793945\n",
      "step 783: loss=0.027918338775634766\n",
      "step 784: loss=0.04948854446411133\n",
      "step 785: loss=0.35353732109069824\n",
      "step 786: loss=0.013762474060058594\n",
      "step 787: loss=0.11862325668334961\n",
      "step 788: loss=1.4354572296142578\n",
      "step 789: loss=0.17925167083740234\n",
      "step 790: loss=0.4444906711578369\n",
      "step 791: loss=0.15286684036254883\n",
      "step 792: loss=0.09328126907348633\n",
      "step 793: loss=0.32610654830932617\n",
      "step 794: loss=0.9303807020187378\n",
      "step 795: loss=0.011697769165039062\n",
      "step 796: loss=0.029285907745361328\n",
      "step 797: loss=0.231231689453125\n",
      "step 798: loss=2.6338624954223633\n",
      "step 799: loss=0.5665862560272217\n",
      "step 800: loss=1.2449233531951904\n",
      "step 801: loss=0.013893604278564453\n",
      "step 802: loss=0.01511383056640625\n",
      "step 803: loss=0.12432408332824707\n",
      "step 804: loss=0.4973611831665039\n",
      "step 805: loss=0.0024280548095703125\n",
      "step 806: loss=0.002131938934326172\n",
      "step 807: loss=0.018868446350097656\n",
      "step 808: loss=0.002613067626953125\n",
      "step 809: loss=0.7255860567092896\n",
      "step 810: loss=0.0009622573852539062\n",
      "step 811: loss=0.9114747047424316\n",
      "step 812: loss=0.3157074451446533\n",
      "step 813: loss=0.07942962646484375\n",
      "step 814: loss=0.461492657661438\n",
      "step 815: loss=0.031975746154785156\n",
      "step 816: loss=0.2038557529449463\n",
      "step 817: loss=0.5515196323394775\n",
      "step 818: loss=0.2785799503326416\n",
      "step 819: loss=0.005415439605712891\n",
      "step 820: loss=0.17008376121520996\n",
      "step 821: loss=0.03214550018310547\n",
      "step 822: loss=0.037631988525390625\n",
      "step 823: loss=0.08344030380249023\n",
      "step 824: loss=0.22780489921569824\n",
      "step 825: loss=2.4254860877990723\n",
      "step 826: loss=0.014660835266113281\n",
      "step 827: loss=2.5519309043884277\n",
      "step 828: loss=0.01477813720703125\n",
      "step 829: loss=0.25369691848754883\n",
      "step 830: loss=0.11380338668823242\n",
      "step 831: loss=0.19821643829345703\n",
      "step 832: loss=0.005218505859375\n",
      "step 833: loss=0.15487146377563477\n",
      "step 834: loss=1.6532922983169556\n",
      "step 835: loss=0.43225550651550293\n",
      "step 836: loss=0.01710987091064453\n",
      "step 837: loss=0.015863895416259766\n",
      "step 838: loss=0.010437965393066406\n",
      "step 839: loss=0.03831672668457031\n",
      "step 840: loss=0.12788152694702148\n",
      "step 841: loss=0.005878925323486328\n",
      "step 842: loss=0.046411991119384766\n",
      "step 843: loss=0.01076507568359375\n",
      "step 844: loss=0.25150251388549805\n",
      "step 845: loss=0.0012178421020507812\n",
      "step 846: loss=0.2321016788482666\n",
      "step 847: loss=0.31379055976867676\n",
      "step 848: loss=0.06747221946716309\n",
      "step 849: loss=0.0008087158203125\n",
      "step 850: loss=0.07829785346984863\n",
      "step 851: loss=0.28147459030151367\n",
      "step 852: loss=0.3888612985610962\n",
      "step 853: loss=0.449662446975708\n",
      "step 854: loss=0.09763097763061523\n",
      "step 855: loss=1.7243692874908447\n",
      "step 856: loss=0.1643972396850586\n",
      "step 857: loss=0.11453390121459961\n",
      "step 858: loss=0.39929258823394775\n",
      "step 859: loss=0.16950511932373047\n",
      "step 860: loss=0.6861879825592041\n",
      "step 861: loss=0.02004098892211914\n",
      "step 862: loss=0.003284931182861328\n",
      "step 863: loss=0.10630512237548828\n",
      "step 864: loss=0.12004256248474121\n",
      "step 865: loss=0.8591494560241699\n",
      "step 866: loss=0.05156230926513672\n",
      "step 867: loss=0.7235795259475708\n",
      "step 868: loss=0.044901371002197266\n",
      "step 869: loss=2.3743138313293457\n",
      "step 870: loss=0.0057239532470703125\n",
      "step 871: loss=0.21019387245178223\n",
      "step 872: loss=0.1750349998474121\n",
      "step 873: loss=0.03566455841064453\n",
      "step 874: loss=1.4096548557281494\n",
      "step 875: loss=0.3176443576812744\n",
      "step 876: loss=1.5774856805801392\n",
      "step 877: loss=0.1490786075592041\n",
      "step 878: loss=0.14512395858764648\n",
      "step 879: loss=0.014568805694580078\n",
      "step 880: loss=0.1957552433013916\n",
      "step 881: loss=0.02507495880126953\n",
      "step 882: loss=0.11042499542236328\n",
      "step 883: loss=0.10820245742797852\n",
      "step 884: loss=0.013692378997802734\n",
      "step 885: loss=0.025472640991210938\n",
      "step 886: loss=1.4292902946472168\n",
      "step 887: loss=0.3023536205291748\n",
      "step 888: loss=0.16981220245361328\n",
      "step 889: loss=0.5759778022766113\n",
      "step 890: loss=0.39397764205932617\n",
      "step 891: loss=0.01878833770751953\n",
      "step 892: loss=0.0006103515625\n",
      "step 893: loss=0.14224529266357422\n",
      "step 894: loss=0.16683459281921387\n",
      "step 895: loss=0.02306652069091797\n",
      "step 896: loss=0.005665779113769531\n",
      "step 897: loss=0.26515984535217285\n",
      "step 898: loss=0.5112065076828003\n",
      "step 899: loss=0.01642131805419922\n",
      "step 900: loss=0.04950976371765137\n",
      "step 901: loss=0.10270214080810547\n",
      "step 902: loss=0.008306503295898438\n",
      "step 903: loss=1.2045660018920898\n",
      "step 904: loss=0.02616596221923828\n",
      "step 905: loss=0.5364210605621338\n",
      "step 906: loss=0.41507232189178467\n",
      "step 907: loss=0.6363639831542969\n",
      "step 908: loss=0.002647876739501953\n",
      "step 909: loss=0.06528759002685547\n",
      "step 910: loss=0.5764923095703125\n",
      "step 911: loss=0.29312682151794434\n",
      "step 912: loss=0.005534172058105469\n",
      "step 913: loss=0.04647350311279297\n",
      "step 914: loss=3.039876699447632\n",
      "step 915: loss=0.04770612716674805\n",
      "step 916: loss=0.23157477378845215\n",
      "step 917: loss=0.34737443923950195\n",
      "step 918: loss=0.5393333435058594\n",
      "step 919: loss=0.3387317657470703\n",
      "step 920: loss=0.10210371017456055\n",
      "step 921: loss=0.2000105381011963\n",
      "step 922: loss=0.46993470191955566\n",
      "step 923: loss=0.056276798248291016\n",
      "step 924: loss=0.006319999694824219\n",
      "step 925: loss=0.01444244384765625\n",
      "step 926: loss=0.02815389633178711\n",
      "step 927: loss=0.13592910766601562\n",
      "step 928: loss=2.0474705696105957\n",
      "step 929: loss=0.08091592788696289\n",
      "step 930: loss=0.23891758918762207\n",
      "step 931: loss=0.1295328140258789\n",
      "step 932: loss=0.03899955749511719\n",
      "step 933: loss=0.02909708023071289\n",
      "step 934: loss=0.022089481353759766\n",
      "step 935: loss=0.010469913482666016\n",
      "step 936: loss=1.1239633560180664\n",
      "step 937: loss=0.004912853240966797\n",
      "step 938: loss=0.24308156967163086\n",
      "step 939: loss=0.5980682373046875\n",
      "step 940: loss=0.12193965911865234\n",
      "step 941: loss=0.02315378189086914\n",
      "step 942: loss=0.37062692642211914\n",
      "step 943: loss=0.009302139282226562\n",
      "step 944: loss=0.011314868927001953\n",
      "step 945: loss=3.5264344215393066\n",
      "step 946: loss=0.0910792350769043\n",
      "step 947: loss=0.15913820266723633\n",
      "step 948: loss=0.01747608184814453\n",
      "step 949: loss=0.11133050918579102\n",
      "step 950: loss=0.09948945045471191\n",
      "step 951: loss=0.052641868591308594\n",
      "step 952: loss=0.00896453857421875\n",
      "step 953: loss=0.01663351058959961\n",
      "step 954: loss=0.05906391143798828\n",
      "step 955: loss=0.002895355224609375\n",
      "step 956: loss=0.07044506072998047\n",
      "step 957: loss=0.007376194000244141\n",
      "step 958: loss=0.2260730266571045\n",
      "step 959: loss=0.16261935234069824\n",
      "step 960: loss=0.6109166145324707\n",
      "step 961: loss=0.5834782123565674\n",
      "step 962: loss=0.05586504936218262\n",
      "step 963: loss=0.004221916198730469\n",
      "step 964: loss=0.033403873443603516\n",
      "step 965: loss=0.793737530708313\n",
      "step 966: loss=0.37858057022094727\n",
      "step 967: loss=0.0952444076538086\n",
      "step 968: loss=0.6360642910003662\n",
      "step 969: loss=0.03384065628051758\n",
      "step 970: loss=0.638343334197998\n",
      "step 971: loss=0.03069305419921875\n",
      "step 972: loss=0.14579248428344727\n",
      "step 973: loss=0.07487893104553223\n",
      "step 974: loss=1.3001110553741455\n",
      "step 975: loss=2.2975776195526123\n",
      "step 976: loss=0.01396322250366211\n",
      "step 977: loss=0.15135955810546875\n",
      "step 978: loss=0.8422958254814148\n",
      "step 979: loss=1.9849610328674316\n",
      "step 980: loss=0.5528028011322021\n",
      "step 981: loss=0.9595495462417603\n",
      "step 982: loss=0.15296173095703125\n",
      "step 983: loss=0.6124076843261719\n",
      "step 984: loss=0.04457712173461914\n",
      "step 985: loss=0.19276797771453857\n",
      "step 986: loss=0.14833617210388184\n",
      "step 987: loss=0.07393813133239746\n",
      "step 988: loss=0.07053279876708984\n",
      "step 989: loss=0.001033782958984375\n",
      "step 990: loss=0.31380701065063477\n",
      "step 991: loss=0.14849281311035156\n",
      "step 992: loss=0.2512540817260742\n",
      "step 993: loss=0.010190486907958984\n",
      "step 994: loss=0.709064245223999\n",
      "step 995: loss=0.2817842960357666\n",
      "step 996: loss=0.0027513504028320312\n",
      "step 997: loss=0.0024824142456054688\n",
      "step 998: loss=0.18323874473571777\n",
      "step 999: loss=0.29120421409606934\n",
      "step 1000: loss=0.002166748046875\n",
      "Mean loss        0.48416767\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 3 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6036dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "092763fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223949670791626\n",
      "step 2: loss=1.4270418882369995\n",
      "step 3: loss=1.1914063692092896\n",
      "step 4: loss=0.8947293162345886\n",
      "step 5: loss=0.8820065259933472\n",
      "step 6: loss=0.6795811057090759\n",
      "step 7: loss=0.6839590072631836\n",
      "step 8: loss=0.7954467535018921\n",
      "step 9: loss=0.7858410477638245\n",
      "step 10: loss=0.7066607475280762\n",
      "step 11: loss=0.508007824420929\n",
      "step 12: loss=0.6086064577102661\n",
      "step 13: loss=0.6874796152114868\n",
      "step 14: loss=0.6014252305030823\n",
      "step 15: loss=0.6547794938087463\n",
      "step 16: loss=0.4428560435771942\n",
      "step 17: loss=0.35594701766967773\n",
      "step 18: loss=0.3600447475910187\n",
      "step 19: loss=0.4020954668521881\n",
      "step 20: loss=0.3246818482875824\n",
      "Mean loss        0.5156536\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1775729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=7.921195030212402\n",
      "step 2: loss=22.573795318603516\n",
      "step 3: loss=15.713532447814941\n",
      "step 4: loss=14.277660369873047\n",
      "step 5: loss=4.392229080200195\n",
      "step 6: loss=2.0070979595184326\n",
      "step 7: loss=1.3166155815124512\n",
      "step 8: loss=1.6341989040374756\n",
      "step 9: loss=1.6222692728042603\n",
      "step 10: loss=1.4166510105133057\n",
      "step 11: loss=1.3110309839248657\n",
      "step 12: loss=1.5402616262435913\n",
      "step 13: loss=1.504196286201477\n",
      "step 14: loss=1.6209816932678223\n",
      "step 15: loss=1.6276339292526245\n",
      "step 16: loss=1.2568767070770264\n",
      "step 17: loss=0.8523489236831665\n",
      "step 18: loss=1.3203682899475098\n",
      "step 19: loss=1.4638503789901733\n",
      "step 20: loss=1.3379669189453125\n",
      "Mean loss        1.8293804\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 3 7 8 3 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = optax.adam(learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6626627",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2 # for all subsequent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f508a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223958015441895\n",
      "step 2: loss=1.4270424842834473\n",
      "step 3: loss=1.191407322883606\n",
      "step 4: loss=0.8947314023971558\n",
      "step 5: loss=0.8820069432258606\n",
      "step 6: loss=0.6795834898948669\n",
      "step 7: loss=0.6839614510536194\n",
      "step 8: loss=0.7954472303390503\n",
      "step 9: loss=0.7858410477638245\n",
      "step 10: loss=0.7066599726676941\n",
      "step 11: loss=0.5080094337463379\n",
      "step 12: loss=0.6086087822914124\n",
      "step 13: loss=0.687477707862854\n",
      "step 14: loss=0.6014255285263062\n",
      "step 15: loss=0.6547796726226807\n",
      "step 16: loss=0.44285672903060913\n",
      "step 17: loss=0.35594889521598816\n",
      "step 18: loss=0.36004677414894104\n",
      "step 19: loss=0.4020947813987732\n",
      "step 20: loss=0.3246852457523346\n",
      "Mean loss        0.51565444\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.adamw(learning_rate, weight_decay=1e-4)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8017b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=1.3223958015441895\n",
      "step 2: loss=1.4270424842834473\n",
      "step 3: loss=1.1914074420928955\n",
      "step 4: loss=0.8947315812110901\n",
      "step 5: loss=0.8820069432258606\n",
      "step 6: loss=0.6795834302902222\n",
      "step 7: loss=0.6839614510536194\n",
      "step 8: loss=0.795447051525116\n",
      "step 9: loss=0.7858409881591797\n",
      "step 10: loss=0.7066601514816284\n",
      "step 11: loss=0.5080092549324036\n",
      "step 12: loss=0.6086088418960571\n",
      "step 13: loss=0.687477707862854\n",
      "step 14: loss=0.6014255881309509\n",
      "step 15: loss=0.6547797918319702\n",
      "step 16: loss=0.44285669922828674\n",
      "step 17: loss=0.3559488356113434\n",
      "step 18: loss=0.3600468337535858\n",
      "step 19: loss=0.4020947813987732\n",
      "step 20: loss=0.3246852159500122\n",
      "Mean loss        0.5156544\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 5 5 1]\n"
     ]
    }
   ],
   "source": [
    "mask_fn = lambda p: jax.tree_util.tree_map(lambda x: x.ndim != 1, p) # mask biases\n",
    "optimizer = optax.adamw(learning_rate, weight_decay=1e-4, mask=mask_fn)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5964dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.329394578933716\n",
      "step 2: loss=2.111168384552002\n",
      "step 3: loss=2.098027229309082\n",
      "step 4: loss=2.115138530731201\n",
      "step 5: loss=2.1225409507751465\n",
      "step 6: loss=1.9438470602035522\n",
      "step 7: loss=1.989481806755066\n",
      "step 8: loss=1.8919346332550049\n",
      "step 9: loss=1.8656792640686035\n",
      "step 10: loss=1.854970932006836\n",
      "step 11: loss=1.867348551750183\n",
      "step 12: loss=1.8192609548568726\n",
      "step 13: loss=1.8986468315124512\n",
      "step 14: loss=1.8100045919418335\n",
      "step 15: loss=1.870030164718628\n",
      "step 16: loss=1.7674648761749268\n",
      "step 17: loss=1.6689090728759766\n",
      "step 18: loss=1.7513816356658936\n",
      "step 19: loss=1.673409342765808\n",
      "step 20: loss=1.7518361806869507\n",
      "Mean loss        1.7164931\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "lr = optax.cosine_decay_schedule(init_value=1e-3, decay_steps=20)\n",
    "optimizer = optax.adamw(lr, weight_decay=1e-4, mask=mask_fn)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd672339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss=2.4460508823394775\n",
      "step 2: loss=2.2159481048583984\n",
      "step 3: loss=2.226898193359375\n",
      "step 4: loss=2.282470703125\n",
      "step 5: loss=2.2132441997528076\n",
      "step 6: loss=2.1017580032348633\n",
      "step 7: loss=2.186561346054077\n",
      "step 8: loss=2.037569284439087\n",
      "step 9: loss=2.0478765964508057\n",
      "step 10: loss=2.033282995223999\n",
      "step 11: loss=2.013596534729004\n",
      "step 12: loss=1.9634274244308472\n",
      "step 13: loss=2.019657850265503\n",
      "step 14: loss=1.8717783689498901\n",
      "step 15: loss=1.937285304069519\n",
      "step 16: loss=1.8528590202331543\n",
      "step 17: loss=1.723109245300293\n",
      "step 18: loss=1.7739412784576416\n",
      "step 19: loss=1.659662127494812\n",
      "step 20: loss=1.698677659034729\n",
      "Mean loss        1.7045716\n",
      "True labels:     [2 0 4 8 7 6 0 6 3 1]\n",
      "Predictions:     [2 0 4 8 7 6 0 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.contrib.muon(learning_rate=learning_rate)\n",
    "params = train_mlp(train_data, optimizer)\n",
    "evaluate_mlp(test_data, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
