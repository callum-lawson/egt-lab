# ðŸŽ¯ Foundations - objectives 

## Build core deep learning skills with MNIST notebooks
- [x] Build backprop from scratch using my version of Karpathy's method
- [x] Use JAX method to understand backprop
- [x] Flax Linen
- [x] Flax NNX
- [x] Optax
- [x] Orbax
- [x] Einsum

# ðŸš€ The Minimal JAX Deep-Learning Stack

This is a useful summary of the JAX ecosystem generated by GPT 5.1. 

## **1. JAX â€” the raw math + autodiff + transforms**

**Youâ€™re using it for:**

* `jax.numpy` (matrix mults, activations, reshaping)
* `jax.grad` (compute gradients)
* `jit`, `vmap` (speed + batching)
* RNGs (`jax.random`)

**Job description:**
**â€œI handle all the math and turn your code into fast GPU/TPU kernels.â€**

ðŸ‘‰ Absolutely essential. Everything else is optional helpers layered on top.

---

## **2. Flax â€” model architecture (layers & parameters)**

**Youâ€™re currently doing:**

* Manually creating weight matrices (`W1`, `b1`, etc.)
* Writing forward functions that manipulate those arrays
* Juggling parameter PyTrees yourself

**Flax does:**

* Provides `Dense`, `Conv`, `Dropout`, etc.
* Organises parameters into a clean structure
* Lets you write models like:

```python
class MLP(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(128)(x)
        x = nn.relu(x)
        x = nn.Dense(10)(x)
        return x
```

**Job description:**
**â€œI manage your layers, parameters, and model structure so you donâ€™t.â€**

Two APIs:

* **Linen** â€” the classic one in most tutorials
* **NNX** â€” newer, simpler one for future projects
  (but they solve the same problem)

ðŸ‘‰ Not required to *learn JAX*, but required to build *bigger, nicer* models without going insane.

---

## **3. Optax â€” gradient updates & optimizers**

**Youâ€™re currently doing:**

* Writing your own SGD / Adam
* Updating parameters manually: `params = params - lr * grads`

**Optax does:**

* Provides Adam, AdamW, SGD, RMSProp, schedules, clipping
* Handles optimizer state automatically

Usage looks like:

```python
opt = optax.adam(learning_rate)
opt_state = opt.init(params)
updates, opt_state = opt.update(grads, opt_state)
params = optax.apply_updates(params, updates)
```

**Job description:**
**â€œI update your model parameters in a clean, reusable way.â€**

ðŸ‘‰ Makes optimizers easy. Almost everyone uses Optax with JAX/Flax.

---

## **4. Orbax â€” saving & loading (checkpointing)**

**Right now:**

* You might be doing nothing or just `pickle`ing parameters.

**Orbax does:**

* Saves model + optimizer state robustly
* Restores checkpoints cleanly
* Supports multi-device training

**Job description:**
**â€œI store your model safely so you can resume training.â€**

ðŸ‘‰ Only really needed for real training workflows, not toy MNIST scripts.


---

## The whole picture in one sentence

> **JAX** does the math and autodiff.

> **Flax** builds the model.

> **Optax** updates the model.

> **Orbax** saves the model.
