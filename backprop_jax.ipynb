{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "181bca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random, tree_util, vmap\n",
    "from jax.nn import relu\n",
    "from jax.scipy.special import logsumexp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "558de4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from TensorFlow Datasets\n",
    "data_dir = '/tmp/tfds' # data_dir = './data/tfds'\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fb40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x, x_max=255.0):\n",
    "    return x / x_max\n",
    "\n",
    "def convert_to_jax(data_np, data_type):\n",
    "    if data_type == \"image\":\n",
    "        data_jax = normalise(jnp.array(data_np, dtype=jnp.float32))\n",
    "    elif data_type == \"label\":\n",
    "        data_jax = jnp.array(data_np)\n",
    "    else:\n",
    "        raise ValueError(\"not image or label\")\n",
    "    return data_jax\n",
    "\n",
    "def flatten_image_for_mlp(data_jax):\n",
    "    n_batch, n_pixels_vertical, n_pixels_horizontal, n_channels = data_jax.shape\n",
    "    data_flattened = data_jax.reshape(n_batch, -1)\n",
    "    return data_flattened\n",
    "\n",
    "def prepare_data(data_dict):\n",
    "    data_jax = {}\n",
    "    for data_type, data_tf in data_dict.items():\n",
    "        data_numpy = data_tf.numpy()\n",
    "        data_jax[data_type] = convert_to_jax(data_numpy, data_type)\n",
    "        if data_type == \"image\":\n",
    "            data_jax[data_type] = flatten_image_for_mlp(data_jax[data_type])\n",
    "    return data_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e662dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tf = \"train\"\n",
    "all_data_tf = mnist_data[dataset_tf]\n",
    "all_data_jax = prepare_data(all_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71d5ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = all_data_jax[\"image\"]\n",
    "labels = all_data_jax[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc5e364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (60000, 784)\n",
      "Labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "239a4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_preactivations(x, W, b):\n",
    "    \"\"\"\n",
    "    x is a row vector (single sample) - shape: (784,)\n",
    "    W is the weights matrix - shape: (784, 128)\n",
    "    b is the bias vector - shape: (128,)\n",
    "    Returns: row vector - shape: (128,)\n",
    "    \"\"\"\n",
    "    return x @ W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7568be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, params_list):\n",
    "    \"\"\"Forward pass through all layers: inputs and outputs are vectors\"\"\"\n",
    "    for layer_number, params in enumerate(params_list):\n",
    "        W, b = params[\"W\"], params[\"b\"]\n",
    "        x = calculate_preactivations(x, W, b)\n",
    "        if layer_number != (len(params_list) - 1):\n",
    "            x = relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "046c9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions_logits, observed_label):\n",
    "    log_probs = predictions_logits - logsumexp(predictions_logits)\n",
    "    return -log_probs[observed_label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a4c7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass_batch = vmap(forward_pass, in_axes=(0, None))\n",
    "calculate_loss_batch = vmap(calculate_loss, in_axes=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_params(dims, key, scale=0.\n",
    "1):\n",
    "    return scale * random.normal(key, dims)\n",
    "\n",
    "def random_layer_params(m, n, key):\n",
    "    \"\"\"Initialize weights and biases for one layer\"\"\"\n",
    "    w_key, b_key = random.split(key)\n",
    "    return {\n",
    "        \"W\": initialise_params((m, n), w_key),\n",
    "        \"b\": initialise_params((n,), b_key)\n",
    "    }\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
    "    params_list = []\n",
    "    keys = random.split(key, len(sizes) - 1)\n",
    "    \n",
    "    for i in range(len(sizes) - 1):\n",
    "        input_size = sizes[i]\n",
    "        output_size = sizes[i + 1]\n",
    "        layer_key = keys[i]\n",
    "        layer_params = random_layer_params(input_size, output_size, layer_key)\n",
    "        params_list.append(layer_params)\n",
    "    \n",
    "    return params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fc34858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:     [4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3 0 9 4 9 0\n",
      " 6 8 4 7 2 6 0 3 1 1 7 2 4 4 6 5 1 9 3 2 4 3 4 4 7 5 8 1 1 4 1 5 3 5 8 4 1\n",
      " 1 4 5 3 2 4 1 4 8 1 2 1 9 0 7 6 7 4 4 9 7 5 6 8 4 6]\n",
      "Predictions:     [7 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 5 1 1 1 7 1 7 7 7 1 1 1 1 1 5 7 1\n",
      " 1 7 5 1 1 7 1 7 1 7 1 1 1 7 7 1 1 5 1 1 5 1 1 5 1 1 1 1 1 7 1 0 1 1 7 7 1\n",
      " 1 5 7 1 1 1 1 7 1 7 1 1 7 1 1 7 1 1 5 1 1 1 1 1 7 1]\n",
      "Match:           [False  True False False False  True False False  True False False False\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True  True False  True False False False False\n",
      " False  True  True False False False False False  True False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [784, 128, 10]\n",
    "params = init_network_params(layer_sizes, random.key(0))\n",
    "\n",
    "trial_set_size = 100\n",
    "test_images = images[:trial_set_size]\n",
    "test_labels = labels[:trial_set_size]\n",
    "\n",
    "logits = forward_pass_batch(test_images, params)\n",
    "predictions = jnp.argmax(logits, axis=1)\n",
    "\n",
    "# Display results\n",
    "print(\"True labels:    \", test_labels)\n",
    "print(\"Predictions:    \", predictions)\n",
    "print(\"Match:          \", predictions == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87189ac9",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- vmap instead of having batch as part of the input matrix dimension\n",
    "- initialisation of parameters\n",
    "- backprop\n",
    "\n",
    "### Data details\n",
    "- n cols x n cells x n colour channels\n",
    "- logits output (10 classes)\n",
    "\n",
    "### Tutorials\n",
    "- https://docs.jax.dev/en/latest/notebooks/neural_network_with_tfds_data.html\n",
    "- https://flax.readthedocs.io/en/latest/mnist_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f586a03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸš€ The Minimal JAX Deep-Learning Stack (High-Level Only)\n",
    "\n",
    "## **1. JAX â€” the raw math + autodiff + transforms**\n",
    "\n",
    "**Youâ€™re using it for:**\n",
    "\n",
    "* `jax.numpy` (matrix mults, activations, reshaping)\n",
    "* `jax.grad` (compute gradients)\n",
    "* `jit`, `vmap` (speed + batching)\n",
    "* RNGs (`jax.random`)\n",
    "\n",
    "**Job description:**\n",
    "**â€œI handle all the math and turn your code into fast GPU/TPU kernels.â€**\n",
    "\n",
    "ðŸ‘‰ Absolutely essential. Everything else is optional helpers layered on top.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Flax â€” model architecture (layers & parameters)**\n",
    "\n",
    "**Youâ€™re currently doing:**\n",
    "\n",
    "* Manually creating weight matrices (`W1`, `b1`, etc.)\n",
    "* Writing forward functions that manipulate those arrays\n",
    "* Juggling parameter PyTrees yourself\n",
    "\n",
    "**Flax does:**\n",
    "\n",
    "* Provides `Dense`, `Conv`, `Dropout`, etc.\n",
    "* Organises parameters into a clean structure\n",
    "* Lets you write models like:\n",
    "\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**Job description:**\n",
    "**â€œI manage your layers, parameters, and model structure so you donâ€™t.â€**\n",
    "\n",
    "Two APIs:\n",
    "\n",
    "* **Linen** â€” the classic one in most tutorials\n",
    "* **NNX** â€” newer, simpler one for future projects\n",
    "  (but they solve the same problem)\n",
    "\n",
    "ðŸ‘‰ Not required to *learn JAX*, but required to build *bigger, nicer* models without going insane.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Optax â€” gradient updates & optimizers**\n",
    "\n",
    "**Youâ€™re currently doing:**\n",
    "\n",
    "* Writing your own SGD / Adam\n",
    "* Updating parameters manually: `params = params - lr * grads`\n",
    "\n",
    "**Optax does:**\n",
    "\n",
    "* Provides Adam, AdamW, SGD, RMSProp, schedules, clipping\n",
    "* Handles optimizer state automatically\n",
    "\n",
    "Usage looks like:\n",
    "\n",
    "```python\n",
    "opt = optax.adam(learning_rate)\n",
    "opt_state = opt.init(params)\n",
    "updates, opt_state = opt.update(grads, opt_state)\n",
    "params = optax.apply_updates(params, updates)\n",
    "```\n",
    "\n",
    "**Job description:**\n",
    "**â€œI update your model parameters in a clean, reusable way.â€**\n",
    "\n",
    "ðŸ‘‰ Makes optimizers easy. Almost everyone uses Optax with JAX/Flax.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Orbax â€” saving & loading (checkpointing)**\n",
    "\n",
    "**Right now:**\n",
    "\n",
    "* You might be doing nothing or just `pickle`ing parameters.\n",
    "\n",
    "**Orbax does:**\n",
    "\n",
    "* Saves model + optimizer state robustly\n",
    "* Restores checkpoints cleanly\n",
    "* Supports multi-device training\n",
    "\n",
    "**Job description:**\n",
    "**â€œI store your model safely so you can resume training.â€**\n",
    "\n",
    "ðŸ‘‰ Only really needed for real training workflows, not toy MNIST scripts.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. TFDS â€” dataset loading (optional)**\n",
    "\n",
    "**Right now:**\n",
    "\n",
    "* You might be writing your own MNIST loader.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ The whole picture in one sentence\n",
    "\n",
    "> **JAX** does the math and autodiff.\n",
    "> **Flax** builds the model.\n",
    "> **Optax** updates the model.\n",
    "> **Orbax** saves the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egt-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
