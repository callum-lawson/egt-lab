{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax import vmap\n",
        "\n",
        "import numpy as np\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "EXPERIMENT_SUFFIX = \"collapse\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2741d19",
      "metadata": {},
      "source": [
        "### Mixture of Gaussians\n",
        "\n",
        "First we're going to generate a mixture of two Gaussians that the generator is supposed to match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ccb3e26d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pmf_gaussian_single(\n",
        "    mu: float,\n",
        "    sigma: float,\n",
        "    n_pixels: int = 784,\n",
        "):\n",
        "    \"\"\"\n",
        "    Discrete Gaussian over 784 bins\n",
        "    Uses probability densities at bin centres rather than \n",
        "    integrated probability masses over the bin intervals\n",
        "    \"\"\"\n",
        "    pixel_midpoints = jnp.arange(n_pixels) + 0.5\n",
        "    pixel_frequencies = jnp.exp(-0.5 * ((pixel_midpoints - mu) / sigma) ** 2)\n",
        "    pixel_probabilities = pixel_frequencies / pixel_frequencies.sum()\n",
        "    return pixel_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f1e74b31",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pmf_gaussian_mixed(\n",
        "    mu1: float,\n",
        "    sigma1: float,\n",
        "    mu2: float,\n",
        "    sigma2: float,\n",
        "    alpha: float,\n",
        "    n_pixels: int = 784,\n",
        "):\n",
        "    \"\"\"pmf for α*N(mean1, std1) + (1-α)*N(mean2, std2) on discrete bins.\"\"\"\n",
        "    p1 = pmf_gaussian_single(mu1, sigma1, n_pixels)\n",
        "    p2 = pmf_gaussian_single(mu2, sigma2, n_pixels)\n",
        "    probs = alpha * p1 + (1.0 - alpha) * p2\n",
        "    return probs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "064ed82b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def r_gaussian_mixed(\n",
        "    key,\n",
        "    mu1: float,\n",
        "    sigma1: float,\n",
        "    mu2: float,\n",
        "    sigma2: float,\n",
        "    alpha: float,\n",
        "    n_pixels: int = 784,\n",
        "    n_samples: int = 100,\n",
        "    n_training_samples: int = 10,\n",
        "):\n",
        "    \"\"\"Draw histograms from the 2-Gaussian mixture.\n",
        "\n",
        "    Returns an array of shape (n_training_samples, n_pixels), where each row\n",
        "    is a histogram (normalized to sum to 1) from `n_samples` draws.\n",
        "    \"\"\"\n",
        "    probs = pmf_gaussian_mixed(mu1, sigma1, mu2, sigma2, alpha, n_pixels)\n",
        "    probs = jnp.broadcast_to(probs, (n_training_samples, n_pixels))\n",
        "    counts = jr.multinomial(key, n=n_samples, p=probs)\n",
        "    return counts / n_samples\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "15a25657",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss_single_label(logits, label):\n",
        "    targets = jnp.full_like(logits, label)\n",
        "    return optax.sigmoid_binary_cross_entropy(logits, targets).mean()\n",
        "\n",
        "def generator_loss_nonsaturating(logits_real_given_fake):\n",
        "    \"\"\"Objective: maximise p(predicted real | fake)\"\"\"\n",
        "    return + cross_entropy_loss_single_label(logits=logits_real_given_fake, label=1)\n",
        "\n",
        "def generator_loss_saturating(logits_real_given_fake):\n",
        "    \"\"\"Objective: minimise p(predicted fake | fake)\"\"\"\n",
        "    return - cross_entropy_loss_single_label(logits=logits_real_given_fake, label=0)\n",
        "\n",
        "def discriminator_loss(logits_real_given_real, logits_real_given_fake):\n",
        "    loss_given_real = cross_entropy_loss_single_label(logits=logits_real_given_real, label=1)\n",
        "    loss_given_fake = cross_entropy_loss_single_label(logits=logits_real_given_fake, label=0)\n",
        "    return (loss_given_real + loss_given_fake) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b22555e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_generator_loss(\n",
        "    generator_params,\n",
        "    discriminator_params,\n",
        "    generator_apply_fn,\n",
        "    discriminator_apply_fn,\n",
        "    z_vector,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "):\n",
        "    # Generator produces logits over histogram bins; enforce simplex constraint\n",
        "    # before passing to the discriminator so it cannot exploit sign/scale/sum.\n",
        "    fake_logits = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    fake_images = nn.softmax(fake_logits, axis=-1)\n",
        "\n",
        "    logits_real_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    if loss_type == \"nonsaturating\":\n",
        "        return generator_loss_nonsaturating(logits_real_given_fake)\n",
        "    elif loss_type == \"saturating\":\n",
        "        return generator_loss_saturating(logits_real_given_fake)\n",
        "    else:\n",
        "        raise ValueError(f\"incorrect loss type specified: {loss_type}\")\n",
        "\n",
        "\n",
        "def calculate_discriminator_loss(\n",
        "    discriminator_params,\n",
        "    generator_params,\n",
        "    generator_apply_fn,\n",
        "    discriminator_apply_fn,\n",
        "    z_vector,\n",
        "    real_images,\n",
        "):\n",
        "    # Transform generator outputs to valid histograms before the discriminator\n",
        "    fake_logits = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    fake_images = nn.softmax(fake_logits, axis=-1)\n",
        "\n",
        "    logits_real_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    logits_real_given_real = discriminator_apply_fn({\"params\": discriminator_params}, real_images)\n",
        "    return discriminator_loss(logits_real_given_real, logits_real_given_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "efbab3ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_generator_step_nonsaturating(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        \"nonsaturating\",\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_generator_step_saturating(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        \"saturating\",\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_discriminator_step(generator, discriminator, z_vector, real_images):\n",
        "    grads_by_params_fn = jax.grad(calculate_discriminator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        discriminator.params,\n",
        "        generator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        real_images,\n",
        "    )\n",
        "    return discriminator.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a156d045",
      "metadata": {},
      "outputs": [],
      "source": [
        "def subsample_images_for_batch(key, images_full_sample, batch_size):\n",
        "    image_ids = jax.random.randint(key, (batch_size,), 0, images_full_sample.shape[0])\n",
        "    return images_full_sample[image_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e79102c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training_gan(\n",
        "    train_data,\n",
        "    n_steps,\n",
        "    generator_training_state,\n",
        "    discriminator_training_state,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    checkpoint_manager,\n",
        "    batch_size: int,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    start_step: int = 0,\n",
        "):\n",
        "    \"\"\"Train a GAN using random mini-batches of real images.\n",
        "\n",
        "    Shapes:\n",
        "      - train_data[\"image\"]: (N, n_pixels)\n",
        "      - real_images_batch:   (batch_size, n_pixels)\n",
        "      - z_vectors:           (batch_size, latent_dim)\n",
        "\n",
        "    Args:\n",
        "      start_step: starting global step index (for resuming from checkpoints).\n",
        "    \"\"\"\n",
        "    real_images = train_data\n",
        "\n",
        "    if loss_type == \"nonsaturating\":\n",
        "        take_generator_step = take_generator_step_nonsaturating\n",
        "    elif loss_type == \"saturating\":\n",
        "        take_generator_step = take_generator_step_saturating\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
        "\n",
        "    for step in range(1, n_steps + 1):\n",
        "        key, key_z_generation, key_real_subsample = jax.random.split(key, 3)\n",
        "\n",
        "        # Random mini-batch of real images\n",
        "        real_images_batch = subsample_images_for_batch(\n",
        "            key_real_subsample,\n",
        "            real_images,\n",
        "            batch_size,\n",
        "        )\n",
        "\n",
        "        # Latent vectors for generator\n",
        "        z_vectors = jax.random.normal(\n",
        "            key_z_generation,\n",
        "            (batch_size, latent_dim),\n",
        "        )\n",
        "\n",
        "        # Update discriminator then generator\n",
        "        discriminator_training_state = take_discriminator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "        generator_training_state = take_generator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "        )\n",
        "\n",
        "        # Monitor losses\n",
        "        generator_loss_value = calculate_generator_loss(\n",
        "            generator_training_state.params,\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            loss_type=loss_type,\n",
        "        )\n",
        "        discriminator_loss_value = calculate_discriminator_loss(\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "\n",
        "        global_step = start_step + step\n",
        "        print(\n",
        "            f\"step {global_step}: generator_loss={generator_loss_value}, \"\n",
        "            f\"discriminator_loss={discriminator_loss_value}\"\n",
        "        )\n",
        "\n",
        "        if step == 1 or global_step % steps_per_save == 0:\n",
        "            checkpoint_manager.save(\n",
        "                global_step,\n",
        "                args=ocp.args.StandardSave(\n",
        "                    {\n",
        "                        \"generator\": generator_training_state,\n",
        "                        \"discriminator\": discriminator_training_state,\n",
        "                    }\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_batch(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4b0c85d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_gan_training(optimizer, key, latent_dim):\n",
        "    N_PIXELS = 784\n",
        "    N_HIDDEN_LAYER = 128\n",
        "    N_BINARY_CATEGORIES = 1\n",
        "\n",
        "    # Generator maps from latent space (latent_dim) to image space (N_PIXELS)\n",
        "    layer_sizes_generator = [latent_dim, N_HIDDEN_LAYER, N_PIXELS]\n",
        "    layer_sizes_discriminator = [N_PIXELS, N_HIDDEN_LAYER, N_BINARY_CATEGORIES]\n",
        "\n",
        "    g_key, d_key = jax.random.split(key)\n",
        "    generator_training_state = create_training_state(layer_sizes_generator, optimizer, g_key)\n",
        "    discriminator_training_state = create_training_state(layer_sizes_discriminator, optimizer, d_key)\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6007df0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(optimizer):\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"gan_{opt_name}\"\n",
        "\n",
        "\n",
        "def make_constraints_experiment_name(optimizer, loss_type: str) -> str:\n",
        "    \"\"\"Experiment name for this notebook's mixture-of-Gaussians GAN runs.\n",
        "\n",
        "    Uses the global `EXPERIMENT_SUFFIX` defined at the top of the notebook\n",
        "    to control the checkpoint directory naming (e.g. \"constraints\", \"collapse\").\n",
        "    \"\"\"\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"gan_{EXPERIMENT_SUFFIX}_{opt_name}_{loss_type}\"\n",
        "\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"gan\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "42aaa206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_gan(\n",
        "    train_data, \n",
        "    optimizer, \n",
        "    n_steps=10**3, \n",
        "    steps_per_save=100, \n",
        "    key=jax.random.key(0),\n",
        "    batch_size: int = 128,\n",
        "    latent_dim: int = 64,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    ):\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=key,\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = run_training_gan(\n",
        "        train_data=train_data,\n",
        "        n_steps=n_steps,\n",
        "        generator_training_state=generator_training_state,\n",
        "        discriminator_training_state=discriminator_training_state,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        batch_size=batch_size,\n",
        "        latent_dim=latent_dim,\n",
        "        loss_type=loss_type,\n",
        "    )\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**4) \n",
        "# test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d1ad17b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.key(0)\n",
        "n_training_samples = 10_000\n",
        "n_pixels = 784\n",
        "n_samples = 10_000\n",
        "\n",
        "mu1 = 200\n",
        "sigma1 = 50\n",
        "mu2 = 350\n",
        "sigma2 = 50 \n",
        "alpha = 0.5\n",
        "train_data = r_gaussian_mixed(key, mu1, sigma1, mu2, sigma2, alpha, n_pixels, n_samples, n_training_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "aa6a1022",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: generator_loss=0.6642202138900757, discriminator_loss=0.6921238899230957\n",
            "step 2: generator_loss=0.6690587401390076, discriminator_loss=0.6911597847938538\n",
            "step 3: generator_loss=0.6739383935928345, discriminator_loss=0.6901169419288635\n",
            "step 4: generator_loss=0.6786852478981018, discriminator_loss=0.689094603061676\n",
            "step 5: generator_loss=0.6830032467842102, discriminator_loss=0.6882257461547852\n",
            "step 6: generator_loss=0.6869418025016785, discriminator_loss=0.6874540448188782\n",
            "step 7: generator_loss=0.690497875213623, discriminator_loss=0.6867116689682007\n",
            "step 8: generator_loss=0.694202721118927, discriminator_loss=0.6857865452766418\n",
            "step 9: generator_loss=0.6976941823959351, discriminator_loss=0.6847840547561646\n",
            "step 10: generator_loss=0.7004246711730957, discriminator_loss=0.6840181946754456\n",
            "step 11: generator_loss=0.7029709815979004, discriminator_loss=0.6831805109977722\n",
            "step 12: generator_loss=0.7049039006233215, discriminator_loss=0.68243408203125\n",
            "step 13: generator_loss=0.706558346748352, discriminator_loss=0.6816374659538269\n",
            "step 14: generator_loss=0.7082406282424927, discriminator_loss=0.6807297468185425\n",
            "step 15: generator_loss=0.7083524465560913, discriminator_loss=0.6804454922676086\n",
            "step 16: generator_loss=0.709067702293396, discriminator_loss=0.6796634197235107\n",
            "step 17: generator_loss=0.7088552117347717, discriminator_loss=0.6792807579040527\n",
            "step 18: generator_loss=0.70882248878479, discriminator_loss=0.6787207722663879\n",
            "step 19: generator_loss=0.7086353302001953, discriminator_loss=0.6781242489814758\n",
            "step 20: generator_loss=0.707321286201477, discriminator_loss=0.6780332326889038\n",
            "step 21: generator_loss=0.7076181173324585, discriminator_loss=0.6772226095199585\n",
            "step 22: generator_loss=0.706308126449585, discriminator_loss=0.6771755814552307\n",
            "step 23: generator_loss=0.7046043872833252, discriminator_loss=0.6773084402084351\n",
            "step 24: generator_loss=0.7051939368247986, discriminator_loss=0.6764549016952515\n",
            "step 25: generator_loss=0.7034168243408203, discriminator_loss=0.6768074035644531\n",
            "step 26: generator_loss=0.7023348808288574, discriminator_loss=0.676898717880249\n",
            "step 27: generator_loss=0.7000482082366943, discriminator_loss=0.6777219772338867\n",
            "step 28: generator_loss=0.7005372643470764, discriminator_loss=0.6773348450660706\n",
            "step 29: generator_loss=0.6988687515258789, discriminator_loss=0.6780774593353271\n",
            "step 30: generator_loss=0.6977907419204712, discriminator_loss=0.6787407994270325\n",
            "step 31: generator_loss=0.6983418464660645, discriminator_loss=0.6787375211715698\n",
            "step 32: generator_loss=0.6976720094680786, discriminator_loss=0.6794165372848511\n",
            "step 33: generator_loss=0.6978139877319336, discriminator_loss=0.6798362731933594\n",
            "step 34: generator_loss=0.6970365643501282, discriminator_loss=0.6808186769485474\n",
            "step 35: generator_loss=0.6972203254699707, discriminator_loss=0.6814041137695312\n",
            "step 36: generator_loss=0.6985081434249878, discriminator_loss=0.6815637350082397\n",
            "step 37: generator_loss=0.6991739273071289, discriminator_loss=0.68216472864151\n",
            "step 38: generator_loss=0.7001965641975403, discriminator_loss=0.6823940277099609\n",
            "step 39: generator_loss=0.7003353834152222, discriminator_loss=0.6831432580947876\n",
            "step 40: generator_loss=0.6998138427734375, discriminator_loss=0.6841640472412109\n",
            "step 41: generator_loss=0.7012803554534912, discriminator_loss=0.6842137575149536\n",
            "step 42: generator_loss=0.7013881802558899, discriminator_loss=0.6847871541976929\n",
            "step 43: generator_loss=0.7005907297134399, discriminator_loss=0.6857060194015503\n",
            "step 44: generator_loss=0.6998679637908936, discriminator_loss=0.6864821910858154\n",
            "step 45: generator_loss=0.6997137069702148, discriminator_loss=0.6869155168533325\n",
            "step 46: generator_loss=0.6989294290542603, discriminator_loss=0.6876370310783386\n",
            "step 47: generator_loss=0.6967751383781433, discriminator_loss=0.6889697313308716\n",
            "step 48: generator_loss=0.6953954696655273, discriminator_loss=0.6899527311325073\n",
            "step 49: generator_loss=0.695839524269104, discriminator_loss=0.6899127960205078\n",
            "step 50: generator_loss=0.6929742097854614, discriminator_loss=0.6914809942245483\n",
            "step 51: generator_loss=0.6928304433822632, discriminator_loss=0.691724419593811\n",
            "step 52: generator_loss=0.6910262107849121, discriminator_loss=0.6927617192268372\n",
            "step 53: generator_loss=0.6904119253158569, discriminator_loss=0.6932689547538757\n",
            "step 54: generator_loss=0.689102053642273, discriminator_loss=0.6941182613372803\n",
            "step 55: generator_loss=0.6888375282287598, discriminator_loss=0.6945880651473999\n",
            "step 56: generator_loss=0.6877000331878662, discriminator_loss=0.695453405380249\n",
            "step 57: generator_loss=0.6868248581886292, discriminator_loss=0.6963099837303162\n",
            "step 58: generator_loss=0.6869630813598633, discriminator_loss=0.6966259479522705\n",
            "step 59: generator_loss=0.6869096159934998, discriminator_loss=0.6970850229263306\n",
            "step 60: generator_loss=0.6875928044319153, discriminator_loss=0.6970708966255188\n",
            "step 61: generator_loss=0.6873232126235962, discriminator_loss=0.697640597820282\n",
            "step 62: generator_loss=0.6867071390151978, discriminator_loss=0.6984051465988159\n",
            "step 63: generator_loss=0.6878490447998047, discriminator_loss=0.6979929208755493\n",
            "step 64: generator_loss=0.6880276203155518, discriminator_loss=0.6981241703033447\n",
            "step 65: generator_loss=0.6879947185516357, discriminator_loss=0.698239803314209\n",
            "step 66: generator_loss=0.6874692440032959, discriminator_loss=0.6986116170883179\n",
            "step 67: generator_loss=0.6882643699645996, discriminator_loss=0.698108434677124\n",
            "step 68: generator_loss=0.689091682434082, discriminator_loss=0.6976014375686646\n",
            "step 69: generator_loss=0.6891496777534485, discriminator_loss=0.6974379420280457\n",
            "step 70: generator_loss=0.6882963180541992, discriminator_loss=0.6975073218345642\n",
            "step 71: generator_loss=0.6895435452461243, discriminator_loss=0.6965295076370239\n",
            "step 72: generator_loss=0.6898539066314697, discriminator_loss=0.6958824396133423\n",
            "step 73: generator_loss=0.6908332705497742, discriminator_loss=0.6949402093887329\n",
            "step 74: generator_loss=0.6906273365020752, discriminator_loss=0.6944782733917236\n",
            "step 75: generator_loss=0.6906757354736328, discriminator_loss=0.6939418315887451\n",
            "step 76: generator_loss=0.6921026706695557, discriminator_loss=0.6926025152206421\n",
            "step 77: generator_loss=0.691887378692627, discriminator_loss=0.6921910643577576\n",
            "step 78: generator_loss=0.6923105716705322, discriminator_loss=0.6915370225906372\n",
            "step 79: generator_loss=0.6932345628738403, discriminator_loss=0.6905385255813599\n",
            "step 80: generator_loss=0.6937021017074585, discriminator_loss=0.6900246143341064\n",
            "step 81: generator_loss=0.6944729685783386, discriminator_loss=0.6892958283424377\n",
            "step 82: generator_loss=0.6961478590965271, discriminator_loss=0.688339114189148\n",
            "step 83: generator_loss=0.6965140104293823, discriminator_loss=0.6877620220184326\n",
            "step 84: generator_loss=0.6965789794921875, discriminator_loss=0.6874971389770508\n",
            "step 85: generator_loss=0.6975529193878174, discriminator_loss=0.6865185499191284\n",
            "step 86: generator_loss=0.6978117823600769, discriminator_loss=0.6861106753349304\n",
            "step 87: generator_loss=0.6961838006973267, discriminator_loss=0.6865006685256958\n",
            "step 88: generator_loss=0.6970974206924438, discriminator_loss=0.6856595277786255\n",
            "step 89: generator_loss=0.6971086859703064, discriminator_loss=0.6851851940155029\n",
            "step 90: generator_loss=0.6947290301322937, discriminator_loss=0.6861588954925537\n",
            "step 91: generator_loss=0.695571780204773, discriminator_loss=0.6855288743972778\n",
            "step 92: generator_loss=0.6946486234664917, discriminator_loss=0.685661256313324\n",
            "step 93: generator_loss=0.6944431066513062, discriminator_loss=0.6857771277427673\n",
            "step 94: generator_loss=0.6944191455841064, discriminator_loss=0.6858842968940735\n",
            "step 95: generator_loss=0.6968924403190613, discriminator_loss=0.6847566366195679\n",
            "step 96: generator_loss=0.6956764459609985, discriminator_loss=0.6856733560562134\n",
            "step 97: generator_loss=0.695409893989563, discriminator_loss=0.6858079433441162\n",
            "step 98: generator_loss=0.6967765092849731, discriminator_loss=0.6854023933410645\n",
            "step 99: generator_loss=0.6985447406768799, discriminator_loss=0.6847548484802246\n",
            "step 100: generator_loss=0.7023855447769165, discriminator_loss=0.6829590201377869\n",
            "step 101: generator_loss=0.7021775245666504, discriminator_loss=0.6830627918243408\n",
            "step 102: generator_loss=0.7023349404335022, discriminator_loss=0.6828819513320923\n",
            "step 103: generator_loss=0.704903781414032, discriminator_loss=0.6812833547592163\n",
            "step 104: generator_loss=0.7037061452865601, discriminator_loss=0.6813639402389526\n",
            "step 105: generator_loss=0.7065273523330688, discriminator_loss=0.6794975996017456\n",
            "step 106: generator_loss=0.7046096324920654, discriminator_loss=0.6799041032791138\n",
            "step 107: generator_loss=0.7037183046340942, discriminator_loss=0.6797811985015869\n",
            "step 108: generator_loss=0.7032536268234253, discriminator_loss=0.6793796420097351\n",
            "step 109: generator_loss=0.7036142945289612, discriminator_loss=0.678676962852478\n",
            "step 110: generator_loss=0.7031424045562744, discriminator_loss=0.6783785820007324\n",
            "step 111: generator_loss=0.7050447463989258, discriminator_loss=0.6771970987319946\n",
            "step 112: generator_loss=0.7065175175666809, discriminator_loss=0.6763021945953369\n",
            "step 113: generator_loss=0.7045621871948242, discriminator_loss=0.6771422028541565\n",
            "step 114: generator_loss=0.7068780064582825, discriminator_loss=0.676068127155304\n",
            "step 115: generator_loss=0.7079248428344727, discriminator_loss=0.6755221486091614\n",
            "step 116: generator_loss=0.7095139026641846, discriminator_loss=0.6746768951416016\n",
            "step 117: generator_loss=0.710928201675415, discriminator_loss=0.6739979386329651\n",
            "step 118: generator_loss=0.7130007147789001, discriminator_loss=0.6729755401611328\n",
            "step 119: generator_loss=0.7165569067001343, discriminator_loss=0.6707959771156311\n",
            "step 120: generator_loss=0.7181844711303711, discriminator_loss=0.6696317791938782\n",
            "step 121: generator_loss=0.7202492952346802, discriminator_loss=0.6682745218276978\n",
            "step 122: generator_loss=0.7215992212295532, discriminator_loss=0.6668294668197632\n",
            "step 123: generator_loss=0.7234361171722412, discriminator_loss=0.6651370525360107\n",
            "step 124: generator_loss=0.7268456220626831, discriminator_loss=0.6625630259513855\n",
            "step 125: generator_loss=0.7257110476493835, discriminator_loss=0.6618537306785583\n",
            "step 126: generator_loss=0.7282628417015076, discriminator_loss=0.6596483588218689\n",
            "step 127: generator_loss=0.7266117334365845, discriminator_loss=0.6592549681663513\n",
            "step 128: generator_loss=0.7292959690093994, discriminator_loss=0.6568180322647095\n",
            "step 129: generator_loss=0.7276318073272705, discriminator_loss=0.6562838554382324\n",
            "step 130: generator_loss=0.7282411456108093, discriminator_loss=0.6549043655395508\n",
            "step 131: generator_loss=0.7280535697937012, discriminator_loss=0.6540601849555969\n",
            "step 132: generator_loss=0.7270845174789429, discriminator_loss=0.6537778377532959\n",
            "step 133: generator_loss=0.7304643392562866, discriminator_loss=0.6517515182495117\n",
            "step 134: generator_loss=0.7329654693603516, discriminator_loss=0.6501822471618652\n",
            "step 135: generator_loss=0.7368453741073608, discriminator_loss=0.647794783115387\n",
            "step 136: generator_loss=0.7371439933776855, discriminator_loss=0.6477645635604858\n",
            "step 137: generator_loss=0.7366410493850708, discriminator_loss=0.6477330327033997\n",
            "step 138: generator_loss=0.7407766580581665, discriminator_loss=0.6458394527435303\n",
            "step 139: generator_loss=0.7450457811355591, discriminator_loss=0.6439160108566284\n",
            "step 140: generator_loss=0.7439839243888855, discriminator_loss=0.6438924074172974\n",
            "step 141: generator_loss=0.7499217391014099, discriminator_loss=0.6406883597373962\n",
            "step 142: generator_loss=0.7445334196090698, discriminator_loss=0.642637312412262\n",
            "step 143: generator_loss=0.7518312335014343, discriminator_loss=0.6386913657188416\n",
            "step 144: generator_loss=0.7526697516441345, discriminator_loss=0.6375299692153931\n",
            "step 145: generator_loss=0.7555006742477417, discriminator_loss=0.6357019543647766\n",
            "step 146: generator_loss=0.7623238563537598, discriminator_loss=0.6315966248512268\n",
            "step 147: generator_loss=0.7705954313278198, discriminator_loss=0.6270128488540649\n",
            "step 148: generator_loss=0.7713965177536011, discriminator_loss=0.6253156661987305\n",
            "step 149: generator_loss=0.7760356664657593, discriminator_loss=0.6219677925109863\n",
            "step 150: generator_loss=0.7802152633666992, discriminator_loss=0.6183195114135742\n",
            "step 151: generator_loss=0.7802557945251465, discriminator_loss=0.6164931654930115\n",
            "step 152: generator_loss=0.783295214176178, discriminator_loss=0.6130670309066772\n",
            "step 153: generator_loss=0.7885515689849854, discriminator_loss=0.6086854934692383\n",
            "step 154: generator_loss=0.7899709939956665, discriminator_loss=0.6062393188476562\n",
            "step 155: generator_loss=0.7923753261566162, discriminator_loss=0.6030595302581787\n",
            "step 156: generator_loss=0.8014967441558838, discriminator_loss=0.5972273945808411\n",
            "step 157: generator_loss=0.8040930032730103, discriminator_loss=0.5941547155380249\n",
            "step 158: generator_loss=0.8085153698921204, discriminator_loss=0.5904643535614014\n",
            "step 159: generator_loss=0.805732786655426, discriminator_loss=0.5898127555847168\n",
            "step 160: generator_loss=0.8160923719406128, discriminator_loss=0.5838758945465088\n",
            "step 161: generator_loss=0.8220808506011963, discriminator_loss=0.5796096324920654\n",
            "step 162: generator_loss=0.814873218536377, discriminator_loss=0.5810645818710327\n",
            "step 163: generator_loss=0.8207111358642578, discriminator_loss=0.5775390267372131\n",
            "step 164: generator_loss=0.8245720267295837, discriminator_loss=0.5746008157730103\n",
            "step 165: generator_loss=0.8142315745353699, discriminator_loss=0.5778412818908691\n",
            "step 166: generator_loss=0.8274673223495483, discriminator_loss=0.5717678070068359\n",
            "step 167: generator_loss=0.8116834163665771, discriminator_loss=0.5776885747909546\n",
            "step 168: generator_loss=0.8218600153923035, discriminator_loss=0.5734431147575378\n",
            "step 169: generator_loss=0.8218337297439575, discriminator_loss=0.573026180267334\n",
            "step 170: generator_loss=0.8186740875244141, discriminator_loss=0.574593186378479\n",
            "step 171: generator_loss=0.8226659297943115, discriminator_loss=0.5727772116661072\n",
            "step 172: generator_loss=0.8224457502365112, discriminator_loss=0.5730608701705933\n",
            "step 173: generator_loss=0.8271888494491577, discriminator_loss=0.5715417861938477\n",
            "step 174: generator_loss=0.8314331769943237, discriminator_loss=0.5702788233757019\n",
            "step 175: generator_loss=0.8439029455184937, discriminator_loss=0.5661705136299133\n",
            "step 176: generator_loss=0.8496842384338379, discriminator_loss=0.5643584728240967\n",
            "step 177: generator_loss=0.8590534329414368, discriminator_loss=0.5607249736785889\n",
            "step 178: generator_loss=0.8675864934921265, discriminator_loss=0.5568342804908752\n",
            "step 179: generator_loss=0.8691964149475098, discriminator_loss=0.555399477481842\n",
            "step 180: generator_loss=0.8758977055549622, discriminator_loss=0.551369309425354\n",
            "step 181: generator_loss=0.8751625418663025, discriminator_loss=0.5495291948318481\n",
            "step 182: generator_loss=0.8738105297088623, discriminator_loss=0.5480104088783264\n",
            "step 183: generator_loss=0.8690773248672485, discriminator_loss=0.5473355054855347\n",
            "step 184: generator_loss=0.8765943050384521, discriminator_loss=0.5419710278511047\n",
            "step 185: generator_loss=0.8687637448310852, discriminator_loss=0.5425479412078857\n",
            "step 186: generator_loss=0.8685839772224426, discriminator_loss=0.5412981510162354\n",
            "step 187: generator_loss=0.8842713832855225, discriminator_loss=0.5336019992828369\n",
            "step 188: generator_loss=0.8777326345443726, discriminator_loss=0.5347685813903809\n",
            "step 189: generator_loss=0.866387665271759, discriminator_loss=0.5374968647956848\n",
            "step 190: generator_loss=0.8734860420227051, discriminator_loss=0.5339474081993103\n",
            "step 191: generator_loss=0.8760902285575867, discriminator_loss=0.5323054194450378\n",
            "step 192: generator_loss=0.884027361869812, discriminator_loss=0.529444694519043\n",
            "step 193: generator_loss=0.8999994993209839, discriminator_loss=0.5235495567321777\n",
            "step 194: generator_loss=0.9181063771247864, discriminator_loss=0.5171220302581787\n",
            "step 195: generator_loss=0.9284301400184631, discriminator_loss=0.5133121609687805\n",
            "step 196: generator_loss=0.9499293565750122, discriminator_loss=0.5052033066749573\n",
            "step 197: generator_loss=0.963000476360321, discriminator_loss=0.4992574155330658\n",
            "step 198: generator_loss=0.9728899002075195, discriminator_loss=0.494110643863678\n",
            "step 199: generator_loss=0.9921627044677734, discriminator_loss=0.4857364296913147\n",
            "step 200: generator_loss=0.9953839778900146, discriminator_loss=0.4815526008605957\n",
            "step 201: generator_loss=1.0129194259643555, discriminator_loss=0.4729885458946228\n",
            "step 202: generator_loss=1.011929988861084, discriminator_loss=0.46976780891418457\n",
            "step 203: generator_loss=1.0242711305618286, discriminator_loss=0.4626811742782593\n",
            "step 204: generator_loss=1.031968593597412, discriminator_loss=0.456210732460022\n",
            "step 205: generator_loss=1.0374878644943237, discriminator_loss=0.4506550133228302\n",
            "step 206: generator_loss=1.0547120571136475, discriminator_loss=0.44197291135787964\n",
            "step 207: generator_loss=1.0557029247283936, discriminator_loss=0.43836233019828796\n",
            "step 208: generator_loss=1.070080041885376, discriminator_loss=0.4304361045360565\n",
            "step 209: generator_loss=1.0874125957489014, discriminator_loss=0.42194175720214844\n",
            "step 210: generator_loss=1.0766234397888184, discriminator_loss=0.4215790033340454\n",
            "step 211: generator_loss=1.0827800035476685, discriminator_loss=0.4162442088127136\n",
            "step 212: generator_loss=1.086277723312378, discriminator_loss=0.4122360348701477\n",
            "step 213: generator_loss=1.091657042503357, discriminator_loss=0.4073348641395569\n",
            "step 214: generator_loss=1.0866674184799194, discriminator_loss=0.40568506717681885\n",
            "step 215: generator_loss=1.0775501728057861, discriminator_loss=0.40593770146369934\n",
            "step 216: generator_loss=1.0780246257781982, discriminator_loss=0.4041267931461334\n",
            "step 217: generator_loss=1.1011860370635986, discriminator_loss=0.3967863917350769\n",
            "step 218: generator_loss=1.0903294086456299, discriminator_loss=0.39810630679130554\n",
            "step 219: generator_loss=1.0755401849746704, discriminator_loss=0.40098416805267334\n",
            "step 220: generator_loss=1.0880554914474487, discriminator_loss=0.3975537419319153\n",
            "step 221: generator_loss=1.0798982381820679, discriminator_loss=0.39967435598373413\n",
            "step 222: generator_loss=1.098231554031372, discriminator_loss=0.3949050307273865\n",
            "step 223: generator_loss=1.112850308418274, discriminator_loss=0.39128318428993225\n",
            "step 224: generator_loss=1.1227421760559082, discriminator_loss=0.3893357515335083\n",
            "step 225: generator_loss=1.1161319017410278, discriminator_loss=0.39144742488861084\n",
            "step 226: generator_loss=1.1277996301651, discriminator_loss=0.3888312578201294\n",
            "step 227: generator_loss=1.1633975505828857, discriminator_loss=0.38071656227111816\n",
            "step 228: generator_loss=1.189274549484253, discriminator_loss=0.3746861219406128\n",
            "step 229: generator_loss=1.172792673110962, discriminator_loss=0.37832725048065186\n",
            "step 230: generator_loss=1.1971642971038818, discriminator_loss=0.3715522885322571\n",
            "step 231: generator_loss=1.2331922054290771, discriminator_loss=0.362656831741333\n",
            "step 232: generator_loss=1.237851619720459, discriminator_loss=0.36162081360816956\n",
            "step 233: generator_loss=1.2612695693969727, discriminator_loss=0.35529372096061707\n",
            "step 234: generator_loss=1.258497953414917, discriminator_loss=0.353554904460907\n",
            "step 235: generator_loss=1.2700293064117432, discriminator_loss=0.3502858281135559\n",
            "step 236: generator_loss=1.264251470565796, discriminator_loss=0.34794169664382935\n",
            "step 237: generator_loss=1.2961130142211914, discriminator_loss=0.3399495780467987\n",
            "step 238: generator_loss=1.2917916774749756, discriminator_loss=0.3379923403263092\n",
            "step 239: generator_loss=1.2993178367614746, discriminator_loss=0.334631085395813\n",
            "step 240: generator_loss=1.3014241456985474, discriminator_loss=0.33087238669395447\n",
            "step 241: generator_loss=1.316620111465454, discriminator_loss=0.32610076665878296\n",
            "step 242: generator_loss=1.3111251592636108, discriminator_loss=0.32420802116394043\n",
            "step 243: generator_loss=1.3463701009750366, discriminator_loss=0.31535324454307556\n",
            "step 244: generator_loss=1.3517117500305176, discriminator_loss=0.31233999133110046\n",
            "step 245: generator_loss=1.379069447517395, discriminator_loss=0.30560237169265747\n",
            "step 246: generator_loss=1.4012532234191895, discriminator_loss=0.2993733882904053\n",
            "step 247: generator_loss=1.4346551895141602, discriminator_loss=0.29194870591163635\n",
            "step 248: generator_loss=1.4487254619598389, discriminator_loss=0.2863803207874298\n",
            "step 249: generator_loss=1.4837703704833984, discriminator_loss=0.2795703113079071\n",
            "step 250: generator_loss=1.498396635055542, discriminator_loss=0.2747499346733093\n",
            "step 251: generator_loss=1.5204397439956665, discriminator_loss=0.26849284768104553\n",
            "step 252: generator_loss=1.5306649208068848, discriminator_loss=0.2662200629711151\n",
            "step 253: generator_loss=1.5257868766784668, discriminator_loss=0.26412832736968994\n",
            "step 254: generator_loss=1.5494396686553955, discriminator_loss=0.25985151529312134\n",
            "step 255: generator_loss=1.525058627128601, discriminator_loss=0.26166659593582153\n",
            "step 256: generator_loss=1.5256645679473877, discriminator_loss=0.25891321897506714\n",
            "step 257: generator_loss=1.4901797771453857, discriminator_loss=0.26195478439331055\n",
            "step 258: generator_loss=1.5406222343444824, discriminator_loss=0.2527414858341217\n",
            "step 259: generator_loss=1.4680485725402832, discriminator_loss=0.26195478439331055\n",
            "step 260: generator_loss=1.472272276878357, discriminator_loss=0.2582359313964844\n",
            "step 261: generator_loss=1.5135090351104736, discriminator_loss=0.25052106380462646\n",
            "step 262: generator_loss=1.4999034404754639, discriminator_loss=0.2507469654083252\n",
            "step 263: generator_loss=1.503501296043396, discriminator_loss=0.2483016848564148\n",
            "step 264: generator_loss=1.5164260864257812, discriminator_loss=0.24466939270496368\n",
            "step 265: generator_loss=1.5959898233413696, discriminator_loss=0.23331862688064575\n",
            "step 266: generator_loss=1.627370834350586, discriminator_loss=0.227814182639122\n",
            "step 267: generator_loss=1.6777082681655884, discriminator_loss=0.22053542733192444\n",
            "step 268: generator_loss=1.73685884475708, discriminator_loss=0.2125064879655838\n",
            "step 269: generator_loss=1.7937206029891968, discriminator_loss=0.20505696535110474\n",
            "step 270: generator_loss=1.8561768531799316, discriminator_loss=0.19707779586315155\n",
            "step 271: generator_loss=1.914494514465332, discriminator_loss=0.18952319025993347\n",
            "step 272: generator_loss=1.9687838554382324, discriminator_loss=0.1826983243227005\n",
            "step 273: generator_loss=2.0335464477539062, discriminator_loss=0.17508335411548615\n",
            "step 274: generator_loss=2.0857574939727783, discriminator_loss=0.16868995130062103\n",
            "step 275: generator_loss=2.125113010406494, discriminator_loss=0.1636667251586914\n",
            "step 276: generator_loss=2.1961374282836914, discriminator_loss=0.15628628432750702\n",
            "step 277: generator_loss=2.227220058441162, discriminator_loss=0.15221036970615387\n",
            "step 278: generator_loss=2.272015333175659, discriminator_loss=0.14741487801074982\n",
            "step 279: generator_loss=2.322671413421631, discriminator_loss=0.1419004648923874\n",
            "step 280: generator_loss=2.352747678756714, discriminator_loss=0.13775670528411865\n",
            "step 281: generator_loss=2.3304474353790283, discriminator_loss=0.14037016034126282\n",
            "step 282: generator_loss=2.3834195137023926, discriminator_loss=0.13357964158058167\n",
            "step 283: generator_loss=2.3561244010925293, discriminator_loss=0.13585416972637177\n",
            "step 284: generator_loss=2.3253979682922363, discriminator_loss=0.13585983216762543\n",
            "step 285: generator_loss=2.3142120838165283, discriminator_loss=0.13909389078617096\n",
            "step 286: generator_loss=2.1990416049957275, discriminator_loss=0.15149745345115662\n",
            "step 287: generator_loss=2.2256383895874023, discriminator_loss=0.14503394067287445\n",
            "step 288: generator_loss=2.032062530517578, discriminator_loss=0.16723139584064484\n",
            "step 289: generator_loss=1.847501277923584, discriminator_loss=0.1952705830335617\n",
            "step 290: generator_loss=1.7538979053497314, discriminator_loss=0.20274445414543152\n",
            "step 291: generator_loss=1.6981719732284546, discriminator_loss=0.2101115882396698\n",
            "step 292: generator_loss=1.5763746500015259, discriminator_loss=0.2244817018508911\n",
            "step 293: generator_loss=1.5897657871246338, discriminator_loss=0.2229980081319809\n",
            "step 294: generator_loss=1.4168667793273926, discriminator_loss=0.2439691126346588\n",
            "step 295: generator_loss=1.4464272260665894, discriminator_loss=0.23943492770195007\n",
            "step 296: generator_loss=1.4551641941070557, discriminator_loss=0.23044532537460327\n",
            "step 297: generator_loss=1.361438512802124, discriminator_loss=0.23867887258529663\n",
            "step 298: generator_loss=1.4116402864456177, discriminator_loss=0.22807061672210693\n",
            "step 299: generator_loss=1.4859493970870972, discriminator_loss=0.2172919362783432\n",
            "step 300: generator_loss=1.5442795753479004, discriminator_loss=0.20805136859416962\n",
            "step 301: generator_loss=1.622096061706543, discriminator_loss=0.1997801810503006\n",
            "step 302: generator_loss=1.6984918117523193, discriminator_loss=0.19203171133995056\n",
            "step 303: generator_loss=1.7957234382629395, discriminator_loss=0.1842281073331833\n",
            "step 304: generator_loss=1.8935034275054932, discriminator_loss=0.17633022367954254\n",
            "step 305: generator_loss=2.000389575958252, discriminator_loss=0.1689530313014984\n",
            "step 306: generator_loss=2.1057705879211426, discriminator_loss=0.16213831305503845\n",
            "step 307: generator_loss=2.197634696960449, discriminator_loss=0.15617123246192932\n",
            "step 308: generator_loss=2.2926902770996094, discriminator_loss=0.15018898248672485\n",
            "step 309: generator_loss=2.3743481636047363, discriminator_loss=0.1449892222881317\n",
            "step 310: generator_loss=2.453108310699463, discriminator_loss=0.13977424800395966\n",
            "step 311: generator_loss=2.523552894592285, discriminator_loss=0.1348237544298172\n",
            "step 312: generator_loss=2.589299201965332, discriminator_loss=0.12983497977256775\n",
            "step 313: generator_loss=2.648070812225342, discriminator_loss=0.12504905462265015\n",
            "step 314: generator_loss=2.7012226581573486, discriminator_loss=0.12051167339086533\n",
            "step 315: generator_loss=2.7504167556762695, discriminator_loss=0.11574896425008774\n",
            "step 316: generator_loss=2.7942957878112793, discriminator_loss=0.11160901188850403\n",
            "step 317: generator_loss=2.8354549407958984, discriminator_loss=0.10730916261672974\n",
            "step 318: generator_loss=2.871461868286133, discriminator_loss=0.10334216803312302\n",
            "step 319: generator_loss=2.906416416168213, discriminator_loss=0.09932307153940201\n",
            "step 320: generator_loss=2.9364564418792725, discriminator_loss=0.09577757120132446\n",
            "step 321: generator_loss=2.965369701385498, discriminator_loss=0.09211175888776779\n",
            "step 322: generator_loss=2.9915425777435303, discriminator_loss=0.08911177515983582\n",
            "step 323: generator_loss=3.017704963684082, discriminator_loss=0.08634012937545776\n",
            "step 324: generator_loss=3.0423154830932617, discriminator_loss=0.08370459824800491\n",
            "step 325: generator_loss=3.0680055618286133, discriminator_loss=0.08142200112342834\n",
            "step 326: generator_loss=3.0926434993743896, discriminator_loss=0.07931162416934967\n",
            "step 327: generator_loss=3.117581605911255, discriminator_loss=0.07719898968935013\n",
            "step 328: generator_loss=3.143017530441284, discriminator_loss=0.07523734867572784\n",
            "step 329: generator_loss=3.1689069271087646, discriminator_loss=0.07327580451965332\n",
            "step 330: generator_loss=3.1954002380371094, discriminator_loss=0.07150141894817352\n",
            "step 331: generator_loss=3.2218666076660156, discriminator_loss=0.06976278871297836\n",
            "step 332: generator_loss=3.248403549194336, discriminator_loss=0.06801960617303848\n",
            "step 333: generator_loss=3.275458812713623, discriminator_loss=0.06636162102222443\n",
            "step 334: generator_loss=3.3019254207611084, discriminator_loss=0.06468633562326431\n",
            "step 335: generator_loss=3.32832670211792, discriminator_loss=0.06315882503986359\n",
            "step 336: generator_loss=3.355414390563965, discriminator_loss=0.06161043047904968\n",
            "step 337: generator_loss=3.3815126419067383, discriminator_loss=0.060175009071826935\n",
            "step 338: generator_loss=3.4077765941619873, discriminator_loss=0.058671627193689346\n",
            "step 339: generator_loss=3.4341201782226562, discriminator_loss=0.057263605296611786\n",
            "step 340: generator_loss=3.4598846435546875, discriminator_loss=0.05596911162137985\n",
            "step 341: generator_loss=3.485029697418213, discriminator_loss=0.054672129452228546\n",
            "step 342: generator_loss=3.509960651397705, discriminator_loss=0.05336344614624977\n",
            "step 343: generator_loss=3.5345935821533203, discriminator_loss=0.05221538990736008\n",
            "step 344: generator_loss=3.558701515197754, discriminator_loss=0.05105049908161163\n",
            "step 345: generator_loss=3.582015037536621, discriminator_loss=0.04992234706878662\n",
            "step 346: generator_loss=3.6052918434143066, discriminator_loss=0.048827365040779114\n",
            "step 347: generator_loss=3.627500534057617, discriminator_loss=0.04776010289788246\n",
            "step 348: generator_loss=3.649467706680298, discriminator_loss=0.04671210050582886\n",
            "step 349: generator_loss=3.67073130607605, discriminator_loss=0.04574919492006302\n",
            "step 350: generator_loss=3.6914496421813965, discriminator_loss=0.0448397696018219\n",
            "step 351: generator_loss=3.71132493019104, discriminator_loss=0.04398152977228165\n",
            "step 352: generator_loss=3.7305078506469727, discriminator_loss=0.0430942177772522\n",
            "step 353: generator_loss=3.7491068840026855, discriminator_loss=0.042282767593860626\n",
            "step 354: generator_loss=3.766725540161133, discriminator_loss=0.04146778956055641\n",
            "step 355: generator_loss=3.7834739685058594, discriminator_loss=0.04069693386554718\n",
            "step 356: generator_loss=3.7995657920837402, discriminator_loss=0.039949312806129456\n",
            "step 357: generator_loss=3.8148250579833984, discriminator_loss=0.03920093923807144\n",
            "step 358: generator_loss=3.829094886779785, discriminator_loss=0.038543909788131714\n",
            "step 359: generator_loss=3.84283185005188, discriminator_loss=0.03786618635058403\n",
            "step 360: generator_loss=3.855726718902588, discriminator_loss=0.03723733872175217\n",
            "step 361: generator_loss=3.8680715560913086, discriminator_loss=0.036591921001672745\n",
            "step 362: generator_loss=3.879939079284668, discriminator_loss=0.03595270588994026\n",
            "step 363: generator_loss=3.891493082046509, discriminator_loss=0.035368867218494415\n",
            "step 364: generator_loss=3.9026145935058594, discriminator_loss=0.034789446741342545\n",
            "step 365: generator_loss=3.9135842323303223, discriminator_loss=0.0342627689242363\n",
            "step 366: generator_loss=3.9244837760925293, discriminator_loss=0.0336703285574913\n",
            "step 367: generator_loss=3.9354138374328613, discriminator_loss=0.0331522673368454\n",
            "step 368: generator_loss=3.9463138580322266, discriminator_loss=0.032580845057964325\n",
            "step 369: generator_loss=3.957435131072998, discriminator_loss=0.03211935982108116\n",
            "step 370: generator_loss=3.9686241149902344, discriminator_loss=0.031677935272455215\n",
            "step 371: generator_loss=3.9799275398254395, discriminator_loss=0.031174244359135628\n",
            "step 372: generator_loss=3.9913992881774902, discriminator_loss=0.030735613778233528\n",
            "step 373: generator_loss=4.002972602844238, discriminator_loss=0.03027036041021347\n",
            "step 374: generator_loss=4.014659404754639, discriminator_loss=0.029827909544110298\n",
            "step 375: generator_loss=4.026583671569824, discriminator_loss=0.029383497312664986\n",
            "step 376: generator_loss=4.038515090942383, discriminator_loss=0.02895866520702839\n",
            "step 377: generator_loss=4.050647258758545, discriminator_loss=0.02855738252401352\n",
            "step 378: generator_loss=4.062829494476318, discriminator_loss=0.02816612273454666\n",
            "step 379: generator_loss=4.075129508972168, discriminator_loss=0.027755729854106903\n",
            "step 380: generator_loss=4.087355613708496, discriminator_loss=0.027395309880375862\n",
            "step 381: generator_loss=4.099719524383545, discriminator_loss=0.026997007429599762\n",
            "step 382: generator_loss=4.111991882324219, discriminator_loss=0.026651032269001007\n",
            "step 383: generator_loss=4.1240997314453125, discriminator_loss=0.026326067745685577\n",
            "step 384: generator_loss=4.136159420013428, discriminator_loss=0.025949904695153236\n",
            "step 385: generator_loss=4.148074150085449, discriminator_loss=0.025634538382291794\n",
            "step 386: generator_loss=4.159931182861328, discriminator_loss=0.025285597890615463\n",
            "step 387: generator_loss=4.171534538269043, discriminator_loss=0.024975761771202087\n",
            "step 388: generator_loss=4.1830291748046875, discriminator_loss=0.024680722504854202\n",
            "step 389: generator_loss=4.194392204284668, discriminator_loss=0.024321969598531723\n",
            "step 390: generator_loss=4.205670356750488, discriminator_loss=0.02400541678071022\n",
            "step 391: generator_loss=4.2168073654174805, discriminator_loss=0.02373906597495079\n",
            "step 392: generator_loss=4.227863311767578, discriminator_loss=0.023454295471310616\n",
            "step 393: generator_loss=4.238912582397461, discriminator_loss=0.023142600432038307\n",
            "step 394: generator_loss=4.249675750732422, discriminator_loss=0.02290717326104641\n",
            "step 395: generator_loss=4.260429382324219, discriminator_loss=0.02257465571165085\n",
            "step 396: generator_loss=4.2710771560668945, discriminator_loss=0.022326786071062088\n",
            "step 397: generator_loss=4.281686782836914, discriminator_loss=0.022062750533223152\n",
            "step 398: generator_loss=4.292302131652832, discriminator_loss=0.021786075085401535\n",
            "step 399: generator_loss=4.302793502807617, discriminator_loss=0.021551813930273056\n",
            "step 400: generator_loss=4.313333988189697, discriminator_loss=0.021273504942655563\n",
            "step 401: generator_loss=4.323783874511719, discriminator_loss=0.021072503179311752\n",
            "step 402: generator_loss=4.334240913391113, discriminator_loss=0.020820144563913345\n",
            "step 403: generator_loss=4.344540119171143, discriminator_loss=0.020585261285305023\n",
            "step 404: generator_loss=4.354772567749023, discriminator_loss=0.02038082480430603\n",
            "step 405: generator_loss=4.364925384521484, discriminator_loss=0.020139582455158234\n",
            "step 406: generator_loss=4.375120162963867, discriminator_loss=0.019901838153600693\n",
            "step 407: generator_loss=4.385287761688232, discriminator_loss=0.01966310478746891\n",
            "step 408: generator_loss=4.395411491394043, discriminator_loss=0.019449839368462563\n",
            "step 409: generator_loss=4.405557632446289, discriminator_loss=0.019262447953224182\n",
            "step 410: generator_loss=4.415594100952148, discriminator_loss=0.019093820825219154\n",
            "step 411: generator_loss=4.425539016723633, discriminator_loss=0.018869582563638687\n",
            "step 412: generator_loss=4.435427665710449, discriminator_loss=0.018668655306100845\n",
            "step 413: generator_loss=4.445314407348633, discriminator_loss=0.01844826340675354\n",
            "step 414: generator_loss=4.455084323883057, discriminator_loss=0.018271740525960922\n",
            "step 415: generator_loss=4.464911460876465, discriminator_loss=0.01808501034975052\n",
            "step 416: generator_loss=4.474552154541016, discriminator_loss=0.017900003120303154\n",
            "step 417: generator_loss=4.484314918518066, discriminator_loss=0.01772473379969597\n",
            "step 418: generator_loss=4.493923187255859, discriminator_loss=0.017535511404275894\n",
            "step 419: generator_loss=4.503539562225342, discriminator_loss=0.017356712371110916\n",
            "step 420: generator_loss=4.5130391120910645, discriminator_loss=0.017182782292366028\n",
            "step 421: generator_loss=4.522517204284668, discriminator_loss=0.01702594757080078\n",
            "step 422: generator_loss=4.5318498611450195, discriminator_loss=0.01683569699525833\n",
            "step 423: generator_loss=4.541302680969238, discriminator_loss=0.016671881079673767\n",
            "step 424: generator_loss=4.550656318664551, discriminator_loss=0.016543852165341377\n",
            "step 425: generator_loss=4.559865474700928, discriminator_loss=0.01638410985469818\n",
            "step 426: generator_loss=4.569016456604004, discriminator_loss=0.016197822988033295\n",
            "step 427: generator_loss=4.578171730041504, discriminator_loss=0.016064021736383438\n",
            "step 428: generator_loss=4.5872697830200195, discriminator_loss=0.015905406326055527\n",
            "step 429: generator_loss=4.596287250518799, discriminator_loss=0.015740174800157547\n",
            "step 430: generator_loss=4.605414390563965, discriminator_loss=0.015590643510222435\n",
            "step 431: generator_loss=4.614335060119629, discriminator_loss=0.01545003056526184\n",
            "step 432: generator_loss=4.623254299163818, discriminator_loss=0.015308896079659462\n",
            "step 433: generator_loss=4.632168769836426, discriminator_loss=0.015191037207841873\n",
            "step 434: generator_loss=4.640977382659912, discriminator_loss=0.01501236017793417\n",
            "step 435: generator_loss=4.6498260498046875, discriminator_loss=0.01489906944334507\n",
            "step 436: generator_loss=4.658556938171387, discriminator_loss=0.014764495193958282\n",
            "step 437: generator_loss=4.667280197143555, discriminator_loss=0.014624333009123802\n",
            "step 438: generator_loss=4.6759033203125, discriminator_loss=0.014490608125925064\n",
            "step 439: generator_loss=4.684538841247559, discriminator_loss=0.014379878528416157\n",
            "step 440: generator_loss=4.6930341720581055, discriminator_loss=0.014239278621971607\n",
            "step 441: generator_loss=4.70151424407959, discriminator_loss=0.014120979234576225\n",
            "step 442: generator_loss=4.710080146789551, discriminator_loss=0.01400278601795435\n",
            "step 443: generator_loss=4.718288898468018, discriminator_loss=0.013871315866708755\n",
            "step 444: generator_loss=4.726770401000977, discriminator_loss=0.013747160322964191\n",
            "step 445: generator_loss=4.735100746154785, discriminator_loss=0.013633733615279198\n",
            "step 446: generator_loss=4.743494510650635, discriminator_loss=0.013508575968444347\n",
            "step 447: generator_loss=4.751784324645996, discriminator_loss=0.01340576820075512\n",
            "step 448: generator_loss=4.760056495666504, discriminator_loss=0.013281414285302162\n",
            "step 449: generator_loss=4.768379211425781, discriminator_loss=0.013172037899494171\n",
            "step 450: generator_loss=4.776540756225586, discriminator_loss=0.013071813620626926\n",
            "step 451: generator_loss=4.784785270690918, discriminator_loss=0.012964477762579918\n",
            "step 452: generator_loss=4.792797565460205, discriminator_loss=0.012839971110224724\n",
            "step 453: generator_loss=4.800884246826172, discriminator_loss=0.01274940650910139\n",
            "step 454: generator_loss=4.8089141845703125, discriminator_loss=0.012632743455469608\n",
            "step 455: generator_loss=4.817060947418213, discriminator_loss=0.012538272887468338\n",
            "step 456: generator_loss=4.8252058029174805, discriminator_loss=0.012438474223017693\n",
            "step 457: generator_loss=4.833088397979736, discriminator_loss=0.01233956590294838\n",
            "step 458: generator_loss=4.841022968292236, discriminator_loss=0.012231425382196903\n",
            "step 459: generator_loss=4.849013328552246, discriminator_loss=0.012140931561589241\n",
            "step 460: generator_loss=4.85689640045166, discriminator_loss=0.012029008008539677\n",
            "step 461: generator_loss=4.864758491516113, discriminator_loss=0.011959460563957691\n",
            "step 462: generator_loss=4.872509479522705, discriminator_loss=0.011829305440187454\n",
            "step 463: generator_loss=4.880298137664795, discriminator_loss=0.011760922148823738\n",
            "step 464: generator_loss=4.888058185577393, discriminator_loss=0.011690529994666576\n",
            "step 465: generator_loss=4.895712375640869, discriminator_loss=0.011580263264477253\n",
            "step 466: generator_loss=4.903168201446533, discriminator_loss=0.01149850431829691\n",
            "step 467: generator_loss=4.910854339599609, discriminator_loss=0.011406639590859413\n",
            "step 468: generator_loss=4.9181365966796875, discriminator_loss=0.011322715319693089\n",
            "step 469: generator_loss=4.925684928894043, discriminator_loss=0.011240373365581036\n",
            "step 470: generator_loss=4.9330291748046875, discriminator_loss=0.011138890869915485\n",
            "step 471: generator_loss=4.9404377937316895, discriminator_loss=0.011044442653656006\n",
            "step 472: generator_loss=4.947745323181152, discriminator_loss=0.010984891094267368\n",
            "step 473: generator_loss=4.955094337463379, discriminator_loss=0.010869543068110943\n",
            "step 474: generator_loss=4.962285041809082, discriminator_loss=0.01080249808728695\n",
            "step 475: generator_loss=4.969560623168945, discriminator_loss=0.01075311191380024\n",
            "step 476: generator_loss=4.976614952087402, discriminator_loss=0.01066218875348568\n",
            "step 477: generator_loss=4.983868598937988, discriminator_loss=0.01057826355099678\n",
            "step 478: generator_loss=4.99095344543457, discriminator_loss=0.010511688888072968\n",
            "step 479: generator_loss=4.998082160949707, discriminator_loss=0.01043096836656332\n",
            "step 480: generator_loss=5.0049262046813965, discriminator_loss=0.010347309522330761\n",
            "step 481: generator_loss=5.012103080749512, discriminator_loss=0.010286803357303143\n",
            "step 482: generator_loss=5.019149303436279, discriminator_loss=0.01018910575658083\n",
            "step 483: generator_loss=5.0260114669799805, discriminator_loss=0.010132299736142159\n",
            "step 484: generator_loss=5.032961368560791, discriminator_loss=0.01006777212023735\n",
            "step 485: generator_loss=5.03973388671875, discriminator_loss=0.009985964745283127\n",
            "step 486: generator_loss=5.046673774719238, discriminator_loss=0.009915095753967762\n",
            "step 487: generator_loss=5.053645133972168, discriminator_loss=0.009842928498983383\n",
            "step 488: generator_loss=5.060488224029541, discriminator_loss=0.009787548333406448\n",
            "step 489: generator_loss=5.067261219024658, discriminator_loss=0.009698858484625816\n",
            "step 490: generator_loss=5.074192047119141, discriminator_loss=0.009647201746702194\n",
            "step 491: generator_loss=5.080850601196289, discriminator_loss=0.009564079344272614\n",
            "step 492: generator_loss=5.087630271911621, discriminator_loss=0.009507929906249046\n",
            "step 493: generator_loss=5.094444274902344, discriminator_loss=0.009440043941140175\n",
            "step 494: generator_loss=5.101325035095215, discriminator_loss=0.00937756709754467\n",
            "step 495: generator_loss=5.107662200927734, discriminator_loss=0.009324247017502785\n",
            "step 496: generator_loss=5.114325523376465, discriminator_loss=0.009261080995202065\n",
            "step 497: generator_loss=5.121077537536621, discriminator_loss=0.009195778518915176\n",
            "step 498: generator_loss=5.127432346343994, discriminator_loss=0.009146757423877716\n",
            "step 499: generator_loss=5.134042263031006, discriminator_loss=0.009070238098502159\n",
            "step 500: generator_loss=5.140480995178223, discriminator_loss=0.009015706367790699\n",
            "step 501: generator_loss=5.146939277648926, discriminator_loss=0.008943622931838036\n",
            "step 502: generator_loss=5.153258800506592, discriminator_loss=0.008899439126253128\n",
            "step 503: generator_loss=5.159726619720459, discriminator_loss=0.008840488269925117\n",
            "step 504: generator_loss=5.16597843170166, discriminator_loss=0.008777359500527382\n",
            "step 505: generator_loss=5.172427654266357, discriminator_loss=0.008714109659194946\n",
            "step 506: generator_loss=5.178865432739258, discriminator_loss=0.008664283901453018\n",
            "step 507: generator_loss=5.185303688049316, discriminator_loss=0.00860867090523243\n",
            "step 508: generator_loss=5.191303253173828, discriminator_loss=0.008549408055841923\n",
            "step 509: generator_loss=5.19795560836792, discriminator_loss=0.008493314497172832\n",
            "step 510: generator_loss=5.2040557861328125, discriminator_loss=0.008441499434411526\n",
            "step 511: generator_loss=5.2103166580200195, discriminator_loss=0.008381349965929985\n",
            "step 512: generator_loss=5.216666221618652, discriminator_loss=0.008329665288329124\n",
            "step 513: generator_loss=5.2227067947387695, discriminator_loss=0.008286014199256897\n",
            "step 514: generator_loss=5.228997707366943, discriminator_loss=0.008229495957493782\n",
            "step 515: generator_loss=5.235101222991943, discriminator_loss=0.008185279555618763\n",
            "step 516: generator_loss=5.241304397583008, discriminator_loss=0.008136795833706856\n",
            "step 517: generator_loss=5.247480869293213, discriminator_loss=0.008071774616837502\n",
            "step 518: generator_loss=5.253499984741211, discriminator_loss=0.008028031326830387\n",
            "step 519: generator_loss=5.259402275085449, discriminator_loss=0.007968109101057053\n",
            "step 520: generator_loss=5.265589714050293, discriminator_loss=0.007936617359519005\n",
            "step 521: generator_loss=5.271537780761719, discriminator_loss=0.00787384994328022\n",
            "step 522: generator_loss=5.277591705322266, discriminator_loss=0.007835268042981625\n",
            "step 523: generator_loss=5.283507347106934, discriminator_loss=0.007791377138346434\n",
            "step 524: generator_loss=5.289314270019531, discriminator_loss=0.007750976365059614\n",
            "step 525: generator_loss=5.295143127441406, discriminator_loss=0.007687273435294628\n",
            "step 526: generator_loss=5.301179885864258, discriminator_loss=0.0076523590832948685\n",
            "step 527: generator_loss=5.307159423828125, discriminator_loss=0.007600628770887852\n",
            "step 528: generator_loss=5.312638282775879, discriminator_loss=0.007550455629825592\n",
            "step 529: generator_loss=5.318448543548584, discriminator_loss=0.007502495311200619\n",
            "step 530: generator_loss=5.324071884155273, discriminator_loss=0.007472630590200424\n",
            "step 531: generator_loss=5.330397605895996, discriminator_loss=0.0074233319610357285\n",
            "step 532: generator_loss=5.335829257965088, discriminator_loss=0.007372480351477861\n",
            "step 533: generator_loss=5.341566562652588, discriminator_loss=0.0073310816660523415\n",
            "step 534: generator_loss=5.347497940063477, discriminator_loss=0.007291626185178757\n",
            "step 535: generator_loss=5.35328483581543, discriminator_loss=0.007252494804561138\n",
            "step 536: generator_loss=5.35875129699707, discriminator_loss=0.0072003998793661594\n",
            "step 537: generator_loss=5.364641189575195, discriminator_loss=0.007158533204346895\n",
            "step 538: generator_loss=5.370303153991699, discriminator_loss=0.007118117064237595\n",
            "step 539: generator_loss=5.37561559677124, discriminator_loss=0.007091068662703037\n",
            "step 540: generator_loss=5.381274223327637, discriminator_loss=0.0070327697321772575\n",
            "step 541: generator_loss=5.387134552001953, discriminator_loss=0.007001548074185848\n",
            "step 542: generator_loss=5.392523765563965, discriminator_loss=0.006965192966163158\n",
            "step 543: generator_loss=5.398275375366211, discriminator_loss=0.006938138045370579\n",
            "step 544: generator_loss=5.403509616851807, discriminator_loss=0.006890617311000824\n",
            "step 545: generator_loss=5.409075736999512, discriminator_loss=0.006849227007478476\n",
            "step 546: generator_loss=5.414590835571289, discriminator_loss=0.006805901415646076\n",
            "step 547: generator_loss=5.419972896575928, discriminator_loss=0.00676435511559248\n",
            "step 548: generator_loss=5.425323486328125, discriminator_loss=0.006723947357386351\n",
            "step 549: generator_loss=5.430889129638672, discriminator_loss=0.006692310329526663\n",
            "step 550: generator_loss=5.436382293701172, discriminator_loss=0.006645243614912033\n",
            "step 551: generator_loss=5.441864967346191, discriminator_loss=0.0066070519387722015\n",
            "step 552: generator_loss=5.447361946105957, discriminator_loss=0.006571468897163868\n",
            "step 553: generator_loss=5.452246189117432, discriminator_loss=0.006546289660036564\n",
            "step 554: generator_loss=5.4577484130859375, discriminator_loss=0.006515573710203171\n",
            "step 555: generator_loss=5.463470458984375, discriminator_loss=0.006482458673417568\n",
            "step 556: generator_loss=5.468854904174805, discriminator_loss=0.006437273696064949\n",
            "step 557: generator_loss=5.473762512207031, discriminator_loss=0.0064080036245286465\n",
            "step 558: generator_loss=5.479346752166748, discriminator_loss=0.006369220092892647\n",
            "step 559: generator_loss=5.484550952911377, discriminator_loss=0.00634548906236887\n",
            "step 560: generator_loss=5.490100860595703, discriminator_loss=0.0063051278702914715\n",
            "step 561: generator_loss=5.4951958656311035, discriminator_loss=0.006282241549342871\n",
            "step 562: generator_loss=5.500520706176758, discriminator_loss=0.006234058178961277\n",
            "step 563: generator_loss=5.505666732788086, discriminator_loss=0.006197099573910236\n",
            "step 564: generator_loss=5.511006832122803, discriminator_loss=0.00617809034883976\n",
            "step 565: generator_loss=5.515936374664307, discriminator_loss=0.006139418110251427\n",
            "step 566: generator_loss=5.521293640136719, discriminator_loss=0.006098290905356407\n",
            "step 567: generator_loss=5.526458740234375, discriminator_loss=0.006066518370062113\n",
            "step 568: generator_loss=5.531334400177002, discriminator_loss=0.006042655557394028\n",
            "step 569: generator_loss=5.536495208740234, discriminator_loss=0.006006411276757717\n",
            "step 570: generator_loss=5.541718482971191, discriminator_loss=0.0059742797166109085\n",
            "step 571: generator_loss=5.546283721923828, discriminator_loss=0.005949307233095169\n",
            "step 572: generator_loss=5.551981449127197, discriminator_loss=0.0059126438573002815\n",
            "step 573: generator_loss=5.557094573974609, discriminator_loss=0.005889163352549076\n",
            "step 574: generator_loss=5.561997890472412, discriminator_loss=0.005854838527739048\n",
            "step 575: generator_loss=5.56723690032959, discriminator_loss=0.0058315591886639595\n",
            "step 576: generator_loss=5.572011947631836, discriminator_loss=0.005793394986540079\n",
            "step 577: generator_loss=5.577089309692383, discriminator_loss=0.005771582946181297\n",
            "step 578: generator_loss=5.582178115844727, discriminator_loss=0.005743145011365414\n",
            "step 579: generator_loss=5.586852073669434, discriminator_loss=0.005715708248317242\n",
            "step 580: generator_loss=5.5918684005737305, discriminator_loss=0.005678665824234486\n",
            "step 581: generator_loss=5.596227169036865, discriminator_loss=0.005661431699991226\n",
            "step 582: generator_loss=5.601203441619873, discriminator_loss=0.005627434700727463\n",
            "step 583: generator_loss=5.6062846183776855, discriminator_loss=0.00560107734054327\n",
            "step 584: generator_loss=5.610959529876709, discriminator_loss=0.0055689821019768715\n",
            "step 585: generator_loss=5.615954399108887, discriminator_loss=0.0055413562804460526\n",
            "step 586: generator_loss=5.620814323425293, discriminator_loss=0.0055111972615122795\n",
            "step 587: generator_loss=5.626047134399414, discriminator_loss=0.005485885310918093\n",
            "step 588: generator_loss=5.630595684051514, discriminator_loss=0.0054555912502110004\n",
            "step 589: generator_loss=5.635353088378906, discriminator_loss=0.005439372267574072\n",
            "step 590: generator_loss=5.63974666595459, discriminator_loss=0.005410987418144941\n",
            "step 591: generator_loss=5.644593715667725, discriminator_loss=0.0053776283748447895\n",
            "step 592: generator_loss=5.649318218231201, discriminator_loss=0.005356454290449619\n",
            "step 593: generator_loss=5.654305458068848, discriminator_loss=0.005327240098267794\n",
            "step 594: generator_loss=5.658923149108887, discriminator_loss=0.005308575928211212\n",
            "step 595: generator_loss=5.663717746734619, discriminator_loss=0.005271606147289276\n",
            "step 596: generator_loss=5.668242454528809, discriminator_loss=0.005252408795058727\n",
            "step 597: generator_loss=5.672746658325195, discriminator_loss=0.005235549062490463\n",
            "step 598: generator_loss=5.677882194519043, discriminator_loss=0.005202070344239473\n",
            "step 599: generator_loss=5.6821770668029785, discriminator_loss=0.005175093654543161\n",
            "step 600: generator_loss=5.686795234680176, discriminator_loss=0.005144896451383829\n",
            "step 601: generator_loss=5.691598892211914, discriminator_loss=0.005125255789607763\n",
            "step 602: generator_loss=5.695888519287109, discriminator_loss=0.005098490044474602\n",
            "step 603: generator_loss=5.7014265060424805, discriminator_loss=0.005073367618024349\n",
            "step 604: generator_loss=5.705656051635742, discriminator_loss=0.005053895525634289\n",
            "step 605: generator_loss=5.710319995880127, discriminator_loss=0.005032015964388847\n",
            "step 606: generator_loss=5.714943885803223, discriminator_loss=0.005004302132874727\n",
            "step 607: generator_loss=5.719457626342773, discriminator_loss=0.004982631653547287\n",
            "step 608: generator_loss=5.723445415496826, discriminator_loss=0.00496746227145195\n",
            "step 609: generator_loss=5.727996826171875, discriminator_loss=0.004940000828355551\n",
            "step 610: generator_loss=5.733734130859375, discriminator_loss=0.004916112404316664\n",
            "step 611: generator_loss=5.738056659698486, discriminator_loss=0.004894430749118328\n",
            "step 612: generator_loss=5.74290132522583, discriminator_loss=0.004873054102063179\n",
            "step 613: generator_loss=5.746681213378906, discriminator_loss=0.004852202720940113\n",
            "step 614: generator_loss=5.751514434814453, discriminator_loss=0.004820020403712988\n",
            "step 615: generator_loss=5.756213188171387, discriminator_loss=0.004796904511749744\n",
            "step 616: generator_loss=5.760615348815918, discriminator_loss=0.00478434469550848\n",
            "step 617: generator_loss=5.764730453491211, discriminator_loss=0.004756678361445665\n",
            "step 618: generator_loss=5.769261360168457, discriminator_loss=0.0047406223602592945\n",
            "step 619: generator_loss=5.773690700531006, discriminator_loss=0.004716657567769289\n",
            "step 620: generator_loss=5.7782769203186035, discriminator_loss=0.004690784495323896\n",
            "step 621: generator_loss=5.782873153686523, discriminator_loss=0.0046686395071446896\n",
            "step 622: generator_loss=5.787444591522217, discriminator_loss=0.004654014483094215\n",
            "step 623: generator_loss=5.791416168212891, discriminator_loss=0.004633487202227116\n",
            "step 624: generator_loss=5.796113967895508, discriminator_loss=0.004611886106431484\n",
            "step 625: generator_loss=5.799871921539307, discriminator_loss=0.004587039817124605\n",
            "step 626: generator_loss=5.804679870605469, discriminator_loss=0.00456314068287611\n",
            "step 627: generator_loss=5.808743953704834, discriminator_loss=0.004553141072392464\n",
            "step 628: generator_loss=5.813379287719727, discriminator_loss=0.00453552883118391\n",
            "step 629: generator_loss=5.817574501037598, discriminator_loss=0.004513021558523178\n",
            "step 630: generator_loss=5.821166515350342, discriminator_loss=0.0044906241819262505\n",
            "step 631: generator_loss=5.8265581130981445, discriminator_loss=0.0044767120853066444\n",
            "step 632: generator_loss=5.830874443054199, discriminator_loss=0.004444668535143137\n",
            "step 633: generator_loss=5.834865570068359, discriminator_loss=0.0044329348020255566\n",
            "step 634: generator_loss=5.838500499725342, discriminator_loss=0.004409388639032841\n",
            "step 635: generator_loss=5.842984199523926, discriminator_loss=0.004390171263366938\n",
            "step 636: generator_loss=5.8482255935668945, discriminator_loss=0.004374029114842415\n",
            "step 637: generator_loss=5.851408004760742, discriminator_loss=0.004350409843027592\n",
            "step 638: generator_loss=5.8559160232543945, discriminator_loss=0.00433830451220274\n",
            "step 639: generator_loss=5.859951019287109, discriminator_loss=0.004329497925937176\n",
            "step 640: generator_loss=5.863420486450195, discriminator_loss=0.004303457215428352\n",
            "step 641: generator_loss=5.867996692657471, discriminator_loss=0.004276093095541\n",
            "step 642: generator_loss=5.872063636779785, discriminator_loss=0.0042598433792591095\n",
            "step 643: generator_loss=5.876735687255859, discriminator_loss=0.004243847914040089\n",
            "step 644: generator_loss=5.881352424621582, discriminator_loss=0.004230412654578686\n",
            "step 645: generator_loss=5.885178089141846, discriminator_loss=0.004204539116472006\n",
            "step 646: generator_loss=5.8891377449035645, discriminator_loss=0.004190446808934212\n",
            "step 647: generator_loss=5.893406867980957, discriminator_loss=0.004171342588961124\n",
            "step 648: generator_loss=5.897825241088867, discriminator_loss=0.00415511429309845\n",
            "step 649: generator_loss=5.901679515838623, discriminator_loss=0.0041398536413908005\n",
            "step 650: generator_loss=5.9055633544921875, discriminator_loss=0.0041106827557086945\n",
            "step 651: generator_loss=5.910412788391113, discriminator_loss=0.0041063688695430756\n",
            "step 652: generator_loss=5.913936138153076, discriminator_loss=0.004082606639713049\n",
            "step 653: generator_loss=5.918008327484131, discriminator_loss=0.004066508263349533\n",
            "step 654: generator_loss=5.921241760253906, discriminator_loss=0.004056160803884268\n",
            "step 655: generator_loss=5.926302909851074, discriminator_loss=0.004036166705191135\n",
            "step 656: generator_loss=5.931102275848389, discriminator_loss=0.004021971486508846\n",
            "step 657: generator_loss=5.9337544441223145, discriminator_loss=0.003994473721832037\n",
            "step 658: generator_loss=5.939384937286377, discriminator_loss=0.0039860885590314865\n",
            "step 659: generator_loss=5.942047119140625, discriminator_loss=0.0039642564952373505\n",
            "step 660: generator_loss=5.9467363357543945, discriminator_loss=0.003952639177441597\n",
            "step 661: generator_loss=5.9506659507751465, discriminator_loss=0.003944583237171173\n",
            "step 662: generator_loss=5.953662395477295, discriminator_loss=0.00391797162592411\n",
            "step 663: generator_loss=5.95877742767334, discriminator_loss=0.0039043514989316463\n",
            "step 664: generator_loss=5.962920188903809, discriminator_loss=0.0038891222793608904\n",
            "step 665: generator_loss=5.966291427612305, discriminator_loss=0.0038702860474586487\n",
            "step 666: generator_loss=5.971719741821289, discriminator_loss=0.0038585818838328123\n",
            "step 667: generator_loss=5.974948883056641, discriminator_loss=0.0038407742977142334\n",
            "step 668: generator_loss=5.978570461273193, discriminator_loss=0.003824922489002347\n",
            "step 669: generator_loss=5.98286247253418, discriminator_loss=0.0038052783347666264\n",
            "step 670: generator_loss=5.9868927001953125, discriminator_loss=0.003802590537816286\n",
            "step 671: generator_loss=5.991363048553467, discriminator_loss=0.0037773612421005964\n",
            "step 672: generator_loss=5.994428634643555, discriminator_loss=0.003765876404941082\n",
            "step 673: generator_loss=5.99749231338501, discriminator_loss=0.0037532125134021044\n",
            "step 674: generator_loss=6.001962661743164, discriminator_loss=0.003732480574399233\n",
            "step 675: generator_loss=6.0057759284973145, discriminator_loss=0.003721242304891348\n",
            "step 676: generator_loss=6.009940147399902, discriminator_loss=0.0037123169749975204\n",
            "step 677: generator_loss=6.013792991638184, discriminator_loss=0.0036911300849169493\n",
            "step 678: generator_loss=6.016369819641113, discriminator_loss=0.0036805965937674046\n",
            "step 679: generator_loss=6.0218729972839355, discriminator_loss=0.003666793694719672\n",
            "step 680: generator_loss=6.025755882263184, discriminator_loss=0.003649607067927718\n",
            "step 681: generator_loss=6.029341697692871, discriminator_loss=0.003635208122432232\n",
            "step 682: generator_loss=6.032196044921875, discriminator_loss=0.0036189211532473564\n",
            "step 683: generator_loss=6.037444114685059, discriminator_loss=0.0036083264276385307\n",
            "step 684: generator_loss=6.039343357086182, discriminator_loss=0.0035968488082289696\n",
            "step 685: generator_loss=6.042021751403809, discriminator_loss=0.003585265949368477\n",
            "step 686: generator_loss=6.047055244445801, discriminator_loss=0.0035647775512188673\n",
            "step 687: generator_loss=6.0512776374816895, discriminator_loss=0.0035538726951926947\n",
            "step 688: generator_loss=6.054471015930176, discriminator_loss=0.0035429983399808407\n",
            "step 689: generator_loss=6.058625221252441, discriminator_loss=0.003520484548062086\n",
            "step 690: generator_loss=6.062795639038086, discriminator_loss=0.003513024188578129\n",
            "step 691: generator_loss=6.0644001960754395, discriminator_loss=0.0034945611841976643\n",
            "step 692: generator_loss=6.070446014404297, discriminator_loss=0.003486518980935216\n",
            "step 693: generator_loss=6.071800231933594, discriminator_loss=0.0034759605769068003\n",
            "step 694: generator_loss=6.076601982116699, discriminator_loss=0.003457572078332305\n",
            "step 695: generator_loss=6.080727577209473, discriminator_loss=0.0034467773512005806\n",
            "step 696: generator_loss=6.084529399871826, discriminator_loss=0.00343166571110487\n",
            "step 697: generator_loss=6.089589595794678, discriminator_loss=0.0034199419897049665\n",
            "step 698: generator_loss=6.089683532714844, discriminator_loss=0.003407931188121438\n",
            "step 699: generator_loss=6.093076705932617, discriminator_loss=0.0033976491540670395\n",
            "step 700: generator_loss=6.098235130310059, discriminator_loss=0.0033857934176921844\n",
            "step 701: generator_loss=6.102284908294678, discriminator_loss=0.0033690151758491993\n",
            "step 702: generator_loss=6.104822635650635, discriminator_loss=0.003353624837473035\n",
            "step 703: generator_loss=6.110910415649414, discriminator_loss=0.0033438452519476414\n",
            "step 704: generator_loss=6.110134124755859, discriminator_loss=0.0033373581245541573\n",
            "step 705: generator_loss=6.1182708740234375, discriminator_loss=0.0033158462028950453\n",
            "step 706: generator_loss=6.120603561401367, discriminator_loss=0.00331138726323843\n",
            "step 707: generator_loss=6.124144077301025, discriminator_loss=0.0033008945174515247\n",
            "step 708: generator_loss=6.1295576095581055, discriminator_loss=0.003279642201960087\n",
            "step 709: generator_loss=6.1309003829956055, discriminator_loss=0.003270187182351947\n",
            "step 710: generator_loss=6.133611679077148, discriminator_loss=0.0032630651257932186\n",
            "step 711: generator_loss=6.138807773590088, discriminator_loss=0.003249102970585227\n",
            "step 712: generator_loss=6.142322063446045, discriminator_loss=0.0032307065557688475\n",
            "step 713: generator_loss=6.143586158752441, discriminator_loss=0.0032274210825562477\n",
            "step 714: generator_loss=6.14816951751709, discriminator_loss=0.003213471733033657\n",
            "step 715: generator_loss=6.150158405303955, discriminator_loss=0.003206190885975957\n",
            "step 716: generator_loss=6.154836654663086, discriminator_loss=0.00319265341386199\n",
            "step 717: generator_loss=6.158859729766846, discriminator_loss=0.003182502929121256\n",
            "step 718: generator_loss=6.159128189086914, discriminator_loss=0.0031680120155215263\n",
            "step 719: generator_loss=6.166236400604248, discriminator_loss=0.0031537748873233795\n",
            "step 720: generator_loss=6.166922569274902, discriminator_loss=0.0031467583030462265\n",
            "step 721: generator_loss=6.172341346740723, discriminator_loss=0.003131712321192026\n",
            "step 722: generator_loss=6.174089431762695, discriminator_loss=0.003128392156213522\n",
            "step 723: generator_loss=6.174818992614746, discriminator_loss=0.0031168535351753235\n",
            "step 724: generator_loss=6.180136203765869, discriminator_loss=0.0030974193941801786\n",
            "step 725: generator_loss=6.1847076416015625, discriminator_loss=0.0030887769535183907\n",
            "step 726: generator_loss=6.1838178634643555, discriminator_loss=0.003083182265982032\n",
            "step 727: generator_loss=6.188047409057617, discriminator_loss=0.003076656023040414\n",
            "step 728: generator_loss=6.1932268142700195, discriminator_loss=0.0030629585962742567\n",
            "step 729: generator_loss=6.192860126495361, discriminator_loss=0.0030493123922497034\n",
            "step 730: generator_loss=6.199095726013184, discriminator_loss=0.003040159586817026\n",
            "step 731: generator_loss=6.198495864868164, discriminator_loss=0.00303269037976861\n",
            "step 732: generator_loss=6.2066330909729, discriminator_loss=0.00301482155919075\n",
            "step 733: generator_loss=6.210007667541504, discriminator_loss=0.003004048950970173\n",
            "step 734: generator_loss=6.211941719055176, discriminator_loss=0.0030022370629012585\n",
            "step 735: generator_loss=6.212580680847168, discriminator_loss=0.0029891766607761383\n",
            "step 736: generator_loss=6.212760925292969, discriminator_loss=0.002983612008392811\n",
            "step 737: generator_loss=6.22263240814209, discriminator_loss=0.0029596728272736073\n",
            "step 738: generator_loss=6.222836971282959, discriminator_loss=0.0029619878623634577\n",
            "step 739: generator_loss=6.219089508056641, discriminator_loss=0.002964899642392993\n",
            "step 740: generator_loss=6.224233627319336, discriminator_loss=0.002948715351521969\n",
            "step 741: generator_loss=6.228906154632568, discriminator_loss=0.0029316237196326256\n",
            "step 742: generator_loss=6.233695983886719, discriminator_loss=0.0029239605646580458\n",
            "step 743: generator_loss=6.234484672546387, discriminator_loss=0.0029171721544116735\n",
            "step 744: generator_loss=6.239480018615723, discriminator_loss=0.0029023666866123676\n",
            "step 745: generator_loss=6.236547470092773, discriminator_loss=0.0029029971919953823\n",
            "step 746: generator_loss=6.2316389083862305, discriminator_loss=0.0029013967141509056\n",
            "step 747: generator_loss=6.246212005615234, discriminator_loss=0.002873550169169903\n",
            "step 748: generator_loss=6.236817836761475, discriminator_loss=0.0028878990560770035\n",
            "step 749: generator_loss=6.233344554901123, discriminator_loss=0.0028785299509763718\n",
            "step 750: generator_loss=6.240893363952637, discriminator_loss=0.002867633244022727\n",
            "step 751: generator_loss=6.254992485046387, discriminator_loss=0.0028436812572181225\n",
            "step 752: generator_loss=6.253155708312988, discriminator_loss=0.0028367675840854645\n",
            "step 753: generator_loss=6.2330217361450195, discriminator_loss=0.002858460182324052\n",
            "step 754: generator_loss=6.2352118492126465, discriminator_loss=0.002854651538655162\n",
            "step 755: generator_loss=6.223968982696533, discriminator_loss=0.0028760088607668877\n",
            "step 756: generator_loss=6.2565202713012695, discriminator_loss=0.002814169507473707\n",
            "step 757: generator_loss=6.239453315734863, discriminator_loss=0.0028256785590201616\n",
            "step 758: generator_loss=6.260851860046387, discriminator_loss=0.0028011316899210215\n",
            "step 759: generator_loss=6.226274490356445, discriminator_loss=0.002842770889401436\n",
            "step 760: generator_loss=6.217679500579834, discriminator_loss=0.002837774809449911\n",
            "step 761: generator_loss=6.242565155029297, discriminator_loss=0.002809758298099041\n",
            "step 762: generator_loss=6.219525337219238, discriminator_loss=0.002822259208187461\n",
            "step 763: generator_loss=6.18431282043457, discriminator_loss=0.00288414116948843\n",
            "step 764: generator_loss=6.2376861572265625, discriminator_loss=0.002816378138959408\n",
            "step 765: generator_loss=6.2204508781433105, discriminator_loss=0.002831581514328718\n",
            "step 766: generator_loss=6.216531753540039, discriminator_loss=0.0028152111917734146\n",
            "step 767: generator_loss=6.207423210144043, discriminator_loss=0.0028306746389716864\n",
            "step 768: generator_loss=6.17092227935791, discriminator_loss=0.002894789446145296\n",
            "step 769: generator_loss=6.156250953674316, discriminator_loss=0.0029742904007434845\n",
            "step 770: generator_loss=6.111062049865723, discriminator_loss=0.0030219871550798416\n",
            "step 771: generator_loss=6.039405345916748, discriminator_loss=0.0032819565385580063\n",
            "step 772: generator_loss=6.111262321472168, discriminator_loss=0.003002314828336239\n",
            "step 773: generator_loss=6.089644908905029, discriminator_loss=0.003034378867596388\n",
            "step 774: generator_loss=6.063751220703125, discriminator_loss=0.0030784374102950096\n",
            "step 775: generator_loss=5.964512825012207, discriminator_loss=0.00327664939686656\n",
            "step 776: generator_loss=5.969995498657227, discriminator_loss=0.0033952612429857254\n",
            "step 777: generator_loss=5.881647109985352, discriminator_loss=0.0034665679559111595\n",
            "step 778: generator_loss=5.9301862716674805, discriminator_loss=0.0038119815289974213\n",
            "step 779: generator_loss=5.863795757293701, discriminator_loss=0.0035438132472336292\n",
            "step 780: generator_loss=5.820119857788086, discriminator_loss=0.003882450982928276\n",
            "step 781: generator_loss=5.792183876037598, discriminator_loss=0.004568167496472597\n",
            "step 782: generator_loss=5.821225166320801, discriminator_loss=0.003780756378546357\n",
            "step 783: generator_loss=5.747365474700928, discriminator_loss=0.0037910861428827047\n",
            "step 784: generator_loss=5.516853332519531, discriminator_loss=0.005174499470740557\n",
            "step 785: generator_loss=5.485065460205078, discriminator_loss=0.005308615975081921\n",
            "step 786: generator_loss=5.454386234283447, discriminator_loss=0.005421314388513565\n",
            "step 787: generator_loss=5.320705413818359, discriminator_loss=0.01544562540948391\n",
            "step 788: generator_loss=5.275214195251465, discriminator_loss=0.009560341946780682\n",
            "step 789: generator_loss=5.144878387451172, discriminator_loss=0.008821001276373863\n",
            "step 790: generator_loss=5.101410865783691, discriminator_loss=0.01375883724540472\n",
            "step 791: generator_loss=5.132770538330078, discriminator_loss=0.010885973460972309\n",
            "step 792: generator_loss=4.894705295562744, discriminator_loss=0.01955011859536171\n",
            "step 793: generator_loss=4.766812801361084, discriminator_loss=0.034916024655103683\n",
            "step 794: generator_loss=5.002135276794434, discriminator_loss=0.013900031335651875\n",
            "step 795: generator_loss=4.624066352844238, discriminator_loss=0.051353905349969864\n",
            "step 796: generator_loss=4.421000003814697, discriminator_loss=0.06375513970851898\n",
            "step 797: generator_loss=4.703116416931152, discriminator_loss=0.07466769218444824\n",
            "step 798: generator_loss=4.646457672119141, discriminator_loss=0.06409931927919388\n",
            "step 799: generator_loss=4.298211097717285, discriminator_loss=0.1302197426557541\n",
            "step 800: generator_loss=4.22432804107666, discriminator_loss=0.12660908699035645\n",
            "step 801: generator_loss=4.257719039916992, discriminator_loss=0.13599920272827148\n",
            "step 802: generator_loss=4.197893142700195, discriminator_loss=0.16867651045322418\n",
            "step 803: generator_loss=4.269608020782471, discriminator_loss=0.13120822608470917\n",
            "step 804: generator_loss=4.783751964569092, discriminator_loss=0.10362175107002258\n",
            "step 805: generator_loss=4.552743911743164, discriminator_loss=0.1327970176935196\n",
            "step 806: generator_loss=4.590968132019043, discriminator_loss=0.1226808950304985\n",
            "step 807: generator_loss=4.347572326660156, discriminator_loss=0.17799507081508636\n",
            "step 808: generator_loss=4.466475963592529, discriminator_loss=0.15977445244789124\n",
            "step 809: generator_loss=4.9379119873046875, discriminator_loss=0.12371588498353958\n",
            "step 810: generator_loss=4.510695934295654, discriminator_loss=0.1559707224369049\n",
            "step 811: generator_loss=4.587772369384766, discriminator_loss=0.15374615788459778\n",
            "step 812: generator_loss=4.7909650802612305, discriminator_loss=0.1568853259086609\n",
            "step 813: generator_loss=4.3951873779296875, discriminator_loss=0.1630585491657257\n",
            "step 814: generator_loss=4.992472171783447, discriminator_loss=0.15063074231147766\n",
            "step 815: generator_loss=4.6915998458862305, discriminator_loss=0.17741121351718903\n",
            "step 816: generator_loss=4.430303573608398, discriminator_loss=0.1778518259525299\n",
            "step 817: generator_loss=4.749251842498779, discriminator_loss=0.17090030014514923\n",
            "step 818: generator_loss=5.2509684562683105, discriminator_loss=0.16527320444583893\n",
            "step 819: generator_loss=4.767631530761719, discriminator_loss=0.19586299359798431\n",
            "step 820: generator_loss=5.031958103179932, discriminator_loss=0.18623468279838562\n",
            "step 821: generator_loss=4.6737565994262695, discriminator_loss=0.20089615881443024\n",
            "step 822: generator_loss=4.345806121826172, discriminator_loss=0.23611024022102356\n",
            "step 823: generator_loss=3.8805432319641113, discriminator_loss=0.2576773762702942\n",
            "step 824: generator_loss=4.47257137298584, discriminator_loss=0.2418137490749359\n",
            "step 825: generator_loss=3.292738914489746, discriminator_loss=0.2905372977256775\n",
            "step 826: generator_loss=3.6216206550598145, discriminator_loss=0.2821926474571228\n",
            "step 827: generator_loss=3.6229300498962402, discriminator_loss=0.301763117313385\n",
            "step 828: generator_loss=3.578885555267334, discriminator_loss=0.31187963485717773\n",
            "step 829: generator_loss=3.034000873565674, discriminator_loss=0.31728595495224\n",
            "step 830: generator_loss=3.4625020027160645, discriminator_loss=0.3185175955295563\n",
            "step 831: generator_loss=3.098923921585083, discriminator_loss=0.344280481338501\n",
            "step 832: generator_loss=2.6136436462402344, discriminator_loss=0.38014650344848633\n",
            "step 833: generator_loss=2.317361831665039, discriminator_loss=0.394131064414978\n",
            "step 834: generator_loss=2.0165843963623047, discriminator_loss=0.397097110748291\n",
            "step 835: generator_loss=1.7320165634155273, discriminator_loss=0.4250999391078949\n",
            "step 836: generator_loss=1.476545810699463, discriminator_loss=0.4025966227054596\n",
            "step 837: generator_loss=1.2207144498825073, discriminator_loss=0.4304215908050537\n",
            "step 838: generator_loss=1.0341908931732178, discriminator_loss=0.472944438457489\n",
            "step 839: generator_loss=1.170017957687378, discriminator_loss=0.4554107189178467\n",
            "step 840: generator_loss=1.169472098350525, discriminator_loss=0.46407073736190796\n",
            "step 841: generator_loss=1.17014741897583, discriminator_loss=0.47850677371025085\n",
            "step 842: generator_loss=1.1280782222747803, discriminator_loss=0.5017768740653992\n",
            "step 843: generator_loss=1.2959450483322144, discriminator_loss=0.478562593460083\n",
            "step 844: generator_loss=1.238669514656067, discriminator_loss=0.4893251955509186\n",
            "step 845: generator_loss=1.367586374282837, discriminator_loss=0.4721100628376007\n",
            "step 846: generator_loss=1.3031666278839111, discriminator_loss=0.48730525374412537\n",
            "step 847: generator_loss=1.2950592041015625, discriminator_loss=0.4959079921245575\n",
            "step 848: generator_loss=1.1406968832015991, discriminator_loss=0.5175031423568726\n",
            "step 849: generator_loss=1.1622893810272217, discriminator_loss=0.51781165599823\n",
            "step 850: generator_loss=1.1684868335723877, discriminator_loss=0.5185014009475708\n",
            "step 851: generator_loss=1.1902860403060913, discriminator_loss=0.5193113088607788\n",
            "step 852: generator_loss=1.1679868698120117, discriminator_loss=0.5078649520874023\n",
            "step 853: generator_loss=1.0755106210708618, discriminator_loss=0.5380488634109497\n",
            "step 854: generator_loss=1.0011649131774902, discriminator_loss=0.5535342693328857\n",
            "step 855: generator_loss=1.0214207172393799, discriminator_loss=0.5433200597763062\n",
            "step 856: generator_loss=1.0137115716934204, discriminator_loss=0.5534875392913818\n",
            "step 857: generator_loss=0.8896654844284058, discriminator_loss=0.5822392702102661\n",
            "step 858: generator_loss=0.960216760635376, discriminator_loss=0.570927619934082\n",
            "step 859: generator_loss=0.9297680854797363, discriminator_loss=0.5958777666091919\n",
            "step 860: generator_loss=0.9468598961830139, discriminator_loss=0.5944135189056396\n",
            "step 861: generator_loss=0.8540666103363037, discriminator_loss=0.6279479265213013\n",
            "step 862: generator_loss=1.0164060592651367, discriminator_loss=0.6115021705627441\n",
            "step 863: generator_loss=0.8568476438522339, discriminator_loss=0.6409623622894287\n",
            "step 864: generator_loss=0.9052647352218628, discriminator_loss=0.6514538526535034\n",
            "step 865: generator_loss=0.9306496977806091, discriminator_loss=0.6512361764907837\n",
            "step 866: generator_loss=0.8608812093734741, discriminator_loss=0.6681113243103027\n",
            "step 867: generator_loss=0.8100566864013672, discriminator_loss=0.6794637441635132\n",
            "step 868: generator_loss=0.6984373331069946, discriminator_loss=0.7193697690963745\n",
            "step 869: generator_loss=0.7351750135421753, discriminator_loss=0.7085959911346436\n",
            "step 870: generator_loss=0.6746167540550232, discriminator_loss=0.7340636253356934\n",
            "step 871: generator_loss=0.6251537799835205, discriminator_loss=0.7680543065071106\n",
            "step 872: generator_loss=0.6632962226867676, discriminator_loss=0.7536340951919556\n",
            "step 873: generator_loss=0.6491931676864624, discriminator_loss=0.7720370888710022\n",
            "step 874: generator_loss=0.6605188250541687, discriminator_loss=0.7729861736297607\n",
            "step 875: generator_loss=0.6318562626838684, discriminator_loss=0.797734260559082\n",
            "step 876: generator_loss=0.6350411176681519, discriminator_loss=0.8070172071456909\n",
            "step 877: generator_loss=0.7391690611839294, discriminator_loss=0.7786417007446289\n",
            "step 878: generator_loss=0.6527411937713623, discriminator_loss=0.8080716133117676\n",
            "step 879: generator_loss=0.710817277431488, discriminator_loss=0.8013263940811157\n",
            "step 880: generator_loss=0.7192051410675049, discriminator_loss=0.7988241314888\n",
            "step 881: generator_loss=0.7140345573425293, discriminator_loss=0.8073329925537109\n",
            "step 882: generator_loss=0.7105239629745483, discriminator_loss=0.7999128103256226\n",
            "step 883: generator_loss=0.6610565185546875, discriminator_loss=0.8331696391105652\n",
            "step 884: generator_loss=0.6608580350875854, discriminator_loss=0.818011999130249\n",
            "step 885: generator_loss=0.6371227502822876, discriminator_loss=0.8198047876358032\n",
            "step 886: generator_loss=0.609552264213562, discriminator_loss=0.8444773554801941\n",
            "step 887: generator_loss=0.607236385345459, discriminator_loss=0.8347698450088501\n",
            "step 888: generator_loss=0.5866285562515259, discriminator_loss=0.8332410454750061\n",
            "step 889: generator_loss=0.5684991478919983, discriminator_loss=0.8535535335540771\n",
            "step 890: generator_loss=0.5695405006408691, discriminator_loss=0.8447365760803223\n",
            "step 891: generator_loss=0.5934094190597534, discriminator_loss=0.839171290397644\n",
            "step 892: generator_loss=0.6151612401008606, discriminator_loss=0.8299810886383057\n",
            "step 893: generator_loss=0.5715548992156982, discriminator_loss=0.8537495136260986\n",
            "step 894: generator_loss=0.6453485488891602, discriminator_loss=0.8152967095375061\n",
            "step 895: generator_loss=0.6313254237174988, discriminator_loss=0.8224797248840332\n",
            "step 896: generator_loss=0.6346292495727539, discriminator_loss=0.8126512765884399\n",
            "step 897: generator_loss=0.6024910807609558, discriminator_loss=0.8305630087852478\n",
            "step 898: generator_loss=0.6083887815475464, discriminator_loss=0.8325717449188232\n",
            "step 899: generator_loss=0.6492284536361694, discriminator_loss=0.8048276901245117\n",
            "step 900: generator_loss=0.6392666101455688, discriminator_loss=0.809208869934082\n",
            "step 901: generator_loss=0.5990585088729858, discriminator_loss=0.8299857378005981\n",
            "step 902: generator_loss=0.6124787330627441, discriminator_loss=0.8232451677322388\n",
            "step 903: generator_loss=0.5958216190338135, discriminator_loss=0.8289775848388672\n",
            "step 904: generator_loss=0.6355406045913696, discriminator_loss=0.7954655885696411\n",
            "step 905: generator_loss=0.5812792778015137, discriminator_loss=0.8380588889122009\n",
            "step 906: generator_loss=0.5772609710693359, discriminator_loss=0.8413020968437195\n",
            "step 907: generator_loss=0.6195327043533325, discriminator_loss=0.8315471410751343\n",
            "step 908: generator_loss=0.5920996069908142, discriminator_loss=0.8466683626174927\n",
            "step 909: generator_loss=0.6210544109344482, discriminator_loss=0.8444048166275024\n",
            "step 910: generator_loss=0.6283714771270752, discriminator_loss=0.8235048055648804\n",
            "step 911: generator_loss=0.6056122779846191, discriminator_loss=0.8533217906951904\n",
            "step 912: generator_loss=0.6760969161987305, discriminator_loss=0.8397994041442871\n",
            "step 913: generator_loss=0.6372933387756348, discriminator_loss=0.8559516668319702\n",
            "step 914: generator_loss=0.6726082563400269, discriminator_loss=0.8433912396430969\n",
            "step 915: generator_loss=0.6289867162704468, discriminator_loss=0.8637713193893433\n",
            "step 916: generator_loss=0.5774407386779785, discriminator_loss=0.8821617960929871\n",
            "step 917: generator_loss=0.6242947578430176, discriminator_loss=0.8615433573722839\n",
            "step 918: generator_loss=0.5811494588851929, discriminator_loss=0.879694938659668\n",
            "step 919: generator_loss=0.5771582126617432, discriminator_loss=0.8715596199035645\n",
            "step 920: generator_loss=0.5265907049179077, discriminator_loss=0.9037774205207825\n",
            "step 921: generator_loss=0.5370393991470337, discriminator_loss=0.8940318822860718\n",
            "step 922: generator_loss=0.538016676902771, discriminator_loss=0.9047878384590149\n",
            "step 923: generator_loss=0.5300594568252563, discriminator_loss=0.9046148061752319\n",
            "step 924: generator_loss=0.5807971358299255, discriminator_loss=0.8754106760025024\n",
            "step 925: generator_loss=0.5903358459472656, discriminator_loss=0.8763278722763062\n",
            "step 926: generator_loss=0.5785391330718994, discriminator_loss=0.8871107697486877\n",
            "step 927: generator_loss=0.6422935724258423, discriminator_loss=0.8518270254135132\n",
            "step 928: generator_loss=0.5839492082595825, discriminator_loss=0.8716434240341187\n",
            "step 929: generator_loss=0.569845974445343, discriminator_loss=0.8741245269775391\n",
            "step 930: generator_loss=0.6483669281005859, discriminator_loss=0.8259996175765991\n",
            "step 931: generator_loss=0.6026753187179565, discriminator_loss=0.8357822895050049\n",
            "step 932: generator_loss=0.5816906690597534, discriminator_loss=0.8470314741134644\n",
            "step 933: generator_loss=0.6051076650619507, discriminator_loss=0.819382905960083\n",
            "step 934: generator_loss=0.5856490135192871, discriminator_loss=0.82123863697052\n",
            "step 935: generator_loss=0.6109830141067505, discriminator_loss=0.8079776763916016\n",
            "step 936: generator_loss=0.6296647787094116, discriminator_loss=0.7930238842964172\n",
            "step 937: generator_loss=0.6502501964569092, discriminator_loss=0.7705711126327515\n",
            "step 938: generator_loss=0.6813849210739136, discriminator_loss=0.7493129968643188\n",
            "step 939: generator_loss=0.7160964608192444, discriminator_loss=0.7288463711738586\n",
            "step 940: generator_loss=0.7245308756828308, discriminator_loss=0.7193107604980469\n",
            "step 941: generator_loss=0.6976682543754578, discriminator_loss=0.7213994264602661\n",
            "step 942: generator_loss=0.801713764667511, discriminator_loss=0.6783992052078247\n",
            "step 943: generator_loss=0.8516314625740051, discriminator_loss=0.6476457715034485\n",
            "step 944: generator_loss=0.8190745115280151, discriminator_loss=0.6491706371307373\n",
            "step 945: generator_loss=0.7810527086257935, discriminator_loss=0.6554015874862671\n",
            "step 946: generator_loss=0.8506115078926086, discriminator_loss=0.6181108951568604\n",
            "step 947: generator_loss=0.8517144918441772, discriminator_loss=0.6084902286529541\n",
            "step 948: generator_loss=0.8513695001602173, discriminator_loss=0.6008359789848328\n",
            "step 949: generator_loss=0.8465341329574585, discriminator_loss=0.5874842405319214\n",
            "step 950: generator_loss=0.9098860025405884, discriminator_loss=0.5619938373565674\n",
            "step 951: generator_loss=0.8929531574249268, discriminator_loss=0.5539329051971436\n",
            "step 952: generator_loss=0.8721339106559753, discriminator_loss=0.5519480109214783\n",
            "step 953: generator_loss=0.9637696743011475, discriminator_loss=0.5225464105606079\n",
            "step 954: generator_loss=0.8741717338562012, discriminator_loss=0.5431492328643799\n",
            "step 955: generator_loss=0.9613621830940247, discriminator_loss=0.5116965770721436\n",
            "step 956: generator_loss=1.00150728225708, discriminator_loss=0.4978867769241333\n",
            "step 957: generator_loss=1.004035234451294, discriminator_loss=0.49238401651382446\n",
            "step 958: generator_loss=1.015409231185913, discriminator_loss=0.48753419518470764\n",
            "step 959: generator_loss=0.9856915473937988, discriminator_loss=0.49807387590408325\n",
            "step 960: generator_loss=1.0767794847488403, discriminator_loss=0.4690018892288208\n",
            "step 961: generator_loss=1.0929923057556152, discriminator_loss=0.45954251289367676\n",
            "step 962: generator_loss=1.1224321126937866, discriminator_loss=0.45041435956954956\n",
            "step 963: generator_loss=1.1917835474014282, discriminator_loss=0.4319041967391968\n",
            "step 964: generator_loss=1.2412354946136475, discriminator_loss=0.41774308681488037\n",
            "step 965: generator_loss=1.2442165613174438, discriminator_loss=0.41159385442733765\n",
            "step 966: generator_loss=1.1610239744186401, discriminator_loss=0.4249197244644165\n",
            "step 967: generator_loss=1.2208106517791748, discriminator_loss=0.40367913246154785\n",
            "step 968: generator_loss=1.2118549346923828, discriminator_loss=0.40652185678482056\n",
            "step 969: generator_loss=1.2364513874053955, discriminator_loss=0.3935360312461853\n",
            "step 970: generator_loss=1.2432674169540405, discriminator_loss=0.39147764444351196\n",
            "step 971: generator_loss=1.2577643394470215, discriminator_loss=0.3831890821456909\n",
            "step 972: generator_loss=1.1958422660827637, discriminator_loss=0.3879045844078064\n",
            "step 973: generator_loss=1.2585179805755615, discriminator_loss=0.37037134170532227\n",
            "step 974: generator_loss=1.2691916227340698, discriminator_loss=0.3611813485622406\n",
            "step 975: generator_loss=1.3713181018829346, discriminator_loss=0.3523719310760498\n",
            "step 976: generator_loss=1.3174268007278442, discriminator_loss=0.35181576013565063\n",
            "step 977: generator_loss=1.3964815139770508, discriminator_loss=0.33491283655166626\n",
            "step 978: generator_loss=1.4187674522399902, discriminator_loss=0.32642000913619995\n",
            "step 979: generator_loss=1.4619097709655762, discriminator_loss=0.31610214710235596\n",
            "step 980: generator_loss=1.3667912483215332, discriminator_loss=0.3268607258796692\n",
            "step 981: generator_loss=1.3618005514144897, discriminator_loss=0.32409727573394775\n",
            "step 982: generator_loss=1.3686912059783936, discriminator_loss=0.319887638092041\n",
            "step 983: generator_loss=1.374699592590332, discriminator_loss=0.3140103220939636\n",
            "step 984: generator_loss=1.4285731315612793, discriminator_loss=0.3093338906764984\n",
            "step 985: generator_loss=1.3989686965942383, discriminator_loss=0.3075574040412903\n",
            "step 986: generator_loss=1.385985016822815, discriminator_loss=0.30468928813934326\n",
            "step 987: generator_loss=1.4339497089385986, discriminator_loss=0.2979110777378082\n",
            "step 988: generator_loss=1.3927351236343384, discriminator_loss=0.30331486463546753\n",
            "step 989: generator_loss=1.4268475770950317, discriminator_loss=0.29411107301712036\n",
            "step 990: generator_loss=1.4672751426696777, discriminator_loss=0.2876169979572296\n",
            "step 991: generator_loss=1.5352933406829834, discriminator_loss=0.2756905257701874\n",
            "step 992: generator_loss=1.4990684986114502, discriminator_loss=0.2812420725822449\n",
            "step 993: generator_loss=1.4570516347885132, discriminator_loss=0.2819993495941162\n",
            "step 994: generator_loss=1.4763824939727783, discriminator_loss=0.2789652347564697\n",
            "step 995: generator_loss=1.5136570930480957, discriminator_loss=0.2727810740470886\n",
            "step 996: generator_loss=1.4030559062957764, discriminator_loss=0.28797224164009094\n",
            "step 997: generator_loss=1.438910722732544, discriminator_loss=0.2786976397037506\n",
            "step 998: generator_loss=1.4418153762817383, discriminator_loss=0.2813474237918854\n",
            "step 999: generator_loss=1.4173449277877808, discriminator_loss=0.2832337021827698\n",
            "step 1000: generator_loss=1.4422075748443604, discriminator_loss=0.2807304263114929\n",
            "step 1001: generator_loss=1.4109740257263184, discriminator_loss=0.28412172198295593\n",
            "step 1002: generator_loss=1.4377524852752686, discriminator_loss=0.2786417007446289\n",
            "step 1003: generator_loss=1.424604892730713, discriminator_loss=0.28121352195739746\n",
            "step 1004: generator_loss=1.4135318994522095, discriminator_loss=0.2818201780319214\n",
            "step 1005: generator_loss=1.4623295068740845, discriminator_loss=0.27492350339889526\n",
            "step 1006: generator_loss=1.4487228393554688, discriminator_loss=0.2754834294319153\n",
            "step 1007: generator_loss=1.4461493492126465, discriminator_loss=0.27614909410476685\n",
            "step 1008: generator_loss=1.4323831796646118, discriminator_loss=0.27932682633399963\n",
            "step 1009: generator_loss=1.4328980445861816, discriminator_loss=0.27774637937545776\n",
            "step 1010: generator_loss=1.4221341609954834, discriminator_loss=0.2821091413497925\n",
            "step 1011: generator_loss=1.4661847352981567, discriminator_loss=0.27428925037384033\n",
            "step 1012: generator_loss=1.4793908596038818, discriminator_loss=0.2715548574924469\n",
            "step 1013: generator_loss=1.4368785619735718, discriminator_loss=0.2774711847305298\n",
            "step 1014: generator_loss=1.5117456912994385, discriminator_loss=0.2675938010215759\n",
            "step 1015: generator_loss=1.5231845378875732, discriminator_loss=0.2633403539657593\n",
            "step 1016: generator_loss=1.5232853889465332, discriminator_loss=0.2621312737464905\n",
            "step 1017: generator_loss=1.516196608543396, discriminator_loss=0.25953227281570435\n",
            "step 1018: generator_loss=1.5559738874435425, discriminator_loss=0.25377312302589417\n",
            "step 1019: generator_loss=1.591045618057251, discriminator_loss=0.24636855721473694\n",
            "step 1020: generator_loss=1.5492753982543945, discriminator_loss=0.24825844168663025\n",
            "step 1021: generator_loss=1.5587513446807861, discriminator_loss=0.2469477653503418\n",
            "step 1022: generator_loss=1.5784580707550049, discriminator_loss=0.2409510761499405\n",
            "step 1023: generator_loss=1.5983514785766602, discriminator_loss=0.23636963963508606\n",
            "step 1024: generator_loss=1.585693359375, discriminator_loss=0.2346230149269104\n",
            "step 1025: generator_loss=1.6409413814544678, discriminator_loss=0.22666937112808228\n",
            "step 1026: generator_loss=1.6849644184112549, discriminator_loss=0.21942752599716187\n",
            "step 1027: generator_loss=1.685257077217102, discriminator_loss=0.2178908884525299\n",
            "step 1028: generator_loss=1.7068595886230469, discriminator_loss=0.213308185338974\n",
            "step 1029: generator_loss=1.6461130380630493, discriminator_loss=0.21981337666511536\n",
            "step 1030: generator_loss=1.7165167331695557, discriminator_loss=0.20922069251537323\n",
            "step 1031: generator_loss=1.7024729251861572, discriminator_loss=0.20934990048408508\n",
            "step 1032: generator_loss=1.7676523923873901, discriminator_loss=0.19857044517993927\n",
            "step 1033: generator_loss=1.686816692352295, discriminator_loss=0.20979458093643188\n",
            "step 1034: generator_loss=1.7250550985336304, discriminator_loss=0.2027919739484787\n",
            "step 1035: generator_loss=1.738908290863037, discriminator_loss=0.19978512823581696\n",
            "step 1036: generator_loss=1.8164513111114502, discriminator_loss=0.18931831419467926\n",
            "step 1037: generator_loss=1.7746241092681885, discriminator_loss=0.19528929889202118\n",
            "step 1038: generator_loss=1.7350589036941528, discriminator_loss=0.20268887281417847\n",
            "step 1039: generator_loss=1.7581642866134644, discriminator_loss=0.20175060629844666\n",
            "step 1040: generator_loss=1.7576603889465332, discriminator_loss=0.1961328387260437\n",
            "step 1041: generator_loss=1.7927308082580566, discriminator_loss=0.1937110424041748\n",
            "step 1042: generator_loss=1.8306385278701782, discriminator_loss=0.19080635905265808\n",
            "step 1043: generator_loss=1.7764639854431152, discriminator_loss=0.1990875005722046\n",
            "step 1044: generator_loss=1.7852556705474854, discriminator_loss=0.1985284835100174\n",
            "step 1045: generator_loss=1.7797520160675049, discriminator_loss=0.1957494020462036\n",
            "step 1046: generator_loss=1.7786431312561035, discriminator_loss=0.19822052121162415\n",
            "step 1047: generator_loss=1.7552956342697144, discriminator_loss=0.2012653946876526\n",
            "step 1048: generator_loss=1.8010262250900269, discriminator_loss=0.2020813226699829\n",
            "step 1049: generator_loss=1.7977410554885864, discriminator_loss=0.19730767607688904\n",
            "step 1050: generator_loss=1.7174768447875977, discriminator_loss=0.21822744607925415\n",
            "step 1051: generator_loss=1.7148898839950562, discriminator_loss=0.22271478176116943\n",
            "step 1052: generator_loss=1.736808180809021, discriminator_loss=0.21616920828819275\n",
            "step 1053: generator_loss=1.7509644031524658, discriminator_loss=0.21300773322582245\n",
            "step 1054: generator_loss=1.7852979898452759, discriminator_loss=0.22492565214633942\n",
            "step 1055: generator_loss=1.790168285369873, discriminator_loss=0.21659880876541138\n",
            "step 1056: generator_loss=1.7323329448699951, discriminator_loss=0.22966699302196503\n",
            "step 1057: generator_loss=1.6580876111984253, discriminator_loss=0.23921026289463043\n",
            "step 1058: generator_loss=1.7613012790679932, discriminator_loss=0.23041605949401855\n",
            "step 1059: generator_loss=1.7745225429534912, discriminator_loss=0.23016023635864258\n",
            "step 1060: generator_loss=1.8718931674957275, discriminator_loss=0.22192305326461792\n",
            "step 1061: generator_loss=1.6593866348266602, discriminator_loss=0.24846526980400085\n",
            "step 1062: generator_loss=1.6635911464691162, discriminator_loss=0.2548661231994629\n",
            "step 1063: generator_loss=1.5701806545257568, discriminator_loss=0.2738399803638458\n",
            "step 1064: generator_loss=1.6894878149032593, discriminator_loss=0.2518622875213623\n",
            "step 1065: generator_loss=1.6311697959899902, discriminator_loss=0.263533353805542\n",
            "step 1066: generator_loss=1.5694470405578613, discriminator_loss=0.26800477504730225\n",
            "step 1067: generator_loss=1.5619282722473145, discriminator_loss=0.2751912474632263\n",
            "step 1068: generator_loss=1.5767229795455933, discriminator_loss=0.28022322058677673\n",
            "step 1069: generator_loss=1.518923282623291, discriminator_loss=0.28675442934036255\n",
            "step 1070: generator_loss=1.4077014923095703, discriminator_loss=0.3118898272514343\n",
            "step 1071: generator_loss=1.3718183040618896, discriminator_loss=0.3205903172492981\n",
            "step 1072: generator_loss=1.278943419456482, discriminator_loss=0.3328058123588562\n",
            "step 1073: generator_loss=1.3750509023666382, discriminator_loss=0.32056573033332825\n",
            "step 1074: generator_loss=1.4242453575134277, discriminator_loss=0.3184140920639038\n",
            "step 1075: generator_loss=1.382127285003662, discriminator_loss=0.32908451557159424\n",
            "step 1076: generator_loss=1.3838043212890625, discriminator_loss=0.33052369952201843\n",
            "step 1077: generator_loss=1.425998330116272, discriminator_loss=0.3241161108016968\n",
            "step 1078: generator_loss=1.2875725030899048, discriminator_loss=0.3498755991458893\n",
            "step 1079: generator_loss=1.348076581954956, discriminator_loss=0.34199583530426025\n",
            "step 1080: generator_loss=1.2633991241455078, discriminator_loss=0.3596068024635315\n",
            "step 1081: generator_loss=1.2795079946517944, discriminator_loss=0.35888227820396423\n",
            "step 1082: generator_loss=1.261456847190857, discriminator_loss=0.3615115284919739\n",
            "step 1083: generator_loss=1.2865638732910156, discriminator_loss=0.3554493188858032\n",
            "step 1084: generator_loss=1.2615994215011597, discriminator_loss=0.36231592297554016\n",
            "step 1085: generator_loss=1.1953587532043457, discriminator_loss=0.38123127818107605\n",
            "step 1086: generator_loss=1.200974702835083, discriminator_loss=0.373843789100647\n",
            "step 1087: generator_loss=1.1965830326080322, discriminator_loss=0.3792751729488373\n",
            "step 1088: generator_loss=1.2453110218048096, discriminator_loss=0.36829760670661926\n",
            "step 1089: generator_loss=1.262804627418518, discriminator_loss=0.36754903197288513\n",
            "step 1090: generator_loss=1.2224094867706299, discriminator_loss=0.37270408868789673\n",
            "step 1091: generator_loss=1.206789493560791, discriminator_loss=0.376911997795105\n",
            "step 1092: generator_loss=1.176558256149292, discriminator_loss=0.38192877173423767\n",
            "step 1093: generator_loss=1.2042945623397827, discriminator_loss=0.3766784071922302\n",
            "step 1094: generator_loss=1.2090649604797363, discriminator_loss=0.37336236238479614\n",
            "step 1095: generator_loss=1.1889448165893555, discriminator_loss=0.37916141748428345\n",
            "step 1096: generator_loss=1.202958345413208, discriminator_loss=0.37552201747894287\n",
            "step 1097: generator_loss=1.167992353439331, discriminator_loss=0.38394105434417725\n",
            "step 1098: generator_loss=1.1595097780227661, discriminator_loss=0.38453707098960876\n",
            "step 1099: generator_loss=1.1397507190704346, discriminator_loss=0.39089450240135193\n",
            "step 1100: generator_loss=1.1563658714294434, discriminator_loss=0.389886736869812\n",
            "step 1101: generator_loss=1.1319934129714966, discriminator_loss=0.39728835225105286\n",
            "step 1102: generator_loss=1.110949158668518, discriminator_loss=0.4008302688598633\n",
            "step 1103: generator_loss=1.1606003046035767, discriminator_loss=0.3898603022098541\n",
            "step 1104: generator_loss=1.14243745803833, discriminator_loss=0.3962983191013336\n",
            "step 1105: generator_loss=1.1528081893920898, discriminator_loss=0.39576321840286255\n",
            "step 1106: generator_loss=1.1558423042297363, discriminator_loss=0.3958609998226166\n",
            "step 1107: generator_loss=1.1787751913070679, discriminator_loss=0.3893364369869232\n",
            "step 1108: generator_loss=1.1236982345581055, discriminator_loss=0.4019853472709656\n",
            "step 1109: generator_loss=1.1161621809005737, discriminator_loss=0.4053761959075928\n",
            "step 1110: generator_loss=1.0966076850891113, discriminator_loss=0.4113820195198059\n",
            "step 1111: generator_loss=1.1062836647033691, discriminator_loss=0.4083074629306793\n",
            "step 1112: generator_loss=1.0903377532958984, discriminator_loss=0.4130624830722809\n",
            "step 1113: generator_loss=1.079115867614746, discriminator_loss=0.415291965007782\n",
            "step 1114: generator_loss=1.096929669380188, discriminator_loss=0.4109373092651367\n",
            "step 1115: generator_loss=1.1110315322875977, discriminator_loss=0.4100587069988251\n",
            "step 1116: generator_loss=1.094215750694275, discriminator_loss=0.41350504755973816\n",
            "step 1117: generator_loss=1.0595786571502686, discriminator_loss=0.4241631329059601\n",
            "step 1118: generator_loss=1.073117733001709, discriminator_loss=0.41961410641670227\n",
            "step 1119: generator_loss=1.0347075462341309, discriminator_loss=0.43244990706443787\n",
            "step 1120: generator_loss=1.047524094581604, discriminator_loss=0.43205249309539795\n",
            "step 1121: generator_loss=1.0424774885177612, discriminator_loss=0.43562817573547363\n",
            "step 1122: generator_loss=1.0503193140029907, discriminator_loss=0.43411344289779663\n",
            "step 1123: generator_loss=1.0452396869659424, discriminator_loss=0.44456911087036133\n",
            "step 1124: generator_loss=1.0368807315826416, discriminator_loss=0.44559401273727417\n",
            "step 1125: generator_loss=1.0550448894500732, discriminator_loss=0.44180455803871155\n",
            "step 1126: generator_loss=1.0213773250579834, discriminator_loss=0.4570229947566986\n",
            "step 1127: generator_loss=1.035980224609375, discriminator_loss=0.45584654808044434\n",
            "step 1128: generator_loss=1.060467004776001, discriminator_loss=0.44515955448150635\n",
            "step 1129: generator_loss=0.9980045557022095, discriminator_loss=0.483930379152298\n",
            "step 1130: generator_loss=1.0322270393371582, discriminator_loss=0.46390217542648315\n",
            "step 1131: generator_loss=1.0289465188980103, discriminator_loss=0.46158933639526367\n",
            "step 1132: generator_loss=0.9976940751075745, discriminator_loss=0.48343127965927124\n",
            "step 1133: generator_loss=1.016498327255249, discriminator_loss=0.4693688750267029\n",
            "step 1134: generator_loss=1.0070629119873047, discriminator_loss=0.4914889931678772\n",
            "step 1135: generator_loss=1.0260324478149414, discriminator_loss=0.47179582715034485\n",
            "step 1136: generator_loss=0.9492076635360718, discriminator_loss=0.5162625312805176\n",
            "step 1137: generator_loss=0.985154390335083, discriminator_loss=0.513755202293396\n",
            "step 1138: generator_loss=0.925145149230957, discriminator_loss=0.5390568375587463\n",
            "step 1139: generator_loss=0.9328227043151855, discriminator_loss=0.5499337911605835\n",
            "step 1140: generator_loss=0.9790273308753967, discriminator_loss=0.5460460186004639\n",
            "step 1141: generator_loss=0.9665166139602661, discriminator_loss=0.5457612872123718\n",
            "step 1142: generator_loss=0.980567216873169, discriminator_loss=0.5493423342704773\n",
            "step 1143: generator_loss=0.9766417741775513, discriminator_loss=0.5558371543884277\n",
            "step 1144: generator_loss=0.9671434760093689, discriminator_loss=0.5703151226043701\n",
            "step 1145: generator_loss=1.0217204093933105, discriminator_loss=0.5494604110717773\n",
            "step 1146: generator_loss=0.9395212531089783, discriminator_loss=0.5847961902618408\n",
            "step 1147: generator_loss=0.968315839767456, discriminator_loss=0.5756493806838989\n",
            "step 1148: generator_loss=0.9513576030731201, discriminator_loss=0.5928583145141602\n",
            "step 1149: generator_loss=0.903660237789154, discriminator_loss=0.6294188499450684\n",
            "step 1150: generator_loss=0.9152045249938965, discriminator_loss=0.5996720194816589\n",
            "step 1151: generator_loss=0.9155986309051514, discriminator_loss=0.6215439438819885\n",
            "step 1152: generator_loss=0.9572813510894775, discriminator_loss=0.6095768809318542\n",
            "step 1153: generator_loss=0.9352911710739136, discriminator_loss=0.6131908297538757\n",
            "step 1154: generator_loss=0.9295327663421631, discriminator_loss=0.6175644397735596\n",
            "step 1155: generator_loss=0.9405934810638428, discriminator_loss=0.624525249004364\n",
            "step 1156: generator_loss=0.856715202331543, discriminator_loss=0.6859091520309448\n",
            "step 1157: generator_loss=0.8586870431900024, discriminator_loss=0.6795989274978638\n",
            "step 1158: generator_loss=0.9511559009552002, discriminator_loss=0.6322799921035767\n",
            "step 1159: generator_loss=0.8565900325775146, discriminator_loss=0.6876059770584106\n",
            "step 1160: generator_loss=0.8471633195877075, discriminator_loss=0.7050834894180298\n",
            "step 1161: generator_loss=0.89760422706604, discriminator_loss=0.6752321720123291\n",
            "step 1162: generator_loss=0.8917335867881775, discriminator_loss=0.6951526403427124\n",
            "step 1163: generator_loss=0.8731126189231873, discriminator_loss=0.7102296948432922\n",
            "step 1164: generator_loss=0.8803812861442566, discriminator_loss=0.7065743207931519\n",
            "step 1165: generator_loss=0.9187581539154053, discriminator_loss=0.681606650352478\n",
            "step 1166: generator_loss=0.8964889049530029, discriminator_loss=0.6938424110412598\n",
            "step 1167: generator_loss=0.9420602321624756, discriminator_loss=0.659076452255249\n",
            "step 1168: generator_loss=0.8915904760360718, discriminator_loss=0.696912407875061\n",
            "step 1169: generator_loss=0.9073696136474609, discriminator_loss=0.6749035120010376\n",
            "step 1170: generator_loss=0.800420343875885, discriminator_loss=0.7132002115249634\n",
            "step 1171: generator_loss=0.847503662109375, discriminator_loss=0.6925328969955444\n",
            "step 1172: generator_loss=0.7691195607185364, discriminator_loss=0.719338059425354\n",
            "step 1173: generator_loss=0.7593744993209839, discriminator_loss=0.7291433811187744\n",
            "step 1174: generator_loss=0.8741419315338135, discriminator_loss=0.6804013848304749\n",
            "step 1175: generator_loss=0.9315855503082275, discriminator_loss=0.6485192179679871\n",
            "step 1176: generator_loss=0.8795958757400513, discriminator_loss=0.6577619314193726\n",
            "step 1177: generator_loss=0.8409724235534668, discriminator_loss=0.6764174699783325\n",
            "step 1178: generator_loss=0.8024494051933289, discriminator_loss=0.6905618906021118\n",
            "step 1179: generator_loss=0.8371810913085938, discriminator_loss=0.6624839305877686\n",
            "step 1180: generator_loss=0.8233395218849182, discriminator_loss=0.6656576991081238\n",
            "step 1181: generator_loss=0.8626857399940491, discriminator_loss=0.6453070640563965\n",
            "step 1182: generator_loss=0.771546483039856, discriminator_loss=0.6767318248748779\n",
            "step 1183: generator_loss=0.7854706048965454, discriminator_loss=0.6662064790725708\n",
            "step 1184: generator_loss=0.7987431883811951, discriminator_loss=0.6527222394943237\n",
            "step 1185: generator_loss=0.8048735857009888, discriminator_loss=0.6559756398200989\n",
            "step 1186: generator_loss=0.7462503910064697, discriminator_loss=0.6744549870491028\n",
            "step 1187: generator_loss=0.7741734385490417, discriminator_loss=0.663020133972168\n",
            "step 1188: generator_loss=0.8108054399490356, discriminator_loss=0.6488689184188843\n",
            "step 1189: generator_loss=0.8228760957717896, discriminator_loss=0.6443901062011719\n",
            "step 1190: generator_loss=0.8081120252609253, discriminator_loss=0.6461719274520874\n",
            "step 1191: generator_loss=0.8463577032089233, discriminator_loss=0.6350361704826355\n",
            "step 1192: generator_loss=0.8118221163749695, discriminator_loss=0.6451127529144287\n",
            "step 1193: generator_loss=0.8639097213745117, discriminator_loss=0.6277914047241211\n",
            "step 1194: generator_loss=0.7605096101760864, discriminator_loss=0.6589019298553467\n",
            "step 1195: generator_loss=0.8583390712738037, discriminator_loss=0.6233946084976196\n",
            "step 1196: generator_loss=0.8609481453895569, discriminator_loss=0.6237940192222595\n",
            "step 1197: generator_loss=0.7993481159210205, discriminator_loss=0.6402773857116699\n",
            "step 1198: generator_loss=0.7908962965011597, discriminator_loss=0.6405800580978394\n",
            "step 1199: generator_loss=0.7814809679985046, discriminator_loss=0.6409769058227539\n",
            "step 1200: generator_loss=0.8022974729537964, discriminator_loss=0.6331181526184082\n",
            "step 1201: generator_loss=0.8113642930984497, discriminator_loss=0.6287510395050049\n",
            "step 1202: generator_loss=0.7584753036499023, discriminator_loss=0.6453039646148682\n",
            "step 1203: generator_loss=0.7537466287612915, discriminator_loss=0.6462734937667847\n",
            "step 1204: generator_loss=0.7243290543556213, discriminator_loss=0.6558254957199097\n",
            "step 1205: generator_loss=0.765794038772583, discriminator_loss=0.6423637270927429\n",
            "step 1206: generator_loss=0.7594490051269531, discriminator_loss=0.6458356976509094\n",
            "step 1207: generator_loss=0.7366849184036255, discriminator_loss=0.6611153483390808\n",
            "step 1208: generator_loss=0.7365899085998535, discriminator_loss=0.6612535715103149\n",
            "step 1209: generator_loss=0.7436753511428833, discriminator_loss=0.666249692440033\n",
            "step 1210: generator_loss=0.7110099792480469, discriminator_loss=0.6780089139938354\n",
            "step 1211: generator_loss=0.7354530096054077, discriminator_loss=0.6738218069076538\n",
            "step 1212: generator_loss=0.7287567853927612, discriminator_loss=0.6846603155136108\n",
            "step 1213: generator_loss=0.7222961187362671, discriminator_loss=0.686687707901001\n",
            "step 1214: generator_loss=0.7647963166236877, discriminator_loss=0.6749335527420044\n",
            "step 1215: generator_loss=0.7369811534881592, discriminator_loss=0.6918504238128662\n",
            "step 1216: generator_loss=0.6927551627159119, discriminator_loss=0.7120532989501953\n",
            "step 1217: generator_loss=0.7403498888015747, discriminator_loss=0.6996778845787048\n",
            "step 1218: generator_loss=0.7072868347167969, discriminator_loss=0.7103515863418579\n",
            "step 1219: generator_loss=0.7105712890625, discriminator_loss=0.7112756967544556\n",
            "step 1220: generator_loss=0.7036490440368652, discriminator_loss=0.7167079448699951\n",
            "step 1221: generator_loss=0.680330216884613, discriminator_loss=0.7278053760528564\n",
            "step 1222: generator_loss=0.6916981935501099, discriminator_loss=0.7295169234275818\n",
            "step 1223: generator_loss=0.6723967790603638, discriminator_loss=0.7352495789527893\n",
            "step 1224: generator_loss=0.6723851561546326, discriminator_loss=0.7346102595329285\n",
            "step 1225: generator_loss=0.6766334176063538, discriminator_loss=0.7376095056533813\n",
            "step 1226: generator_loss=0.6501302123069763, discriminator_loss=0.7428956031799316\n",
            "step 1227: generator_loss=0.6335139274597168, discriminator_loss=0.7540721893310547\n",
            "step 1228: generator_loss=0.6148852109909058, discriminator_loss=0.7691121697425842\n",
            "step 1229: generator_loss=0.6433112025260925, discriminator_loss=0.7594113945960999\n",
            "step 1230: generator_loss=0.6233628988265991, discriminator_loss=0.767095685005188\n",
            "step 1231: generator_loss=0.6511982679367065, discriminator_loss=0.770211935043335\n",
            "step 1232: generator_loss=0.6565201878547668, discriminator_loss=0.76667320728302\n",
            "step 1233: generator_loss=0.6449097990989685, discriminator_loss=0.7728045582771301\n",
            "step 1234: generator_loss=0.6224578619003296, discriminator_loss=0.7879471778869629\n",
            "step 1235: generator_loss=0.6281626224517822, discriminator_loss=0.7874190807342529\n",
            "step 1236: generator_loss=0.6318253874778748, discriminator_loss=0.7864112854003906\n",
            "step 1237: generator_loss=0.622125506401062, discriminator_loss=0.7954816818237305\n",
            "step 1238: generator_loss=0.6024425029754639, discriminator_loss=0.8023790717124939\n",
            "step 1239: generator_loss=0.613169252872467, discriminator_loss=0.8014692068099976\n",
            "step 1240: generator_loss=0.6349203586578369, discriminator_loss=0.7872797250747681\n",
            "step 1241: generator_loss=0.6159565448760986, discriminator_loss=0.7949674725532532\n",
            "step 1242: generator_loss=0.6181920766830444, discriminator_loss=0.7908269166946411\n",
            "step 1243: generator_loss=0.6033563613891602, discriminator_loss=0.79463791847229\n",
            "step 1244: generator_loss=0.5877227187156677, discriminator_loss=0.8008408546447754\n",
            "step 1245: generator_loss=0.6388086676597595, discriminator_loss=0.7725704908370972\n",
            "step 1246: generator_loss=0.5980141162872314, discriminator_loss=0.7881722450256348\n",
            "step 1247: generator_loss=0.6152207851409912, discriminator_loss=0.7744120359420776\n",
            "step 1248: generator_loss=0.6115663051605225, discriminator_loss=0.7718978524208069\n",
            "step 1249: generator_loss=0.5975925922393799, discriminator_loss=0.7749086022377014\n",
            "step 1250: generator_loss=0.6170743703842163, discriminator_loss=0.7587193846702576\n",
            "step 1251: generator_loss=0.6281163692474365, discriminator_loss=0.7499573826789856\n",
            "step 1252: generator_loss=0.6279661655426025, discriminator_loss=0.7490224838256836\n",
            "step 1253: generator_loss=0.6470062732696533, discriminator_loss=0.7354042530059814\n",
            "step 1254: generator_loss=0.6586039662361145, discriminator_loss=0.7247843742370605\n",
            "step 1255: generator_loss=0.6772130131721497, discriminator_loss=0.7130091190338135\n",
            "step 1256: generator_loss=0.6827623844146729, discriminator_loss=0.708672285079956\n",
            "step 1257: generator_loss=0.6827752590179443, discriminator_loss=0.7065409421920776\n",
            "step 1258: generator_loss=0.6989009976387024, discriminator_loss=0.6984566450119019\n",
            "step 1259: generator_loss=0.7077563405036926, discriminator_loss=0.6953413486480713\n",
            "step 1260: generator_loss=0.7206252217292786, discriminator_loss=0.6869580745697021\n",
            "step 1261: generator_loss=0.7365087270736694, discriminator_loss=0.6784921884536743\n",
            "step 1262: generator_loss=0.7397439479827881, discriminator_loss=0.676877498626709\n",
            "step 1263: generator_loss=0.7490451335906982, discriminator_loss=0.6717318296432495\n",
            "step 1264: generator_loss=0.7550931572914124, discriminator_loss=0.6677833795547485\n",
            "step 1265: generator_loss=0.7718449234962463, discriminator_loss=0.6602663397789001\n",
            "step 1266: generator_loss=0.7694841027259827, discriminator_loss=0.6586573123931885\n",
            "step 1267: generator_loss=0.7727705240249634, discriminator_loss=0.6557467579841614\n",
            "step 1268: generator_loss=0.7772175073623657, discriminator_loss=0.651347279548645\n",
            "step 1269: generator_loss=0.7682743668556213, discriminator_loss=0.6508393287658691\n",
            "step 1270: generator_loss=0.7832669615745544, discriminator_loss=0.6425155997276306\n",
            "step 1271: generator_loss=0.7778061628341675, discriminator_loss=0.6411560773849487\n",
            "step 1272: generator_loss=0.7640331983566284, discriminator_loss=0.644048810005188\n",
            "step 1273: generator_loss=0.7816989421844482, discriminator_loss=0.6327745318412781\n",
            "step 1274: generator_loss=0.7735032439231873, discriminator_loss=0.6328919529914856\n",
            "step 1275: generator_loss=0.7677369117736816, discriminator_loss=0.6328253746032715\n",
            "step 1276: generator_loss=0.7695642709732056, discriminator_loss=0.6282801628112793\n",
            "step 1277: generator_loss=0.7822703123092651, discriminator_loss=0.6210591793060303\n",
            "step 1278: generator_loss=0.7796642184257507, discriminator_loss=0.6208642721176147\n",
            "step 1279: generator_loss=0.7809122204780579, discriminator_loss=0.6191044449806213\n",
            "step 1280: generator_loss=0.7812163829803467, discriminator_loss=0.6168836355209351\n",
            "step 1281: generator_loss=0.7897230982780457, discriminator_loss=0.612358033657074\n",
            "step 1282: generator_loss=0.7863577604293823, discriminator_loss=0.6121154427528381\n",
            "step 1283: generator_loss=0.8055464625358582, discriminator_loss=0.6034051179885864\n",
            "step 1284: generator_loss=0.8032599687576294, discriminator_loss=0.602682888507843\n",
            "step 1285: generator_loss=0.8220537900924683, discriminator_loss=0.5951537489891052\n",
            "step 1286: generator_loss=0.8172925710678101, discriminator_loss=0.5963971614837646\n",
            "step 1287: generator_loss=0.8179966807365417, discriminator_loss=0.5945481657981873\n",
            "step 1288: generator_loss=0.8139289617538452, discriminator_loss=0.5960003733634949\n",
            "step 1289: generator_loss=0.8236872553825378, discriminator_loss=0.5918744206428528\n",
            "step 1290: generator_loss=0.8257139921188354, discriminator_loss=0.5885847806930542\n",
            "step 1291: generator_loss=0.8347355723381042, discriminator_loss=0.584820032119751\n",
            "step 1292: generator_loss=0.828382134437561, discriminator_loss=0.5864521861076355\n",
            "step 1293: generator_loss=0.837165117263794, discriminator_loss=0.5815144181251526\n",
            "step 1294: generator_loss=0.8427218794822693, discriminator_loss=0.5777150392532349\n",
            "step 1295: generator_loss=0.8514728546142578, discriminator_loss=0.5743744373321533\n",
            "step 1296: generator_loss=0.8535102605819702, discriminator_loss=0.5729719400405884\n",
            "step 1297: generator_loss=0.8426583409309387, discriminator_loss=0.5759283900260925\n",
            "step 1298: generator_loss=0.8516628742218018, discriminator_loss=0.5711801648139954\n",
            "step 1299: generator_loss=0.8596899509429932, discriminator_loss=0.5671696662902832\n",
            "step 1300: generator_loss=0.8394407033920288, discriminator_loss=0.5732206702232361\n",
            "step 1301: generator_loss=0.8653697967529297, discriminator_loss=0.5624781847000122\n",
            "step 1302: generator_loss=0.8611370325088501, discriminator_loss=0.5625454783439636\n",
            "step 1303: generator_loss=0.8799676299095154, discriminator_loss=0.5541440844535828\n",
            "step 1304: generator_loss=0.8673701286315918, discriminator_loss=0.5578317642211914\n",
            "step 1305: generator_loss=0.8481377363204956, discriminator_loss=0.5634249448776245\n",
            "step 1306: generator_loss=0.8674696683883667, discriminator_loss=0.5562707781791687\n",
            "step 1307: generator_loss=0.8860985040664673, discriminator_loss=0.547713041305542\n",
            "step 1308: generator_loss=0.8790654540061951, discriminator_loss=0.5488319396972656\n",
            "step 1309: generator_loss=0.8856523633003235, discriminator_loss=0.5461199879646301\n",
            "step 1310: generator_loss=0.883418083190918, discriminator_loss=0.5462234020233154\n",
            "step 1311: generator_loss=0.8817390203475952, discriminator_loss=0.5461081266403198\n",
            "step 1312: generator_loss=0.8934718370437622, discriminator_loss=0.5415331125259399\n",
            "step 1313: generator_loss=0.9129936695098877, discriminator_loss=0.5335755348205566\n",
            "step 1314: generator_loss=0.9269939661026001, discriminator_loss=0.5284744501113892\n",
            "step 1315: generator_loss=0.9207181930541992, discriminator_loss=0.5296386480331421\n",
            "step 1316: generator_loss=0.9198245406150818, discriminator_loss=0.5287994742393494\n",
            "step 1317: generator_loss=0.9417920112609863, discriminator_loss=0.5212104320526123\n",
            "step 1318: generator_loss=0.9505183100700378, discriminator_loss=0.5156468152999878\n",
            "step 1319: generator_loss=0.9449436664581299, discriminator_loss=0.5156557559967041\n",
            "step 1320: generator_loss=0.9392716884613037, discriminator_loss=0.516175389289856\n",
            "step 1321: generator_loss=0.9253876805305481, discriminator_loss=0.5189108848571777\n",
            "step 1322: generator_loss=0.9453320503234863, discriminator_loss=0.5110412836074829\n",
            "step 1323: generator_loss=0.9444646835327148, discriminator_loss=0.5090937614440918\n",
            "step 1324: generator_loss=0.9728147983551025, discriminator_loss=0.4988890290260315\n",
            "step 1325: generator_loss=0.9738961458206177, discriminator_loss=0.49544623494148254\n",
            "step 1326: generator_loss=0.9654978513717651, discriminator_loss=0.49656203389167786\n",
            "step 1327: generator_loss=0.9647877812385559, discriminator_loss=0.4939315915107727\n",
            "step 1328: generator_loss=0.9837710857391357, discriminator_loss=0.4870142936706543\n",
            "step 1329: generator_loss=1.0034637451171875, discriminator_loss=0.4789087772369385\n",
            "step 1330: generator_loss=0.9968786239624023, discriminator_loss=0.47936588525772095\n",
            "step 1331: generator_loss=1.010909080505371, discriminator_loss=0.4723564684391022\n",
            "step 1332: generator_loss=1.011361837387085, discriminator_loss=0.47033458948135376\n",
            "step 1333: generator_loss=1.0393668413162231, discriminator_loss=0.4594542682170868\n",
            "step 1334: generator_loss=1.031748652458191, discriminator_loss=0.4584985077381134\n",
            "step 1335: generator_loss=1.0262563228607178, discriminator_loss=0.4581271708011627\n",
            "step 1336: generator_loss=1.0554044246673584, discriminator_loss=0.44745928049087524\n",
            "step 1337: generator_loss=1.0871295928955078, discriminator_loss=0.43717169761657715\n",
            "step 1338: generator_loss=1.089339256286621, discriminator_loss=0.4363703727722168\n",
            "step 1339: generator_loss=1.1007678508758545, discriminator_loss=0.43307366967201233\n",
            "step 1340: generator_loss=1.1089097261428833, discriminator_loss=0.4296066164970398\n",
            "step 1341: generator_loss=1.1233208179473877, discriminator_loss=0.4241480231285095\n",
            "step 1342: generator_loss=1.1588921546936035, discriminator_loss=0.41478919982910156\n",
            "step 1343: generator_loss=1.1474709510803223, discriminator_loss=0.41571277379989624\n",
            "step 1344: generator_loss=1.1954468488693237, discriminator_loss=0.40301740169525146\n",
            "step 1345: generator_loss=1.1647968292236328, discriminator_loss=0.4076925814151764\n",
            "step 1346: generator_loss=1.1765010356903076, discriminator_loss=0.4043046236038208\n",
            "step 1347: generator_loss=1.2135119438171387, discriminator_loss=0.39257556200027466\n",
            "step 1348: generator_loss=1.193651795387268, discriminator_loss=0.3965113162994385\n",
            "step 1349: generator_loss=1.2031618356704712, discriminator_loss=0.39257144927978516\n",
            "step 1350: generator_loss=1.194846510887146, discriminator_loss=0.39074862003326416\n",
            "step 1351: generator_loss=1.1716954708099365, discriminator_loss=0.39803579449653625\n",
            "step 1352: generator_loss=1.2096364498138428, discriminator_loss=0.38755854964256287\n",
            "step 1353: generator_loss=1.1951837539672852, discriminator_loss=0.38869887590408325\n",
            "step 1354: generator_loss=1.1742970943450928, discriminator_loss=0.39215680956840515\n",
            "step 1355: generator_loss=1.1923491954803467, discriminator_loss=0.38657286763191223\n",
            "step 1356: generator_loss=1.2083840370178223, discriminator_loss=0.3815753161907196\n",
            "step 1357: generator_loss=1.1608821153640747, discriminator_loss=0.38982734084129333\n",
            "step 1358: generator_loss=1.167656660079956, discriminator_loss=0.39096105098724365\n",
            "step 1359: generator_loss=1.166006088256836, discriminator_loss=0.3880462050437927\n",
            "step 1360: generator_loss=1.1922662258148193, discriminator_loss=0.3812997043132782\n",
            "step 1361: generator_loss=1.1558101177215576, discriminator_loss=0.3875507414340973\n",
            "step 1362: generator_loss=1.1613355875015259, discriminator_loss=0.3856101632118225\n",
            "step 1363: generator_loss=1.169320821762085, discriminator_loss=0.3831968903541565\n",
            "step 1364: generator_loss=1.1335066556930542, discriminator_loss=0.3897548317909241\n",
            "step 1365: generator_loss=1.165687084197998, discriminator_loss=0.3826490044593811\n",
            "step 1366: generator_loss=1.1683192253112793, discriminator_loss=0.38176417350769043\n",
            "step 1367: generator_loss=1.1407716274261475, discriminator_loss=0.3872033655643463\n",
            "step 1368: generator_loss=1.1360304355621338, discriminator_loss=0.3871873915195465\n",
            "step 1369: generator_loss=1.1843194961547852, discriminator_loss=0.37487512826919556\n",
            "step 1370: generator_loss=1.1778364181518555, discriminator_loss=0.3771243691444397\n",
            "step 1371: generator_loss=1.1644843816757202, discriminator_loss=0.3793797791004181\n",
            "step 1372: generator_loss=1.1728827953338623, discriminator_loss=0.3792710304260254\n",
            "step 1373: generator_loss=1.1685084104537964, discriminator_loss=0.37843000888824463\n",
            "step 1374: generator_loss=1.2294833660125732, discriminator_loss=0.36453497409820557\n",
            "step 1375: generator_loss=1.1910209655761719, discriminator_loss=0.3731452226638794\n",
            "step 1376: generator_loss=1.2111808061599731, discriminator_loss=0.3683125972747803\n",
            "step 1377: generator_loss=1.2263808250427246, discriminator_loss=0.36468183994293213\n",
            "step 1378: generator_loss=1.2158088684082031, discriminator_loss=0.36564821004867554\n",
            "step 1379: generator_loss=1.1868534088134766, discriminator_loss=0.37218141555786133\n",
            "step 1380: generator_loss=1.2634230852127075, discriminator_loss=0.3550495505332947\n",
            "step 1381: generator_loss=1.2702693939208984, discriminator_loss=0.3531368672847748\n",
            "step 1382: generator_loss=1.2870969772338867, discriminator_loss=0.3494550585746765\n",
            "step 1383: generator_loss=1.2715870141983032, discriminator_loss=0.351056307554245\n",
            "step 1384: generator_loss=1.263688325881958, discriminator_loss=0.3528072237968445\n",
            "step 1385: generator_loss=1.2791409492492676, discriminator_loss=0.35097941756248474\n",
            "step 1386: generator_loss=1.2922393083572388, discriminator_loss=0.34359580278396606\n",
            "step 1387: generator_loss=1.265259861946106, discriminator_loss=0.3493656814098358\n",
            "step 1388: generator_loss=1.2794461250305176, discriminator_loss=0.3475656509399414\n",
            "step 1389: generator_loss=1.3072144985198975, discriminator_loss=0.3423013389110565\n",
            "step 1390: generator_loss=1.288902997970581, discriminator_loss=0.34577637910842896\n",
            "step 1391: generator_loss=1.2763564586639404, discriminator_loss=0.34616321325302124\n",
            "step 1392: generator_loss=1.3223148584365845, discriminator_loss=0.33804935216903687\n",
            "step 1393: generator_loss=1.3272641897201538, discriminator_loss=0.33515220880508423\n",
            "step 1394: generator_loss=1.2835121154785156, discriminator_loss=0.3437816798686981\n",
            "step 1395: generator_loss=1.264145016670227, discriminator_loss=0.34333518147468567\n",
            "step 1396: generator_loss=1.2971752882003784, discriminator_loss=0.3397482633590698\n",
            "step 1397: generator_loss=1.32733952999115, discriminator_loss=0.3335341513156891\n",
            "step 1398: generator_loss=1.2788809537887573, discriminator_loss=0.34313204884529114\n",
            "step 1399: generator_loss=1.2989369630813599, discriminator_loss=0.3375301957130432\n",
            "step 1400: generator_loss=1.3213136196136475, discriminator_loss=0.3330655097961426\n",
            "step 1401: generator_loss=1.260288119316101, discriminator_loss=0.35069969296455383\n",
            "step 1402: generator_loss=1.238889217376709, discriminator_loss=0.35368263721466064\n",
            "step 1403: generator_loss=1.2035067081451416, discriminator_loss=0.3616423010826111\n",
            "step 1404: generator_loss=1.2349873781204224, discriminator_loss=0.35556885600090027\n",
            "step 1405: generator_loss=1.2725651264190674, discriminator_loss=0.35078495740890503\n",
            "step 1406: generator_loss=1.320847511291504, discriminator_loss=0.34217435121536255\n",
            "step 1407: generator_loss=1.3188235759735107, discriminator_loss=0.3445683717727661\n",
            "step 1408: generator_loss=1.2424442768096924, discriminator_loss=0.35995885729789734\n",
            "step 1409: generator_loss=1.203660249710083, discriminator_loss=0.3677799105644226\n",
            "step 1410: generator_loss=1.2990710735321045, discriminator_loss=0.3455829620361328\n",
            "step 1411: generator_loss=1.2470452785491943, discriminator_loss=0.36002016067504883\n",
            "step 1412: generator_loss=1.2336872816085815, discriminator_loss=0.364703506231308\n",
            "step 1413: generator_loss=1.2382519245147705, discriminator_loss=0.3614296615123749\n",
            "step 1414: generator_loss=1.1821751594543457, discriminator_loss=0.3764215111732483\n",
            "step 1415: generator_loss=1.2211637496948242, discriminator_loss=0.3667355179786682\n",
            "step 1416: generator_loss=1.2265689373016357, discriminator_loss=0.36850982904434204\n",
            "step 1417: generator_loss=1.2566213607788086, discriminator_loss=0.3659679591655731\n",
            "step 1418: generator_loss=1.2061539888381958, discriminator_loss=0.3763021230697632\n",
            "step 1419: generator_loss=1.2349488735198975, discriminator_loss=0.3684430420398712\n",
            "step 1420: generator_loss=1.2037185430526733, discriminator_loss=0.3759576678276062\n",
            "step 1421: generator_loss=1.2103469371795654, discriminator_loss=0.37743866443634033\n",
            "step 1422: generator_loss=1.219710350036621, discriminator_loss=0.3744974732398987\n",
            "step 1423: generator_loss=1.2182044982910156, discriminator_loss=0.37420135736465454\n",
            "step 1424: generator_loss=1.172762155532837, discriminator_loss=0.38495850563049316\n",
            "step 1425: generator_loss=1.1837294101715088, discriminator_loss=0.3857332170009613\n",
            "step 1426: generator_loss=1.156233787536621, discriminator_loss=0.38926374912261963\n",
            "step 1427: generator_loss=1.1236655712127686, discriminator_loss=0.40121304988861084\n",
            "step 1428: generator_loss=1.220197081565857, discriminator_loss=0.3792317509651184\n",
            "step 1429: generator_loss=1.1718389987945557, discriminator_loss=0.38935142755508423\n",
            "step 1430: generator_loss=1.1375199556350708, discriminator_loss=0.3979417681694031\n",
            "step 1431: generator_loss=1.1528739929199219, discriminator_loss=0.3967370092868805\n",
            "step 1432: generator_loss=1.1473095417022705, discriminator_loss=0.3994220197200775\n",
            "step 1433: generator_loss=1.1699824333190918, discriminator_loss=0.3962457776069641\n",
            "step 1434: generator_loss=1.1290020942687988, discriminator_loss=0.4015694558620453\n",
            "step 1435: generator_loss=1.1418298482894897, discriminator_loss=0.40327656269073486\n",
            "step 1436: generator_loss=1.132443904876709, discriminator_loss=0.40318918228149414\n",
            "step 1437: generator_loss=1.1180341243743896, discriminator_loss=0.4076879620552063\n",
            "step 1438: generator_loss=1.0556092262268066, discriminator_loss=0.420803040266037\n",
            "step 1439: generator_loss=1.1313767433166504, discriminator_loss=0.40383923053741455\n",
            "step 1440: generator_loss=1.1183607578277588, discriminator_loss=0.41167980432510376\n",
            "step 1441: generator_loss=1.1409143209457397, discriminator_loss=0.4078131318092346\n",
            "step 1442: generator_loss=1.0822868347167969, discriminator_loss=0.422075480222702\n",
            "step 1443: generator_loss=1.1302355527877808, discriminator_loss=0.4100877642631531\n",
            "step 1444: generator_loss=1.1085131168365479, discriminator_loss=0.41683250665664673\n",
            "step 1445: generator_loss=1.1351349353790283, discriminator_loss=0.4097820818424225\n",
            "step 1446: generator_loss=1.086379885673523, discriminator_loss=0.4192056357860565\n",
            "step 1447: generator_loss=1.0518049001693726, discriminator_loss=0.4294852316379547\n",
            "step 1448: generator_loss=1.0841162204742432, discriminator_loss=0.4223679304122925\n",
            "step 1449: generator_loss=1.0839550495147705, discriminator_loss=0.42336505651474\n",
            "step 1450: generator_loss=1.1063201427459717, discriminator_loss=0.4180316627025604\n",
            "step 1451: generator_loss=1.0299992561340332, discriminator_loss=0.4343307912349701\n",
            "step 1452: generator_loss=1.051443338394165, discriminator_loss=0.4305744171142578\n",
            "step 1453: generator_loss=1.0441540479660034, discriminator_loss=0.4326193332672119\n",
            "step 1454: generator_loss=1.03987717628479, discriminator_loss=0.43484988808631897\n",
            "step 1455: generator_loss=1.061759114265442, discriminator_loss=0.43011778593063354\n",
            "step 1456: generator_loss=1.0723459720611572, discriminator_loss=0.4280194044113159\n",
            "step 1457: generator_loss=1.0509746074676514, discriminator_loss=0.43436992168426514\n",
            "step 1458: generator_loss=1.045212745666504, discriminator_loss=0.43615251779556274\n",
            "step 1459: generator_loss=1.0458292961120605, discriminator_loss=0.43632271885871887\n",
            "step 1460: generator_loss=1.0389492511749268, discriminator_loss=0.43737053871154785\n",
            "step 1461: generator_loss=1.0251058340072632, discriminator_loss=0.443623423576355\n",
            "step 1462: generator_loss=1.0331733226776123, discriminator_loss=0.44188830256462097\n",
            "step 1463: generator_loss=1.0276179313659668, discriminator_loss=0.4446081519126892\n",
            "step 1464: generator_loss=1.0310256481170654, discriminator_loss=0.44398343563079834\n",
            "step 1465: generator_loss=1.0190508365631104, discriminator_loss=0.4490108788013458\n",
            "step 1466: generator_loss=1.0270166397094727, discriminator_loss=0.44735991954803467\n",
            "step 1467: generator_loss=1.0359277725219727, discriminator_loss=0.4446040391921997\n",
            "step 1468: generator_loss=1.0164510011672974, discriminator_loss=0.44861942529678345\n",
            "step 1469: generator_loss=1.0172300338745117, discriminator_loss=0.44958585500717163\n",
            "step 1470: generator_loss=1.0200867652893066, discriminator_loss=0.45022881031036377\n",
            "step 1471: generator_loss=1.0474474430084229, discriminator_loss=0.44357138872146606\n",
            "step 1472: generator_loss=1.0146832466125488, discriminator_loss=0.4517367482185364\n",
            "step 1473: generator_loss=1.029227375984192, discriminator_loss=0.44978147745132446\n",
            "step 1474: generator_loss=1.0200624465942383, discriminator_loss=0.4496496319770813\n",
            "step 1475: generator_loss=1.0097596645355225, discriminator_loss=0.45294854044914246\n",
            "step 1476: generator_loss=1.01079523563385, discriminator_loss=0.4527280926704407\n",
            "step 1477: generator_loss=1.0047739744186401, discriminator_loss=0.45443353056907654\n",
            "step 1478: generator_loss=0.999140202999115, discriminator_loss=0.4551909267902374\n",
            "step 1479: generator_loss=1.020050287246704, discriminator_loss=0.4513748586177826\n",
            "step 1480: generator_loss=1.0137698650360107, discriminator_loss=0.45193713903427124\n",
            "step 1481: generator_loss=1.0148149728775024, discriminator_loss=0.4542052745819092\n",
            "step 1482: generator_loss=0.9993367195129395, discriminator_loss=0.4581570625305176\n",
            "step 1483: generator_loss=1.0077738761901855, discriminator_loss=0.4551466405391693\n",
            "step 1484: generator_loss=1.0206313133239746, discriminator_loss=0.4521014094352722\n",
            "step 1485: generator_loss=1.007533073425293, discriminator_loss=0.456747829914093\n",
            "step 1486: generator_loss=1.0014688968658447, discriminator_loss=0.45710912346839905\n",
            "step 1487: generator_loss=1.00950026512146, discriminator_loss=0.4557188153266907\n",
            "step 1488: generator_loss=1.006310224533081, discriminator_loss=0.45721837878227234\n",
            "step 1489: generator_loss=1.015246868133545, discriminator_loss=0.45452767610549927\n",
            "step 1490: generator_loss=1.0018510818481445, discriminator_loss=0.45722341537475586\n",
            "step 1491: generator_loss=1.0062205791473389, discriminator_loss=0.45598772168159485\n",
            "step 1492: generator_loss=1.0061593055725098, discriminator_loss=0.4564075469970703\n",
            "step 1493: generator_loss=1.0039374828338623, discriminator_loss=0.457183301448822\n",
            "step 1494: generator_loss=0.9925495386123657, discriminator_loss=0.46176594495773315\n",
            "step 1495: generator_loss=1.0132205486297607, discriminator_loss=0.4563613533973694\n",
            "step 1496: generator_loss=1.0210741758346558, discriminator_loss=0.45436620712280273\n",
            "step 1497: generator_loss=0.9953131675720215, discriminator_loss=0.4619443416595459\n",
            "step 1498: generator_loss=1.0024391412734985, discriminator_loss=0.459696888923645\n",
            "step 1499: generator_loss=0.9911857843399048, discriminator_loss=0.4629999101161957\n",
            "step 1500: generator_loss=0.9985451698303223, discriminator_loss=0.46211254596710205\n",
            "step 1501: generator_loss=0.9966305494308472, discriminator_loss=0.463378369808197\n",
            "step 1502: generator_loss=1.0029741525650024, discriminator_loss=0.46066632866859436\n",
            "step 1503: generator_loss=0.9908190965652466, discriminator_loss=0.4649849534034729\n",
            "step 1504: generator_loss=1.0027546882629395, discriminator_loss=0.4622335433959961\n",
            "step 1505: generator_loss=0.9972138404846191, discriminator_loss=0.46553993225097656\n",
            "step 1506: generator_loss=0.9758589863777161, discriminator_loss=0.4715059995651245\n",
            "step 1507: generator_loss=0.9922139048576355, discriminator_loss=0.46831029653549194\n",
            "step 1508: generator_loss=0.9913278818130493, discriminator_loss=0.4695686995983124\n",
            "step 1509: generator_loss=0.996990442276001, discriminator_loss=0.4692380428314209\n",
            "step 1510: generator_loss=0.9832179546356201, discriminator_loss=0.473219633102417\n",
            "step 1511: generator_loss=0.9766587615013123, discriminator_loss=0.4754827618598938\n",
            "step 1512: generator_loss=0.9851837754249573, discriminator_loss=0.4723922610282898\n",
            "step 1513: generator_loss=0.9644889831542969, discriminator_loss=0.479739248752594\n",
            "step 1514: generator_loss=0.9764245748519897, discriminator_loss=0.4773732125759125\n",
            "step 1515: generator_loss=0.9584002494812012, discriminator_loss=0.48636162281036377\n",
            "step 1516: generator_loss=0.9598730206489563, discriminator_loss=0.48768481612205505\n",
            "step 1517: generator_loss=0.9790961146354675, discriminator_loss=0.48066455125808716\n",
            "step 1518: generator_loss=0.955105185508728, discriminator_loss=0.48864853382110596\n",
            "step 1519: generator_loss=0.9439646005630493, discriminator_loss=0.494951069355011\n",
            "step 1520: generator_loss=0.9681184887886047, discriminator_loss=0.48587214946746826\n",
            "step 1521: generator_loss=0.9534957408905029, discriminator_loss=0.4934244155883789\n",
            "step 1522: generator_loss=0.9569473266601562, discriminator_loss=0.49274492263793945\n",
            "step 1523: generator_loss=0.9286904335021973, discriminator_loss=0.5072216987609863\n",
            "step 1524: generator_loss=0.9631034135818481, discriminator_loss=0.4953082799911499\n",
            "step 1525: generator_loss=0.9422634840011597, discriminator_loss=0.5023421049118042\n",
            "step 1526: generator_loss=0.9497370719909668, discriminator_loss=0.5004969835281372\n",
            "step 1527: generator_loss=0.938135027885437, discriminator_loss=0.5055719614028931\n",
            "step 1528: generator_loss=0.9243361353874207, discriminator_loss=0.5109550952911377\n",
            "step 1529: generator_loss=0.9233641624450684, discriminator_loss=0.5113765001296997\n",
            "step 1530: generator_loss=0.8996487855911255, discriminator_loss=0.5221896767616272\n",
            "step 1531: generator_loss=0.9320234656333923, discriminator_loss=0.510663628578186\n",
            "step 1532: generator_loss=0.9217999577522278, discriminator_loss=0.5152931213378906\n",
            "step 1533: generator_loss=0.8868126273155212, discriminator_loss=0.5267645120620728\n",
            "step 1534: generator_loss=0.9084361791610718, discriminator_loss=0.5213676691055298\n",
            "step 1535: generator_loss=0.9078207015991211, discriminator_loss=0.5240983963012695\n",
            "step 1536: generator_loss=0.8803348541259766, discriminator_loss=0.5340837240219116\n",
            "step 1537: generator_loss=0.8701738715171814, discriminator_loss=0.541530430316925\n",
            "step 1538: generator_loss=0.892279863357544, discriminator_loss=0.5353715419769287\n",
            "step 1539: generator_loss=0.908094048500061, discriminator_loss=0.5276460647583008\n",
            "step 1540: generator_loss=0.8873008489608765, discriminator_loss=0.541174054145813\n",
            "step 1541: generator_loss=0.9182207584381104, discriminator_loss=0.532961368560791\n",
            "step 1542: generator_loss=0.9306378364562988, discriminator_loss=0.5298802256584167\n",
            "step 1543: generator_loss=0.9109115600585938, discriminator_loss=0.5407949686050415\n",
            "step 1544: generator_loss=0.9042351245880127, discriminator_loss=0.542389988899231\n",
            "step 1545: generator_loss=0.8793861865997314, discriminator_loss=0.551795244216919\n",
            "step 1546: generator_loss=0.8725938200950623, discriminator_loss=0.5546297430992126\n",
            "step 1547: generator_loss=0.8877393007278442, discriminator_loss=0.5528700351715088\n",
            "step 1548: generator_loss=0.8775729537010193, discriminator_loss=0.5582780241966248\n",
            "step 1549: generator_loss=0.8883538246154785, discriminator_loss=0.5524986982345581\n",
            "step 1550: generator_loss=0.8850013017654419, discriminator_loss=0.5554065704345703\n",
            "step 1551: generator_loss=0.8975120186805725, discriminator_loss=0.550123393535614\n",
            "step 1552: generator_loss=0.8718445301055908, discriminator_loss=0.561684250831604\n",
            "step 1553: generator_loss=0.8747838735580444, discriminator_loss=0.5598071813583374\n",
            "step 1554: generator_loss=0.8892231583595276, discriminator_loss=0.5557806491851807\n",
            "step 1555: generator_loss=0.8541058301925659, discriminator_loss=0.5695511698722839\n",
            "step 1556: generator_loss=0.8486126661300659, discriminator_loss=0.5706537961959839\n",
            "step 1557: generator_loss=0.865450382232666, discriminator_loss=0.5618909597396851\n",
            "step 1558: generator_loss=0.888073742389679, discriminator_loss=0.555417537689209\n",
            "step 1559: generator_loss=0.8556774854660034, discriminator_loss=0.5671386122703552\n",
            "step 1560: generator_loss=0.8312392830848694, discriminator_loss=0.5745055079460144\n",
            "step 1561: generator_loss=0.8297175168991089, discriminator_loss=0.5784870982170105\n",
            "step 1562: generator_loss=0.8012329339981079, discriminator_loss=0.586694061756134\n",
            "step 1563: generator_loss=0.8341967463493347, discriminator_loss=0.5760923624038696\n",
            "step 1564: generator_loss=0.8321049213409424, discriminator_loss=0.5776693820953369\n",
            "step 1565: generator_loss=0.8460681438446045, discriminator_loss=0.5735326409339905\n",
            "step 1566: generator_loss=0.8459877967834473, discriminator_loss=0.5725460648536682\n",
            "step 1567: generator_loss=0.8288289308547974, discriminator_loss=0.5801772475242615\n",
            "step 1568: generator_loss=0.8182664513587952, discriminator_loss=0.5881870985031128\n",
            "step 1569: generator_loss=0.8274046778678894, discriminator_loss=0.5883631706237793\n",
            "step 1570: generator_loss=0.813402533531189, discriminator_loss=0.5978692770004272\n",
            "step 1571: generator_loss=0.8180387020111084, discriminator_loss=0.598900556564331\n",
            "step 1572: generator_loss=0.8121926784515381, discriminator_loss=0.6031659245491028\n",
            "step 1573: generator_loss=0.8020062446594238, discriminator_loss=0.6110406517982483\n",
            "step 1574: generator_loss=0.8223260641098022, discriminator_loss=0.6061357259750366\n",
            "step 1575: generator_loss=0.8068121671676636, discriminator_loss=0.6134166717529297\n",
            "step 1576: generator_loss=0.7967884540557861, discriminator_loss=0.6172881126403809\n",
            "step 1577: generator_loss=0.8012511730194092, discriminator_loss=0.6173537969589233\n",
            "step 1578: generator_loss=0.7925717830657959, discriminator_loss=0.622551441192627\n",
            "step 1579: generator_loss=0.8096833229064941, discriminator_loss=0.6155047416687012\n",
            "step 1580: generator_loss=0.8105591535568237, discriminator_loss=0.6158337593078613\n",
            "step 1581: generator_loss=0.7746484875679016, discriminator_loss=0.6297932267189026\n",
            "step 1582: generator_loss=0.8144522905349731, discriminator_loss=0.6145411729812622\n",
            "step 1583: generator_loss=0.7527783513069153, discriminator_loss=0.6403102874755859\n",
            "step 1584: generator_loss=0.7630579471588135, discriminator_loss=0.6341349482536316\n",
            "step 1585: generator_loss=0.7560070753097534, discriminator_loss=0.6367590427398682\n",
            "step 1586: generator_loss=0.7402019500732422, discriminator_loss=0.6448650360107422\n",
            "step 1587: generator_loss=0.745118260383606, discriminator_loss=0.6430211663246155\n",
            "step 1588: generator_loss=0.7695229649543762, discriminator_loss=0.6349338293075562\n",
            "step 1589: generator_loss=0.7577476501464844, discriminator_loss=0.6380500793457031\n",
            "step 1590: generator_loss=0.7629308700561523, discriminator_loss=0.6384040117263794\n",
            "step 1591: generator_loss=0.760104238986969, discriminator_loss=0.6396147012710571\n",
            "step 1592: generator_loss=0.778404951095581, discriminator_loss=0.6335110664367676\n",
            "step 1593: generator_loss=0.7590296268463135, discriminator_loss=0.6397227048873901\n",
            "step 1594: generator_loss=0.7635819315910339, discriminator_loss=0.6387559771537781\n",
            "step 1595: generator_loss=0.7778517007827759, discriminator_loss=0.6355766654014587\n",
            "step 1596: generator_loss=0.7683072090148926, discriminator_loss=0.6404982805252075\n",
            "step 1597: generator_loss=0.781029462814331, discriminator_loss=0.6350120306015015\n",
            "step 1598: generator_loss=0.7703676223754883, discriminator_loss=0.6399544477462769\n",
            "step 1599: generator_loss=0.7370697259902954, discriminator_loss=0.6537284851074219\n",
            "step 1600: generator_loss=0.7467995285987854, discriminator_loss=0.6498194336891174\n",
            "step 1601: generator_loss=0.755437970161438, discriminator_loss=0.6468991637229919\n",
            "step 1602: generator_loss=0.7661705613136292, discriminator_loss=0.6432358622550964\n",
            "step 1603: generator_loss=0.7508388757705688, discriminator_loss=0.6487680673599243\n",
            "step 1604: generator_loss=0.7295676469802856, discriminator_loss=0.6564398407936096\n",
            "step 1605: generator_loss=0.7568735480308533, discriminator_loss=0.6478733420372009\n",
            "step 1606: generator_loss=0.7630812525749207, discriminator_loss=0.6454477310180664\n",
            "step 1607: generator_loss=0.7671124935150146, discriminator_loss=0.6453019976615906\n",
            "step 1608: generator_loss=0.7753278017044067, discriminator_loss=0.6409401297569275\n",
            "step 1609: generator_loss=0.7468767166137695, discriminator_loss=0.6527214646339417\n",
            "step 1610: generator_loss=0.7524831891059875, discriminator_loss=0.6499766111373901\n",
            "step 1611: generator_loss=0.761805534362793, discriminator_loss=0.644944965839386\n",
            "step 1612: generator_loss=0.7576978206634521, discriminator_loss=0.6449438333511353\n",
            "step 1613: generator_loss=0.7487704753875732, discriminator_loss=0.6499930620193481\n",
            "step 1614: generator_loss=0.7416250705718994, discriminator_loss=0.6502155661582947\n",
            "step 1615: generator_loss=0.7544853687286377, discriminator_loss=0.6455807089805603\n",
            "step 1616: generator_loss=0.7304000854492188, discriminator_loss=0.654551088809967\n",
            "step 1617: generator_loss=0.7350717782974243, discriminator_loss=0.6535850763320923\n",
            "step 1618: generator_loss=0.7216394543647766, discriminator_loss=0.658737063407898\n",
            "step 1619: generator_loss=0.7252026796340942, discriminator_loss=0.6573889255523682\n",
            "step 1620: generator_loss=0.7368947267532349, discriminator_loss=0.6530507206916809\n",
            "step 1621: generator_loss=0.7403339147567749, discriminator_loss=0.6522674560546875\n",
            "step 1622: generator_loss=0.7185161113739014, discriminator_loss=0.661627471446991\n",
            "step 1623: generator_loss=0.7372121810913086, discriminator_loss=0.6537338495254517\n",
            "step 1624: generator_loss=0.7276376485824585, discriminator_loss=0.6578488945960999\n",
            "step 1625: generator_loss=0.7327965497970581, discriminator_loss=0.6568142175674438\n",
            "step 1626: generator_loss=0.7307131290435791, discriminator_loss=0.6579549908638\n",
            "step 1627: generator_loss=0.7341269850730896, discriminator_loss=0.6569197773933411\n",
            "step 1628: generator_loss=0.7315599918365479, discriminator_loss=0.6593697667121887\n",
            "step 1629: generator_loss=0.7321310639381409, discriminator_loss=0.6581926345825195\n",
            "step 1630: generator_loss=0.7333686351776123, discriminator_loss=0.6587744951248169\n",
            "step 1631: generator_loss=0.7550760507583618, discriminator_loss=0.649006724357605\n",
            "step 1632: generator_loss=0.7418403625488281, discriminator_loss=0.6558264493942261\n",
            "step 1633: generator_loss=0.7565704584121704, discriminator_loss=0.6491855382919312\n",
            "step 1634: generator_loss=0.7357611656188965, discriminator_loss=0.657598614692688\n",
            "step 1635: generator_loss=0.7498154640197754, discriminator_loss=0.6504048109054565\n",
            "step 1636: generator_loss=0.7438211441040039, discriminator_loss=0.6528700590133667\n",
            "step 1637: generator_loss=0.7355283498764038, discriminator_loss=0.656470000743866\n",
            "step 1638: generator_loss=0.7322357892990112, discriminator_loss=0.6573970317840576\n",
            "step 1639: generator_loss=0.7318654656410217, discriminator_loss=0.6572279930114746\n",
            "step 1640: generator_loss=0.7340933084487915, discriminator_loss=0.6557603478431702\n",
            "step 1641: generator_loss=0.7415060997009277, discriminator_loss=0.652639627456665\n",
            "step 1642: generator_loss=0.7494600415229797, discriminator_loss=0.648771345615387\n",
            "step 1643: generator_loss=0.7401959896087646, discriminator_loss=0.6523581147193909\n",
            "step 1644: generator_loss=0.7437309622764587, discriminator_loss=0.6500048637390137\n",
            "step 1645: generator_loss=0.7503759264945984, discriminator_loss=0.6475414037704468\n",
            "step 1646: generator_loss=0.7454470992088318, discriminator_loss=0.6486006379127502\n",
            "step 1647: generator_loss=0.7390803098678589, discriminator_loss=0.6507686376571655\n",
            "step 1648: generator_loss=0.7465258836746216, discriminator_loss=0.6475731134414673\n",
            "step 1649: generator_loss=0.7517799735069275, discriminator_loss=0.6451876163482666\n",
            "step 1650: generator_loss=0.755793571472168, discriminator_loss=0.6429541707038879\n",
            "step 1651: generator_loss=0.7556874752044678, discriminator_loss=0.6424708962440491\n",
            "step 1652: generator_loss=0.7567934989929199, discriminator_loss=0.6412705183029175\n",
            "step 1653: generator_loss=0.7580544948577881, discriminator_loss=0.6401399970054626\n",
            "step 1654: generator_loss=0.7562622427940369, discriminator_loss=0.6399624347686768\n",
            "step 1655: generator_loss=0.7658781409263611, discriminator_loss=0.6351288557052612\n",
            "step 1656: generator_loss=0.7611604332923889, discriminator_loss=0.6362568140029907\n",
            "step 1657: generator_loss=0.7518090605735779, discriminator_loss=0.6398661732673645\n",
            "step 1658: generator_loss=0.7548149228096008, discriminator_loss=0.637292742729187\n",
            "step 1659: generator_loss=0.7609580159187317, discriminator_loss=0.6335869431495667\n",
            "step 1660: generator_loss=0.750111997127533, discriminator_loss=0.6380794048309326\n",
            "step 1661: generator_loss=0.7512307167053223, discriminator_loss=0.6370562314987183\n",
            "step 1662: generator_loss=0.7642402648925781, discriminator_loss=0.6320707201957703\n",
            "step 1663: generator_loss=0.7628345489501953, discriminator_loss=0.6311610341072083\n",
            "step 1664: generator_loss=0.7679373025894165, discriminator_loss=0.6289098858833313\n",
            "step 1665: generator_loss=0.7609450817108154, discriminator_loss=0.6308233737945557\n",
            "step 1666: generator_loss=0.7651727199554443, discriminator_loss=0.6287264823913574\n",
            "step 1667: generator_loss=0.7755393385887146, discriminator_loss=0.6235107183456421\n",
            "step 1668: generator_loss=0.7680293321609497, discriminator_loss=0.6268923282623291\n",
            "step 1669: generator_loss=0.7694909572601318, discriminator_loss=0.6245831847190857\n",
            "step 1670: generator_loss=0.7769240140914917, discriminator_loss=0.621117889881134\n",
            "step 1671: generator_loss=0.7824344038963318, discriminator_loss=0.6186110973358154\n",
            "step 1672: generator_loss=0.7773963809013367, discriminator_loss=0.6196738481521606\n",
            "step 1673: generator_loss=0.7821590900421143, discriminator_loss=0.6169878244400024\n",
            "step 1674: generator_loss=0.7804725170135498, discriminator_loss=0.6167192459106445\n",
            "step 1675: generator_loss=0.7838954925537109, discriminator_loss=0.6151758432388306\n",
            "step 1676: generator_loss=0.783693790435791, discriminator_loss=0.6138783097267151\n",
            "step 1677: generator_loss=0.7814122438430786, discriminator_loss=0.6138737201690674\n",
            "step 1678: generator_loss=0.7844792604446411, discriminator_loss=0.6123100519180298\n",
            "step 1679: generator_loss=0.7981215119361877, discriminator_loss=0.604893684387207\n",
            "step 1680: generator_loss=0.7919738292694092, discriminator_loss=0.6071046590805054\n",
            "step 1681: generator_loss=0.7963778972625732, discriminator_loss=0.6044309139251709\n",
            "step 1682: generator_loss=0.7867855429649353, discriminator_loss=0.607235312461853\n",
            "step 1683: generator_loss=0.7911041975021362, discriminator_loss=0.6047400236129761\n",
            "step 1684: generator_loss=0.7977681159973145, discriminator_loss=0.6016765236854553\n",
            "step 1685: generator_loss=0.8022907972335815, discriminator_loss=0.599024772644043\n",
            "step 1686: generator_loss=0.8126745223999023, discriminator_loss=0.5942952632904053\n",
            "step 1687: generator_loss=0.8025071620941162, discriminator_loss=0.5975536704063416\n",
            "step 1688: generator_loss=0.8190172910690308, discriminator_loss=0.58961021900177\n",
            "step 1689: generator_loss=0.8123552799224854, discriminator_loss=0.5912991762161255\n",
            "step 1690: generator_loss=0.8161501884460449, discriminator_loss=0.589387834072113\n",
            "step 1691: generator_loss=0.8213562965393066, discriminator_loss=0.5872208476066589\n",
            "step 1692: generator_loss=0.8169659376144409, discriminator_loss=0.5870612263679504\n",
            "step 1693: generator_loss=0.8370683193206787, discriminator_loss=0.5794932842254639\n",
            "step 1694: generator_loss=0.8308523893356323, discriminator_loss=0.5806338787078857\n",
            "step 1695: generator_loss=0.8092945218086243, discriminator_loss=0.5880602598190308\n",
            "step 1696: generator_loss=0.8171855211257935, discriminator_loss=0.5841532945632935\n",
            "step 1697: generator_loss=0.8230066895484924, discriminator_loss=0.5815572738647461\n",
            "step 1698: generator_loss=0.8357458114624023, discriminator_loss=0.5761749744415283\n",
            "step 1699: generator_loss=0.8376799821853638, discriminator_loss=0.5741055607795715\n",
            "step 1700: generator_loss=0.833899974822998, discriminator_loss=0.5743756890296936\n",
            "step 1701: generator_loss=0.8255163431167603, discriminator_loss=0.5785032510757446\n",
            "step 1702: generator_loss=0.8382813930511475, discriminator_loss=0.5735487341880798\n",
            "step 1703: generator_loss=0.8406545519828796, discriminator_loss=0.5722549557685852\n",
            "step 1704: generator_loss=0.835399866104126, discriminator_loss=0.5741536021232605\n",
            "step 1705: generator_loss=0.837264358997345, discriminator_loss=0.573534369468689\n",
            "step 1706: generator_loss=0.8464226126670837, discriminator_loss=0.5689773559570312\n",
            "step 1707: generator_loss=0.8235552906990051, discriminator_loss=0.5775034427642822\n",
            "step 1708: generator_loss=0.838024914264679, discriminator_loss=0.5702211856842041\n",
            "step 1709: generator_loss=0.8292573690414429, discriminator_loss=0.5742952227592468\n",
            "step 1710: generator_loss=0.838567852973938, discriminator_loss=0.5709317922592163\n",
            "step 1711: generator_loss=0.8372312188148499, discriminator_loss=0.5706299543380737\n",
            "step 1712: generator_loss=0.8301984071731567, discriminator_loss=0.5735647082328796\n",
            "step 1713: generator_loss=0.8115418553352356, discriminator_loss=0.5803406834602356\n",
            "step 1714: generator_loss=0.8093608617782593, discriminator_loss=0.5822009444236755\n",
            "step 1715: generator_loss=0.8304264545440674, discriminator_loss=0.5749664306640625\n",
            "step 1716: generator_loss=0.8265166282653809, discriminator_loss=0.5776191353797913\n",
            "step 1717: generator_loss=0.842024028301239, discriminator_loss=0.5721161961555481\n",
            "step 1718: generator_loss=0.8312361836433411, discriminator_loss=0.5754733085632324\n",
            "step 1719: generator_loss=0.8406727313995361, discriminator_loss=0.571523904800415\n",
            "step 1720: generator_loss=0.8346024751663208, discriminator_loss=0.5735076665878296\n",
            "step 1721: generator_loss=0.8278646469116211, discriminator_loss=0.5767094492912292\n",
            "step 1722: generator_loss=0.8263893723487854, discriminator_loss=0.5768274664878845\n",
            "step 1723: generator_loss=0.8094286322593689, discriminator_loss=0.5842355489730835\n",
            "step 1724: generator_loss=0.8181352019309998, discriminator_loss=0.5799451470375061\n",
            "step 1725: generator_loss=0.8295214176177979, discriminator_loss=0.5759119987487793\n",
            "step 1726: generator_loss=0.8304538726806641, discriminator_loss=0.5770659446716309\n",
            "step 1727: generator_loss=0.8201377391815186, discriminator_loss=0.5805855989456177\n",
            "step 1728: generator_loss=0.8215532302856445, discriminator_loss=0.5803704261779785\n",
            "step 1729: generator_loss=0.8230974674224854, discriminator_loss=0.5805954337120056\n",
            "step 1730: generator_loss=0.8267160654067993, discriminator_loss=0.5797669887542725\n",
            "step 1731: generator_loss=0.8249441981315613, discriminator_loss=0.5805743932723999\n",
            "step 1732: generator_loss=0.8312162756919861, discriminator_loss=0.5781130790710449\n",
            "step 1733: generator_loss=0.8259527683258057, discriminator_loss=0.5796041488647461\n",
            "step 1734: generator_loss=0.8466100096702576, discriminator_loss=0.5724762082099915\n",
            "step 1735: generator_loss=0.8205991983413696, discriminator_loss=0.5805046558380127\n",
            "step 1736: generator_loss=0.8123130798339844, discriminator_loss=0.5843367576599121\n",
            "step 1737: generator_loss=0.816104531288147, discriminator_loss=0.5834580659866333\n",
            "step 1738: generator_loss=0.8337782621383667, discriminator_loss=0.5760371088981628\n",
            "step 1739: generator_loss=0.8272385597229004, discriminator_loss=0.5785610675811768\n",
            "step 1740: generator_loss=0.800588846206665, discriminator_loss=0.5884737968444824\n",
            "step 1741: generator_loss=0.8394983410835266, discriminator_loss=0.5740708708763123\n",
            "step 1742: generator_loss=0.8291769027709961, discriminator_loss=0.5787327885627747\n",
            "step 1743: generator_loss=0.8058385848999023, discriminator_loss=0.5869205594062805\n",
            "step 1744: generator_loss=0.8224872946739197, discriminator_loss=0.5808576345443726\n",
            "step 1745: generator_loss=0.7972266674041748, discriminator_loss=0.5906003713607788\n",
            "step 1746: generator_loss=0.8144741654396057, discriminator_loss=0.585568904876709\n",
            "step 1747: generator_loss=0.8159206509590149, discriminator_loss=0.5843670964241028\n",
            "step 1748: generator_loss=0.8254765868186951, discriminator_loss=0.5826400518417358\n",
            "step 1749: generator_loss=0.816180944442749, discriminator_loss=0.5857207179069519\n",
            "step 1750: generator_loss=0.8237333297729492, discriminator_loss=0.5841758251190186\n",
            "step 1751: generator_loss=0.8189544677734375, discriminator_loss=0.5870722532272339\n",
            "step 1752: generator_loss=0.8111864328384399, discriminator_loss=0.5904412269592285\n",
            "step 1753: generator_loss=0.8112896680831909, discriminator_loss=0.5901198387145996\n",
            "step 1754: generator_loss=0.8205766081809998, discriminator_loss=0.5878865718841553\n",
            "step 1755: generator_loss=0.7973541021347046, discriminator_loss=0.5963925123214722\n",
            "step 1756: generator_loss=0.8002433776855469, discriminator_loss=0.5949575901031494\n",
            "step 1757: generator_loss=0.8194645643234253, discriminator_loss=0.5888659954071045\n",
            "step 1758: generator_loss=0.7924966216087341, discriminator_loss=0.5981999635696411\n",
            "step 1759: generator_loss=0.8047564029693604, discriminator_loss=0.5939632058143616\n",
            "step 1760: generator_loss=0.8064510822296143, discriminator_loss=0.5945838689804077\n",
            "step 1761: generator_loss=0.8064473867416382, discriminator_loss=0.5959622859954834\n",
            "step 1762: generator_loss=0.786861002445221, discriminator_loss=0.6024932861328125\n",
            "step 1763: generator_loss=0.8008185029029846, discriminator_loss=0.5968109369277954\n",
            "step 1764: generator_loss=0.7853422164916992, discriminator_loss=0.6048954725265503\n",
            "step 1765: generator_loss=0.7781341671943665, discriminator_loss=0.60907381772995\n",
            "step 1766: generator_loss=0.7889382839202881, discriminator_loss=0.6063546538352966\n",
            "step 1767: generator_loss=0.7867380380630493, discriminator_loss=0.6073065400123596\n",
            "step 1768: generator_loss=0.7692698240280151, discriminator_loss=0.6154050230979919\n",
            "step 1769: generator_loss=0.7872744798660278, discriminator_loss=0.6093872785568237\n",
            "step 1770: generator_loss=0.7786176800727844, discriminator_loss=0.6132379174232483\n",
            "step 1771: generator_loss=0.7931957840919495, discriminator_loss=0.6097335815429688\n",
            "step 1772: generator_loss=0.7835697531700134, discriminator_loss=0.613047182559967\n",
            "step 1773: generator_loss=0.7725951075553894, discriminator_loss=0.6170207858085632\n",
            "step 1774: generator_loss=0.782102644443512, discriminator_loss=0.6132128834724426\n",
            "step 1775: generator_loss=0.7862474918365479, discriminator_loss=0.6127779483795166\n",
            "step 1776: generator_loss=0.7845973968505859, discriminator_loss=0.6148386001586914\n",
            "step 1777: generator_loss=0.7756099104881287, discriminator_loss=0.6168404817581177\n",
            "step 1778: generator_loss=0.7809603214263916, discriminator_loss=0.6147326231002808\n",
            "step 1779: generator_loss=0.7868184447288513, discriminator_loss=0.6107633709907532\n",
            "step 1780: generator_loss=0.767437756061554, discriminator_loss=0.6193294525146484\n",
            "step 1781: generator_loss=0.7832428216934204, discriminator_loss=0.61379474401474\n",
            "step 1782: generator_loss=0.7774733304977417, discriminator_loss=0.617343544960022\n",
            "step 1783: generator_loss=0.7740423679351807, discriminator_loss=0.6193612217903137\n",
            "step 1784: generator_loss=0.7856255173683167, discriminator_loss=0.6149053573608398\n",
            "step 1785: generator_loss=0.7898457050323486, discriminator_loss=0.6135457754135132\n",
            "step 1786: generator_loss=0.7782593369483948, discriminator_loss=0.6194590330123901\n",
            "step 1787: generator_loss=0.7897969484329224, discriminator_loss=0.6139960289001465\n",
            "step 1788: generator_loss=0.7975707054138184, discriminator_loss=0.6113636493682861\n",
            "step 1789: generator_loss=0.7921431064605713, discriminator_loss=0.6141854524612427\n",
            "step 1790: generator_loss=0.792678952217102, discriminator_loss=0.6140888333320618\n",
            "step 1791: generator_loss=0.8110707998275757, discriminator_loss=0.607069730758667\n",
            "step 1792: generator_loss=0.8118374943733215, discriminator_loss=0.6060304641723633\n",
            "step 1793: generator_loss=0.8159065246582031, discriminator_loss=0.6037402153015137\n",
            "step 1794: generator_loss=0.8105467557907104, discriminator_loss=0.6045387983322144\n",
            "step 1795: generator_loss=0.8119734525680542, discriminator_loss=0.6024783849716187\n",
            "step 1796: generator_loss=0.808851957321167, discriminator_loss=0.6016748547554016\n",
            "step 1797: generator_loss=0.7973326444625854, discriminator_loss=0.6056298017501831\n",
            "step 1798: generator_loss=0.7989099025726318, discriminator_loss=0.6032025814056396\n",
            "step 1799: generator_loss=0.8148385286331177, discriminator_loss=0.5955591797828674\n",
            "step 1800: generator_loss=0.8092455267906189, discriminator_loss=0.5979604125022888\n",
            "step 1801: generator_loss=0.8157861232757568, discriminator_loss=0.594833493232727\n",
            "step 1802: generator_loss=0.8191225528717041, discriminator_loss=0.5914011597633362\n",
            "step 1803: generator_loss=0.8108221292495728, discriminator_loss=0.5943578481674194\n",
            "step 1804: generator_loss=0.8130793571472168, discriminator_loss=0.5911670923233032\n",
            "step 1805: generator_loss=0.8144916296005249, discriminator_loss=0.5879536271095276\n",
            "step 1806: generator_loss=0.8102236986160278, discriminator_loss=0.588375449180603\n",
            "step 1807: generator_loss=0.824262261390686, discriminator_loss=0.5817161798477173\n",
            "step 1808: generator_loss=0.8164148330688477, discriminator_loss=0.583358645439148\n",
            "step 1809: generator_loss=0.8076567649841309, discriminator_loss=0.5866221189498901\n",
            "step 1810: generator_loss=0.8146283626556396, discriminator_loss=0.5826955437660217\n",
            "step 1811: generator_loss=0.8344550132751465, discriminator_loss=0.5750448107719421\n",
            "step 1812: generator_loss=0.8246821165084839, discriminator_loss=0.5812877416610718\n",
            "step 1813: generator_loss=0.8299798965454102, discriminator_loss=0.5784765481948853\n",
            "step 1814: generator_loss=0.8516907691955566, discriminator_loss=0.5705387592315674\n",
            "step 1815: generator_loss=0.8380637168884277, discriminator_loss=0.5755670666694641\n",
            "step 1816: generator_loss=0.8364042639732361, discriminator_loss=0.5767731070518494\n",
            "step 1817: generator_loss=0.8551630973815918, discriminator_loss=0.569243311882019\n",
            "step 1818: generator_loss=0.8387547135353088, discriminator_loss=0.5740641355514526\n",
            "step 1819: generator_loss=0.8347358703613281, discriminator_loss=0.5748940706253052\n",
            "step 1820: generator_loss=0.828371524810791, discriminator_loss=0.5753297805786133\n",
            "step 1821: generator_loss=0.8478763103485107, discriminator_loss=0.5676317811012268\n",
            "step 1822: generator_loss=0.8401741981506348, discriminator_loss=0.569986879825592\n",
            "step 1823: generator_loss=0.8446387648582458, discriminator_loss=0.5675525069236755\n",
            "step 1824: generator_loss=0.8363953828811646, discriminator_loss=0.5701234340667725\n",
            "step 1825: generator_loss=0.8378065228462219, discriminator_loss=0.5697277784347534\n",
            "step 1826: generator_loss=0.8378259539604187, discriminator_loss=0.5684896111488342\n",
            "step 1827: generator_loss=0.8412439227104187, discriminator_loss=0.5677542686462402\n",
            "step 1828: generator_loss=0.8394731283187866, discriminator_loss=0.5668380856513977\n",
            "step 1829: generator_loss=0.843144416809082, discriminator_loss=0.5658955574035645\n",
            "step 1830: generator_loss=0.8357252478599548, discriminator_loss=0.5702232122421265\n",
            "step 1831: generator_loss=0.8333009481430054, discriminator_loss=0.5707715749740601\n",
            "step 1832: generator_loss=0.8404701948165894, discriminator_loss=0.5675843954086304\n",
            "step 1833: generator_loss=0.8405822515487671, discriminator_loss=0.5683149099349976\n",
            "step 1834: generator_loss=0.839003324508667, discriminator_loss=0.5684864521026611\n",
            "step 1835: generator_loss=0.841495156288147, discriminator_loss=0.5677863359451294\n",
            "step 1836: generator_loss=0.8331840634346008, discriminator_loss=0.570492148399353\n",
            "step 1837: generator_loss=0.8252816796302795, discriminator_loss=0.5726839303970337\n",
            "step 1838: generator_loss=0.8291220664978027, discriminator_loss=0.571557343006134\n",
            "step 1839: generator_loss=0.8302209377288818, discriminator_loss=0.5713046789169312\n",
            "step 1840: generator_loss=0.8288355469703674, discriminator_loss=0.5727598667144775\n",
            "step 1841: generator_loss=0.8182821869850159, discriminator_loss=0.5773855447769165\n",
            "step 1842: generator_loss=0.8190881013870239, discriminator_loss=0.5770103931427002\n",
            "step 1843: generator_loss=0.8269613981246948, discriminator_loss=0.5747032165527344\n",
            "step 1844: generator_loss=0.8250295519828796, discriminator_loss=0.5753337144851685\n",
            "step 1845: generator_loss=0.8155339956283569, discriminator_loss=0.5823476910591125\n",
            "step 1846: generator_loss=0.8190821409225464, discriminator_loss=0.5811597108840942\n",
            "step 1847: generator_loss=0.8193309307098389, discriminator_loss=0.5808365941047668\n",
            "step 1848: generator_loss=0.8117929100990295, discriminator_loss=0.5866513252258301\n",
            "step 1849: generator_loss=0.8118807077407837, discriminator_loss=0.5873045921325684\n",
            "step 1850: generator_loss=0.8090946078300476, discriminator_loss=0.5889760255813599\n",
            "step 1851: generator_loss=0.8033900260925293, discriminator_loss=0.592013955116272\n",
            "step 1852: generator_loss=0.7996500730514526, discriminator_loss=0.5908336639404297\n",
            "step 1853: generator_loss=0.8091999292373657, discriminator_loss=0.589076042175293\n",
            "step 1854: generator_loss=0.7989691495895386, discriminator_loss=0.5931326150894165\n",
            "step 1855: generator_loss=0.791782557964325, discriminator_loss=0.5955777764320374\n",
            "step 1856: generator_loss=0.7901673316955566, discriminator_loss=0.5973673462867737\n",
            "step 1857: generator_loss=0.7922072410583496, discriminator_loss=0.5980385541915894\n",
            "step 1858: generator_loss=0.7962127923965454, discriminator_loss=0.5975635051727295\n",
            "step 1859: generator_loss=0.7823569774627686, discriminator_loss=0.6032200455665588\n",
            "step 1860: generator_loss=0.7784172892570496, discriminator_loss=0.6063034534454346\n",
            "step 1861: generator_loss=0.7865396738052368, discriminator_loss=0.6049991250038147\n",
            "step 1862: generator_loss=0.7848605513572693, discriminator_loss=0.6053414940834045\n",
            "step 1863: generator_loss=0.7960942983627319, discriminator_loss=0.6026324033737183\n",
            "step 1864: generator_loss=0.7806403636932373, discriminator_loss=0.6078228950500488\n",
            "step 1865: generator_loss=0.7792708873748779, discriminator_loss=0.611190676689148\n",
            "step 1866: generator_loss=0.7854609489440918, discriminator_loss=0.6072290539741516\n",
            "step 1867: generator_loss=0.7865678071975708, discriminator_loss=0.6071165204048157\n",
            "step 1868: generator_loss=0.7872209548950195, discriminator_loss=0.6076377034187317\n",
            "step 1869: generator_loss=0.7766348123550415, discriminator_loss=0.6109477877616882\n",
            "step 1870: generator_loss=0.7842183113098145, discriminator_loss=0.6083723902702332\n",
            "step 1871: generator_loss=0.7844061851501465, discriminator_loss=0.6079984903335571\n",
            "step 1872: generator_loss=0.7767416834831238, discriminator_loss=0.6121060252189636\n",
            "step 1873: generator_loss=0.7668916583061218, discriminator_loss=0.6166864633560181\n",
            "step 1874: generator_loss=0.774416983127594, discriminator_loss=0.6148080825805664\n",
            "step 1875: generator_loss=0.7668602466583252, discriminator_loss=0.61821448802948\n",
            "step 1876: generator_loss=0.7657254934310913, discriminator_loss=0.6198708415031433\n",
            "step 1877: generator_loss=0.7808756828308105, discriminator_loss=0.6143701076507568\n",
            "step 1878: generator_loss=0.7738598585128784, discriminator_loss=0.616939902305603\n",
            "step 1879: generator_loss=0.777625560760498, discriminator_loss=0.6174309253692627\n",
            "step 1880: generator_loss=0.7737617492675781, discriminator_loss=0.6189002394676208\n",
            "step 1881: generator_loss=0.7658230662345886, discriminator_loss=0.6232620477676392\n",
            "step 1882: generator_loss=0.7689075469970703, discriminator_loss=0.620943009853363\n",
            "step 1883: generator_loss=0.770607590675354, discriminator_loss=0.6223964095115662\n",
            "step 1884: generator_loss=0.7693488001823425, discriminator_loss=0.623444676399231\n",
            "step 1885: generator_loss=0.7624651193618774, discriminator_loss=0.626192569732666\n",
            "step 1886: generator_loss=0.7675480246543884, discriminator_loss=0.6252322793006897\n",
            "step 1887: generator_loss=0.7681007981300354, discriminator_loss=0.6252334713935852\n",
            "step 1888: generator_loss=0.7698051929473877, discriminator_loss=0.6251029968261719\n",
            "step 1889: generator_loss=0.7644081115722656, discriminator_loss=0.6267014741897583\n",
            "step 1890: generator_loss=0.7714367508888245, discriminator_loss=0.625235915184021\n",
            "step 1891: generator_loss=0.7768786549568176, discriminator_loss=0.6235440969467163\n",
            "step 1892: generator_loss=0.7632718086242676, discriminator_loss=0.627914547920227\n",
            "step 1893: generator_loss=0.7630388736724854, discriminator_loss=0.6267843246459961\n",
            "step 1894: generator_loss=0.7623172998428345, discriminator_loss=0.6271549463272095\n",
            "step 1895: generator_loss=0.7566589117050171, discriminator_loss=0.629580020904541\n",
            "step 1896: generator_loss=0.7656240463256836, discriminator_loss=0.6259655952453613\n",
            "step 1897: generator_loss=0.7546458840370178, discriminator_loss=0.6294402480125427\n",
            "step 1898: generator_loss=0.7546161413192749, discriminator_loss=0.6297677755355835\n",
            "step 1899: generator_loss=0.7570122480392456, discriminator_loss=0.6286255121231079\n",
            "step 1900: generator_loss=0.7561256885528564, discriminator_loss=0.6299133896827698\n",
            "step 1901: generator_loss=0.7597517967224121, discriminator_loss=0.6283255815505981\n",
            "step 1902: generator_loss=0.7550044059753418, discriminator_loss=0.6309342384338379\n",
            "step 1903: generator_loss=0.759697437286377, discriminator_loss=0.6294517517089844\n",
            "step 1904: generator_loss=0.7574459314346313, discriminator_loss=0.6311017274856567\n",
            "step 1905: generator_loss=0.7568600177764893, discriminator_loss=0.6311286687850952\n",
            "step 1906: generator_loss=0.7663967609405518, discriminator_loss=0.6282649040222168\n",
            "step 1907: generator_loss=0.7634680867195129, discriminator_loss=0.6301684975624084\n",
            "step 1908: generator_loss=0.758827805519104, discriminator_loss=0.6310730576515198\n",
            "step 1909: generator_loss=0.7639960050582886, discriminator_loss=0.6300927400588989\n",
            "step 1910: generator_loss=0.7629967331886292, discriminator_loss=0.6300004124641418\n",
            "step 1911: generator_loss=0.7668812274932861, discriminator_loss=0.6272286176681519\n",
            "step 1912: generator_loss=0.7569478154182434, discriminator_loss=0.631643533706665\n",
            "step 1913: generator_loss=0.7579582929611206, discriminator_loss=0.630473256111145\n",
            "step 1914: generator_loss=0.756464421749115, discriminator_loss=0.6313037276268005\n",
            "step 1915: generator_loss=0.7612478733062744, discriminator_loss=0.6297699809074402\n",
            "step 1916: generator_loss=0.7646678686141968, discriminator_loss=0.628846287727356\n",
            "step 1917: generator_loss=0.7641086578369141, discriminator_loss=0.6297600269317627\n",
            "step 1918: generator_loss=0.7634433507919312, discriminator_loss=0.6307993531227112\n",
            "step 1919: generator_loss=0.7625585794448853, discriminator_loss=0.6312023401260376\n",
            "step 1920: generator_loss=0.7634640336036682, discriminator_loss=0.6317883133888245\n",
            "step 1921: generator_loss=0.7678865194320679, discriminator_loss=0.6290018558502197\n",
            "step 1922: generator_loss=0.7625907063484192, discriminator_loss=0.6306560039520264\n",
            "step 1923: generator_loss=0.7631523609161377, discriminator_loss=0.629797101020813\n",
            "step 1924: generator_loss=0.7636356353759766, discriminator_loss=0.6293505430221558\n",
            "step 1925: generator_loss=0.7665454745292664, discriminator_loss=0.6282405853271484\n",
            "step 1926: generator_loss=0.7593383193016052, discriminator_loss=0.630765974521637\n",
            "step 1927: generator_loss=0.7621212005615234, discriminator_loss=0.6292710304260254\n",
            "step 1928: generator_loss=0.7580109238624573, discriminator_loss=0.6316589713096619\n",
            "step 1929: generator_loss=0.7555553913116455, discriminator_loss=0.6325360536575317\n",
            "step 1930: generator_loss=0.7554982900619507, discriminator_loss=0.6331808567047119\n",
            "step 1931: generator_loss=0.7674980759620667, discriminator_loss=0.6260216236114502\n",
            "step 1932: generator_loss=0.7593926787376404, discriminator_loss=0.6307921409606934\n",
            "step 1933: generator_loss=0.7602552771568298, discriminator_loss=0.6310638785362244\n",
            "step 1934: generator_loss=0.7617471218109131, discriminator_loss=0.630109429359436\n",
            "step 1935: generator_loss=0.765099287033081, discriminator_loss=0.6291351318359375\n",
            "step 1936: generator_loss=0.7667673826217651, discriminator_loss=0.6284646987915039\n",
            "step 1937: generator_loss=0.7607322931289673, discriminator_loss=0.6307485103607178\n",
            "step 1938: generator_loss=0.7668525576591492, discriminator_loss=0.6278928518295288\n",
            "step 1939: generator_loss=0.7648055553436279, discriminator_loss=0.6283137202262878\n",
            "step 1940: generator_loss=0.7606889009475708, discriminator_loss=0.6310682892799377\n",
            "step 1941: generator_loss=0.7613153457641602, discriminator_loss=0.6288840770721436\n",
            "step 1942: generator_loss=0.7580230236053467, discriminator_loss=0.6300991773605347\n",
            "step 1943: generator_loss=0.7583174705505371, discriminator_loss=0.6311510801315308\n",
            "step 1944: generator_loss=0.765333890914917, discriminator_loss=0.62850022315979\n",
            "step 1945: generator_loss=0.7600840330123901, discriminator_loss=0.6304475665092468\n",
            "step 1946: generator_loss=0.7612611055374146, discriminator_loss=0.62953782081604\n",
            "step 1947: generator_loss=0.7604578137397766, discriminator_loss=0.6311455965042114\n",
            "step 1948: generator_loss=0.7593675851821899, discriminator_loss=0.6312450170516968\n",
            "step 1949: generator_loss=0.7613768577575684, discriminator_loss=0.6315186023712158\n",
            "step 1950: generator_loss=0.7636135816574097, discriminator_loss=0.6297124624252319\n",
            "step 1951: generator_loss=0.7598819732666016, discriminator_loss=0.6320686340332031\n",
            "step 1952: generator_loss=0.7629750967025757, discriminator_loss=0.630439043045044\n",
            "step 1953: generator_loss=0.7694540023803711, discriminator_loss=0.6266697645187378\n",
            "step 1954: generator_loss=0.7594311833381653, discriminator_loss=0.6312639713287354\n",
            "step 1955: generator_loss=0.7620401978492737, discriminator_loss=0.6294276714324951\n",
            "step 1956: generator_loss=0.7623391151428223, discriminator_loss=0.6290565133094788\n",
            "step 1957: generator_loss=0.763795793056488, discriminator_loss=0.6289779543876648\n",
            "step 1958: generator_loss=0.7621721029281616, discriminator_loss=0.6294959187507629\n",
            "step 1959: generator_loss=0.7598960399627686, discriminator_loss=0.6304977536201477\n",
            "step 1960: generator_loss=0.7589583396911621, discriminator_loss=0.6303601861000061\n",
            "step 1961: generator_loss=0.7561565637588501, discriminator_loss=0.6319026947021484\n",
            "step 1962: generator_loss=0.7571283578872681, discriminator_loss=0.6320033073425293\n",
            "step 1963: generator_loss=0.7596110105514526, discriminator_loss=0.6312350630760193\n",
            "step 1964: generator_loss=0.7618260383605957, discriminator_loss=0.6296265125274658\n",
            "step 1965: generator_loss=0.7572246789932251, discriminator_loss=0.632581353187561\n",
            "step 1966: generator_loss=0.761145830154419, discriminator_loss=0.6317042112350464\n",
            "step 1967: generator_loss=0.759181797504425, discriminator_loss=0.6325919032096863\n",
            "step 1968: generator_loss=0.763522207736969, discriminator_loss=0.6307166814804077\n",
            "step 1969: generator_loss=0.7623857855796814, discriminator_loss=0.6314113736152649\n",
            "step 1970: generator_loss=0.7610465288162231, discriminator_loss=0.6311765313148499\n",
            "step 1971: generator_loss=0.7606604695320129, discriminator_loss=0.6315906047821045\n",
            "step 1972: generator_loss=0.762758731842041, discriminator_loss=0.6305719614028931\n",
            "step 1973: generator_loss=0.7578354477882385, discriminator_loss=0.6332586407661438\n",
            "step 1974: generator_loss=0.7561768889427185, discriminator_loss=0.6325922012329102\n",
            "step 1975: generator_loss=0.7598005533218384, discriminator_loss=0.6322136521339417\n",
            "step 1976: generator_loss=0.7597201466560364, discriminator_loss=0.6319774389266968\n",
            "step 1977: generator_loss=0.7635047435760498, discriminator_loss=0.6318420171737671\n",
            "step 1978: generator_loss=0.7590678930282593, discriminator_loss=0.6328116059303284\n",
            "step 1979: generator_loss=0.7572361826896667, discriminator_loss=0.632185697555542\n",
            "step 1980: generator_loss=0.7556909918785095, discriminator_loss=0.6342566609382629\n",
            "step 1981: generator_loss=0.7506558299064636, discriminator_loss=0.6362863779067993\n",
            "step 1982: generator_loss=0.7606421709060669, discriminator_loss=0.6324042081832886\n",
            "step 1983: generator_loss=0.7580288648605347, discriminator_loss=0.6338679790496826\n",
            "step 1984: generator_loss=0.7624780535697937, discriminator_loss=0.6328483819961548\n",
            "step 1985: generator_loss=0.7617955207824707, discriminator_loss=0.6330937743186951\n",
            "step 1986: generator_loss=0.7537554502487183, discriminator_loss=0.6365088224411011\n",
            "step 1987: generator_loss=0.7603777050971985, discriminator_loss=0.6328006386756897\n",
            "step 1988: generator_loss=0.755278468132019, discriminator_loss=0.6346731185913086\n",
            "step 1989: generator_loss=0.7552555799484253, discriminator_loss=0.6341387033462524\n",
            "step 1990: generator_loss=0.7525734305381775, discriminator_loss=0.6351810097694397\n",
            "step 1991: generator_loss=0.7505366802215576, discriminator_loss=0.6360219120979309\n",
            "step 1992: generator_loss=0.7589173316955566, discriminator_loss=0.6336787343025208\n",
            "step 1993: generator_loss=0.7568237781524658, discriminator_loss=0.6346868276596069\n",
            "step 1994: generator_loss=0.7533884644508362, discriminator_loss=0.6365234851837158\n",
            "step 1995: generator_loss=0.7584896087646484, discriminator_loss=0.6345614194869995\n",
            "step 1996: generator_loss=0.7567981481552124, discriminator_loss=0.6357305645942688\n",
            "step 1997: generator_loss=0.7510598301887512, discriminator_loss=0.6384109854698181\n",
            "step 1998: generator_loss=0.7542558312416077, discriminator_loss=0.6369922757148743\n",
            "step 1999: generator_loss=0.7538578510284424, discriminator_loss=0.6369420886039734\n",
            "step 2000: generator_loss=0.756037175655365, discriminator_loss=0.6354647874832153\n",
            "step 2001: generator_loss=0.7556408643722534, discriminator_loss=0.6364943981170654\n",
            "step 2002: generator_loss=0.7521174550056458, discriminator_loss=0.637412428855896\n",
            "step 2003: generator_loss=0.7597174644470215, discriminator_loss=0.6339507102966309\n",
            "step 2004: generator_loss=0.7514703273773193, discriminator_loss=0.6375223994255066\n",
            "step 2005: generator_loss=0.7494853734970093, discriminator_loss=0.6371774077415466\n",
            "step 2006: generator_loss=0.7557283639907837, discriminator_loss=0.6351395845413208\n",
            "step 2007: generator_loss=0.7485843300819397, discriminator_loss=0.6381791830062866\n",
            "step 2008: generator_loss=0.7535037994384766, discriminator_loss=0.6371764540672302\n",
            "step 2009: generator_loss=0.747629702091217, discriminator_loss=0.6398934125900269\n",
            "step 2010: generator_loss=0.7537460923194885, discriminator_loss=0.6377942562103271\n",
            "step 2011: generator_loss=0.7531372308731079, discriminator_loss=0.6377825736999512\n",
            "step 2012: generator_loss=0.7508037090301514, discriminator_loss=0.6398064494132996\n",
            "step 2013: generator_loss=0.7563656568527222, discriminator_loss=0.6375200748443604\n",
            "step 2014: generator_loss=0.75282222032547, discriminator_loss=0.638391375541687\n",
            "step 2015: generator_loss=0.7558857202529907, discriminator_loss=0.6372925043106079\n",
            "step 2016: generator_loss=0.7520186901092529, discriminator_loss=0.6389798521995544\n",
            "step 2017: generator_loss=0.752111554145813, discriminator_loss=0.6383453011512756\n",
            "step 2018: generator_loss=0.7536212205886841, discriminator_loss=0.6366358995437622\n",
            "step 2019: generator_loss=0.7502796649932861, discriminator_loss=0.6385689973831177\n",
            "step 2020: generator_loss=0.7525303959846497, discriminator_loss=0.6381533741950989\n",
            "step 2021: generator_loss=0.7512953281402588, discriminator_loss=0.638108491897583\n",
            "step 2022: generator_loss=0.7505085468292236, discriminator_loss=0.6391213536262512\n",
            "step 2023: generator_loss=0.7504876852035522, discriminator_loss=0.6386063694953918\n",
            "step 2024: generator_loss=0.7546307444572449, discriminator_loss=0.6387890577316284\n",
            "step 2025: generator_loss=0.7513295412063599, discriminator_loss=0.6398224234580994\n",
            "step 2026: generator_loss=0.7541497945785522, discriminator_loss=0.6388605833053589\n",
            "step 2027: generator_loss=0.7576062679290771, discriminator_loss=0.6370186805725098\n",
            "step 2028: generator_loss=0.7525757551193237, discriminator_loss=0.6391897201538086\n",
            "step 2029: generator_loss=0.7474352121353149, discriminator_loss=0.6413540244102478\n",
            "step 2030: generator_loss=0.7583861947059631, discriminator_loss=0.6361467838287354\n",
            "step 2031: generator_loss=0.7517713904380798, discriminator_loss=0.6389976739883423\n",
            "step 2032: generator_loss=0.7494391202926636, discriminator_loss=0.6387461423873901\n",
            "step 2033: generator_loss=0.7493518590927124, discriminator_loss=0.6382167339324951\n",
            "step 2034: generator_loss=0.7499805688858032, discriminator_loss=0.638771653175354\n",
            "step 2035: generator_loss=0.7464689612388611, discriminator_loss=0.6404631733894348\n",
            "step 2036: generator_loss=0.7532333135604858, discriminator_loss=0.6376376748085022\n",
            "step 2037: generator_loss=0.7461275458335876, discriminator_loss=0.641745388507843\n",
            "step 2038: generator_loss=0.749835193157196, discriminator_loss=0.6394616961479187\n",
            "step 2039: generator_loss=0.748900294303894, discriminator_loss=0.6404807567596436\n",
            "step 2040: generator_loss=0.7507989406585693, discriminator_loss=0.6397236585617065\n",
            "step 2041: generator_loss=0.7541007995605469, discriminator_loss=0.6380820870399475\n",
            "step 2042: generator_loss=0.75612473487854, discriminator_loss=0.6369693279266357\n",
            "step 2043: generator_loss=0.7526642084121704, discriminator_loss=0.638036847114563\n",
            "step 2044: generator_loss=0.7494139671325684, discriminator_loss=0.6384547352790833\n",
            "step 2045: generator_loss=0.7453727722167969, discriminator_loss=0.6405153870582581\n",
            "step 2046: generator_loss=0.7446329593658447, discriminator_loss=0.6413919925689697\n",
            "step 2047: generator_loss=0.7453644871711731, discriminator_loss=0.6398127675056458\n",
            "step 2048: generator_loss=0.7493230104446411, discriminator_loss=0.6387145519256592\n",
            "step 2049: generator_loss=0.74688720703125, discriminator_loss=0.6397509574890137\n",
            "step 2050: generator_loss=0.7541818618774414, discriminator_loss=0.6368106603622437\n",
            "step 2051: generator_loss=0.7465742826461792, discriminator_loss=0.6410956382751465\n",
            "step 2052: generator_loss=0.7590924501419067, discriminator_loss=0.6359588503837585\n",
            "step 2053: generator_loss=0.7545956373214722, discriminator_loss=0.6372538805007935\n",
            "step 2054: generator_loss=0.7483664155006409, discriminator_loss=0.6411104202270508\n",
            "step 2055: generator_loss=0.7538647055625916, discriminator_loss=0.638972282409668\n",
            "step 2056: generator_loss=0.7578638195991516, discriminator_loss=0.637942373752594\n",
            "step 2057: generator_loss=0.7550204992294312, discriminator_loss=0.6387895345687866\n",
            "step 2058: generator_loss=0.7492079734802246, discriminator_loss=0.6403922438621521\n",
            "step 2059: generator_loss=0.7530080080032349, discriminator_loss=0.6386547088623047\n",
            "step 2060: generator_loss=0.7544006109237671, discriminator_loss=0.6383949518203735\n",
            "step 2061: generator_loss=0.7530751824378967, discriminator_loss=0.6368821859359741\n",
            "step 2062: generator_loss=0.7485499382019043, discriminator_loss=0.6375963091850281\n",
            "step 2063: generator_loss=0.7477928400039673, discriminator_loss=0.6379520893096924\n",
            "step 2064: generator_loss=0.751000702381134, discriminator_loss=0.6364448070526123\n",
            "step 2065: generator_loss=0.7513687014579773, discriminator_loss=0.6363922357559204\n",
            "step 2066: generator_loss=0.7508609890937805, discriminator_loss=0.6368800401687622\n",
            "step 2067: generator_loss=0.7558852434158325, discriminator_loss=0.6353363990783691\n",
            "step 2068: generator_loss=0.7522413730621338, discriminator_loss=0.6372126340866089\n",
            "step 2069: generator_loss=0.7533935308456421, discriminator_loss=0.6372385025024414\n",
            "step 2070: generator_loss=0.7548080682754517, discriminator_loss=0.6370210647583008\n",
            "step 2071: generator_loss=0.7526484727859497, discriminator_loss=0.6395153999328613\n",
            "step 2072: generator_loss=0.75230872631073, discriminator_loss=0.638379693031311\n",
            "step 2073: generator_loss=0.7533893585205078, discriminator_loss=0.6376721262931824\n",
            "step 2074: generator_loss=0.7525885105133057, discriminator_loss=0.6383202075958252\n",
            "step 2075: generator_loss=0.7522245049476624, discriminator_loss=0.6378669738769531\n",
            "step 2076: generator_loss=0.7586123943328857, discriminator_loss=0.6348633766174316\n",
            "step 2077: generator_loss=0.749853253364563, discriminator_loss=0.6388606429100037\n",
            "step 2078: generator_loss=0.7547299265861511, discriminator_loss=0.6362168788909912\n",
            "step 2079: generator_loss=0.761852502822876, discriminator_loss=0.6322147846221924\n",
            "step 2080: generator_loss=0.7530416250228882, discriminator_loss=0.6352778673171997\n",
            "step 2081: generator_loss=0.7569586038589478, discriminator_loss=0.6332768201828003\n",
            "step 2082: generator_loss=0.7555395364761353, discriminator_loss=0.6343201994895935\n",
            "step 2083: generator_loss=0.7539929151535034, discriminator_loss=0.6361879706382751\n",
            "step 2084: generator_loss=0.7601103782653809, discriminator_loss=0.6341172456741333\n",
            "step 2085: generator_loss=0.7610159516334534, discriminator_loss=0.6339683532714844\n",
            "step 2086: generator_loss=0.7616605758666992, discriminator_loss=0.632327675819397\n",
            "step 2087: generator_loss=0.7609819173812866, discriminator_loss=0.6337329149246216\n",
            "step 2088: generator_loss=0.7671597003936768, discriminator_loss=0.6299358606338501\n",
            "step 2089: generator_loss=0.7602118253707886, discriminator_loss=0.6335362792015076\n",
            "step 2090: generator_loss=0.7641300559043884, discriminator_loss=0.6307848691940308\n",
            "step 2091: generator_loss=0.7621306777000427, discriminator_loss=0.6308857202529907\n",
            "step 2092: generator_loss=0.7580159902572632, discriminator_loss=0.6321495771408081\n",
            "step 2093: generator_loss=0.751178503036499, discriminator_loss=0.6347962617874146\n",
            "step 2094: generator_loss=0.7605858445167542, discriminator_loss=0.6310616731643677\n",
            "step 2095: generator_loss=0.7585241198539734, discriminator_loss=0.6307518482208252\n",
            "step 2096: generator_loss=0.7612835168838501, discriminator_loss=0.6292558908462524\n",
            "step 2097: generator_loss=0.754953145980835, discriminator_loss=0.6322106719017029\n",
            "step 2098: generator_loss=0.7554712891578674, discriminator_loss=0.6319420337677002\n",
            "step 2099: generator_loss=0.7519442439079285, discriminator_loss=0.6344426870346069\n",
            "step 2100: generator_loss=0.7669869661331177, discriminator_loss=0.6287089586257935\n",
            "step 2101: generator_loss=0.7637288570404053, discriminator_loss=0.630994975566864\n",
            "step 2102: generator_loss=0.7626726031303406, discriminator_loss=0.6316034197807312\n",
            "step 2103: generator_loss=0.760250449180603, discriminator_loss=0.6317830681800842\n",
            "step 2104: generator_loss=0.7603307962417603, discriminator_loss=0.6323689222335815\n",
            "step 2105: generator_loss=0.7633811235427856, discriminator_loss=0.6309406161308289\n",
            "step 2106: generator_loss=0.751710832118988, discriminator_loss=0.6353874206542969\n",
            "step 2107: generator_loss=0.7613036036491394, discriminator_loss=0.6309672594070435\n",
            "step 2108: generator_loss=0.7583210468292236, discriminator_loss=0.6316978335380554\n",
            "step 2109: generator_loss=0.7609356045722961, discriminator_loss=0.6303693652153015\n",
            "step 2110: generator_loss=0.7557868957519531, discriminator_loss=0.6321547627449036\n",
            "step 2111: generator_loss=0.7572797536849976, discriminator_loss=0.6314232349395752\n",
            "step 2112: generator_loss=0.7554650902748108, discriminator_loss=0.6326852440834045\n",
            "step 2113: generator_loss=0.7561765909194946, discriminator_loss=0.6320065259933472\n",
            "step 2114: generator_loss=0.7598576545715332, discriminator_loss=0.6313704252243042\n",
            "step 2115: generator_loss=0.7567232251167297, discriminator_loss=0.6340794563293457\n",
            "step 2116: generator_loss=0.7538324594497681, discriminator_loss=0.6341597437858582\n",
            "step 2117: generator_loss=0.7574094533920288, discriminator_loss=0.634399950504303\n",
            "step 2118: generator_loss=0.7572277784347534, discriminator_loss=0.6351156234741211\n",
            "step 2119: generator_loss=0.7536256313323975, discriminator_loss=0.6369448900222778\n",
            "step 2120: generator_loss=0.7566639184951782, discriminator_loss=0.636417806148529\n",
            "step 2121: generator_loss=0.7551808953285217, discriminator_loss=0.63737553358078\n",
            "step 2122: generator_loss=0.7585629224777222, discriminator_loss=0.635839581489563\n",
            "step 2123: generator_loss=0.7582463622093201, discriminator_loss=0.6364902257919312\n",
            "step 2124: generator_loss=0.7583327293395996, discriminator_loss=0.6357169151306152\n",
            "step 2125: generator_loss=0.7615764141082764, discriminator_loss=0.6335833072662354\n",
            "step 2126: generator_loss=0.7524740099906921, discriminator_loss=0.6363891959190369\n",
            "step 2127: generator_loss=0.7439166307449341, discriminator_loss=0.6398831009864807\n",
            "step 2128: generator_loss=0.7421457767486572, discriminator_loss=0.6398732662200928\n",
            "step 2129: generator_loss=0.7473874092102051, discriminator_loss=0.6384985446929932\n",
            "step 2130: generator_loss=0.7446831464767456, discriminator_loss=0.6404517889022827\n",
            "step 2131: generator_loss=0.7467167377471924, discriminator_loss=0.641136646270752\n",
            "step 2132: generator_loss=0.7454794645309448, discriminator_loss=0.641437292098999\n",
            "step 2133: generator_loss=0.7502856254577637, discriminator_loss=0.641461968421936\n",
            "step 2134: generator_loss=0.7557180523872375, discriminator_loss=0.6396893262863159\n",
            "step 2135: generator_loss=0.7456467747688293, discriminator_loss=0.6433199644088745\n",
            "step 2136: generator_loss=0.754462718963623, discriminator_loss=0.6385321617126465\n",
            "step 2137: generator_loss=0.7427888512611389, discriminator_loss=0.6428730487823486\n",
            "step 2138: generator_loss=0.7366459965705872, discriminator_loss=0.6454426050186157\n",
            "step 2139: generator_loss=0.740584135055542, discriminator_loss=0.6424241065979004\n",
            "step 2140: generator_loss=0.740342915058136, discriminator_loss=0.6437679529190063\n",
            "step 2141: generator_loss=0.7404297590255737, discriminator_loss=0.6437814831733704\n",
            "step 2142: generator_loss=0.7387841939926147, discriminator_loss=0.6451351642608643\n",
            "step 2143: generator_loss=0.7388939261436462, discriminator_loss=0.6455990076065063\n",
            "step 2144: generator_loss=0.7469825744628906, discriminator_loss=0.6449342966079712\n",
            "step 2145: generator_loss=0.7448915839195251, discriminator_loss=0.6447709202766418\n",
            "step 2146: generator_loss=0.7500951290130615, discriminator_loss=0.6436949372291565\n",
            "step 2147: generator_loss=0.7530871033668518, discriminator_loss=0.6433178186416626\n",
            "step 2148: generator_loss=0.7440463304519653, discriminator_loss=0.6474781036376953\n",
            "step 2149: generator_loss=0.745911717414856, discriminator_loss=0.6468678712844849\n",
            "step 2150: generator_loss=0.740341305732727, discriminator_loss=0.6493874788284302\n",
            "step 2151: generator_loss=0.7491366863250732, discriminator_loss=0.645622193813324\n",
            "step 2152: generator_loss=0.7409098148345947, discriminator_loss=0.6485680341720581\n",
            "step 2153: generator_loss=0.7372074127197266, discriminator_loss=0.650309145450592\n",
            "step 2154: generator_loss=0.7320523262023926, discriminator_loss=0.6535053253173828\n",
            "step 2155: generator_loss=0.7350286245346069, discriminator_loss=0.652588427066803\n",
            "step 2156: generator_loss=0.742121160030365, discriminator_loss=0.6487776637077332\n",
            "step 2157: generator_loss=0.7368081212043762, discriminator_loss=0.6527359485626221\n",
            "step 2158: generator_loss=0.7371089458465576, discriminator_loss=0.6525750756263733\n",
            "step 2159: generator_loss=0.7358725070953369, discriminator_loss=0.6532317399978638\n",
            "step 2160: generator_loss=0.7347772121429443, discriminator_loss=0.6537668704986572\n",
            "step 2161: generator_loss=0.7356753349304199, discriminator_loss=0.6534386873245239\n",
            "step 2162: generator_loss=0.7289587259292603, discriminator_loss=0.6571149826049805\n",
            "step 2163: generator_loss=0.7334550619125366, discriminator_loss=0.6558029055595398\n",
            "step 2164: generator_loss=0.7273959517478943, discriminator_loss=0.657685399055481\n",
            "step 2165: generator_loss=0.7247439622879028, discriminator_loss=0.658424973487854\n",
            "step 2166: generator_loss=0.7220816612243652, discriminator_loss=0.6589460372924805\n",
            "step 2167: generator_loss=0.7252531051635742, discriminator_loss=0.6581566333770752\n",
            "step 2168: generator_loss=0.7245962619781494, discriminator_loss=0.6592398881912231\n",
            "step 2169: generator_loss=0.728234589099884, discriminator_loss=0.6570531129837036\n",
            "step 2170: generator_loss=0.7278870344161987, discriminator_loss=0.6581035852432251\n",
            "step 2171: generator_loss=0.7261326313018799, discriminator_loss=0.6583358645439148\n",
            "step 2172: generator_loss=0.7285696864128113, discriminator_loss=0.6583563089370728\n",
            "step 2173: generator_loss=0.7254456281661987, discriminator_loss=0.6605440974235535\n",
            "step 2174: generator_loss=0.7196787595748901, discriminator_loss=0.6652315258979797\n",
            "step 2175: generator_loss=0.7217766046524048, discriminator_loss=0.6639302372932434\n",
            "step 2176: generator_loss=0.7329121828079224, discriminator_loss=0.6581542491912842\n",
            "step 2177: generator_loss=0.7326868772506714, discriminator_loss=0.6589508056640625\n",
            "step 2178: generator_loss=0.7253156900405884, discriminator_loss=0.6628300547599792\n",
            "step 2179: generator_loss=0.7218949794769287, discriminator_loss=0.6638553142547607\n",
            "step 2180: generator_loss=0.7192551493644714, discriminator_loss=0.6637859344482422\n",
            "step 2181: generator_loss=0.7212725877761841, discriminator_loss=0.6640986800193787\n",
            "step 2182: generator_loss=0.7220525741577148, discriminator_loss=0.6626203656196594\n",
            "step 2183: generator_loss=0.7223681807518005, discriminator_loss=0.6625462770462036\n",
            "step 2184: generator_loss=0.7214560508728027, discriminator_loss=0.6640989780426025\n",
            "step 2185: generator_loss=0.7227486371994019, discriminator_loss=0.6636646389961243\n",
            "step 2186: generator_loss=0.7185829281806946, discriminator_loss=0.6659746170043945\n",
            "step 2187: generator_loss=0.7174654006958008, discriminator_loss=0.6655569672584534\n",
            "step 2188: generator_loss=0.7156280279159546, discriminator_loss=0.6666492819786072\n",
            "step 2189: generator_loss=0.7131012678146362, discriminator_loss=0.6666414141654968\n",
            "step 2190: generator_loss=0.7111443877220154, discriminator_loss=0.6675302982330322\n",
            "step 2191: generator_loss=0.7149480581283569, discriminator_loss=0.6671208143234253\n",
            "step 2192: generator_loss=0.7165887355804443, discriminator_loss=0.6655211448669434\n",
            "step 2193: generator_loss=0.7181602120399475, discriminator_loss=0.665482759475708\n",
            "step 2194: generator_loss=0.7225194573402405, discriminator_loss=0.6636241674423218\n",
            "step 2195: generator_loss=0.7247804999351501, discriminator_loss=0.6630938649177551\n",
            "step 2196: generator_loss=0.7246976494789124, discriminator_loss=0.6632094383239746\n",
            "step 2197: generator_loss=0.7290781736373901, discriminator_loss=0.6623129844665527\n",
            "step 2198: generator_loss=0.7209031581878662, discriminator_loss=0.6665396690368652\n",
            "step 2199: generator_loss=0.721732497215271, discriminator_loss=0.667233943939209\n",
            "step 2200: generator_loss=0.7265137434005737, discriminator_loss=0.6650838851928711\n",
            "step 2201: generator_loss=0.7197023034095764, discriminator_loss=0.6681439876556396\n",
            "step 2202: generator_loss=0.7222987413406372, discriminator_loss=0.6668511033058167\n",
            "step 2203: generator_loss=0.7211830019950867, discriminator_loss=0.6687442064285278\n",
            "step 2204: generator_loss=0.7251236438751221, discriminator_loss=0.6649991273880005\n",
            "step 2205: generator_loss=0.7232825756072998, discriminator_loss=0.6668644547462463\n",
            "step 2206: generator_loss=0.716825544834137, discriminator_loss=0.6702733039855957\n",
            "step 2207: generator_loss=0.7251055240631104, discriminator_loss=0.6657538414001465\n",
            "step 2208: generator_loss=0.7295435667037964, discriminator_loss=0.6650147438049316\n",
            "step 2209: generator_loss=0.7259352207183838, discriminator_loss=0.6675345301628113\n",
            "step 2210: generator_loss=0.7302172183990479, discriminator_loss=0.6651949882507324\n",
            "step 2211: generator_loss=0.7273011803627014, discriminator_loss=0.6671373844146729\n",
            "step 2212: generator_loss=0.725187361240387, discriminator_loss=0.66705721616745\n",
            "step 2213: generator_loss=0.7240932583808899, discriminator_loss=0.6660878658294678\n",
            "step 2214: generator_loss=0.7196462154388428, discriminator_loss=0.6675615310668945\n",
            "step 2215: generator_loss=0.7132164239883423, discriminator_loss=0.6687788367271423\n",
            "step 2216: generator_loss=0.7119715213775635, discriminator_loss=0.6690806746482849\n",
            "step 2217: generator_loss=0.7091376185417175, discriminator_loss=0.6693960428237915\n",
            "step 2218: generator_loss=0.7082067728042603, discriminator_loss=0.668856143951416\n",
            "step 2219: generator_loss=0.7028599977493286, discriminator_loss=0.6706662178039551\n",
            "step 2220: generator_loss=0.7058761119842529, discriminator_loss=0.6703572273254395\n",
            "step 2221: generator_loss=0.712925136089325, discriminator_loss=0.666709303855896\n",
            "step 2222: generator_loss=0.7158344984054565, discriminator_loss=0.6660239696502686\n",
            "step 2223: generator_loss=0.7214915752410889, discriminator_loss=0.6644861698150635\n",
            "step 2224: generator_loss=0.7235404849052429, discriminator_loss=0.6651700139045715\n",
            "step 2225: generator_loss=0.7260371446609497, discriminator_loss=0.6657817363739014\n",
            "step 2226: generator_loss=0.734569787979126, discriminator_loss=0.663567841053009\n",
            "step 2227: generator_loss=0.7319334745407104, discriminator_loss=0.6685233116149902\n",
            "step 2228: generator_loss=0.734870195388794, discriminator_loss=0.6681863069534302\n",
            "step 2229: generator_loss=0.7382460236549377, discriminator_loss=0.6680200695991516\n",
            "step 2230: generator_loss=0.7317873239517212, discriminator_loss=0.6699024438858032\n",
            "step 2231: generator_loss=0.7294425964355469, discriminator_loss=0.6712186336517334\n",
            "step 2232: generator_loss=0.7298779487609863, discriminator_loss=0.6701177358627319\n",
            "step 2233: generator_loss=0.7270472645759583, discriminator_loss=0.6704708337783813\n",
            "step 2234: generator_loss=0.7282013893127441, discriminator_loss=0.6690431833267212\n",
            "step 2235: generator_loss=0.7288639545440674, discriminator_loss=0.6669045686721802\n",
            "step 2236: generator_loss=0.7244728803634644, discriminator_loss=0.6675158739089966\n",
            "step 2237: generator_loss=0.7218241691589355, discriminator_loss=0.6672480702400208\n",
            "step 2238: generator_loss=0.7206772565841675, discriminator_loss=0.667098879814148\n",
            "step 2239: generator_loss=0.7258334159851074, discriminator_loss=0.6655672788619995\n",
            "step 2240: generator_loss=0.7300664782524109, discriminator_loss=0.6634461283683777\n",
            "step 2241: generator_loss=0.7259840965270996, discriminator_loss=0.6662474870681763\n",
            "step 2242: generator_loss=0.7359015941619873, discriminator_loss=0.6615216732025146\n",
            "step 2243: generator_loss=0.7302030324935913, discriminator_loss=0.6648836135864258\n",
            "step 2244: generator_loss=0.7268716096878052, discriminator_loss=0.6659568548202515\n",
            "step 2245: generator_loss=0.7196194529533386, discriminator_loss=0.6685856580734253\n",
            "step 2246: generator_loss=0.7229305505752563, discriminator_loss=0.665542483329773\n",
            "step 2247: generator_loss=0.720465362071991, discriminator_loss=0.6656984090805054\n",
            "step 2248: generator_loss=0.7107906341552734, discriminator_loss=0.6698454022407532\n",
            "step 2249: generator_loss=0.7117199897766113, discriminator_loss=0.6684617400169373\n",
            "step 2250: generator_loss=0.7019038200378418, discriminator_loss=0.672153115272522\n",
            "step 2251: generator_loss=0.705883264541626, discriminator_loss=0.6687608957290649\n",
            "step 2252: generator_loss=0.7059580087661743, discriminator_loss=0.6687182188034058\n",
            "step 2253: generator_loss=0.707640528678894, discriminator_loss=0.6679641008377075\n",
            "step 2254: generator_loss=0.7150528430938721, discriminator_loss=0.6650406122207642\n",
            "step 2255: generator_loss=0.7110576033592224, discriminator_loss=0.6677776575088501\n",
            "step 2256: generator_loss=0.7197780609130859, discriminator_loss=0.6645220518112183\n",
            "step 2257: generator_loss=0.7257137298583984, discriminator_loss=0.662319004535675\n",
            "step 2258: generator_loss=0.7259600758552551, discriminator_loss=0.6634719967842102\n",
            "step 2259: generator_loss=0.7267053723335266, discriminator_loss=0.6648932695388794\n",
            "step 2260: generator_loss=0.7299767732620239, discriminator_loss=0.6644313931465149\n",
            "step 2261: generator_loss=0.7335758805274963, discriminator_loss=0.6645691990852356\n",
            "step 2262: generator_loss=0.7284643650054932, discriminator_loss=0.6671962738037109\n",
            "step 2263: generator_loss=0.7365783452987671, discriminator_loss=0.6638848781585693\n",
            "step 2264: generator_loss=0.73084557056427, discriminator_loss=0.667676568031311\n",
            "step 2265: generator_loss=0.7309927940368652, discriminator_loss=0.6669167876243591\n",
            "step 2266: generator_loss=0.7350537776947021, discriminator_loss=0.6637906432151794\n",
            "step 2267: generator_loss=0.729328453540802, discriminator_loss=0.6663792133331299\n",
            "step 2268: generator_loss=0.7278797626495361, discriminator_loss=0.6655260324478149\n",
            "step 2269: generator_loss=0.7285206317901611, discriminator_loss=0.6648023128509521\n",
            "step 2270: generator_loss=0.7245175838470459, discriminator_loss=0.6655260324478149\n",
            "step 2271: generator_loss=0.7199527621269226, discriminator_loss=0.6670226454734802\n",
            "step 2272: generator_loss=0.7221522331237793, discriminator_loss=0.6648977398872375\n",
            "step 2273: generator_loss=0.721623420715332, discriminator_loss=0.6637705564498901\n",
            "step 2274: generator_loss=0.7191218137741089, discriminator_loss=0.6649993658065796\n",
            "step 2275: generator_loss=0.7200101613998413, discriminator_loss=0.6633354425430298\n",
            "step 2276: generator_loss=0.7145084738731384, discriminator_loss=0.6654676198959351\n",
            "step 2277: generator_loss=0.7166657447814941, discriminator_loss=0.6635804176330566\n",
            "step 2278: generator_loss=0.717953622341156, discriminator_loss=0.6608685851097107\n",
            "step 2279: generator_loss=0.7152019739151001, discriminator_loss=0.6624672412872314\n",
            "step 2280: generator_loss=0.7161072492599487, discriminator_loss=0.6602842211723328\n",
            "step 2281: generator_loss=0.7152889370918274, discriminator_loss=0.660622239112854\n",
            "step 2282: generator_loss=0.7153345346450806, discriminator_loss=0.6601177453994751\n",
            "step 2283: generator_loss=0.7181426882743835, discriminator_loss=0.6592934131622314\n",
            "step 2284: generator_loss=0.7182505130767822, discriminator_loss=0.659666121006012\n",
            "step 2285: generator_loss=0.7179163694381714, discriminator_loss=0.661400556564331\n",
            "step 2286: generator_loss=0.7222287654876709, discriminator_loss=0.659959077835083\n",
            "step 2287: generator_loss=0.730867862701416, discriminator_loss=0.656991183757782\n",
            "step 2288: generator_loss=0.7354977130889893, discriminator_loss=0.6566059589385986\n",
            "step 2289: generator_loss=0.7328052520751953, discriminator_loss=0.659641683101654\n",
            "step 2290: generator_loss=0.7350268363952637, discriminator_loss=0.6598820090293884\n",
            "step 2291: generator_loss=0.7424167394638062, discriminator_loss=0.6577008366584778\n",
            "step 2292: generator_loss=0.7452470064163208, discriminator_loss=0.6574091911315918\n",
            "step 2293: generator_loss=0.7392903566360474, discriminator_loss=0.6600977182388306\n",
            "step 2294: generator_loss=0.743632972240448, discriminator_loss=0.6577816605567932\n",
            "step 2295: generator_loss=0.7432723045349121, discriminator_loss=0.6572242379188538\n",
            "step 2296: generator_loss=0.740963876247406, discriminator_loss=0.658546507358551\n",
            "step 2297: generator_loss=0.7359187602996826, discriminator_loss=0.6585801839828491\n",
            "step 2298: generator_loss=0.7326587438583374, discriminator_loss=0.6595250964164734\n",
            "step 2299: generator_loss=0.7321146726608276, discriminator_loss=0.658788800239563\n",
            "step 2300: generator_loss=0.729457676410675, discriminator_loss=0.6588068008422852\n",
            "step 2301: generator_loss=0.7299153804779053, discriminator_loss=0.6576583981513977\n",
            "step 2302: generator_loss=0.7251566648483276, discriminator_loss=0.65933758020401\n",
            "step 2303: generator_loss=0.7275945544242859, discriminator_loss=0.657791018486023\n",
            "step 2304: generator_loss=0.7250961065292358, discriminator_loss=0.6586215496063232\n",
            "step 2305: generator_loss=0.723617672920227, discriminator_loss=0.6603611707687378\n",
            "step 2306: generator_loss=0.7296767234802246, discriminator_loss=0.6574912667274475\n",
            "step 2307: generator_loss=0.7248433828353882, discriminator_loss=0.6600478887557983\n",
            "step 2308: generator_loss=0.7297264337539673, discriminator_loss=0.6586849689483643\n",
            "step 2309: generator_loss=0.7308489084243774, discriminator_loss=0.6582057476043701\n",
            "step 2310: generator_loss=0.7296429872512817, discriminator_loss=0.6584843993186951\n",
            "step 2311: generator_loss=0.7280261516571045, discriminator_loss=0.6598180532455444\n",
            "step 2312: generator_loss=0.7340809106826782, discriminator_loss=0.6569181680679321\n",
            "step 2313: generator_loss=0.7337335348129272, discriminator_loss=0.6574325561523438\n",
            "step 2314: generator_loss=0.7321310043334961, discriminator_loss=0.6577218770980835\n",
            "step 2315: generator_loss=0.7303107976913452, discriminator_loss=0.6582326889038086\n",
            "step 2316: generator_loss=0.7293707132339478, discriminator_loss=0.6582973003387451\n",
            "step 2317: generator_loss=0.7279961109161377, discriminator_loss=0.6588969230651855\n",
            "step 2318: generator_loss=0.7294524908065796, discriminator_loss=0.6571699380874634\n",
            "step 2319: generator_loss=0.7245157361030579, discriminator_loss=0.6596832275390625\n",
            "step 2320: generator_loss=0.7257980108261108, discriminator_loss=0.6586506366729736\n",
            "step 2321: generator_loss=0.7275561094284058, discriminator_loss=0.6579595804214478\n",
            "step 2322: generator_loss=0.72709059715271, discriminator_loss=0.6575967073440552\n",
            "step 2323: generator_loss=0.7295151948928833, discriminator_loss=0.6569662690162659\n",
            "step 2324: generator_loss=0.73017418384552, discriminator_loss=0.6565499901771545\n",
            "step 2325: generator_loss=0.732435405254364, discriminator_loss=0.6571211814880371\n",
            "step 2326: generator_loss=0.7323551774024963, discriminator_loss=0.6570028066635132\n",
            "step 2327: generator_loss=0.7359168529510498, discriminator_loss=0.6562352776527405\n",
            "step 2328: generator_loss=0.7355062961578369, discriminator_loss=0.6568969488143921\n",
            "step 2329: generator_loss=0.737339198589325, discriminator_loss=0.6570448279380798\n",
            "step 2330: generator_loss=0.7368742227554321, discriminator_loss=0.6568661332130432\n",
            "step 2331: generator_loss=0.7359281182289124, discriminator_loss=0.6573375463485718\n",
            "step 2332: generator_loss=0.735688328742981, discriminator_loss=0.6561341881752014\n",
            "step 2333: generator_loss=0.7340254783630371, discriminator_loss=0.6554450988769531\n",
            "step 2334: generator_loss=0.7311204671859741, discriminator_loss=0.6565525531768799\n",
            "step 2335: generator_loss=0.7312552332878113, discriminator_loss=0.6554741263389587\n",
            "step 2336: generator_loss=0.7323647737503052, discriminator_loss=0.6548991203308105\n",
            "step 2337: generator_loss=0.7287160158157349, discriminator_loss=0.6558338403701782\n",
            "step 2338: generator_loss=0.7355610132217407, discriminator_loss=0.6528369188308716\n",
            "step 2339: generator_loss=0.7339896559715271, discriminator_loss=0.6535837054252625\n",
            "step 2340: generator_loss=0.7336586117744446, discriminator_loss=0.6543747186660767\n",
            "step 2341: generator_loss=0.7379486560821533, discriminator_loss=0.6529943943023682\n",
            "step 2342: generator_loss=0.7374186515808105, discriminator_loss=0.6536309719085693\n",
            "step 2343: generator_loss=0.739315390586853, discriminator_loss=0.6539908051490784\n",
            "step 2344: generator_loss=0.741868257522583, discriminator_loss=0.6520879864692688\n",
            "step 2345: generator_loss=0.7408581376075745, discriminator_loss=0.6529303789138794\n",
            "step 2346: generator_loss=0.738379180431366, discriminator_loss=0.6534196138381958\n",
            "step 2347: generator_loss=0.7352476119995117, discriminator_loss=0.6538543701171875\n",
            "step 2348: generator_loss=0.7322050333023071, discriminator_loss=0.6546053886413574\n",
            "step 2349: generator_loss=0.7322856783866882, discriminator_loss=0.6541422605514526\n",
            "step 2350: generator_loss=0.7314745187759399, discriminator_loss=0.6531673073768616\n",
            "step 2351: generator_loss=0.7338273525238037, discriminator_loss=0.6516519784927368\n",
            "step 2352: generator_loss=0.7338807582855225, discriminator_loss=0.6505756378173828\n",
            "step 2353: generator_loss=0.7320631742477417, discriminator_loss=0.652258038520813\n",
            "step 2354: generator_loss=0.7321349382400513, discriminator_loss=0.6516242027282715\n",
            "step 2355: generator_loss=0.7321146130561829, discriminator_loss=0.6522729396820068\n",
            "step 2356: generator_loss=0.7320603132247925, discriminator_loss=0.6528487205505371\n",
            "step 2357: generator_loss=0.7379699945449829, discriminator_loss=0.651316225528717\n",
            "step 2358: generator_loss=0.7370210886001587, discriminator_loss=0.6517593860626221\n",
            "step 2359: generator_loss=0.7361106872558594, discriminator_loss=0.6530348062515259\n",
            "step 2360: generator_loss=0.7396487593650818, discriminator_loss=0.6510676145553589\n",
            "step 2361: generator_loss=0.7362679839134216, discriminator_loss=0.6535881757736206\n",
            "step 2362: generator_loss=0.7386718392372131, discriminator_loss=0.6521064043045044\n",
            "step 2363: generator_loss=0.7371953129768372, discriminator_loss=0.6537878513336182\n",
            "step 2364: generator_loss=0.7399749755859375, discriminator_loss=0.6524167060852051\n",
            "step 2365: generator_loss=0.7346817851066589, discriminator_loss=0.6537961363792419\n",
            "step 2366: generator_loss=0.7413467764854431, discriminator_loss=0.6505017280578613\n",
            "step 2367: generator_loss=0.7328001260757446, discriminator_loss=0.6539114713668823\n",
            "step 2368: generator_loss=0.7340953946113586, discriminator_loss=0.6525734663009644\n",
            "step 2369: generator_loss=0.7322420477867126, discriminator_loss=0.653002142906189\n",
            "step 2370: generator_loss=0.7312771677970886, discriminator_loss=0.6539056301116943\n",
            "step 2371: generator_loss=0.7275303602218628, discriminator_loss=0.6551048755645752\n",
            "step 2372: generator_loss=0.7311570048332214, discriminator_loss=0.6542431116104126\n",
            "step 2373: generator_loss=0.7309485673904419, discriminator_loss=0.6548161506652832\n",
            "step 2374: generator_loss=0.7323258519172668, discriminator_loss=0.653972864151001\n",
            "step 2375: generator_loss=0.7364975810050964, discriminator_loss=0.6531506180763245\n",
            "step 2376: generator_loss=0.7348512411117554, discriminator_loss=0.6551731824874878\n",
            "step 2377: generator_loss=0.7329853773117065, discriminator_loss=0.6571912169456482\n",
            "step 2378: generator_loss=0.7365334033966064, discriminator_loss=0.6557275056838989\n",
            "step 2379: generator_loss=0.7343828082084656, discriminator_loss=0.6584295034408569\n",
            "step 2380: generator_loss=0.7357598543167114, discriminator_loss=0.656577467918396\n",
            "step 2381: generator_loss=0.7358002066612244, discriminator_loss=0.6565794348716736\n",
            "step 2382: generator_loss=0.730971097946167, discriminator_loss=0.6586267948150635\n",
            "step 2383: generator_loss=0.7287769317626953, discriminator_loss=0.6596187949180603\n",
            "step 2384: generator_loss=0.724937915802002, discriminator_loss=0.6608378887176514\n",
            "step 2385: generator_loss=0.7278331518173218, discriminator_loss=0.6592947244644165\n",
            "step 2386: generator_loss=0.7267823219299316, discriminator_loss=0.6599003076553345\n",
            "step 2387: generator_loss=0.7234187126159668, discriminator_loss=0.6612893342971802\n",
            "step 2388: generator_loss=0.7263201475143433, discriminator_loss=0.6606411337852478\n",
            "step 2389: generator_loss=0.7233576774597168, discriminator_loss=0.6618523597717285\n",
            "step 2390: generator_loss=0.7235867977142334, discriminator_loss=0.6606794595718384\n",
            "step 2391: generator_loss=0.7185345888137817, discriminator_loss=0.664141058921814\n",
            "step 2392: generator_loss=0.7220808863639832, discriminator_loss=0.6628726720809937\n",
            "step 2393: generator_loss=0.7192783951759338, discriminator_loss=0.6640615463256836\n",
            "step 2394: generator_loss=0.7247726917266846, discriminator_loss=0.6617633700370789\n",
            "step 2395: generator_loss=0.7202561497688293, discriminator_loss=0.6643629670143127\n",
            "step 2396: generator_loss=0.7202225923538208, discriminator_loss=0.6654611825942993\n",
            "step 2397: generator_loss=0.7105554342269897, discriminator_loss=0.6700782775878906\n",
            "step 2398: generator_loss=0.7144913077354431, discriminator_loss=0.6678588390350342\n",
            "step 2399: generator_loss=0.7141460180282593, discriminator_loss=0.6692464351654053\n",
            "step 2400: generator_loss=0.7129929065704346, discriminator_loss=0.66985684633255\n",
            "step 2401: generator_loss=0.710864782333374, discriminator_loss=0.6701469421386719\n",
            "step 2402: generator_loss=0.7162344455718994, discriminator_loss=0.6681157350540161\n",
            "step 2403: generator_loss=0.713105320930481, discriminator_loss=0.6702080368995667\n",
            "step 2404: generator_loss=0.7182615995407104, discriminator_loss=0.6685259342193604\n",
            "step 2405: generator_loss=0.7203309535980225, discriminator_loss=0.6679672002792358\n",
            "step 2406: generator_loss=0.7213732600212097, discriminator_loss=0.6673377752304077\n",
            "step 2407: generator_loss=0.716513991355896, discriminator_loss=0.6693203449249268\n",
            "step 2408: generator_loss=0.7153990268707275, discriminator_loss=0.6705471277236938\n",
            "step 2409: generator_loss=0.7157735824584961, discriminator_loss=0.6701362133026123\n",
            "step 2410: generator_loss=0.7155544757843018, discriminator_loss=0.6716181039810181\n",
            "step 2411: generator_loss=0.718640148639679, discriminator_loss=0.671371579170227\n",
            "step 2412: generator_loss=0.71144700050354, discriminator_loss=0.6764487028121948\n",
            "step 2413: generator_loss=0.7152450084686279, discriminator_loss=0.6748292446136475\n",
            "step 2414: generator_loss=0.7081167697906494, discriminator_loss=0.6782660484313965\n",
            "step 2415: generator_loss=0.710885763168335, discriminator_loss=0.678108811378479\n",
            "step 2416: generator_loss=0.7094935178756714, discriminator_loss=0.6781469583511353\n",
            "step 2417: generator_loss=0.7090418934822083, discriminator_loss=0.6791914105415344\n",
            "step 2418: generator_loss=0.7067683935165405, discriminator_loss=0.6810075044631958\n",
            "step 2419: generator_loss=0.7099024653434753, discriminator_loss=0.6796969771385193\n",
            "step 2420: generator_loss=0.7119486331939697, discriminator_loss=0.6783069968223572\n",
            "step 2421: generator_loss=0.7141197919845581, discriminator_loss=0.6761351823806763\n",
            "step 2422: generator_loss=0.7159793376922607, discriminator_loss=0.6760140061378479\n",
            "step 2423: generator_loss=0.7190883159637451, discriminator_loss=0.6721397638320923\n",
            "step 2424: generator_loss=0.7221245765686035, discriminator_loss=0.6715154647827148\n",
            "step 2425: generator_loss=0.7286566495895386, discriminator_loss=0.6675379276275635\n",
            "step 2426: generator_loss=0.7291842699050903, discriminator_loss=0.6678071618080139\n",
            "step 2427: generator_loss=0.7318345308303833, discriminator_loss=0.665947675704956\n",
            "step 2428: generator_loss=0.7283638715744019, discriminator_loss=0.6672479510307312\n",
            "step 2429: generator_loss=0.7219330072402954, discriminator_loss=0.6707340478897095\n",
            "step 2430: generator_loss=0.7217040061950684, discriminator_loss=0.6709506511688232\n",
            "step 2431: generator_loss=0.7217717170715332, discriminator_loss=0.6716248393058777\n",
            "step 2432: generator_loss=0.7140161395072937, discriminator_loss=0.6763809323310852\n",
            "step 2433: generator_loss=0.7122146487236023, discriminator_loss=0.6774195432662964\n",
            "step 2434: generator_loss=0.7077730894088745, discriminator_loss=0.6798660755157471\n",
            "step 2435: generator_loss=0.697301983833313, discriminator_loss=0.6856005787849426\n",
            "step 2436: generator_loss=0.6976388692855835, discriminator_loss=0.6859279870986938\n",
            "step 2437: generator_loss=0.6952623128890991, discriminator_loss=0.6874978542327881\n",
            "step 2438: generator_loss=0.6889197826385498, discriminator_loss=0.6908124685287476\n",
            "step 2439: generator_loss=0.689372181892395, discriminator_loss=0.6907271146774292\n",
            "step 2440: generator_loss=0.688493013381958, discriminator_loss=0.6918748617172241\n",
            "step 2441: generator_loss=0.69090735912323, discriminator_loss=0.6917604804039001\n",
            "step 2442: generator_loss=0.701716959476471, discriminator_loss=0.6867554187774658\n",
            "step 2443: generator_loss=0.7090100646018982, discriminator_loss=0.6845653057098389\n",
            "step 2444: generator_loss=0.7191780209541321, discriminator_loss=0.682642936706543\n",
            "step 2445: generator_loss=0.7255948781967163, discriminator_loss=0.6807575225830078\n",
            "step 2446: generator_loss=0.7324637770652771, discriminator_loss=0.6785742044448853\n",
            "step 2447: generator_loss=0.7436290979385376, discriminator_loss=0.6716928482055664\n",
            "step 2448: generator_loss=0.7449824213981628, discriminator_loss=0.6698961853981018\n",
            "step 2449: generator_loss=0.7454546689987183, discriminator_loss=0.6660931706428528\n",
            "step 2450: generator_loss=0.7411726713180542, discriminator_loss=0.6645259857177734\n",
            "step 2451: generator_loss=0.739325761795044, discriminator_loss=0.6616511940956116\n",
            "step 2452: generator_loss=0.7313937544822693, discriminator_loss=0.662670373916626\n",
            "step 2453: generator_loss=0.7228312492370605, discriminator_loss=0.6632595658302307\n",
            "step 2454: generator_loss=0.7160838842391968, discriminator_loss=0.66371750831604\n",
            "step 2455: generator_loss=0.7112658023834229, discriminator_loss=0.6622511148452759\n",
            "step 2456: generator_loss=0.703631579875946, discriminator_loss=0.664865255355835\n",
            "step 2457: generator_loss=0.6985505819320679, discriminator_loss=0.6672137975692749\n",
            "step 2458: generator_loss=0.6958514451980591, discriminator_loss=0.6694009900093079\n",
            "step 2459: generator_loss=0.703883707523346, discriminator_loss=0.6699800491333008\n",
            "step 2460: generator_loss=0.6935761570930481, discriminator_loss=0.6785372495651245\n",
            "step 2461: generator_loss=0.6992749571800232, discriminator_loss=0.6809166669845581\n",
            "step 2462: generator_loss=0.6889392137527466, discriminator_loss=0.6909763216972351\n",
            "step 2463: generator_loss=0.6937826871871948, discriminator_loss=0.6909517049789429\n",
            "step 2464: generator_loss=0.6941594481468201, discriminator_loss=0.6943802833557129\n",
            "step 2465: generator_loss=0.6921301484107971, discriminator_loss=0.6981199383735657\n",
            "step 2466: generator_loss=0.6985709071159363, discriminator_loss=0.6960325241088867\n",
            "step 2467: generator_loss=0.6958785057067871, discriminator_loss=0.6987278461456299\n",
            "step 2468: generator_loss=0.7010096311569214, discriminator_loss=0.6955296993255615\n",
            "step 2469: generator_loss=0.7046058177947998, discriminator_loss=0.6941001415252686\n",
            "step 2470: generator_loss=0.7085776329040527, discriminator_loss=0.690699577331543\n",
            "step 2471: generator_loss=0.7092666625976562, discriminator_loss=0.689665675163269\n",
            "step 2472: generator_loss=0.7146818041801453, discriminator_loss=0.6851979494094849\n",
            "step 2473: generator_loss=0.7203203439712524, discriminator_loss=0.6816431879997253\n",
            "step 2474: generator_loss=0.7250673770904541, discriminator_loss=0.6775286197662354\n",
            "step 2475: generator_loss=0.7267787456512451, discriminator_loss=0.6745581030845642\n",
            "step 2476: generator_loss=0.7255192995071411, discriminator_loss=0.6707298755645752\n",
            "step 2477: generator_loss=0.7357752323150635, discriminator_loss=0.6629728078842163\n",
            "step 2478: generator_loss=0.7394729852676392, discriminator_loss=0.6569274067878723\n",
            "step 2479: generator_loss=0.7394518852233887, discriminator_loss=0.6552761793136597\n",
            "step 2480: generator_loss=0.7530170679092407, discriminator_loss=0.6483567953109741\n",
            "step 2481: generator_loss=0.7537845373153687, discriminator_loss=0.6479495763778687\n",
            "step 2482: generator_loss=0.7565183639526367, discriminator_loss=0.6459836363792419\n",
            "step 2483: generator_loss=0.7594730854034424, discriminator_loss=0.6474409103393555\n",
            "step 2484: generator_loss=0.7584444284439087, discriminator_loss=0.6502092480659485\n",
            "step 2485: generator_loss=0.7516160011291504, discriminator_loss=0.6556191444396973\n",
            "step 2486: generator_loss=0.745813250541687, discriminator_loss=0.6584104895591736\n",
            "step 2487: generator_loss=0.7374919652938843, discriminator_loss=0.6638617515563965\n",
            "step 2488: generator_loss=0.7296366691589355, discriminator_loss=0.6653296947479248\n",
            "step 2489: generator_loss=0.715938150882721, discriminator_loss=0.6698895692825317\n",
            "step 2490: generator_loss=0.7047576904296875, discriminator_loss=0.6737959384918213\n",
            "step 2491: generator_loss=0.6962940692901611, discriminator_loss=0.6762012243270874\n",
            "step 2492: generator_loss=0.6896939277648926, discriminator_loss=0.6797823905944824\n",
            "step 2493: generator_loss=0.6867022514343262, discriminator_loss=0.6817160844802856\n",
            "step 2494: generator_loss=0.6851441264152527, discriminator_loss=0.6828470230102539\n",
            "step 2495: generator_loss=0.6815430521965027, discriminator_loss=0.6859721541404724\n",
            "step 2496: generator_loss=0.6853592395782471, discriminator_loss=0.6859498023986816\n",
            "step 2497: generator_loss=0.6879382133483887, discriminator_loss=0.6871106624603271\n",
            "step 2498: generator_loss=0.68800950050354, discriminator_loss=0.6880691051483154\n",
            "step 2499: generator_loss=0.6925528049468994, discriminator_loss=0.6865770816802979\n",
            "step 2500: generator_loss=0.6957593560218811, discriminator_loss=0.6865233182907104\n",
            "step 2501: generator_loss=0.6997987031936646, discriminator_loss=0.6855985522270203\n",
            "step 2502: generator_loss=0.7002779841423035, discriminator_loss=0.6851027011871338\n",
            "step 2503: generator_loss=0.7055890560150146, discriminator_loss=0.6831282377243042\n",
            "step 2504: generator_loss=0.7102169990539551, discriminator_loss=0.679890513420105\n",
            "step 2505: generator_loss=0.7181209325790405, discriminator_loss=0.6764217019081116\n",
            "step 2506: generator_loss=0.7234530448913574, discriminator_loss=0.6743226051330566\n",
            "step 2507: generator_loss=0.7288751602172852, discriminator_loss=0.669876217842102\n",
            "step 2508: generator_loss=0.7348394393920898, discriminator_loss=0.666734516620636\n",
            "step 2509: generator_loss=0.7395762205123901, discriminator_loss=0.664324939250946\n",
            "step 2510: generator_loss=0.7419629096984863, discriminator_loss=0.6622464656829834\n",
            "step 2511: generator_loss=0.7397531270980835, discriminator_loss=0.6611820459365845\n",
            "step 2512: generator_loss=0.7401880025863647, discriminator_loss=0.6593307256698608\n",
            "step 2513: generator_loss=0.7465519905090332, discriminator_loss=0.6559256911277771\n",
            "step 2514: generator_loss=0.7500411868095398, discriminator_loss=0.653048574924469\n",
            "step 2515: generator_loss=0.7468862533569336, discriminator_loss=0.6536358594894409\n",
            "step 2516: generator_loss=0.7504680156707764, discriminator_loss=0.6511517763137817\n",
            "step 2517: generator_loss=0.7424407601356506, discriminator_loss=0.6547448635101318\n",
            "step 2518: generator_loss=0.7370371222496033, discriminator_loss=0.6586686372756958\n",
            "step 2519: generator_loss=0.7362041473388672, discriminator_loss=0.6577812433242798\n",
            "step 2520: generator_loss=0.7352107763290405, discriminator_loss=0.6585601568222046\n",
            "step 2521: generator_loss=0.7270572781562805, discriminator_loss=0.6615740656852722\n",
            "step 2522: generator_loss=0.7250896692276001, discriminator_loss=0.6639513969421387\n",
            "step 2523: generator_loss=0.7181683778762817, discriminator_loss=0.6673967838287354\n",
            "step 2524: generator_loss=0.7115495204925537, discriminator_loss=0.6704351902008057\n",
            "step 2525: generator_loss=0.7170332670211792, discriminator_loss=0.6699278354644775\n",
            "step 2526: generator_loss=0.7116290330886841, discriminator_loss=0.6749618053436279\n",
            "step 2527: generator_loss=0.7147249579429626, discriminator_loss=0.6753156185150146\n",
            "step 2528: generator_loss=0.7144206762313843, discriminator_loss=0.6779577732086182\n",
            "step 2529: generator_loss=0.712958574295044, discriminator_loss=0.6796480417251587\n",
            "step 2530: generator_loss=0.7069970965385437, discriminator_loss=0.6825525760650635\n",
            "step 2531: generator_loss=0.7007482647895813, discriminator_loss=0.6869212985038757\n",
            "step 2532: generator_loss=0.6966880559921265, discriminator_loss=0.6871891021728516\n",
            "step 2533: generator_loss=0.694361686706543, discriminator_loss=0.6901779174804688\n",
            "step 2534: generator_loss=0.6871340274810791, discriminator_loss=0.6918518543243408\n",
            "step 2535: generator_loss=0.6909418106079102, discriminator_loss=0.6891212463378906\n",
            "step 2536: generator_loss=0.6938228011131287, discriminator_loss=0.6874837875366211\n",
            "step 2537: generator_loss=0.6981391310691833, discriminator_loss=0.6860253810882568\n",
            "step 2538: generator_loss=0.7017371654510498, discriminator_loss=0.6849631667137146\n",
            "step 2539: generator_loss=0.7034251689910889, discriminator_loss=0.684014618396759\n",
            "step 2540: generator_loss=0.7069027423858643, discriminator_loss=0.6832593679428101\n",
            "step 2541: generator_loss=0.7149820923805237, discriminator_loss=0.6782400608062744\n",
            "step 2542: generator_loss=0.7204882502555847, discriminator_loss=0.6746141910552979\n",
            "step 2543: generator_loss=0.7263868451118469, discriminator_loss=0.6718287467956543\n",
            "step 2544: generator_loss=0.7316896319389343, discriminator_loss=0.6688792705535889\n",
            "step 2545: generator_loss=0.743686854839325, discriminator_loss=0.6622139811515808\n",
            "step 2546: generator_loss=0.7480090856552124, discriminator_loss=0.6587494611740112\n",
            "step 2547: generator_loss=0.7532079219818115, discriminator_loss=0.6549040079116821\n",
            "step 2548: generator_loss=0.7512184381484985, discriminator_loss=0.653897762298584\n",
            "step 2549: generator_loss=0.7532137632369995, discriminator_loss=0.6519997119903564\n",
            "step 2550: generator_loss=0.753803014755249, discriminator_loss=0.6506216526031494\n",
            "step 2551: generator_loss=0.7533203363418579, discriminator_loss=0.6490486860275269\n",
            "step 2552: generator_loss=0.744146466255188, discriminator_loss=0.6516903638839722\n",
            "step 2553: generator_loss=0.7463961243629456, discriminator_loss=0.6466275453567505\n",
            "step 2554: generator_loss=0.7415943741798401, discriminator_loss=0.6468035578727722\n",
            "step 2555: generator_loss=0.743161678314209, discriminator_loss=0.6433572769165039\n",
            "step 2556: generator_loss=0.7396380305290222, discriminator_loss=0.6434329748153687\n",
            "step 2557: generator_loss=0.7411633729934692, discriminator_loss=0.6424329876899719\n",
            "step 2558: generator_loss=0.7431336641311646, discriminator_loss=0.6430675983428955\n",
            "step 2559: generator_loss=0.7457382082939148, discriminator_loss=0.6430145502090454\n",
            "step 2560: generator_loss=0.7434452772140503, discriminator_loss=0.6463834047317505\n",
            "step 2561: generator_loss=0.7359213829040527, discriminator_loss=0.6532605886459351\n",
            "step 2562: generator_loss=0.7343742847442627, discriminator_loss=0.6548141837120056\n",
            "step 2563: generator_loss=0.7350082397460938, discriminator_loss=0.655563235282898\n",
            "step 2564: generator_loss=0.735818088054657, discriminator_loss=0.6564885377883911\n",
            "step 2565: generator_loss=0.7343864440917969, discriminator_loss=0.6579449772834778\n",
            "step 2566: generator_loss=0.7254164218902588, discriminator_loss=0.6638871431350708\n",
            "step 2567: generator_loss=0.7234586477279663, discriminator_loss=0.6647103428840637\n",
            "step 2568: generator_loss=0.7128015756607056, discriminator_loss=0.6710776090621948\n",
            "step 2569: generator_loss=0.714949369430542, discriminator_loss=0.6718952059745789\n",
            "step 2570: generator_loss=0.7059503793716431, discriminator_loss=0.6759698390960693\n",
            "step 2571: generator_loss=0.7010663747787476, discriminator_loss=0.6794148087501526\n",
            "step 2572: generator_loss=0.6989935636520386, discriminator_loss=0.6815577149391174\n",
            "step 2573: generator_loss=0.6932730078697205, discriminator_loss=0.6843115091323853\n",
            "step 2574: generator_loss=0.6867619752883911, discriminator_loss=0.6901916861534119\n",
            "step 2575: generator_loss=0.689303457736969, discriminator_loss=0.6912063360214233\n",
            "step 2576: generator_loss=0.6878849864006042, discriminator_loss=0.6939631700515747\n",
            "step 2577: generator_loss=0.6819270849227905, discriminator_loss=0.6981347799301147\n",
            "step 2578: generator_loss=0.6871650218963623, discriminator_loss=0.6988469362258911\n",
            "step 2579: generator_loss=0.6881369352340698, discriminator_loss=0.69999760389328\n",
            "step 2580: generator_loss=0.6879013776779175, discriminator_loss=0.7011328339576721\n",
            "step 2581: generator_loss=0.682086706161499, discriminator_loss=0.7075469493865967\n",
            "step 2582: generator_loss=0.6793347597122192, discriminator_loss=0.709449827671051\n",
            "step 2583: generator_loss=0.6775929927825928, discriminator_loss=0.7124236226081848\n",
            "step 2584: generator_loss=0.6779137253761292, discriminator_loss=0.7143006324768066\n",
            "step 2585: generator_loss=0.6808698177337646, discriminator_loss=0.7115769982337952\n",
            "step 2586: generator_loss=0.6675089597702026, discriminator_loss=0.7172902822494507\n",
            "step 2587: generator_loss=0.6663175821304321, discriminator_loss=0.7185434103012085\n",
            "step 2588: generator_loss=0.6644535660743713, discriminator_loss=0.7189663648605347\n",
            "step 2589: generator_loss=0.6629664897918701, discriminator_loss=0.7183650732040405\n",
            "step 2590: generator_loss=0.6624117493629456, discriminator_loss=0.7190582156181335\n",
            "step 2591: generator_loss=0.6643626689910889, discriminator_loss=0.7198609709739685\n",
            "step 2592: generator_loss=0.665675163269043, discriminator_loss=0.7198727130889893\n",
            "step 2593: generator_loss=0.6728545427322388, discriminator_loss=0.7173377275466919\n",
            "step 2594: generator_loss=0.6727967262268066, discriminator_loss=0.7186304330825806\n",
            "step 2595: generator_loss=0.6742404699325562, discriminator_loss=0.7177373766899109\n",
            "step 2596: generator_loss=0.679470419883728, discriminator_loss=0.7144998908042908\n",
            "step 2597: generator_loss=0.683565080165863, discriminator_loss=0.7112365961074829\n",
            "step 2598: generator_loss=0.6870670914649963, discriminator_loss=0.7074933648109436\n",
            "step 2599: generator_loss=0.6906818151473999, discriminator_loss=0.7029937505722046\n",
            "step 2600: generator_loss=0.696829080581665, discriminator_loss=0.6972053050994873\n",
            "step 2601: generator_loss=0.7007415294647217, discriminator_loss=0.6907704472541809\n",
            "step 2602: generator_loss=0.7056174278259277, discriminator_loss=0.6857073307037354\n",
            "step 2603: generator_loss=0.7114114165306091, discriminator_loss=0.681100606918335\n",
            "step 2604: generator_loss=0.7173283100128174, discriminator_loss=0.676188588142395\n",
            "step 2605: generator_loss=0.7208312749862671, discriminator_loss=0.6754740476608276\n",
            "step 2606: generator_loss=0.7260165214538574, discriminator_loss=0.6723666191101074\n",
            "step 2607: generator_loss=0.7278856039047241, discriminator_loss=0.6699082255363464\n",
            "step 2608: generator_loss=0.7314071655273438, discriminator_loss=0.6666824221611023\n",
            "step 2609: generator_loss=0.7278311252593994, discriminator_loss=0.668047308921814\n",
            "step 2610: generator_loss=0.731637179851532, discriminator_loss=0.664851188659668\n",
            "step 2611: generator_loss=0.7297873497009277, discriminator_loss=0.6646819114685059\n",
            "step 2612: generator_loss=0.7272974252700806, discriminator_loss=0.6636223793029785\n",
            "step 2613: generator_loss=0.7261512875556946, discriminator_loss=0.6628581285476685\n",
            "step 2614: generator_loss=0.7296972274780273, discriminator_loss=0.6588343381881714\n",
            "step 2615: generator_loss=0.7266849279403687, discriminator_loss=0.6587268114089966\n",
            "step 2616: generator_loss=0.7229699492454529, discriminator_loss=0.6590116024017334\n",
            "step 2617: generator_loss=0.725591778755188, discriminator_loss=0.6569777727127075\n",
            "step 2618: generator_loss=0.7280089855194092, discriminator_loss=0.6552819013595581\n",
            "step 2619: generator_loss=0.7281321883201599, discriminator_loss=0.6547483801841736\n",
            "step 2620: generator_loss=0.7316538691520691, discriminator_loss=0.6549407243728638\n",
            "step 2621: generator_loss=0.7357068061828613, discriminator_loss=0.6536558866500854\n",
            "step 2622: generator_loss=0.7340296506881714, discriminator_loss=0.655754029750824\n",
            "step 2623: generator_loss=0.7301586866378784, discriminator_loss=0.6588691473007202\n",
            "step 2624: generator_loss=0.7322871088981628, discriminator_loss=0.6570627689361572\n",
            "step 2625: generator_loss=0.7268620729446411, discriminator_loss=0.6605441570281982\n",
            "step 2626: generator_loss=0.7234960198402405, discriminator_loss=0.6619534492492676\n",
            "step 2627: generator_loss=0.7237703800201416, discriminator_loss=0.6611229181289673\n",
            "step 2628: generator_loss=0.7266783118247986, discriminator_loss=0.6592320203781128\n",
            "step 2629: generator_loss=0.7198361754417419, discriminator_loss=0.6619552373886108\n",
            "step 2630: generator_loss=0.715342104434967, discriminator_loss=0.6640788316726685\n",
            "step 2631: generator_loss=0.7139221429824829, discriminator_loss=0.6637431383132935\n",
            "step 2632: generator_loss=0.7155195474624634, discriminator_loss=0.6637373566627502\n",
            "step 2633: generator_loss=0.7118455767631531, discriminator_loss=0.665497899055481\n",
            "step 2634: generator_loss=0.7128715515136719, discriminator_loss=0.663823127746582\n",
            "step 2635: generator_loss=0.7078167796134949, discriminator_loss=0.6669059991836548\n",
            "step 2636: generator_loss=0.7108696103096008, discriminator_loss=0.6677488684654236\n",
            "step 2637: generator_loss=0.7045907378196716, discriminator_loss=0.6715308427810669\n",
            "step 2638: generator_loss=0.7108026742935181, discriminator_loss=0.6708170175552368\n",
            "step 2639: generator_loss=0.7082204818725586, discriminator_loss=0.6732372641563416\n",
            "step 2640: generator_loss=0.7146861553192139, discriminator_loss=0.671618640422821\n",
            "step 2641: generator_loss=0.713379442691803, discriminator_loss=0.6731181144714355\n",
            "step 2642: generator_loss=0.7216805815696716, discriminator_loss=0.6723184585571289\n",
            "step 2643: generator_loss=0.7422385215759277, discriminator_loss=0.6693534851074219\n",
            "step 2644: generator_loss=0.7133968472480774, discriminator_loss=0.6729773283004761\n",
            "step 2645: generator_loss=0.7082505226135254, discriminator_loss=0.6755915284156799\n",
            "step 2646: generator_loss=0.705938994884491, discriminator_loss=0.6771904230117798\n",
            "step 2647: generator_loss=0.711536169052124, discriminator_loss=0.6755170822143555\n",
            "step 2648: generator_loss=0.7103424072265625, discriminator_loss=0.6763598322868347\n",
            "step 2649: generator_loss=0.7089608907699585, discriminator_loss=0.6767106652259827\n",
            "step 2650: generator_loss=0.7121643424034119, discriminator_loss=0.6755512952804565\n",
            "step 2651: generator_loss=0.7118097543716431, discriminator_loss=0.6776399612426758\n",
            "step 2652: generator_loss=0.7128643989562988, discriminator_loss=0.6765387058258057\n",
            "step 2653: generator_loss=0.7059972286224365, discriminator_loss=0.6773998737335205\n",
            "step 2654: generator_loss=0.7073636054992676, discriminator_loss=0.6767523288726807\n",
            "step 2655: generator_loss=0.7092994451522827, discriminator_loss=0.6749780774116516\n",
            "step 2656: generator_loss=0.7099704742431641, discriminator_loss=0.6736024022102356\n",
            "step 2657: generator_loss=0.7099798321723938, discriminator_loss=0.6716383099555969\n",
            "step 2658: generator_loss=0.721103847026825, discriminator_loss=0.6649311780929565\n",
            "step 2659: generator_loss=0.7170373201370239, discriminator_loss=0.6670191287994385\n",
            "step 2660: generator_loss=0.728021502494812, discriminator_loss=0.660801887512207\n",
            "step 2661: generator_loss=0.7283424139022827, discriminator_loss=0.6616836190223694\n",
            "step 2662: generator_loss=0.7337861061096191, discriminator_loss=0.6580948829650879\n",
            "step 2663: generator_loss=0.7370002865791321, discriminator_loss=0.6548203229904175\n",
            "step 2664: generator_loss=0.7402749061584473, discriminator_loss=0.6523573398590088\n",
            "step 2665: generator_loss=0.744032621383667, discriminator_loss=0.6479936242103577\n",
            "step 2666: generator_loss=0.745155930519104, discriminator_loss=0.645628809928894\n",
            "step 2667: generator_loss=0.7445552945137024, discriminator_loss=0.6438010334968567\n",
            "step 2668: generator_loss=0.7560563087463379, discriminator_loss=0.6361287832260132\n",
            "step 2669: generator_loss=0.7501233816146851, discriminator_loss=0.637981116771698\n",
            "step 2670: generator_loss=0.7645275592803955, discriminator_loss=0.6308317184448242\n",
            "step 2671: generator_loss=0.7595362663269043, discriminator_loss=0.6358122825622559\n",
            "step 2672: generator_loss=0.7557801008224487, discriminator_loss=0.6380139589309692\n",
            "step 2673: generator_loss=0.7531052827835083, discriminator_loss=0.6403954029083252\n",
            "step 2674: generator_loss=0.749137282371521, discriminator_loss=0.6420406103134155\n",
            "step 2675: generator_loss=0.7446916103363037, discriminator_loss=0.6443716287612915\n",
            "step 2676: generator_loss=0.7361313104629517, discriminator_loss=0.6483926773071289\n",
            "step 2677: generator_loss=0.7328294515609741, discriminator_loss=0.6481239795684814\n",
            "step 2678: generator_loss=0.7217668294906616, discriminator_loss=0.6541202068328857\n",
            "step 2679: generator_loss=0.7242203950881958, discriminator_loss=0.653800368309021\n",
            "step 2680: generator_loss=0.7206736207008362, discriminator_loss=0.6568974256515503\n",
            "step 2681: generator_loss=0.7109693288803101, discriminator_loss=0.6642321944236755\n",
            "step 2682: generator_loss=0.7031898498535156, discriminator_loss=0.6698548793792725\n",
            "step 2683: generator_loss=0.698830783367157, discriminator_loss=0.6745204329490662\n",
            "step 2684: generator_loss=0.6974283456802368, discriminator_loss=0.6773369312286377\n",
            "step 2685: generator_loss=0.6961613297462463, discriminator_loss=0.6813483834266663\n",
            "step 2686: generator_loss=0.6925305128097534, discriminator_loss=0.6863824129104614\n",
            "step 2687: generator_loss=0.6902981996536255, discriminator_loss=0.688637375831604\n",
            "step 2688: generator_loss=0.6870237588882446, discriminator_loss=0.6933817267417908\n",
            "step 2689: generator_loss=0.6867488622665405, discriminator_loss=0.6951984167098999\n",
            "step 2690: generator_loss=0.6905709505081177, discriminator_loss=0.6942530274391174\n",
            "step 2691: generator_loss=0.6860508918762207, discriminator_loss=0.6981858015060425\n",
            "step 2692: generator_loss=0.6826728582382202, discriminator_loss=0.7022614479064941\n",
            "step 2693: generator_loss=0.6873586177825928, discriminator_loss=0.7018866539001465\n",
            "step 2694: generator_loss=0.6907074451446533, discriminator_loss=0.7012270092964172\n",
            "step 2695: generator_loss=0.6881123781204224, discriminator_loss=0.702132523059845\n",
            "step 2696: generator_loss=0.6882420778274536, discriminator_loss=0.7033304572105408\n",
            "step 2697: generator_loss=0.6864737272262573, discriminator_loss=0.7056399583816528\n",
            "step 2698: generator_loss=0.685998797416687, discriminator_loss=0.703021764755249\n",
            "step 2699: generator_loss=0.6878955960273743, discriminator_loss=0.7000635266304016\n",
            "step 2700: generator_loss=0.6843311190605164, discriminator_loss=0.6994890570640564\n",
            "step 2701: generator_loss=0.6855093240737915, discriminator_loss=0.6964963674545288\n",
            "step 2702: generator_loss=0.6907103061676025, discriminator_loss=0.6901335716247559\n",
            "step 2703: generator_loss=0.6956908106803894, discriminator_loss=0.6855654716491699\n",
            "step 2704: generator_loss=0.7036046981811523, discriminator_loss=0.6824504137039185\n",
            "step 2705: generator_loss=0.708185076713562, discriminator_loss=0.6784543991088867\n",
            "step 2706: generator_loss=0.7140140533447266, discriminator_loss=0.675674557685852\n",
            "step 2707: generator_loss=0.7198935747146606, discriminator_loss=0.671904981136322\n",
            "step 2708: generator_loss=0.7212406992912292, discriminator_loss=0.669919490814209\n",
            "step 2709: generator_loss=0.7321364879608154, discriminator_loss=0.6634324789047241\n",
            "step 2710: generator_loss=0.7298418283462524, discriminator_loss=0.6626988649368286\n",
            "step 2711: generator_loss=0.734010636806488, discriminator_loss=0.656948447227478\n",
            "step 2712: generator_loss=0.7314327359199524, discriminator_loss=0.6577480435371399\n",
            "step 2713: generator_loss=0.7324569225311279, discriminator_loss=0.6568734645843506\n",
            "step 2714: generator_loss=0.7369059920310974, discriminator_loss=0.6540359258651733\n",
            "step 2715: generator_loss=0.738438606262207, discriminator_loss=0.6516947150230408\n",
            "step 2716: generator_loss=0.7379573583602905, discriminator_loss=0.6512340903282166\n",
            "step 2717: generator_loss=0.7365643978118896, discriminator_loss=0.6506744623184204\n",
            "step 2718: generator_loss=0.743268609046936, discriminator_loss=0.6491761207580566\n",
            "step 2719: generator_loss=0.7387919425964355, discriminator_loss=0.6528179049491882\n",
            "step 2720: generator_loss=0.7482857704162598, discriminator_loss=0.6501524448394775\n",
            "step 2721: generator_loss=0.7400072813034058, discriminator_loss=0.6542665362358093\n",
            "step 2722: generator_loss=0.7394750714302063, discriminator_loss=0.6551668643951416\n",
            "step 2723: generator_loss=0.7390066385269165, discriminator_loss=0.6568830609321594\n",
            "step 2724: generator_loss=0.7414295077323914, discriminator_loss=0.6565048694610596\n",
            "step 2725: generator_loss=0.7342373132705688, discriminator_loss=0.65963214635849\n",
            "step 2726: generator_loss=0.720000147819519, discriminator_loss=0.664221465587616\n",
            "step 2727: generator_loss=0.7124234437942505, discriminator_loss=0.6692497730255127\n",
            "step 2728: generator_loss=0.7101669311523438, discriminator_loss=0.6707313060760498\n",
            "step 2729: generator_loss=0.7079120874404907, discriminator_loss=0.6714454889297485\n",
            "step 2730: generator_loss=0.7102574110031128, discriminator_loss=0.6705598831176758\n",
            "step 2731: generator_loss=0.7091178894042969, discriminator_loss=0.6726595163345337\n",
            "step 2732: generator_loss=0.7089657783508301, discriminator_loss=0.6753425002098083\n",
            "step 2733: generator_loss=0.6999179720878601, discriminator_loss=0.681999683380127\n",
            "step 2734: generator_loss=0.7105996012687683, discriminator_loss=0.6785577535629272\n",
            "step 2735: generator_loss=0.7063854932785034, discriminator_loss=0.6835610270500183\n",
            "step 2736: generator_loss=0.6991148591041565, discriminator_loss=0.6879673004150391\n",
            "step 2737: generator_loss=0.6982237696647644, discriminator_loss=0.6898856163024902\n",
            "step 2738: generator_loss=0.6967639923095703, discriminator_loss=0.6922754049301147\n",
            "step 2739: generator_loss=0.6926983594894409, discriminator_loss=0.6965125799179077\n",
            "step 2740: generator_loss=0.6889474391937256, discriminator_loss=0.6987640261650085\n",
            "step 2741: generator_loss=0.6841807961463928, discriminator_loss=0.7027149796485901\n",
            "step 2742: generator_loss=0.6769706010818481, discriminator_loss=0.7071995139122009\n",
            "step 2743: generator_loss=0.6718916893005371, discriminator_loss=0.7105228900909424\n",
            "step 2744: generator_loss=0.6674081087112427, discriminator_loss=0.713276743888855\n",
            "step 2745: generator_loss=0.6661674976348877, discriminator_loss=0.7148675918579102\n",
            "step 2746: generator_loss=0.663395345211029, discriminator_loss=0.718018651008606\n",
            "step 2747: generator_loss=0.6675413846969604, discriminator_loss=0.716128945350647\n",
            "step 2748: generator_loss=0.6616455316543579, discriminator_loss=0.7210303544998169\n",
            "step 2749: generator_loss=0.6612420082092285, discriminator_loss=0.722566545009613\n",
            "step 2750: generator_loss=0.6587212085723877, discriminator_loss=0.7251371145248413\n",
            "step 2751: generator_loss=0.6574484705924988, discriminator_loss=0.7267202138900757\n",
            "step 2752: generator_loss=0.65920490026474, discriminator_loss=0.7273556590080261\n",
            "step 2753: generator_loss=0.6625657081604004, discriminator_loss=0.7269530296325684\n",
            "step 2754: generator_loss=0.6586405038833618, discriminator_loss=0.7299113273620605\n",
            "step 2755: generator_loss=0.6593658328056335, discriminator_loss=0.7295494675636292\n",
            "step 2756: generator_loss=0.663571834564209, discriminator_loss=0.7270544767379761\n",
            "step 2757: generator_loss=0.6641308069229126, discriminator_loss=0.7260887622833252\n",
            "step 2758: generator_loss=0.6616865396499634, discriminator_loss=0.7276871800422668\n",
            "step 2759: generator_loss=0.6668351888656616, discriminator_loss=0.7229385375976562\n",
            "step 2760: generator_loss=0.6710333824157715, discriminator_loss=0.7186727523803711\n",
            "step 2761: generator_loss=0.675449013710022, discriminator_loss=0.714547336101532\n",
            "step 2762: generator_loss=0.6789884567260742, discriminator_loss=0.7112827301025391\n",
            "step 2763: generator_loss=0.685056209564209, discriminator_loss=0.7063870429992676\n",
            "step 2764: generator_loss=0.6880056858062744, discriminator_loss=0.70237797498703\n",
            "step 2765: generator_loss=0.6929205656051636, discriminator_loss=0.6977646350860596\n",
            "step 2766: generator_loss=0.6958678364753723, discriminator_loss=0.6935615539550781\n",
            "step 2767: generator_loss=0.6990572214126587, discriminator_loss=0.6890678405761719\n",
            "step 2768: generator_loss=0.702400803565979, discriminator_loss=0.6853020191192627\n",
            "step 2769: generator_loss=0.707531213760376, discriminator_loss=0.681835949420929\n",
            "step 2770: generator_loss=0.7134566307067871, discriminator_loss=0.6774321794509888\n",
            "step 2771: generator_loss=0.7225525975227356, discriminator_loss=0.6717942953109741\n",
            "step 2772: generator_loss=0.7283439636230469, discriminator_loss=0.6661967039108276\n",
            "step 2773: generator_loss=0.7340872287750244, discriminator_loss=0.662910521030426\n",
            "step 2774: generator_loss=0.7400084137916565, discriminator_loss=0.6600477695465088\n",
            "step 2775: generator_loss=0.7460578083992004, discriminator_loss=0.6562991738319397\n",
            "step 2776: generator_loss=0.7540477514266968, discriminator_loss=0.653301477432251\n",
            "step 2777: generator_loss=0.7551056146621704, discriminator_loss=0.652634859085083\n",
            "step 2778: generator_loss=0.7628437280654907, discriminator_loss=0.6483587026596069\n",
            "step 2779: generator_loss=0.7637646198272705, discriminator_loss=0.6455621719360352\n",
            "step 2780: generator_loss=0.760134220123291, discriminator_loss=0.6461470723152161\n",
            "step 2781: generator_loss=0.7639951705932617, discriminator_loss=0.6421414613723755\n",
            "step 2782: generator_loss=0.7630317807197571, discriminator_loss=0.6422697305679321\n",
            "step 2783: generator_loss=0.759681224822998, discriminator_loss=0.6429498195648193\n",
            "step 2784: generator_loss=0.7501254081726074, discriminator_loss=0.6463211178779602\n",
            "step 2785: generator_loss=0.7498416304588318, discriminator_loss=0.6465511918067932\n",
            "step 2786: generator_loss=0.7459918856620789, discriminator_loss=0.6472930908203125\n",
            "step 2787: generator_loss=0.7375121712684631, discriminator_loss=0.6507581472396851\n",
            "step 2788: generator_loss=0.73406583070755, discriminator_loss=0.6526296138763428\n",
            "step 2789: generator_loss=0.7256916165351868, discriminator_loss=0.656938910484314\n",
            "step 2790: generator_loss=0.7200843095779419, discriminator_loss=0.658826470375061\n",
            "step 2791: generator_loss=0.718428373336792, discriminator_loss=0.6609710454940796\n",
            "step 2792: generator_loss=0.7137184143066406, discriminator_loss=0.664311945438385\n",
            "step 2793: generator_loss=0.712470531463623, discriminator_loss=0.666660726070404\n",
            "step 2794: generator_loss=0.7107628583908081, discriminator_loss=0.6691358089447021\n",
            "step 2795: generator_loss=0.7038344740867615, discriminator_loss=0.6734519600868225\n",
            "step 2796: generator_loss=0.7080416083335876, discriminator_loss=0.6734347939491272\n",
            "step 2797: generator_loss=0.6964271068572998, discriminator_loss=0.6822646856307983\n",
            "step 2798: generator_loss=0.6996184587478638, discriminator_loss=0.6829602718353271\n",
            "step 2799: generator_loss=0.6969068050384521, discriminator_loss=0.687743067741394\n",
            "step 2800: generator_loss=0.7034648656845093, discriminator_loss=0.6863299608230591\n",
            "step 2801: generator_loss=0.6947455406188965, discriminator_loss=0.6915867328643799\n",
            "step 2802: generator_loss=0.6965055465698242, discriminator_loss=0.6946232318878174\n",
            "step 2803: generator_loss=0.6928369998931885, discriminator_loss=0.6966745257377625\n",
            "step 2804: generator_loss=0.6933318972587585, discriminator_loss=0.6964112520217896\n",
            "step 2805: generator_loss=0.6911157369613647, discriminator_loss=0.6987211108207703\n",
            "step 2806: generator_loss=0.6962635517120361, discriminator_loss=0.6956595182418823\n",
            "step 2807: generator_loss=0.6964426040649414, discriminator_loss=0.695554256439209\n",
            "step 2808: generator_loss=0.6956871747970581, discriminator_loss=0.6957308053970337\n",
            "step 2809: generator_loss=0.6972211599349976, discriminator_loss=0.6949661374092102\n",
            "step 2810: generator_loss=0.7009866237640381, discriminator_loss=0.6914678812026978\n",
            "step 2811: generator_loss=0.7061684131622314, discriminator_loss=0.6880452632904053\n",
            "step 2812: generator_loss=0.7101662755012512, discriminator_loss=0.685471773147583\n",
            "step 2813: generator_loss=0.7151652574539185, discriminator_loss=0.6837393045425415\n",
            "step 2814: generator_loss=0.720601499080658, discriminator_loss=0.6809109449386597\n",
            "step 2815: generator_loss=0.7228070497512817, discriminator_loss=0.679915189743042\n",
            "step 2816: generator_loss=0.7202883362770081, discriminator_loss=0.6798287630081177\n",
            "step 2817: generator_loss=0.7242141962051392, discriminator_loss=0.6777714490890503\n",
            "step 2818: generator_loss=0.7193996906280518, discriminator_loss=0.6783405542373657\n",
            "step 2819: generator_loss=0.7129611968994141, discriminator_loss=0.6792199611663818\n",
            "step 2820: generator_loss=0.7158011198043823, discriminator_loss=0.676433801651001\n",
            "step 2821: generator_loss=0.713820219039917, discriminator_loss=0.6739927530288696\n",
            "step 2822: generator_loss=0.7160414457321167, discriminator_loss=0.6713640093803406\n",
            "step 2823: generator_loss=0.713180661201477, discriminator_loss=0.6715120077133179\n",
            "step 2824: generator_loss=0.7138307094573975, discriminator_loss=0.6693318486213684\n",
            "step 2825: generator_loss=0.7149811387062073, discriminator_loss=0.6689674854278564\n",
            "step 2826: generator_loss=0.7144574522972107, discriminator_loss=0.6680722236633301\n",
            "step 2827: generator_loss=0.7162420749664307, discriminator_loss=0.66665118932724\n",
            "step 2828: generator_loss=0.7141849994659424, discriminator_loss=0.6668546199798584\n",
            "step 2829: generator_loss=0.7165161371231079, discriminator_loss=0.6660324335098267\n",
            "step 2830: generator_loss=0.7231569886207581, discriminator_loss=0.6608738303184509\n",
            "step 2831: generator_loss=0.7252225875854492, discriminator_loss=0.6598647832870483\n",
            "step 2832: generator_loss=0.7287108898162842, discriminator_loss=0.6571401357650757\n",
            "step 2833: generator_loss=0.7329636812210083, discriminator_loss=0.6543478965759277\n",
            "step 2834: generator_loss=0.7324903011322021, discriminator_loss=0.6534175872802734\n",
            "step 2835: generator_loss=0.7353028059005737, discriminator_loss=0.6509143114089966\n",
            "step 2836: generator_loss=0.7388794422149658, discriminator_loss=0.6475182175636292\n",
            "step 2837: generator_loss=0.737568736076355, discriminator_loss=0.6476307511329651\n",
            "step 2838: generator_loss=0.7402034997940063, discriminator_loss=0.6463485360145569\n",
            "step 2839: generator_loss=0.7396190762519836, discriminator_loss=0.6470340490341187\n",
            "step 2840: generator_loss=0.7339407205581665, discriminator_loss=0.6532946228981018\n",
            "step 2841: generator_loss=0.7451186180114746, discriminator_loss=0.6504241228103638\n",
            "step 2842: generator_loss=0.7342689633369446, discriminator_loss=0.6574820280075073\n",
            "step 2843: generator_loss=0.733845055103302, discriminator_loss=0.6585071086883545\n",
            "step 2844: generator_loss=0.7250484228134155, discriminator_loss=0.6652106046676636\n",
            "step 2845: generator_loss=0.7213906645774841, discriminator_loss=0.6668577194213867\n",
            "step 2846: generator_loss=0.7139803767204285, discriminator_loss=0.6707710027694702\n",
            "step 2847: generator_loss=0.7114428281784058, discriminator_loss=0.6735891103744507\n",
            "step 2848: generator_loss=0.6936209797859192, discriminator_loss=0.6821765899658203\n",
            "step 2849: generator_loss=0.7008452415466309, discriminator_loss=0.6808595061302185\n",
            "step 2850: generator_loss=0.7002423405647278, discriminator_loss=0.681736946105957\n",
            "step 2851: generator_loss=0.6944209337234497, discriminator_loss=0.6858468055725098\n",
            "step 2852: generator_loss=0.6734943389892578, discriminator_loss=0.699634313583374\n",
            "step 2853: generator_loss=0.6743234395980835, discriminator_loss=0.6999684572219849\n",
            "step 2854: generator_loss=0.6731716990470886, discriminator_loss=0.7041481733322144\n",
            "step 2855: generator_loss=0.672420084476471, discriminator_loss=0.7061722278594971\n",
            "step 2856: generator_loss=0.6687026023864746, discriminator_loss=0.7103735208511353\n",
            "step 2857: generator_loss=0.6686845421791077, discriminator_loss=0.7140436172485352\n",
            "step 2858: generator_loss=0.6570019721984863, discriminator_loss=0.7226967811584473\n",
            "step 2859: generator_loss=0.6567360758781433, discriminator_loss=0.7237414121627808\n",
            "step 2860: generator_loss=0.6603866815567017, discriminator_loss=0.7259439826011658\n",
            "step 2861: generator_loss=0.6566346883773804, discriminator_loss=0.728715181350708\n",
            "step 2862: generator_loss=0.6572474241256714, discriminator_loss=0.7282947301864624\n",
            "step 2863: generator_loss=0.6587029695510864, discriminator_loss=0.7267788648605347\n",
            "step 2864: generator_loss=0.6588634848594666, discriminator_loss=0.7269855737686157\n",
            "step 2865: generator_loss=0.6592307686805725, discriminator_loss=0.727493405342102\n",
            "step 2866: generator_loss=0.6628583669662476, discriminator_loss=0.7251094579696655\n",
            "step 2867: generator_loss=0.6609625220298767, discriminator_loss=0.7231033444404602\n",
            "step 2868: generator_loss=0.6650024652481079, discriminator_loss=0.7191252708435059\n",
            "step 2869: generator_loss=0.6646035313606262, discriminator_loss=0.7156654000282288\n",
            "step 2870: generator_loss=0.6668375134468079, discriminator_loss=0.7117549180984497\n",
            "step 2871: generator_loss=0.6672568321228027, discriminator_loss=0.7081087231636047\n",
            "step 2872: generator_loss=0.672413170337677, discriminator_loss=0.7060827016830444\n",
            "step 2873: generator_loss=0.6735154390335083, discriminator_loss=0.7028152942657471\n",
            "step 2874: generator_loss=0.6811438798904419, discriminator_loss=0.7002092599868774\n",
            "step 2875: generator_loss=0.6849184036254883, discriminator_loss=0.7003406286239624\n",
            "step 2876: generator_loss=0.68787682056427, discriminator_loss=0.7013433575630188\n",
            "step 2877: generator_loss=0.6876181960105896, discriminator_loss=0.7024102210998535\n",
            "step 2878: generator_loss=0.6881159543991089, discriminator_loss=0.7042274475097656\n",
            "step 2879: generator_loss=0.6906821727752686, discriminator_loss=0.7021244764328003\n",
            "step 2880: generator_loss=0.6874936819076538, discriminator_loss=0.702781081199646\n",
            "step 2881: generator_loss=0.685861349105835, discriminator_loss=0.7037718296051025\n",
            "step 2882: generator_loss=0.686206579208374, discriminator_loss=0.7008985877037048\n",
            "step 2883: generator_loss=0.6832607388496399, discriminator_loss=0.7016453742980957\n",
            "step 2884: generator_loss=0.6848931312561035, discriminator_loss=0.6983012557029724\n",
            "step 2885: generator_loss=0.6834069490432739, discriminator_loss=0.697192907333374\n",
            "step 2886: generator_loss=0.6827392578125, discriminator_loss=0.6947710514068604\n",
            "step 2887: generator_loss=0.6843799352645874, discriminator_loss=0.6930145621299744\n",
            "step 2888: generator_loss=0.6847965717315674, discriminator_loss=0.6915284395217896\n",
            "step 2889: generator_loss=0.686880350112915, discriminator_loss=0.6910481452941895\n",
            "step 2890: generator_loss=0.6894787549972534, discriminator_loss=0.6899124979972839\n",
            "step 2891: generator_loss=0.6915779113769531, discriminator_loss=0.6900693774223328\n",
            "step 2892: generator_loss=0.6923690438270569, discriminator_loss=0.6907658576965332\n",
            "step 2893: generator_loss=0.6946419477462769, discriminator_loss=0.6907427310943604\n",
            "step 2894: generator_loss=0.6971354484558105, discriminator_loss=0.6918425559997559\n",
            "step 2895: generator_loss=0.6980061531066895, discriminator_loss=0.6929912567138672\n",
            "step 2896: generator_loss=0.698993444442749, discriminator_loss=0.6931468844413757\n",
            "step 2897: generator_loss=0.6967636346817017, discriminator_loss=0.6950045228004456\n",
            "step 2898: generator_loss=0.6951125860214233, discriminator_loss=0.6965430974960327\n",
            "step 2899: generator_loss=0.6942545175552368, discriminator_loss=0.696478545665741\n",
            "step 2900: generator_loss=0.6892483234405518, discriminator_loss=0.6981828212738037\n",
            "step 2901: generator_loss=0.6868867874145508, discriminator_loss=0.6994304656982422\n",
            "step 2902: generator_loss=0.6854227781295776, discriminator_loss=0.6990485191345215\n",
            "step 2903: generator_loss=0.682476282119751, discriminator_loss=0.7001039385795593\n",
            "step 2904: generator_loss=0.6798322200775146, discriminator_loss=0.7014819383621216\n",
            "step 2905: generator_loss=0.6787576675415039, discriminator_loss=0.7017746567726135\n",
            "step 2906: generator_loss=0.6727125644683838, discriminator_loss=0.704835832118988\n",
            "step 2907: generator_loss=0.6735373735427856, discriminator_loss=0.7059488296508789\n",
            "step 2908: generator_loss=0.676173210144043, discriminator_loss=0.7050272226333618\n",
            "step 2909: generator_loss=0.6753484010696411, discriminator_loss=0.7054290771484375\n",
            "step 2910: generator_loss=0.6745048761367798, discriminator_loss=0.7073432207107544\n",
            "step 2911: generator_loss=0.6753162145614624, discriminator_loss=0.7080963850021362\n",
            "step 2912: generator_loss=0.674639105796814, discriminator_loss=0.7096410393714905\n",
            "step 2913: generator_loss=0.6744296550750732, discriminator_loss=0.7111133933067322\n",
            "step 2914: generator_loss=0.6765284538269043, discriminator_loss=0.7101114988327026\n",
            "step 2915: generator_loss=0.6759052872657776, discriminator_loss=0.7112743854522705\n",
            "step 2916: generator_loss=0.6767891049385071, discriminator_loss=0.7131165266036987\n",
            "step 2917: generator_loss=0.6744000911712646, discriminator_loss=0.7129436731338501\n",
            "step 2918: generator_loss=0.6790942549705505, discriminator_loss=0.7114199995994568\n",
            "step 2919: generator_loss=0.6772140264511108, discriminator_loss=0.7124501466751099\n",
            "step 2920: generator_loss=0.6751586198806763, discriminator_loss=0.7132242321968079\n",
            "step 2921: generator_loss=0.6780347228050232, discriminator_loss=0.7116763591766357\n",
            "step 2922: generator_loss=0.6790361404418945, discriminator_loss=0.7119045853614807\n",
            "step 2923: generator_loss=0.6804995536804199, discriminator_loss=0.710597038269043\n",
            "step 2924: generator_loss=0.6801860332489014, discriminator_loss=0.7113630771636963\n",
            "step 2925: generator_loss=0.6781656742095947, discriminator_loss=0.7115249633789062\n",
            "step 2926: generator_loss=0.6783510446548462, discriminator_loss=0.7112996578216553\n",
            "step 2927: generator_loss=0.6794999837875366, discriminator_loss=0.7095915079116821\n",
            "step 2928: generator_loss=0.6808035969734192, discriminator_loss=0.7089032530784607\n",
            "step 2929: generator_loss=0.6785355806350708, discriminator_loss=0.7093915939331055\n",
            "step 2930: generator_loss=0.6796352863311768, discriminator_loss=0.7083760499954224\n",
            "step 2931: generator_loss=0.6757168173789978, discriminator_loss=0.7110845446586609\n",
            "step 2932: generator_loss=0.6751713752746582, discriminator_loss=0.7131080031394958\n",
            "step 2933: generator_loss=0.6807419657707214, discriminator_loss=0.7098426222801208\n",
            "step 2934: generator_loss=0.674265444278717, discriminator_loss=0.7131933569908142\n",
            "step 2935: generator_loss=0.6698933839797974, discriminator_loss=0.7137904167175293\n",
            "step 2936: generator_loss=0.6655478477478027, discriminator_loss=0.716562032699585\n",
            "step 2937: generator_loss=0.6640785932540894, discriminator_loss=0.7172433137893677\n",
            "step 2938: generator_loss=0.6688299179077148, discriminator_loss=0.7147557139396667\n",
            "step 2939: generator_loss=0.6623996496200562, discriminator_loss=0.7193195223808289\n",
            "step 2940: generator_loss=0.6668163537979126, discriminator_loss=0.7169365882873535\n",
            "step 2941: generator_loss=0.665080189704895, discriminator_loss=0.7189159989356995\n",
            "step 2942: generator_loss=0.6647415161132812, discriminator_loss=0.7197397351264954\n",
            "step 2943: generator_loss=0.6649868488311768, discriminator_loss=0.7205426692962646\n",
            "step 2944: generator_loss=0.6663118600845337, discriminator_loss=0.7215976715087891\n",
            "step 2945: generator_loss=0.6671279668807983, discriminator_loss=0.7210357785224915\n",
            "step 2946: generator_loss=0.6679556369781494, discriminator_loss=0.7210602760314941\n",
            "step 2947: generator_loss=0.6697570085525513, discriminator_loss=0.7210215330123901\n",
            "step 2948: generator_loss=0.6684596538543701, discriminator_loss=0.7216896414756775\n",
            "step 2949: generator_loss=0.6651707291603088, discriminator_loss=0.7224477529525757\n",
            "step 2950: generator_loss=0.6666458249092102, discriminator_loss=0.7225849628448486\n",
            "step 2951: generator_loss=0.6664133071899414, discriminator_loss=0.7231725454330444\n",
            "step 2952: generator_loss=0.6683089733123779, discriminator_loss=0.7227628231048584\n",
            "step 2953: generator_loss=0.6698355674743652, discriminator_loss=0.721566915512085\n",
            "step 2954: generator_loss=0.6736999750137329, discriminator_loss=0.7180709838867188\n",
            "step 2955: generator_loss=0.6713466048240662, discriminator_loss=0.7183623313903809\n",
            "step 2956: generator_loss=0.6740058064460754, discriminator_loss=0.7162902355194092\n",
            "step 2957: generator_loss=0.6734579801559448, discriminator_loss=0.7157910466194153\n",
            "step 2958: generator_loss=0.6734153032302856, discriminator_loss=0.7149524092674255\n",
            "step 2959: generator_loss=0.6744288206100464, discriminator_loss=0.7136164903640747\n",
            "step 2960: generator_loss=0.6743361949920654, discriminator_loss=0.7134196758270264\n",
            "step 2961: generator_loss=0.673569917678833, discriminator_loss=0.7125757932662964\n",
            "step 2962: generator_loss=0.6701428890228271, discriminator_loss=0.7155694961547852\n",
            "step 2963: generator_loss=0.6708962917327881, discriminator_loss=0.7152521014213562\n",
            "step 2964: generator_loss=0.6719217300415039, discriminator_loss=0.7153164148330688\n",
            "step 2965: generator_loss=0.6733300685882568, discriminator_loss=0.7135288715362549\n",
            "step 2966: generator_loss=0.671768307685852, discriminator_loss=0.7135882377624512\n",
            "step 2967: generator_loss=0.6730122566223145, discriminator_loss=0.7126132249832153\n",
            "step 2968: generator_loss=0.6708482503890991, discriminator_loss=0.7135249972343445\n",
            "step 2969: generator_loss=0.6721780896186829, discriminator_loss=0.7128416299819946\n",
            "step 2970: generator_loss=0.671902060508728, discriminator_loss=0.712542712688446\n",
            "step 2971: generator_loss=0.6724783778190613, discriminator_loss=0.7119616270065308\n",
            "step 2972: generator_loss=0.6763497591018677, discriminator_loss=0.7096781730651855\n",
            "step 2973: generator_loss=0.6744877696037292, discriminator_loss=0.7092972993850708\n",
            "step 2974: generator_loss=0.6800665855407715, discriminator_loss=0.7065951824188232\n",
            "step 2975: generator_loss=0.6793782711029053, discriminator_loss=0.706364631652832\n",
            "step 2976: generator_loss=0.6813931465148926, discriminator_loss=0.7039462327957153\n",
            "step 2977: generator_loss=0.6873621940612793, discriminator_loss=0.7001085877418518\n",
            "step 2978: generator_loss=0.6895577907562256, discriminator_loss=0.6969771385192871\n",
            "step 2979: generator_loss=0.6968080401420593, discriminator_loss=0.691259503364563\n",
            "step 2980: generator_loss=0.6995835304260254, discriminator_loss=0.6897571682929993\n",
            "step 2981: generator_loss=0.703711748123169, discriminator_loss=0.687151312828064\n",
            "step 2982: generator_loss=0.7077487707138062, discriminator_loss=0.686237633228302\n",
            "step 2983: generator_loss=0.7094024419784546, discriminator_loss=0.685897946357727\n",
            "step 2984: generator_loss=0.7100229263305664, discriminator_loss=0.6865987777709961\n",
            "step 2985: generator_loss=0.7079454064369202, discriminator_loss=0.687788724899292\n",
            "step 2986: generator_loss=0.7051817774772644, discriminator_loss=0.6885000467300415\n",
            "step 2987: generator_loss=0.7055749297142029, discriminator_loss=0.6868971586227417\n",
            "step 2988: generator_loss=0.7001726031303406, discriminator_loss=0.6881359815597534\n",
            "step 2989: generator_loss=0.6980421543121338, discriminator_loss=0.6872694492340088\n",
            "step 2990: generator_loss=0.6943688988685608, discriminator_loss=0.6879128217697144\n",
            "step 2991: generator_loss=0.6907048225402832, discriminator_loss=0.6892106533050537\n",
            "step 2992: generator_loss=0.688626766204834, discriminator_loss=0.6886810064315796\n",
            "step 2993: generator_loss=0.6875736713409424, discriminator_loss=0.6875792741775513\n",
            "step 2994: generator_loss=0.6866413354873657, discriminator_loss=0.6877323985099792\n",
            "step 2995: generator_loss=0.6890714168548584, discriminator_loss=0.6864644289016724\n",
            "step 2996: generator_loss=0.6913152933120728, discriminator_loss=0.6841700077056885\n",
            "step 2997: generator_loss=0.6949060559272766, discriminator_loss=0.6821008920669556\n",
            "step 2998: generator_loss=0.7002654075622559, discriminator_loss=0.6797535419464111\n",
            "step 2999: generator_loss=0.7057561874389648, discriminator_loss=0.6766043305397034\n",
            "step 3000: generator_loss=0.7142741680145264, discriminator_loss=0.6734550595283508\n",
            "step 3001: generator_loss=0.7208343744277954, discriminator_loss=0.6722079515457153\n",
            "step 3002: generator_loss=0.7248031497001648, discriminator_loss=0.6717221736907959\n",
            "step 3003: generator_loss=0.7299196124076843, discriminator_loss=0.6693561673164368\n",
            "step 3004: generator_loss=0.7343304753303528, discriminator_loss=0.6672823429107666\n",
            "step 3005: generator_loss=0.7343356013298035, discriminator_loss=0.6680401563644409\n",
            "step 3006: generator_loss=0.7375350594520569, discriminator_loss=0.66517573595047\n",
            "step 3007: generator_loss=0.7352800369262695, discriminator_loss=0.6650062799453735\n",
            "step 3008: generator_loss=0.7323963642120361, discriminator_loss=0.6635805368423462\n",
            "step 3009: generator_loss=0.7311193943023682, discriminator_loss=0.6621907949447632\n",
            "step 3010: generator_loss=0.729299783706665, discriminator_loss=0.6602367758750916\n",
            "step 3011: generator_loss=0.7258082032203674, discriminator_loss=0.6586174368858337\n",
            "step 3012: generator_loss=0.7265945672988892, discriminator_loss=0.6560790538787842\n",
            "step 3013: generator_loss=0.7280887365341187, discriminator_loss=0.6550323367118835\n",
            "step 3014: generator_loss=0.7288264036178589, discriminator_loss=0.6534326672554016\n",
            "step 3015: generator_loss=0.7292793989181519, discriminator_loss=0.6527705192565918\n",
            "step 3016: generator_loss=0.7352111339569092, discriminator_loss=0.649539589881897\n",
            "step 3017: generator_loss=0.7372130751609802, discriminator_loss=0.6482832431793213\n",
            "step 3018: generator_loss=0.7380104064941406, discriminator_loss=0.646432638168335\n",
            "step 3019: generator_loss=0.7451884150505066, discriminator_loss=0.6435028314590454\n",
            "step 3020: generator_loss=0.7483398914337158, discriminator_loss=0.6419550180435181\n",
            "step 3021: generator_loss=0.7501085996627808, discriminator_loss=0.6420458555221558\n",
            "step 3022: generator_loss=0.7507492303848267, discriminator_loss=0.6410183906555176\n",
            "step 3023: generator_loss=0.7520615458488464, discriminator_loss=0.639984667301178\n",
            "step 3024: generator_loss=0.7519025206565857, discriminator_loss=0.6383789777755737\n",
            "step 3025: generator_loss=0.7550340890884399, discriminator_loss=0.6359597444534302\n",
            "step 3026: generator_loss=0.7531591653823853, discriminator_loss=0.6365633010864258\n",
            "step 3027: generator_loss=0.752792477607727, discriminator_loss=0.634634792804718\n",
            "step 3028: generator_loss=0.7538309693336487, discriminator_loss=0.632571280002594\n",
            "step 3029: generator_loss=0.7517628073692322, discriminator_loss=0.6322498321533203\n",
            "step 3030: generator_loss=0.7566452622413635, discriminator_loss=0.6287833452224731\n",
            "step 3031: generator_loss=0.7545179128646851, discriminator_loss=0.6317083835601807\n",
            "step 3032: generator_loss=0.7580907344818115, discriminator_loss=0.6301904916763306\n",
            "step 3033: generator_loss=0.7624255418777466, discriminator_loss=0.6279296875\n",
            "step 3034: generator_loss=0.7611968517303467, discriminator_loss=0.6285982131958008\n",
            "step 3035: generator_loss=0.7674572467803955, discriminator_loss=0.6256555318832397\n",
            "step 3036: generator_loss=0.7642012238502502, discriminator_loss=0.6270371675491333\n",
            "step 3037: generator_loss=0.7639334797859192, discriminator_loss=0.6265182495117188\n",
            "step 3038: generator_loss=0.7667375802993774, discriminator_loss=0.6257903575897217\n",
            "step 3039: generator_loss=0.765658974647522, discriminator_loss=0.626577615737915\n",
            "step 3040: generator_loss=0.7629638910293579, discriminator_loss=0.6273693442344666\n",
            "step 3041: generator_loss=0.7651274800300598, discriminator_loss=0.6270602941513062\n",
            "step 3042: generator_loss=0.7605490684509277, discriminator_loss=0.6293267011642456\n",
            "step 3043: generator_loss=0.7533867359161377, discriminator_loss=0.6322060823440552\n",
            "step 3044: generator_loss=0.7583096027374268, discriminator_loss=0.630578875541687\n",
            "step 3045: generator_loss=0.7579847574234009, discriminator_loss=0.6307241916656494\n",
            "step 3046: generator_loss=0.7485781311988831, discriminator_loss=0.6358895897865295\n",
            "step 3047: generator_loss=0.751366138458252, discriminator_loss=0.634109616279602\n",
            "step 3048: generator_loss=0.7521276473999023, discriminator_loss=0.6357620358467102\n",
            "step 3049: generator_loss=0.7460042834281921, discriminator_loss=0.6380698680877686\n",
            "step 3050: generator_loss=0.7477971315383911, discriminator_loss=0.6374768614768982\n",
            "step 3051: generator_loss=0.7474130392074585, discriminator_loss=0.6390708684921265\n",
            "step 3052: generator_loss=0.749839186668396, discriminator_loss=0.6382534503936768\n",
            "step 3053: generator_loss=0.7443526387214661, discriminator_loss=0.6430811882019043\n",
            "step 3054: generator_loss=0.7454489469528198, discriminator_loss=0.641781210899353\n",
            "step 3055: generator_loss=0.7474426031112671, discriminator_loss=0.6428194046020508\n",
            "step 3056: generator_loss=0.7391589879989624, discriminator_loss=0.6473569869995117\n",
            "step 3057: generator_loss=0.742110550403595, discriminator_loss=0.6462510824203491\n",
            "step 3058: generator_loss=0.7438546419143677, discriminator_loss=0.6478093266487122\n",
            "step 3059: generator_loss=0.7405345439910889, discriminator_loss=0.6507071256637573\n",
            "step 3060: generator_loss=0.7396162748336792, discriminator_loss=0.6500970125198364\n",
            "step 3061: generator_loss=0.7355231642723083, discriminator_loss=0.6547321677207947\n",
            "step 3062: generator_loss=0.7312357425689697, discriminator_loss=0.6569353342056274\n",
            "step 3063: generator_loss=0.7287235856056213, discriminator_loss=0.6574277281761169\n",
            "step 3064: generator_loss=0.7250109910964966, discriminator_loss=0.6587775349617004\n",
            "step 3065: generator_loss=0.7237695455551147, discriminator_loss=0.6607535481452942\n",
            "step 3066: generator_loss=0.724420428276062, discriminator_loss=0.6626501083374023\n",
            "step 3067: generator_loss=0.7199243307113647, discriminator_loss=0.6637331247329712\n",
            "step 3068: generator_loss=0.7133712768554688, discriminator_loss=0.6690552830696106\n",
            "step 3069: generator_loss=0.7054343223571777, discriminator_loss=0.6732258200645447\n",
            "step 3070: generator_loss=0.709102988243103, discriminator_loss=0.6717383861541748\n",
            "step 3071: generator_loss=0.7057361006736755, discriminator_loss=0.675464928150177\n",
            "step 3072: generator_loss=0.7042113542556763, discriminator_loss=0.6782917976379395\n",
            "step 3073: generator_loss=0.7052052021026611, discriminator_loss=0.6781514883041382\n",
            "step 3074: generator_loss=0.7007597088813782, discriminator_loss=0.684349775314331\n",
            "step 3075: generator_loss=0.7062627077102661, discriminator_loss=0.6827372312545776\n",
            "step 3076: generator_loss=0.7041491270065308, discriminator_loss=0.6846572160720825\n",
            "step 3077: generator_loss=0.697800874710083, discriminator_loss=0.6887500286102295\n",
            "step 3078: generator_loss=0.6966279745101929, discriminator_loss=0.6913582682609558\n",
            "step 3079: generator_loss=0.6890119314193726, discriminator_loss=0.6955592632293701\n",
            "step 3080: generator_loss=0.6969594359397888, discriminator_loss=0.6916829347610474\n",
            "step 3081: generator_loss=0.691597580909729, discriminator_loss=0.695082426071167\n",
            "step 3082: generator_loss=0.6921793222427368, discriminator_loss=0.6946765780448914\n",
            "step 3083: generator_loss=0.6883866190910339, discriminator_loss=0.6967076659202576\n",
            "step 3084: generator_loss=0.6868207454681396, discriminator_loss=0.6975351572036743\n",
            "step 3085: generator_loss=0.683661937713623, discriminator_loss=0.7001981735229492\n",
            "step 3086: generator_loss=0.6890424489974976, discriminator_loss=0.6989407539367676\n",
            "step 3087: generator_loss=0.6854872703552246, discriminator_loss=0.7019791603088379\n",
            "step 3088: generator_loss=0.6847354173660278, discriminator_loss=0.7032039165496826\n",
            "step 3089: generator_loss=0.6739501953125, discriminator_loss=0.7098599672317505\n",
            "step 3090: generator_loss=0.6779292225837708, discriminator_loss=0.7087013721466064\n",
            "step 3091: generator_loss=0.6757844686508179, discriminator_loss=0.7101478576660156\n",
            "step 3092: generator_loss=0.675884485244751, discriminator_loss=0.709807276725769\n",
            "step 3093: generator_loss=0.6750266551971436, discriminator_loss=0.7092359066009521\n",
            "step 3094: generator_loss=0.6811223030090332, discriminator_loss=0.7058659791946411\n",
            "step 3095: generator_loss=0.6765822172164917, discriminator_loss=0.708842396736145\n",
            "step 3096: generator_loss=0.6785587668418884, discriminator_loss=0.7074888944625854\n",
            "step 3097: generator_loss=0.671894907951355, discriminator_loss=0.7110803127288818\n",
            "step 3098: generator_loss=0.6730496287345886, discriminator_loss=0.7111157178878784\n",
            "step 3099: generator_loss=0.6764057874679565, discriminator_loss=0.7080338001251221\n",
            "step 3100: generator_loss=0.6752970218658447, discriminator_loss=0.7098995447158813\n",
            "step 3101: generator_loss=0.6706012487411499, discriminator_loss=0.7126095294952393\n",
            "step 3102: generator_loss=0.6714692115783691, discriminator_loss=0.7128099799156189\n",
            "step 3103: generator_loss=0.6759604215621948, discriminator_loss=0.7091259956359863\n",
            "step 3104: generator_loss=0.6739526391029358, discriminator_loss=0.7113084197044373\n",
            "step 3105: generator_loss=0.6739696264266968, discriminator_loss=0.7084985971450806\n",
            "step 3106: generator_loss=0.6792140007019043, discriminator_loss=0.7075546979904175\n",
            "step 3107: generator_loss=0.6850571632385254, discriminator_loss=0.7040020227432251\n",
            "step 3108: generator_loss=0.6828227043151855, discriminator_loss=0.7048665285110474\n",
            "step 3109: generator_loss=0.6855641603469849, discriminator_loss=0.7044359445571899\n",
            "step 3110: generator_loss=0.6921958327293396, discriminator_loss=0.7015373706817627\n",
            "step 3111: generator_loss=0.6880871653556824, discriminator_loss=0.7035591006278992\n",
            "step 3112: generator_loss=0.693062424659729, discriminator_loss=0.6996402740478516\n",
            "step 3113: generator_loss=0.689105749130249, discriminator_loss=0.6999160051345825\n",
            "step 3114: generator_loss=0.6910979747772217, discriminator_loss=0.6982518434524536\n",
            "step 3115: generator_loss=0.6944514513015747, discriminator_loss=0.6933954954147339\n",
            "step 3116: generator_loss=0.7007800936698914, discriminator_loss=0.6889946460723877\n",
            "step 3117: generator_loss=0.704933762550354, discriminator_loss=0.6834285259246826\n",
            "step 3118: generator_loss=0.7031508088111877, discriminator_loss=0.6834979057312012\n",
            "step 3119: generator_loss=0.713431715965271, discriminator_loss=0.6765012741088867\n",
            "step 3120: generator_loss=0.7217546701431274, discriminator_loss=0.6704345345497131\n",
            "step 3121: generator_loss=0.7273014187812805, discriminator_loss=0.6674437522888184\n",
            "step 3122: generator_loss=0.7269462943077087, discriminator_loss=0.6666672825813293\n",
            "step 3123: generator_loss=0.7287531495094299, discriminator_loss=0.6651852130889893\n",
            "step 3124: generator_loss=0.731948733329773, discriminator_loss=0.6628211140632629\n",
            "step 3125: generator_loss=0.7431603670120239, discriminator_loss=0.656827986240387\n",
            "step 3126: generator_loss=0.7473781108856201, discriminator_loss=0.6525430679321289\n",
            "step 3127: generator_loss=0.7389030456542969, discriminator_loss=0.6541284322738647\n",
            "step 3128: generator_loss=0.7489699125289917, discriminator_loss=0.6487241983413696\n",
            "step 3129: generator_loss=0.7431535124778748, discriminator_loss=0.6496217250823975\n",
            "step 3130: generator_loss=0.7523725032806396, discriminator_loss=0.6450034379959106\n",
            "step 3131: generator_loss=0.7454557418823242, discriminator_loss=0.6483559608459473\n",
            "step 3132: generator_loss=0.746635913848877, discriminator_loss=0.6469336748123169\n",
            "step 3133: generator_loss=0.7505211234092712, discriminator_loss=0.6451293230056763\n",
            "step 3134: generator_loss=0.7406508922576904, discriminator_loss=0.651090145111084\n",
            "step 3135: generator_loss=0.7371352910995483, discriminator_loss=0.6520546674728394\n",
            "step 3136: generator_loss=0.7355459928512573, discriminator_loss=0.6525419354438782\n",
            "step 3137: generator_loss=0.7331345081329346, discriminator_loss=0.6537734270095825\n",
            "step 3138: generator_loss=0.7298716306686401, discriminator_loss=0.6547771692276001\n",
            "step 3139: generator_loss=0.7288162708282471, discriminator_loss=0.6550254225730896\n",
            "step 3140: generator_loss=0.7308114767074585, discriminator_loss=0.6535316705703735\n",
            "step 3141: generator_loss=0.7251678109169006, discriminator_loss=0.6580851078033447\n",
            "step 3142: generator_loss=0.7360833883285522, discriminator_loss=0.6532368063926697\n",
            "step 3143: generator_loss=0.7342701554298401, discriminator_loss=0.6542062759399414\n",
            "step 3144: generator_loss=0.7338674068450928, discriminator_loss=0.6569464802742004\n",
            "step 3145: generator_loss=0.7325536608695984, discriminator_loss=0.6595406532287598\n",
            "step 3146: generator_loss=0.7436884641647339, discriminator_loss=0.6561390161514282\n",
            "step 3147: generator_loss=0.7413358092308044, discriminator_loss=0.6589681506156921\n",
            "step 3148: generator_loss=0.7344202399253845, discriminator_loss=0.6620032787322998\n",
            "step 3149: generator_loss=0.7334890365600586, discriminator_loss=0.6613248586654663\n",
            "step 3150: generator_loss=0.7280551195144653, discriminator_loss=0.662973165512085\n",
            "step 3151: generator_loss=0.7275255918502808, discriminator_loss=0.6625896692276001\n",
            "step 3152: generator_loss=0.7218667268753052, discriminator_loss=0.6655926704406738\n",
            "step 3153: generator_loss=0.7163091897964478, discriminator_loss=0.6670434474945068\n",
            "step 3154: generator_loss=0.718238353729248, discriminator_loss=0.6654700636863708\n",
            "step 3155: generator_loss=0.7112193703651428, discriminator_loss=0.6682661771774292\n",
            "step 3156: generator_loss=0.7117026448249817, discriminator_loss=0.6666139364242554\n",
            "step 3157: generator_loss=0.7073529958724976, discriminator_loss=0.6681252717971802\n",
            "step 3158: generator_loss=0.7149118185043335, discriminator_loss=0.6662373542785645\n",
            "step 3159: generator_loss=0.7148481011390686, discriminator_loss=0.6669315099716187\n",
            "step 3160: generator_loss=0.7186765074729919, discriminator_loss=0.6665066480636597\n",
            "step 3161: generator_loss=0.7232075929641724, discriminator_loss=0.6670739650726318\n",
            "step 3162: generator_loss=0.7295600771903992, discriminator_loss=0.6644339561462402\n",
            "step 3163: generator_loss=0.7308422327041626, discriminator_loss=0.6639214754104614\n",
            "step 3164: generator_loss=0.7265130281448364, discriminator_loss=0.6670225858688354\n",
            "step 3165: generator_loss=0.7277147769927979, discriminator_loss=0.6653140187263489\n",
            "step 3166: generator_loss=0.7237419486045837, discriminator_loss=0.6663025617599487\n",
            "step 3167: generator_loss=0.7278841733932495, discriminator_loss=0.6652417182922363\n",
            "step 3168: generator_loss=0.7311921715736389, discriminator_loss=0.6619886159896851\n",
            "step 3169: generator_loss=0.7252307534217834, discriminator_loss=0.6636461019515991\n",
            "step 3170: generator_loss=0.7195349931716919, discriminator_loss=0.6662812232971191\n",
            "step 3171: generator_loss=0.7260949611663818, discriminator_loss=0.6607425212860107\n",
            "step 3172: generator_loss=0.7265356779098511, discriminator_loss=0.6588547229766846\n",
            "step 3173: generator_loss=0.722091794013977, discriminator_loss=0.6593480110168457\n",
            "step 3174: generator_loss=0.7319234609603882, discriminator_loss=0.6527554988861084\n",
            "step 3175: generator_loss=0.731139063835144, discriminator_loss=0.6517165899276733\n",
            "step 3176: generator_loss=0.7336443662643433, discriminator_loss=0.6473010778427124\n",
            "step 3177: generator_loss=0.7487713098526001, discriminator_loss=0.6392655968666077\n",
            "step 3178: generator_loss=0.7546628713607788, discriminator_loss=0.6341361999511719\n",
            "step 3179: generator_loss=0.7537326812744141, discriminator_loss=0.6340165138244629\n",
            "step 3180: generator_loss=0.7654289603233337, discriminator_loss=0.6296583414077759\n",
            "step 3181: generator_loss=0.7713510394096375, discriminator_loss=0.6304038763046265\n",
            "step 3182: generator_loss=0.7745612859725952, discriminator_loss=0.6318733096122742\n",
            "step 3183: generator_loss=0.7791352272033691, discriminator_loss=0.6295623779296875\n",
            "step 3184: generator_loss=0.7709519863128662, discriminator_loss=0.6335674524307251\n",
            "step 3185: generator_loss=0.7765374779701233, discriminator_loss=0.6309980154037476\n",
            "step 3186: generator_loss=0.773289680480957, discriminator_loss=0.6309418082237244\n",
            "step 3187: generator_loss=0.7661316394805908, discriminator_loss=0.6331262588500977\n",
            "step 3188: generator_loss=0.7669596672058105, discriminator_loss=0.6301113367080688\n",
            "step 3189: generator_loss=0.7610716819763184, discriminator_loss=0.6307568550109863\n",
            "step 3190: generator_loss=0.755459725856781, discriminator_loss=0.6317735910415649\n",
            "step 3191: generator_loss=0.7443128228187561, discriminator_loss=0.6354902982711792\n",
            "step 3192: generator_loss=0.746147096157074, discriminator_loss=0.6345844864845276\n",
            "step 3193: generator_loss=0.7347739934921265, discriminator_loss=0.6401721239089966\n",
            "step 3194: generator_loss=0.737882137298584, discriminator_loss=0.6376926302909851\n",
            "step 3195: generator_loss=0.7384805679321289, discriminator_loss=0.6396061778068542\n",
            "step 3196: generator_loss=0.7418248653411865, discriminator_loss=0.6416953206062317\n",
            "step 3197: generator_loss=0.7444248199462891, discriminator_loss=0.6422319412231445\n",
            "step 3198: generator_loss=0.7496840357780457, discriminator_loss=0.6411999464035034\n",
            "step 3199: generator_loss=0.7489595413208008, discriminator_loss=0.6425981521606445\n",
            "step 3200: generator_loss=0.7523887157440186, discriminator_loss=0.6435569524765015\n",
            "step 3201: generator_loss=0.7438415288925171, discriminator_loss=0.648749589920044\n",
            "step 3202: generator_loss=0.7500380277633667, discriminator_loss=0.6467490196228027\n",
            "step 3203: generator_loss=0.7465747594833374, discriminator_loss=0.6494499444961548\n",
            "step 3204: generator_loss=0.7454109191894531, discriminator_loss=0.6510103344917297\n",
            "step 3205: generator_loss=0.7429211735725403, discriminator_loss=0.6523972749710083\n",
            "step 3206: generator_loss=0.7320677042007446, discriminator_loss=0.6574410200119019\n",
            "step 3207: generator_loss=0.7309693098068237, discriminator_loss=0.658251941204071\n",
            "step 3208: generator_loss=0.7255144119262695, discriminator_loss=0.6614439487457275\n",
            "step 3209: generator_loss=0.7206553220748901, discriminator_loss=0.6625060439109802\n",
            "step 3210: generator_loss=0.7216343879699707, discriminator_loss=0.6636699438095093\n",
            "step 3211: generator_loss=0.7101805210113525, discriminator_loss=0.6685478687286377\n",
            "step 3212: generator_loss=0.7072985768318176, discriminator_loss=0.6718524098396301\n",
            "step 3213: generator_loss=0.7080550789833069, discriminator_loss=0.6719926595687866\n",
            "step 3214: generator_loss=0.7080437541007996, discriminator_loss=0.6736010313034058\n",
            "step 3215: generator_loss=0.7065081000328064, discriminator_loss=0.676176905632019\n",
            "step 3216: generator_loss=0.7023880481719971, discriminator_loss=0.6797865033149719\n",
            "step 3217: generator_loss=0.6992427110671997, discriminator_loss=0.6815038919448853\n",
            "step 3218: generator_loss=0.7052145600318909, discriminator_loss=0.6813901662826538\n",
            "step 3219: generator_loss=0.7016336917877197, discriminator_loss=0.6850866079330444\n",
            "step 3220: generator_loss=0.6963273286819458, discriminator_loss=0.68798828125\n",
            "step 3221: generator_loss=0.6957882046699524, discriminator_loss=0.6903092861175537\n",
            "step 3222: generator_loss=0.6980708837509155, discriminator_loss=0.6906169652938843\n",
            "step 3223: generator_loss=0.689732551574707, discriminator_loss=0.6958075165748596\n",
            "step 3224: generator_loss=0.6975682377815247, discriminator_loss=0.689896285533905\n",
            "step 3225: generator_loss=0.7002322673797607, discriminator_loss=0.6897556781768799\n",
            "step 3226: generator_loss=0.6955739259719849, discriminator_loss=0.6933728456497192\n",
            "step 3227: generator_loss=0.697009801864624, discriminator_loss=0.6898095011711121\n",
            "step 3228: generator_loss=0.7025972604751587, discriminator_loss=0.6865049600601196\n",
            "step 3229: generator_loss=0.6941159963607788, discriminator_loss=0.6898891925811768\n",
            "step 3230: generator_loss=0.6981425285339355, discriminator_loss=0.6898593306541443\n",
            "step 3231: generator_loss=0.7011081576347351, discriminator_loss=0.6881126165390015\n",
            "step 3232: generator_loss=0.6976450085639954, discriminator_loss=0.6896555423736572\n",
            "step 3233: generator_loss=0.6972993612289429, discriminator_loss=0.6903160810470581\n",
            "step 3234: generator_loss=0.6911851167678833, discriminator_loss=0.6927767395973206\n",
            "step 3235: generator_loss=0.7038670182228088, discriminator_loss=0.6851212382316589\n",
            "step 3236: generator_loss=0.6973241567611694, discriminator_loss=0.6892096400260925\n",
            "step 3237: generator_loss=0.6940535306930542, discriminator_loss=0.6898657083511353\n",
            "step 3238: generator_loss=0.6934112906455994, discriminator_loss=0.6900663375854492\n",
            "step 3239: generator_loss=0.6893613934516907, discriminator_loss=0.6935005187988281\n",
            "step 3240: generator_loss=0.6954864859580994, discriminator_loss=0.6906838417053223\n",
            "step 3241: generator_loss=0.6938294172286987, discriminator_loss=0.6922023296356201\n",
            "step 3242: generator_loss=0.6923220753669739, discriminator_loss=0.6920379400253296\n",
            "step 3243: generator_loss=0.686013400554657, discriminator_loss=0.6954939961433411\n",
            "step 3244: generator_loss=0.6762112379074097, discriminator_loss=0.7000491619110107\n",
            "step 3245: generator_loss=0.6833919286727905, discriminator_loss=0.6972102522850037\n",
            "step 3246: generator_loss=0.6884210109710693, discriminator_loss=0.6942715644836426\n",
            "step 3247: generator_loss=0.6803692579269409, discriminator_loss=0.6978169679641724\n",
            "step 3248: generator_loss=0.6789678335189819, discriminator_loss=0.7001651525497437\n",
            "step 3249: generator_loss=0.6829590797424316, discriminator_loss=0.7001807689666748\n",
            "step 3250: generator_loss=0.6850792169570923, discriminator_loss=0.6997580528259277\n",
            "step 3251: generator_loss=0.6857550740242004, discriminator_loss=0.7002825736999512\n",
            "step 3252: generator_loss=0.6830688714981079, discriminator_loss=0.7049916982650757\n",
            "step 3253: generator_loss=0.6860532760620117, discriminator_loss=0.7036361694335938\n",
            "step 3254: generator_loss=0.680798351764679, discriminator_loss=0.7079002857208252\n",
            "step 3255: generator_loss=0.6763139963150024, discriminator_loss=0.7114349603652954\n",
            "step 3256: generator_loss=0.6798565983772278, discriminator_loss=0.7082971334457397\n",
            "step 3257: generator_loss=0.677984356880188, discriminator_loss=0.7096356153488159\n",
            "step 3258: generator_loss=0.6703490018844604, discriminator_loss=0.7139603495597839\n",
            "step 3259: generator_loss=0.6694079637527466, discriminator_loss=0.7133610248565674\n",
            "step 3260: generator_loss=0.6643843650817871, discriminator_loss=0.7171453833580017\n",
            "step 3261: generator_loss=0.6571568846702576, discriminator_loss=0.7213284969329834\n",
            "step 3262: generator_loss=0.6608449220657349, discriminator_loss=0.7186648845672607\n",
            "step 3263: generator_loss=0.6546242237091064, discriminator_loss=0.7238651514053345\n",
            "step 3264: generator_loss=0.6601293683052063, discriminator_loss=0.7198008894920349\n",
            "step 3265: generator_loss=0.6583831310272217, discriminator_loss=0.7228779792785645\n",
            "step 3266: generator_loss=0.6594961285591125, discriminator_loss=0.7226091623306274\n",
            "step 3267: generator_loss=0.6607613563537598, discriminator_loss=0.7236047983169556\n",
            "step 3268: generator_loss=0.6558146476745605, discriminator_loss=0.7250516414642334\n",
            "step 3269: generator_loss=0.6602399945259094, discriminator_loss=0.7255572080612183\n",
            "step 3270: generator_loss=0.6609140634536743, discriminator_loss=0.7260963916778564\n",
            "step 3271: generator_loss=0.6607598662376404, discriminator_loss=0.7283740043640137\n",
            "step 3272: generator_loss=0.6592191457748413, discriminator_loss=0.7293641567230225\n",
            "step 3273: generator_loss=0.6609076857566833, discriminator_loss=0.7273941040039062\n",
            "step 3274: generator_loss=0.6598249673843384, discriminator_loss=0.7289383411407471\n",
            "step 3275: generator_loss=0.6601741313934326, discriminator_loss=0.7307947874069214\n",
            "step 3276: generator_loss=0.6669116020202637, discriminator_loss=0.7260876297950745\n",
            "step 3277: generator_loss=0.6591058969497681, discriminator_loss=0.7299128174781799\n",
            "step 3278: generator_loss=0.6723330020904541, discriminator_loss=0.7219610214233398\n",
            "step 3279: generator_loss=0.6718257665634155, discriminator_loss=0.7218878865242004\n",
            "step 3280: generator_loss=0.6710841059684753, discriminator_loss=0.7229021191596985\n",
            "step 3281: generator_loss=0.6758196353912354, discriminator_loss=0.7211782932281494\n",
            "step 3282: generator_loss=0.6771166324615479, discriminator_loss=0.718899130821228\n",
            "step 3283: generator_loss=0.6833245754241943, discriminator_loss=0.7144012451171875\n",
            "step 3284: generator_loss=0.6788363456726074, discriminator_loss=0.7132978439331055\n",
            "step 3285: generator_loss=0.6750192642211914, discriminator_loss=0.7135436534881592\n",
            "step 3286: generator_loss=0.6824007034301758, discriminator_loss=0.7099401950836182\n",
            "step 3287: generator_loss=0.6773279905319214, discriminator_loss=0.7085261940956116\n",
            "step 3288: generator_loss=0.6794472932815552, discriminator_loss=0.7071189880371094\n",
            "step 3289: generator_loss=0.688795268535614, discriminator_loss=0.7000607252120972\n",
            "step 3290: generator_loss=0.6900048851966858, discriminator_loss=0.7014496922492981\n",
            "step 3291: generator_loss=0.6999365091323853, discriminator_loss=0.6978896856307983\n",
            "step 3292: generator_loss=0.708798885345459, discriminator_loss=0.6951943635940552\n",
            "step 3293: generator_loss=0.7040488719940186, discriminator_loss=0.6960456967353821\n",
            "step 3294: generator_loss=0.7079542279243469, discriminator_loss=0.6933982372283936\n",
            "step 3295: generator_loss=0.7130523920059204, discriminator_loss=0.6902261972427368\n",
            "step 3296: generator_loss=0.7116556167602539, discriminator_loss=0.6882248520851135\n",
            "step 3297: generator_loss=0.7098643183708191, discriminator_loss=0.6868309378623962\n",
            "step 3298: generator_loss=0.7094367742538452, discriminator_loss=0.6818274259567261\n",
            "step 3299: generator_loss=0.699945867061615, discriminator_loss=0.6832681894302368\n",
            "step 3300: generator_loss=0.7066215872764587, discriminator_loss=0.6771524548530579\n",
            "step 3301: generator_loss=0.7022984027862549, discriminator_loss=0.6759394407272339\n",
            "step 3302: generator_loss=0.7061296701431274, discriminator_loss=0.6757543087005615\n",
            "step 3303: generator_loss=0.7170141339302063, discriminator_loss=0.6703095436096191\n",
            "step 3304: generator_loss=0.7247559428215027, discriminator_loss=0.6668484210968018\n",
            "step 3305: generator_loss=0.7211648225784302, discriminator_loss=0.6701421737670898\n",
            "step 3306: generator_loss=0.728614866733551, discriminator_loss=0.6670749187469482\n",
            "step 3307: generator_loss=0.7287548780441284, discriminator_loss=0.6658556461334229\n",
            "step 3308: generator_loss=0.7252753973007202, discriminator_loss=0.666305661201477\n",
            "step 3309: generator_loss=0.7170929312705994, discriminator_loss=0.6696471571922302\n",
            "step 3310: generator_loss=0.7228508591651917, discriminator_loss=0.6651700735092163\n",
            "step 3311: generator_loss=0.7074645757675171, discriminator_loss=0.6715831160545349\n",
            "step 3312: generator_loss=0.7215925455093384, discriminator_loss=0.6615087985992432\n",
            "step 3313: generator_loss=0.7320504784584045, discriminator_loss=0.656526505947113\n",
            "step 3314: generator_loss=0.727670431137085, discriminator_loss=0.6577997803688049\n",
            "step 3315: generator_loss=0.7378722429275513, discriminator_loss=0.6534276008605957\n",
            "step 3316: generator_loss=0.7430978417396545, discriminator_loss=0.6517245769500732\n",
            "step 3317: generator_loss=0.7385241389274597, discriminator_loss=0.6546205878257751\n",
            "step 3318: generator_loss=0.752331018447876, discriminator_loss=0.6495088338851929\n",
            "step 3319: generator_loss=0.7566445469856262, discriminator_loss=0.6467722654342651\n",
            "step 3320: generator_loss=0.7626244425773621, discriminator_loss=0.6438406705856323\n",
            "step 3321: generator_loss=0.747509777545929, discriminator_loss=0.6491189002990723\n",
            "step 3322: generator_loss=0.741094708442688, discriminator_loss=0.6522014141082764\n",
            "step 3323: generator_loss=0.7630420923233032, discriminator_loss=0.6415607333183289\n",
            "step 3324: generator_loss=0.7514914274215698, discriminator_loss=0.6444072723388672\n",
            "step 3325: generator_loss=0.7457169890403748, discriminator_loss=0.6464414596557617\n",
            "step 3326: generator_loss=0.7489710450172424, discriminator_loss=0.6443188190460205\n",
            "step 3327: generator_loss=0.7446560859680176, discriminator_loss=0.6459144353866577\n",
            "step 3328: generator_loss=0.735756516456604, discriminator_loss=0.6508023738861084\n",
            "step 3329: generator_loss=0.741096019744873, discriminator_loss=0.6495572328567505\n",
            "step 3330: generator_loss=0.7341065406799316, discriminator_loss=0.6541828513145447\n",
            "step 3331: generator_loss=0.7459679841995239, discriminator_loss=0.6503294110298157\n",
            "step 3332: generator_loss=0.7425921559333801, discriminator_loss=0.6538862586021423\n",
            "step 3333: generator_loss=0.7382535338401794, discriminator_loss=0.6551135778427124\n",
            "step 3334: generator_loss=0.7201590538024902, discriminator_loss=0.6673703193664551\n",
            "step 3335: generator_loss=0.732161283493042, discriminator_loss=0.6604136228561401\n",
            "step 3336: generator_loss=0.7412945032119751, discriminator_loss=0.6571096777915955\n",
            "step 3337: generator_loss=0.7274186015129089, discriminator_loss=0.6657480597496033\n",
            "step 3338: generator_loss=0.7127452492713928, discriminator_loss=0.6761033535003662\n",
            "step 3339: generator_loss=0.7306572198867798, discriminator_loss=0.666182816028595\n",
            "step 3340: generator_loss=0.7303777933120728, discriminator_loss=0.6682844161987305\n",
            "step 3341: generator_loss=0.7237462997436523, discriminator_loss=0.6729749441146851\n",
            "step 3342: generator_loss=0.7178388833999634, discriminator_loss=0.6764845848083496\n",
            "step 3343: generator_loss=0.7113803625106812, discriminator_loss=0.6805194020271301\n",
            "step 3344: generator_loss=0.7257462739944458, discriminator_loss=0.672418475151062\n",
            "step 3345: generator_loss=0.720825731754303, discriminator_loss=0.6756280064582825\n",
            "step 3346: generator_loss=0.7162007093429565, discriminator_loss=0.6784343123435974\n",
            "step 3347: generator_loss=0.7297663688659668, discriminator_loss=0.669098973274231\n",
            "step 3348: generator_loss=0.7368450164794922, discriminator_loss=0.6646968126296997\n",
            "step 3349: generator_loss=0.734201967716217, discriminator_loss=0.6659204959869385\n",
            "step 3350: generator_loss=0.7106299996376038, discriminator_loss=0.6787722110748291\n",
            "step 3351: generator_loss=0.7299312949180603, discriminator_loss=0.6669418811798096\n",
            "step 3352: generator_loss=0.7035090327262878, discriminator_loss=0.6814253330230713\n",
            "step 3353: generator_loss=0.7124173641204834, discriminator_loss=0.6765971779823303\n",
            "step 3354: generator_loss=0.7137882113456726, discriminator_loss=0.6759883165359497\n",
            "step 3355: generator_loss=0.7133602499961853, discriminator_loss=0.6765815615653992\n",
            "step 3356: generator_loss=0.7149751782417297, discriminator_loss=0.6772226691246033\n",
            "step 3357: generator_loss=0.7281715869903564, discriminator_loss=0.6717071533203125\n",
            "step 3358: generator_loss=0.7090997099876404, discriminator_loss=0.684715747833252\n",
            "step 3359: generator_loss=0.7468469738960266, discriminator_loss=0.6627180576324463\n",
            "step 3360: generator_loss=0.7290318608283997, discriminator_loss=0.6728237867355347\n",
            "step 3361: generator_loss=0.7170367240905762, discriminator_loss=0.6792894601821899\n",
            "step 3362: generator_loss=0.7380204796791077, discriminator_loss=0.6655124425888062\n",
            "step 3363: generator_loss=0.7326358556747437, discriminator_loss=0.668157160282135\n",
            "step 3364: generator_loss=0.7225736379623413, discriminator_loss=0.6731404066085815\n",
            "step 3365: generator_loss=0.7187982797622681, discriminator_loss=0.6735427975654602\n",
            "step 3366: generator_loss=0.7241148352622986, discriminator_loss=0.669289231300354\n",
            "step 3367: generator_loss=0.7224937677383423, discriminator_loss=0.6695435643196106\n",
            "step 3368: generator_loss=0.7130416035652161, discriminator_loss=0.6740015745162964\n",
            "step 3369: generator_loss=0.7373987436294556, discriminator_loss=0.6593941450119019\n",
            "step 3370: generator_loss=0.7353233098983765, discriminator_loss=0.6598084568977356\n",
            "step 3371: generator_loss=0.7389211058616638, discriminator_loss=0.6568156480789185\n",
            "step 3372: generator_loss=0.734016478061676, discriminator_loss=0.6619631052017212\n",
            "step 3373: generator_loss=0.7409391403198242, discriminator_loss=0.6585772037506104\n",
            "step 3374: generator_loss=0.7343851327896118, discriminator_loss=0.6625680327415466\n",
            "step 3375: generator_loss=0.7520377039909363, discriminator_loss=0.6548680067062378\n",
            "step 3376: generator_loss=0.7433707118034363, discriminator_loss=0.6605420112609863\n",
            "step 3377: generator_loss=0.7347838282585144, discriminator_loss=0.6644185781478882\n",
            "step 3378: generator_loss=0.7293558120727539, discriminator_loss=0.6668549180030823\n",
            "step 3379: generator_loss=0.7394883632659912, discriminator_loss=0.6591815948486328\n",
            "step 3380: generator_loss=0.7411837577819824, discriminator_loss=0.6586912870407104\n",
            "step 3381: generator_loss=0.7277671098709106, discriminator_loss=0.6632294654846191\n",
            "step 3382: generator_loss=0.718513548374176, discriminator_loss=0.6677724123001099\n",
            "step 3383: generator_loss=0.7420167922973633, discriminator_loss=0.6548324823379517\n",
            "step 3384: generator_loss=0.7226426005363464, discriminator_loss=0.6634525060653687\n",
            "step 3385: generator_loss=0.7370761036872864, discriminator_loss=0.6551440954208374\n",
            "step 3386: generator_loss=0.7247493267059326, discriminator_loss=0.6608321070671082\n",
            "step 3387: generator_loss=0.7251473665237427, discriminator_loss=0.658606767654419\n",
            "step 3388: generator_loss=0.7215894460678101, discriminator_loss=0.6578029990196228\n",
            "step 3389: generator_loss=0.7309887409210205, discriminator_loss=0.6521851420402527\n",
            "step 3390: generator_loss=0.72688889503479, discriminator_loss=0.6527341604232788\n",
            "step 3391: generator_loss=0.7231961488723755, discriminator_loss=0.6538017988204956\n",
            "step 3392: generator_loss=0.7161422967910767, discriminator_loss=0.657712459564209\n",
            "step 3393: generator_loss=0.7268593311309814, discriminator_loss=0.6502904295921326\n",
            "step 3394: generator_loss=0.7274526357650757, discriminator_loss=0.6495823860168457\n",
            "step 3395: generator_loss=0.7357406616210938, discriminator_loss=0.6460051536560059\n",
            "step 3396: generator_loss=0.7421491742134094, discriminator_loss=0.6432485580444336\n",
            "step 3397: generator_loss=0.7499099969863892, discriminator_loss=0.6415115594863892\n",
            "step 3398: generator_loss=0.7509523630142212, discriminator_loss=0.6397255659103394\n",
            "step 3399: generator_loss=0.7577385902404785, discriminator_loss=0.6383434534072876\n",
            "step 3400: generator_loss=0.7625083923339844, discriminator_loss=0.634799063205719\n",
            "step 3401: generator_loss=0.7672842741012573, discriminator_loss=0.6334742903709412\n",
            "step 3402: generator_loss=0.7687617540359497, discriminator_loss=0.6312835812568665\n",
            "step 3403: generator_loss=0.769721269607544, discriminator_loss=0.6305299997329712\n",
            "step 3404: generator_loss=0.7748370170593262, discriminator_loss=0.6270765066146851\n",
            "step 3405: generator_loss=0.7774444818496704, discriminator_loss=0.624983549118042\n",
            "step 3406: generator_loss=0.7774943113327026, discriminator_loss=0.6233903169631958\n",
            "step 3407: generator_loss=0.7814236879348755, discriminator_loss=0.6199116706848145\n",
            "step 3408: generator_loss=0.7841796875, discriminator_loss=0.6181339025497437\n",
            "step 3409: generator_loss=0.7838380336761475, discriminator_loss=0.6161037683486938\n",
            "step 3410: generator_loss=0.782608151435852, discriminator_loss=0.6168941259384155\n",
            "step 3411: generator_loss=0.790132999420166, discriminator_loss=0.6137808561325073\n",
            "step 3412: generator_loss=0.7935879230499268, discriminator_loss=0.6130342483520508\n",
            "step 3413: generator_loss=0.7925683856010437, discriminator_loss=0.6114922761917114\n",
            "step 3414: generator_loss=0.7893590927124023, discriminator_loss=0.6131381392478943\n",
            "step 3415: generator_loss=0.7965587377548218, discriminator_loss=0.6092693209648132\n",
            "step 3416: generator_loss=0.77662193775177, discriminator_loss=0.6170106530189514\n",
            "step 3417: generator_loss=0.7900014519691467, discriminator_loss=0.611275315284729\n",
            "step 3418: generator_loss=0.7823723554611206, discriminator_loss=0.6133979558944702\n",
            "step 3419: generator_loss=0.7781840562820435, discriminator_loss=0.6155187487602234\n",
            "step 3420: generator_loss=0.7731743454933167, discriminator_loss=0.6179993748664856\n",
            "step 3421: generator_loss=0.7863756418228149, discriminator_loss=0.6113826036453247\n",
            "step 3422: generator_loss=0.7682779431343079, discriminator_loss=0.6198207139968872\n",
            "step 3423: generator_loss=0.7829176783561707, discriminator_loss=0.6131944060325623\n",
            "step 3424: generator_loss=0.775673508644104, discriminator_loss=0.6154978275299072\n",
            "step 3425: generator_loss=0.7666233777999878, discriminator_loss=0.6191115379333496\n",
            "step 3426: generator_loss=0.7746989130973816, discriminator_loss=0.6165436506271362\n",
            "step 3427: generator_loss=0.7832016348838806, discriminator_loss=0.61326003074646\n",
            "step 3428: generator_loss=0.7696107625961304, discriminator_loss=0.6187149286270142\n",
            "step 3429: generator_loss=0.7767193913459778, discriminator_loss=0.6173171997070312\n",
            "step 3430: generator_loss=0.7738838195800781, discriminator_loss=0.6190721988677979\n",
            "step 3431: generator_loss=0.7760816216468811, discriminator_loss=0.6210668087005615\n",
            "step 3432: generator_loss=0.7725322246551514, discriminator_loss=0.6218622326850891\n",
            "step 3433: generator_loss=0.7609368562698364, discriminator_loss=0.6261770129203796\n",
            "step 3434: generator_loss=0.7756540775299072, discriminator_loss=0.6227457523345947\n",
            "step 3435: generator_loss=0.7727949619293213, discriminator_loss=0.6248774528503418\n",
            "step 3436: generator_loss=0.7577807903289795, discriminator_loss=0.6320358514785767\n",
            "step 3437: generator_loss=0.7632617950439453, discriminator_loss=0.6306745409965515\n",
            "step 3438: generator_loss=0.772140383720398, discriminator_loss=0.6253523230552673\n",
            "step 3439: generator_loss=0.7756967544555664, discriminator_loss=0.6253185272216797\n",
            "step 3440: generator_loss=0.7451701164245605, discriminator_loss=0.6379785537719727\n",
            "step 3441: generator_loss=0.7392528653144836, discriminator_loss=0.6401731967926025\n",
            "step 3442: generator_loss=0.7526570558547974, discriminator_loss=0.6355868577957153\n",
            "step 3443: generator_loss=0.7478588819503784, discriminator_loss=0.6381812691688538\n",
            "step 3444: generator_loss=0.753868579864502, discriminator_loss=0.6358109712600708\n",
            "step 3445: generator_loss=0.7416547536849976, discriminator_loss=0.6412668228149414\n",
            "step 3446: generator_loss=0.7467213869094849, discriminator_loss=0.6405093669891357\n",
            "step 3447: generator_loss=0.7598997354507446, discriminator_loss=0.6362521648406982\n",
            "step 3448: generator_loss=0.7395003437995911, discriminator_loss=0.6465198397636414\n",
            "step 3449: generator_loss=0.7504857182502747, discriminator_loss=0.6444483995437622\n",
            "step 3450: generator_loss=0.7607225179672241, discriminator_loss=0.6423884630203247\n",
            "step 3451: generator_loss=0.7645155191421509, discriminator_loss=0.6420814990997314\n",
            "step 3452: generator_loss=0.7498438358306885, discriminator_loss=0.6483147144317627\n",
            "step 3453: generator_loss=0.7551560401916504, discriminator_loss=0.6481826305389404\n",
            "step 3454: generator_loss=0.7580844759941101, discriminator_loss=0.6471989154815674\n",
            "step 3455: generator_loss=0.7604719996452332, discriminator_loss=0.6465535759925842\n",
            "step 3456: generator_loss=0.748331606388092, discriminator_loss=0.6512773036956787\n",
            "step 3457: generator_loss=0.7359874844551086, discriminator_loss=0.6571226716041565\n",
            "step 3458: generator_loss=0.7692599892616272, discriminator_loss=0.6437257528305054\n",
            "step 3459: generator_loss=0.7419679164886475, discriminator_loss=0.6553294658660889\n",
            "step 3460: generator_loss=0.7745869159698486, discriminator_loss=0.6421847343444824\n",
            "step 3461: generator_loss=0.7467837333679199, discriminator_loss=0.6542842984199524\n",
            "step 3462: generator_loss=0.7467629909515381, discriminator_loss=0.6513804793357849\n",
            "step 3463: generator_loss=0.7210257053375244, discriminator_loss=0.6645357608795166\n",
            "step 3464: generator_loss=0.7116121053695679, discriminator_loss=0.668817400932312\n",
            "step 3465: generator_loss=0.7182537913322449, discriminator_loss=0.668168306350708\n",
            "step 3466: generator_loss=0.719152569770813, discriminator_loss=0.6691162586212158\n",
            "step 3467: generator_loss=0.7091373205184937, discriminator_loss=0.6745080947875977\n",
            "step 3468: generator_loss=0.7036774158477783, discriminator_loss=0.6788052320480347\n",
            "step 3469: generator_loss=0.7034144997596741, discriminator_loss=0.6828564405441284\n",
            "step 3470: generator_loss=0.7084856033325195, discriminator_loss=0.6822166442871094\n",
            "step 3471: generator_loss=0.6842046976089478, discriminator_loss=0.6951457262039185\n",
            "step 3472: generator_loss=0.7169966697692871, discriminator_loss=0.6831957697868347\n",
            "step 3473: generator_loss=0.7046645879745483, discriminator_loss=0.6896057724952698\n",
            "step 3474: generator_loss=0.7123152017593384, discriminator_loss=0.6865072250366211\n",
            "step 3475: generator_loss=0.7150131464004517, discriminator_loss=0.6873183846473694\n",
            "step 3476: generator_loss=0.6983486413955688, discriminator_loss=0.6957605481147766\n",
            "step 3477: generator_loss=0.6976332664489746, discriminator_loss=0.6951167583465576\n",
            "step 3478: generator_loss=0.6798133850097656, discriminator_loss=0.7038787603378296\n",
            "step 3479: generator_loss=0.6738024353981018, discriminator_loss=0.7075952291488647\n",
            "step 3480: generator_loss=0.6804653406143188, discriminator_loss=0.7049746513366699\n",
            "step 3481: generator_loss=0.6933525800704956, discriminator_loss=0.7001680135726929\n",
            "step 3482: generator_loss=0.6985352039337158, discriminator_loss=0.697562575340271\n",
            "step 3483: generator_loss=0.6791508793830872, discriminator_loss=0.7058619260787964\n",
            "step 3484: generator_loss=0.6815434694290161, discriminator_loss=0.7093989849090576\n",
            "step 3485: generator_loss=0.678092360496521, discriminator_loss=0.711351752281189\n",
            "step 3486: generator_loss=0.6843769550323486, discriminator_loss=0.7097408771514893\n",
            "step 3487: generator_loss=0.6800912618637085, discriminator_loss=0.7127877473831177\n",
            "step 3488: generator_loss=0.6841692924499512, discriminator_loss=0.7102453112602234\n",
            "step 3489: generator_loss=0.6681357622146606, discriminator_loss=0.7182618975639343\n",
            "step 3490: generator_loss=0.6793993711471558, discriminator_loss=0.7138357758522034\n",
            "step 3491: generator_loss=0.6822431087493896, discriminator_loss=0.7109867334365845\n",
            "step 3492: generator_loss=0.675390899181366, discriminator_loss=0.7157394886016846\n",
            "step 3493: generator_loss=0.6728070974349976, discriminator_loss=0.716221272945404\n",
            "step 3494: generator_loss=0.6805018186569214, discriminator_loss=0.7113281488418579\n",
            "step 3495: generator_loss=0.6682454347610474, discriminator_loss=0.7148078680038452\n",
            "step 3496: generator_loss=0.6806348562240601, discriminator_loss=0.7059844136238098\n",
            "step 3497: generator_loss=0.6638691425323486, discriminator_loss=0.7139413356781006\n",
            "step 3498: generator_loss=0.6922181844711304, discriminator_loss=0.7014099359512329\n",
            "step 3499: generator_loss=0.6799864172935486, discriminator_loss=0.7062736749649048\n",
            "step 3500: generator_loss=0.6987365484237671, discriminator_loss=0.6986966729164124\n",
            "step 3501: generator_loss=0.6982942223548889, discriminator_loss=0.6993335485458374\n",
            "step 3502: generator_loss=0.6984856128692627, discriminator_loss=0.7009186744689941\n",
            "step 3503: generator_loss=0.7056880593299866, discriminator_loss=0.6988734006881714\n",
            "step 3504: generator_loss=0.713159441947937, discriminator_loss=0.6956601142883301\n",
            "step 3505: generator_loss=0.7145061492919922, discriminator_loss=0.6939364671707153\n",
            "step 3506: generator_loss=0.7197046279907227, discriminator_loss=0.6905182600021362\n",
            "step 3507: generator_loss=0.7106519341468811, discriminator_loss=0.6930748820304871\n",
            "step 3508: generator_loss=0.7035274505615234, discriminator_loss=0.6932907700538635\n",
            "step 3509: generator_loss=0.6939886212348938, discriminator_loss=0.6944024562835693\n",
            "step 3510: generator_loss=0.6894212961196899, discriminator_loss=0.6935828924179077\n",
            "step 3511: generator_loss=0.6849640607833862, discriminator_loss=0.6919808387756348\n",
            "step 3512: generator_loss=0.6911634802818298, discriminator_loss=0.6856480836868286\n",
            "step 3513: generator_loss=0.689184308052063, discriminator_loss=0.6837360262870789\n",
            "step 3514: generator_loss=0.688896894454956, discriminator_loss=0.6832519769668579\n",
            "step 3515: generator_loss=0.6883895397186279, discriminator_loss=0.6841402649879456\n",
            "step 3516: generator_loss=0.6882469654083252, discriminator_loss=0.6838773488998413\n",
            "step 3517: generator_loss=0.7024260759353638, discriminator_loss=0.6779853105545044\n",
            "step 3518: generator_loss=0.7006882429122925, discriminator_loss=0.6786257028579712\n",
            "step 3519: generator_loss=0.7058283090591431, discriminator_loss=0.6770792007446289\n",
            "step 3520: generator_loss=0.7182128429412842, discriminator_loss=0.6717923879623413\n",
            "step 3521: generator_loss=0.7181755304336548, discriminator_loss=0.6730337738990784\n",
            "step 3522: generator_loss=0.7300901412963867, discriminator_loss=0.6688495874404907\n",
            "step 3523: generator_loss=0.7371647357940674, discriminator_loss=0.6663849353790283\n",
            "step 3524: generator_loss=0.7363455295562744, discriminator_loss=0.6667091846466064\n",
            "step 3525: generator_loss=0.737555205821991, discriminator_loss=0.6656168103218079\n",
            "step 3526: generator_loss=0.7305548191070557, discriminator_loss=0.6686978340148926\n",
            "step 3527: generator_loss=0.7357242107391357, discriminator_loss=0.6654744744300842\n",
            "step 3528: generator_loss=0.7282763719558716, discriminator_loss=0.6670383810997009\n",
            "step 3529: generator_loss=0.7254570722579956, discriminator_loss=0.6663732528686523\n",
            "step 3530: generator_loss=0.723872184753418, discriminator_loss=0.6654554605484009\n",
            "step 3531: generator_loss=0.7199689149856567, discriminator_loss=0.6657567620277405\n",
            "step 3532: generator_loss=0.7195409536361694, discriminator_loss=0.6640459895133972\n",
            "step 3533: generator_loss=0.7193974852561951, discriminator_loss=0.664241373538971\n",
            "step 3534: generator_loss=0.7177691459655762, discriminator_loss=0.6639127731323242\n",
            "step 3535: generator_loss=0.715031623840332, discriminator_loss=0.6648208498954773\n",
            "step 3536: generator_loss=0.7141748666763306, discriminator_loss=0.6639348268508911\n",
            "step 3537: generator_loss=0.7100467681884766, discriminator_loss=0.665919303894043\n",
            "step 3538: generator_loss=0.7148498296737671, discriminator_loss=0.6634455919265747\n",
            "step 3539: generator_loss=0.7166537642478943, discriminator_loss=0.6631625890731812\n",
            "step 3540: generator_loss=0.7151477336883545, discriminator_loss=0.664255678653717\n",
            "step 3541: generator_loss=0.7182453870773315, discriminator_loss=0.6636474132537842\n",
            "step 3542: generator_loss=0.7217153906822205, discriminator_loss=0.6629608273506165\n",
            "step 3543: generator_loss=0.7298915386199951, discriminator_loss=0.6597898006439209\n",
            "step 3544: generator_loss=0.7238861918449402, discriminator_loss=0.6640366315841675\n",
            "step 3545: generator_loss=0.7244440317153931, discriminator_loss=0.6626906394958496\n",
            "step 3546: generator_loss=0.7326933741569519, discriminator_loss=0.6605377197265625\n",
            "step 3547: generator_loss=0.7339433431625366, discriminator_loss=0.6589503288269043\n",
            "step 3548: generator_loss=0.7322180271148682, discriminator_loss=0.6609500646591187\n",
            "step 3549: generator_loss=0.7376933097839355, discriminator_loss=0.657047688961029\n",
            "step 3550: generator_loss=0.7367762923240662, discriminator_loss=0.6572268009185791\n",
            "step 3551: generator_loss=0.7323998212814331, discriminator_loss=0.6589678525924683\n",
            "step 3552: generator_loss=0.7348504066467285, discriminator_loss=0.6556640863418579\n",
            "step 3553: generator_loss=0.7327531576156616, discriminator_loss=0.6561747789382935\n",
            "step 3554: generator_loss=0.7281190156936646, discriminator_loss=0.657052218914032\n",
            "step 3555: generator_loss=0.7302190065383911, discriminator_loss=0.6549868583679199\n",
            "step 3556: generator_loss=0.7262405157089233, discriminator_loss=0.6556549072265625\n",
            "step 3557: generator_loss=0.7255299091339111, discriminator_loss=0.65611732006073\n",
            "step 3558: generator_loss=0.726026713848114, discriminator_loss=0.6563047170639038\n",
            "step 3559: generator_loss=0.7311075925827026, discriminator_loss=0.6529961824417114\n",
            "step 3560: generator_loss=0.732661247253418, discriminator_loss=0.6532601118087769\n",
            "step 3561: generator_loss=0.7402099967002869, discriminator_loss=0.6503628492355347\n",
            "step 3562: generator_loss=0.7435095310211182, discriminator_loss=0.6486443281173706\n",
            "step 3563: generator_loss=0.7467679977416992, discriminator_loss=0.6490461230278015\n",
            "step 3564: generator_loss=0.7523149251937866, discriminator_loss=0.6460698246955872\n",
            "step 3565: generator_loss=0.749427080154419, discriminator_loss=0.6476297378540039\n",
            "step 3566: generator_loss=0.7544521689414978, discriminator_loss=0.6439425945281982\n",
            "step 3567: generator_loss=0.7561877965927124, discriminator_loss=0.6416959166526794\n",
            "step 3568: generator_loss=0.751751184463501, discriminator_loss=0.6411807537078857\n",
            "step 3569: generator_loss=0.7482139468193054, discriminator_loss=0.6413143277168274\n",
            "step 3570: generator_loss=0.7395458221435547, discriminator_loss=0.6431884765625\n",
            "step 3571: generator_loss=0.7466698884963989, discriminator_loss=0.6401122808456421\n",
            "step 3572: generator_loss=0.7417739629745483, discriminator_loss=0.6414229869842529\n",
            "step 3573: generator_loss=0.7410620450973511, discriminator_loss=0.642017126083374\n",
            "step 3574: generator_loss=0.7415743470191956, discriminator_loss=0.6418464183807373\n",
            "step 3575: generator_loss=0.747067928314209, discriminator_loss=0.6419780254364014\n",
            "step 3576: generator_loss=0.7534171342849731, discriminator_loss=0.6406809091567993\n",
            "step 3577: generator_loss=0.7482413649559021, discriminator_loss=0.6449189782142639\n",
            "step 3578: generator_loss=0.7496473789215088, discriminator_loss=0.6450920104980469\n",
            "step 3579: generator_loss=0.7440608739852905, discriminator_loss=0.6476224660873413\n",
            "step 3580: generator_loss=0.7428320050239563, discriminator_loss=0.6470284461975098\n",
            "step 3581: generator_loss=0.7390052676200867, discriminator_loss=0.6488116979598999\n",
            "step 3582: generator_loss=0.7332302331924438, discriminator_loss=0.6518993377685547\n",
            "step 3583: generator_loss=0.7300862669944763, discriminator_loss=0.6525946855545044\n",
            "step 3584: generator_loss=0.7255145907402039, discriminator_loss=0.6541060209274292\n",
            "step 3585: generator_loss=0.7224260568618774, discriminator_loss=0.6562119126319885\n",
            "step 3586: generator_loss=0.7263953685760498, discriminator_loss=0.6543185114860535\n",
            "step 3587: generator_loss=0.7214318513870239, discriminator_loss=0.657629132270813\n",
            "step 3588: generator_loss=0.7225390672683716, discriminator_loss=0.6588523387908936\n",
            "step 3589: generator_loss=0.7202747464179993, discriminator_loss=0.6607042551040649\n",
            "step 3590: generator_loss=0.721358060836792, discriminator_loss=0.661746621131897\n",
            "step 3591: generator_loss=0.7163894176483154, discriminator_loss=0.6651144027709961\n",
            "step 3592: generator_loss=0.714439868927002, discriminator_loss=0.6665838956832886\n",
            "step 3593: generator_loss=0.7240006923675537, discriminator_loss=0.6629063487052917\n",
            "step 3594: generator_loss=0.7152526378631592, discriminator_loss=0.6687506437301636\n",
            "step 3595: generator_loss=0.7157753109931946, discriminator_loss=0.6678962111473083\n",
            "step 3596: generator_loss=0.7131929993629456, discriminator_loss=0.669971227645874\n",
            "step 3597: generator_loss=0.7115833759307861, discriminator_loss=0.672088623046875\n",
            "step 3598: generator_loss=0.7072296738624573, discriminator_loss=0.6743099689483643\n",
            "step 3599: generator_loss=0.7066882848739624, discriminator_loss=0.675589382648468\n",
            "step 3600: generator_loss=0.7064894437789917, discriminator_loss=0.6764141321182251\n",
            "step 3601: generator_loss=0.7027050256729126, discriminator_loss=0.6778342723846436\n",
            "step 3602: generator_loss=0.707476794719696, discriminator_loss=0.6774318814277649\n",
            "step 3603: generator_loss=0.698816180229187, discriminator_loss=0.68337082862854\n",
            "step 3604: generator_loss=0.7027453184127808, discriminator_loss=0.6807299256324768\n",
            "step 3605: generator_loss=0.6980670690536499, discriminator_loss=0.6845885515213013\n",
            "step 3606: generator_loss=0.7031517028808594, discriminator_loss=0.6835565567016602\n",
            "step 3607: generator_loss=0.7026219964027405, discriminator_loss=0.6843547224998474\n",
            "step 3608: generator_loss=0.6954113245010376, discriminator_loss=0.6894824504852295\n",
            "step 3609: generator_loss=0.700981855392456, discriminator_loss=0.6871845722198486\n",
            "step 3610: generator_loss=0.6965194940567017, discriminator_loss=0.6888105273246765\n",
            "step 3611: generator_loss=0.6918952465057373, discriminator_loss=0.6922767162322998\n",
            "step 3612: generator_loss=0.6864417791366577, discriminator_loss=0.6935953497886658\n",
            "step 3613: generator_loss=0.6872686147689819, discriminator_loss=0.6917691826820374\n",
            "step 3614: generator_loss=0.6871329545974731, discriminator_loss=0.6920567750930786\n",
            "step 3615: generator_loss=0.6842485666275024, discriminator_loss=0.6928355693817139\n",
            "step 3616: generator_loss=0.6877201795578003, discriminator_loss=0.6907512545585632\n",
            "step 3617: generator_loss=0.6873699426651001, discriminator_loss=0.6918384432792664\n",
            "step 3618: generator_loss=0.6930949091911316, discriminator_loss=0.6907547116279602\n",
            "step 3619: generator_loss=0.6955739259719849, discriminator_loss=0.6897698640823364\n",
            "step 3620: generator_loss=0.6986487507820129, discriminator_loss=0.6893030405044556\n",
            "step 3621: generator_loss=0.7004895806312561, discriminator_loss=0.6885302066802979\n",
            "step 3622: generator_loss=0.7074593305587769, discriminator_loss=0.6876800656318665\n",
            "step 3623: generator_loss=0.7139617800712585, discriminator_loss=0.6857427954673767\n",
            "step 3624: generator_loss=0.7143813967704773, discriminator_loss=0.6860992312431335\n",
            "step 3625: generator_loss=0.7180984020233154, discriminator_loss=0.6835858821868896\n",
            "step 3626: generator_loss=0.7174306511878967, discriminator_loss=0.685081422328949\n",
            "step 3627: generator_loss=0.7132848501205444, discriminator_loss=0.6853110194206238\n",
            "step 3628: generator_loss=0.7127405405044556, discriminator_loss=0.6856464147567749\n",
            "step 3629: generator_loss=0.7114220857620239, discriminator_loss=0.684938907623291\n",
            "step 3630: generator_loss=0.7092222571372986, discriminator_loss=0.6848927140235901\n",
            "step 3631: generator_loss=0.7051124572753906, discriminator_loss=0.6856703162193298\n",
            "step 3632: generator_loss=0.7041394710540771, discriminator_loss=0.6849988102912903\n",
            "step 3633: generator_loss=0.6990481615066528, discriminator_loss=0.6854735612869263\n",
            "step 3634: generator_loss=0.6976332664489746, discriminator_loss=0.6862434148788452\n",
            "step 3635: generator_loss=0.6965826749801636, discriminator_loss=0.6856930255889893\n",
            "step 3636: generator_loss=0.6970410346984863, discriminator_loss=0.6840762495994568\n",
            "step 3637: generator_loss=0.6968344449996948, discriminator_loss=0.6837974786758423\n",
            "step 3638: generator_loss=0.6976743340492249, discriminator_loss=0.6835422515869141\n",
            "step 3639: generator_loss=0.700773298740387, discriminator_loss=0.6828588247299194\n",
            "step 3640: generator_loss=0.7052981853485107, discriminator_loss=0.6805689930915833\n",
            "step 3641: generator_loss=0.7083210945129395, discriminator_loss=0.6799824237823486\n",
            "step 3642: generator_loss=0.7081944942474365, discriminator_loss=0.6802783608436584\n",
            "step 3643: generator_loss=0.7102757692337036, discriminator_loss=0.6815062761306763\n",
            "step 3644: generator_loss=0.7111274003982544, discriminator_loss=0.6809134483337402\n",
            "step 3645: generator_loss=0.7107682824134827, discriminator_loss=0.6817029714584351\n",
            "step 3646: generator_loss=0.7081291079521179, discriminator_loss=0.6824520230293274\n",
            "step 3647: generator_loss=0.7085734009742737, discriminator_loss=0.6831172108650208\n",
            "step 3648: generator_loss=0.7069872617721558, discriminator_loss=0.6821024417877197\n",
            "step 3649: generator_loss=0.7033960223197937, discriminator_loss=0.6841610074043274\n",
            "step 3650: generator_loss=0.7034448385238647, discriminator_loss=0.683193564414978\n",
            "step 3651: generator_loss=0.6961942315101624, discriminator_loss=0.6856337785720825\n",
            "step 3652: generator_loss=0.6969987750053406, discriminator_loss=0.6847171783447266\n",
            "step 3653: generator_loss=0.6939293146133423, discriminator_loss=0.6873071193695068\n",
            "step 3654: generator_loss=0.6949993968009949, discriminator_loss=0.6893187165260315\n",
            "step 3655: generator_loss=0.6928837299346924, discriminator_loss=0.6900418996810913\n",
            "step 3656: generator_loss=0.6919738054275513, discriminator_loss=0.6903030872344971\n",
            "step 3657: generator_loss=0.6908670663833618, discriminator_loss=0.6933665871620178\n",
            "step 3658: generator_loss=0.6928935647010803, discriminator_loss=0.6928209066390991\n",
            "step 3659: generator_loss=0.6905005574226379, discriminator_loss=0.6953808069229126\n",
            "step 3660: generator_loss=0.6900046467781067, discriminator_loss=0.696905255317688\n",
            "step 3661: generator_loss=0.6882373094558716, discriminator_loss=0.6982613801956177\n",
            "step 3662: generator_loss=0.6841481924057007, discriminator_loss=0.699342668056488\n",
            "step 3663: generator_loss=0.6841398477554321, discriminator_loss=0.7007715106010437\n",
            "step 3664: generator_loss=0.6799540519714355, discriminator_loss=0.7021752595901489\n",
            "step 3665: generator_loss=0.6792910099029541, discriminator_loss=0.703887939453125\n",
            "step 3666: generator_loss=0.6792670488357544, discriminator_loss=0.7037321925163269\n",
            "step 3667: generator_loss=0.6806764602661133, discriminator_loss=0.7035888433456421\n",
            "step 3668: generator_loss=0.6801395416259766, discriminator_loss=0.7055172324180603\n",
            "step 3669: generator_loss=0.6763267517089844, discriminator_loss=0.7093740701675415\n",
            "step 3670: generator_loss=0.676016092300415, discriminator_loss=0.7081325054168701\n",
            "step 3671: generator_loss=0.6736397743225098, discriminator_loss=0.7101896405220032\n",
            "step 3672: generator_loss=0.6718707084655762, discriminator_loss=0.7102850079536438\n",
            "step 3673: generator_loss=0.6754988431930542, discriminator_loss=0.7095966339111328\n",
            "step 3674: generator_loss=0.6755906343460083, discriminator_loss=0.7088983058929443\n",
            "step 3675: generator_loss=0.6749167442321777, discriminator_loss=0.7085130214691162\n",
            "step 3676: generator_loss=0.6749109029769897, discriminator_loss=0.7083656787872314\n",
            "step 3677: generator_loss=0.6797071099281311, discriminator_loss=0.7055996656417847\n",
            "step 3678: generator_loss=0.6837878227233887, discriminator_loss=0.7028684616088867\n",
            "step 3679: generator_loss=0.6866121292114258, discriminator_loss=0.7008730173110962\n",
            "step 3680: generator_loss=0.6888892650604248, discriminator_loss=0.6997985243797302\n",
            "step 3681: generator_loss=0.6900351047515869, discriminator_loss=0.7008222341537476\n",
            "step 3682: generator_loss=0.6928423643112183, discriminator_loss=0.7000524997711182\n",
            "step 3683: generator_loss=0.6953914165496826, discriminator_loss=0.6983968019485474\n",
            "step 3684: generator_loss=0.6945635080337524, discriminator_loss=0.6981686949729919\n",
            "step 3685: generator_loss=0.6937279105186462, discriminator_loss=0.6973721385002136\n",
            "step 3686: generator_loss=0.6927620768547058, discriminator_loss=0.696210503578186\n",
            "step 3687: generator_loss=0.6916124820709229, discriminator_loss=0.6964658498764038\n",
            "step 3688: generator_loss=0.6926169395446777, discriminator_loss=0.694355845451355\n",
            "step 3689: generator_loss=0.6952764987945557, discriminator_loss=0.6920922994613647\n",
            "step 3690: generator_loss=0.6974732875823975, discriminator_loss=0.6906065940856934\n",
            "step 3691: generator_loss=0.6989496946334839, discriminator_loss=0.6887683868408203\n",
            "step 3692: generator_loss=0.6996153593063354, discriminator_loss=0.6885045766830444\n",
            "step 3693: generator_loss=0.7025306820869446, discriminator_loss=0.6876156330108643\n",
            "step 3694: generator_loss=0.7046599984169006, discriminator_loss=0.6865453720092773\n",
            "step 3695: generator_loss=0.7047357559204102, discriminator_loss=0.6864292621612549\n",
            "step 3696: generator_loss=0.7061408758163452, discriminator_loss=0.6844068765640259\n",
            "step 3697: generator_loss=0.7070838212966919, discriminator_loss=0.6824425458908081\n",
            "step 3698: generator_loss=0.7109552621841431, discriminator_loss=0.6801656484603882\n",
            "step 3699: generator_loss=0.7134333848953247, discriminator_loss=0.6773715019226074\n",
            "step 3700: generator_loss=0.7118395566940308, discriminator_loss=0.6784125566482544\n",
            "step 3701: generator_loss=0.7151386141777039, discriminator_loss=0.6754909753799438\n",
            "step 3702: generator_loss=0.7161942720413208, discriminator_loss=0.6745381355285645\n",
            "step 3703: generator_loss=0.7146406769752502, discriminator_loss=0.674639880657196\n",
            "step 3704: generator_loss=0.7163592576980591, discriminator_loss=0.6755293607711792\n",
            "step 3705: generator_loss=0.7153996825218201, discriminator_loss=0.676345944404602\n",
            "step 3706: generator_loss=0.7131237983703613, discriminator_loss=0.6772147417068481\n",
            "step 3707: generator_loss=0.7115020155906677, discriminator_loss=0.6781926155090332\n",
            "step 3708: generator_loss=0.7118157744407654, discriminator_loss=0.6785497665405273\n",
            "step 3709: generator_loss=0.7114334106445312, discriminator_loss=0.6789294481277466\n",
            "step 3710: generator_loss=0.7090826034545898, discriminator_loss=0.6782696843147278\n",
            "step 3711: generator_loss=0.7063140869140625, discriminator_loss=0.6805984973907471\n",
            "step 3712: generator_loss=0.7044388055801392, discriminator_loss=0.681315541267395\n",
            "step 3713: generator_loss=0.697578489780426, discriminator_loss=0.6836905479431152\n",
            "step 3714: generator_loss=0.7027943134307861, discriminator_loss=0.6821428537368774\n",
            "step 3715: generator_loss=0.6999523639678955, discriminator_loss=0.6834924817085266\n",
            "step 3716: generator_loss=0.7030463814735413, discriminator_loss=0.6835953593254089\n",
            "step 3717: generator_loss=0.70237797498703, discriminator_loss=0.6838343143463135\n",
            "step 3718: generator_loss=0.699108362197876, discriminator_loss=0.685448169708252\n",
            "step 3719: generator_loss=0.7007834315299988, discriminator_loss=0.6851639747619629\n",
            "step 3720: generator_loss=0.6978720426559448, discriminator_loss=0.6876980066299438\n",
            "step 3721: generator_loss=0.7005330324172974, discriminator_loss=0.6864382028579712\n",
            "step 3722: generator_loss=0.6984273791313171, discriminator_loss=0.6885719299316406\n",
            "step 3723: generator_loss=0.7020761370658875, discriminator_loss=0.6867172718048096\n",
            "step 3724: generator_loss=0.7016065120697021, discriminator_loss=0.6871229410171509\n",
            "step 3725: generator_loss=0.7032455205917358, discriminator_loss=0.6871398091316223\n",
            "step 3726: generator_loss=0.6996289491653442, discriminator_loss=0.6898921728134155\n",
            "step 3727: generator_loss=0.7021777629852295, discriminator_loss=0.6880830526351929\n",
            "step 3728: generator_loss=0.6992228627204895, discriminator_loss=0.6886895895004272\n",
            "step 3729: generator_loss=0.6988677382469177, discriminator_loss=0.6886320114135742\n",
            "step 3730: generator_loss=0.6962568759918213, discriminator_loss=0.6904065012931824\n",
            "step 3731: generator_loss=0.6949401497840881, discriminator_loss=0.6898236870765686\n",
            "step 3732: generator_loss=0.6944567561149597, discriminator_loss=0.6886714696884155\n",
            "step 3733: generator_loss=0.6916250586509705, discriminator_loss=0.6893277168273926\n",
            "step 3734: generator_loss=0.68850177526474, discriminator_loss=0.6909213066101074\n",
            "step 3735: generator_loss=0.6912184953689575, discriminator_loss=0.6895325183868408\n",
            "step 3736: generator_loss=0.6928276419639587, discriminator_loss=0.6887497901916504\n",
            "step 3737: generator_loss=0.6978946924209595, discriminator_loss=0.6850050687789917\n",
            "step 3738: generator_loss=0.6964563131332397, discriminator_loss=0.6854135990142822\n",
            "step 3739: generator_loss=0.6969859004020691, discriminator_loss=0.6838767528533936\n",
            "step 3740: generator_loss=0.7036759257316589, discriminator_loss=0.6783363223075867\n",
            "step 3741: generator_loss=0.7052643299102783, discriminator_loss=0.6754741072654724\n",
            "step 3742: generator_loss=0.7072020769119263, discriminator_loss=0.6739755868911743\n",
            "step 3743: generator_loss=0.7092952728271484, discriminator_loss=0.6733614802360535\n",
            "step 3744: generator_loss=0.7125293612480164, discriminator_loss=0.6732105612754822\n",
            "step 3745: generator_loss=0.7187841534614563, discriminator_loss=0.6697708368301392\n",
            "step 3746: generator_loss=0.7217252254486084, discriminator_loss=0.6687823534011841\n",
            "step 3747: generator_loss=0.724611759185791, discriminator_loss=0.667271614074707\n",
            "step 3748: generator_loss=0.7254023551940918, discriminator_loss=0.6674914360046387\n",
            "step 3749: generator_loss=0.724804162979126, discriminator_loss=0.6679778099060059\n",
            "step 3750: generator_loss=0.7264949083328247, discriminator_loss=0.6668754816055298\n",
            "step 3751: generator_loss=0.7223905324935913, discriminator_loss=0.66762375831604\n",
            "step 3752: generator_loss=0.7229369878768921, discriminator_loss=0.6665855646133423\n",
            "step 3753: generator_loss=0.7213841676712036, discriminator_loss=0.6661222577095032\n",
            "step 3754: generator_loss=0.7226128578186035, discriminator_loss=0.6657659411430359\n",
            "step 3755: generator_loss=0.7185604572296143, discriminator_loss=0.6664503812789917\n",
            "step 3756: generator_loss=0.7223005294799805, discriminator_loss=0.6637572050094604\n",
            "step 3757: generator_loss=0.7205983400344849, discriminator_loss=0.6641808748245239\n",
            "step 3758: generator_loss=0.7198679447174072, discriminator_loss=0.6632441282272339\n",
            "step 3759: generator_loss=0.719380795955658, discriminator_loss=0.6632587909698486\n",
            "step 3760: generator_loss=0.7224046587944031, discriminator_loss=0.6601598262786865\n",
            "step 3761: generator_loss=0.7250200510025024, discriminator_loss=0.6583809852600098\n",
            "step 3762: generator_loss=0.726468026638031, discriminator_loss=0.6573857665061951\n",
            "step 3763: generator_loss=0.7294800281524658, discriminator_loss=0.6561171412467957\n",
            "step 3764: generator_loss=0.7314819693565369, discriminator_loss=0.6563510298728943\n",
            "step 3765: generator_loss=0.7344309091567993, discriminator_loss=0.6555026769638062\n",
            "step 3766: generator_loss=0.73518306016922, discriminator_loss=0.6559637784957886\n",
            "step 3767: generator_loss=0.7380234599113464, discriminator_loss=0.6560162305831909\n",
            "step 3768: generator_loss=0.7377159595489502, discriminator_loss=0.6573317050933838\n",
            "step 3769: generator_loss=0.7366882562637329, discriminator_loss=0.6564208269119263\n",
            "step 3770: generator_loss=0.7339076399803162, discriminator_loss=0.6582083106040955\n",
            "step 3771: generator_loss=0.7329052686691284, discriminator_loss=0.6589174270629883\n",
            "step 3772: generator_loss=0.7318922281265259, discriminator_loss=0.6584458351135254\n",
            "step 3773: generator_loss=0.731303334236145, discriminator_loss=0.657917857170105\n",
            "step 3774: generator_loss=0.7281569242477417, discriminator_loss=0.6589466333389282\n",
            "step 3775: generator_loss=0.7259616255760193, discriminator_loss=0.6597512364387512\n",
            "step 3776: generator_loss=0.7238640785217285, discriminator_loss=0.6609330773353577\n",
            "step 3777: generator_loss=0.7205958366394043, discriminator_loss=0.6621417999267578\n",
            "step 3778: generator_loss=0.7209592461585999, discriminator_loss=0.662487804889679\n",
            "step 3779: generator_loss=0.7209393382072449, discriminator_loss=0.6624245643615723\n",
            "step 3780: generator_loss=0.7230454683303833, discriminator_loss=0.6629287600517273\n",
            "step 3781: generator_loss=0.7214866280555725, discriminator_loss=0.6646236181259155\n",
            "step 3782: generator_loss=0.7178982496261597, discriminator_loss=0.6662053465843201\n",
            "step 3783: generator_loss=0.7245177030563354, discriminator_loss=0.664459228515625\n",
            "step 3784: generator_loss=0.7241811752319336, discriminator_loss=0.6649518013000488\n",
            "step 3785: generator_loss=0.7217515110969543, discriminator_loss=0.6669012308120728\n",
            "step 3786: generator_loss=0.7234623432159424, discriminator_loss=0.6666033864021301\n",
            "step 3787: generator_loss=0.7205693125724792, discriminator_loss=0.6683154106140137\n",
            "step 3788: generator_loss=0.7250884771347046, discriminator_loss=0.6663811206817627\n",
            "step 3789: generator_loss=0.7222806811332703, discriminator_loss=0.6670416593551636\n",
            "step 3790: generator_loss=0.7222263813018799, discriminator_loss=0.6670697331428528\n",
            "step 3791: generator_loss=0.726064920425415, discriminator_loss=0.6656222939491272\n",
            "step 3792: generator_loss=0.7177527546882629, discriminator_loss=0.671091616153717\n",
            "step 3793: generator_loss=0.7201367020606995, discriminator_loss=0.670210599899292\n",
            "step 3794: generator_loss=0.7202577590942383, discriminator_loss=0.6720370054244995\n",
            "step 3795: generator_loss=0.719971776008606, discriminator_loss=0.6714491844177246\n",
            "step 3796: generator_loss=0.7204610705375671, discriminator_loss=0.6702534556388855\n",
            "step 3797: generator_loss=0.71416836977005, discriminator_loss=0.6744572520256042\n",
            "step 3798: generator_loss=0.712243378162384, discriminator_loss=0.6754186749458313\n",
            "step 3799: generator_loss=0.7122264504432678, discriminator_loss=0.674799382686615\n",
            "step 3800: generator_loss=0.709738552570343, discriminator_loss=0.6764075756072998\n",
            "step 3801: generator_loss=0.6983511447906494, discriminator_loss=0.6814941167831421\n",
            "step 3802: generator_loss=0.7018337249755859, discriminator_loss=0.6812440156936646\n",
            "step 3803: generator_loss=0.7017769813537598, discriminator_loss=0.6817597150802612\n",
            "step 3804: generator_loss=0.7014658451080322, discriminator_loss=0.6825399398803711\n",
            "step 3805: generator_loss=0.6915382146835327, discriminator_loss=0.6886104941368103\n",
            "step 3806: generator_loss=0.6900061368942261, discriminator_loss=0.6895862817764282\n",
            "step 3807: generator_loss=0.6934595108032227, discriminator_loss=0.6879985332489014\n",
            "step 3808: generator_loss=0.6928269863128662, discriminator_loss=0.6894660592079163\n",
            "step 3809: generator_loss=0.6949446201324463, discriminator_loss=0.6897853016853333\n",
            "step 3810: generator_loss=0.6918041706085205, discriminator_loss=0.6918559074401855\n",
            "step 3811: generator_loss=0.6904480457305908, discriminator_loss=0.694426953792572\n",
            "step 3812: generator_loss=0.6935116648674011, discriminator_loss=0.6943275928497314\n",
            "step 3813: generator_loss=0.6887505054473877, discriminator_loss=0.6968671083450317\n",
            "step 3814: generator_loss=0.6875168085098267, discriminator_loss=0.6981500387191772\n",
            "step 3815: generator_loss=0.6940207481384277, discriminator_loss=0.6954911947250366\n",
            "step 3816: generator_loss=0.6887507438659668, discriminator_loss=0.6978715658187866\n",
            "step 3817: generator_loss=0.6922541856765747, discriminator_loss=0.6967822313308716\n",
            "step 3818: generator_loss=0.6934055089950562, discriminator_loss=0.6938432455062866\n",
            "step 3819: generator_loss=0.6884128451347351, discriminator_loss=0.6969071626663208\n",
            "step 3820: generator_loss=0.6941205263137817, discriminator_loss=0.6928502321243286\n",
            "step 3821: generator_loss=0.6912028789520264, discriminator_loss=0.6936308145523071\n",
            "step 3822: generator_loss=0.6926232576370239, discriminator_loss=0.6944952011108398\n",
            "step 3823: generator_loss=0.6937011480331421, discriminator_loss=0.6943005323410034\n",
            "step 3824: generator_loss=0.6956437826156616, discriminator_loss=0.6951027512550354\n",
            "step 3825: generator_loss=0.6947470903396606, discriminator_loss=0.6959889531135559\n",
            "step 3826: generator_loss=0.6984858512878418, discriminator_loss=0.6949671506881714\n",
            "step 3827: generator_loss=0.7003144025802612, discriminator_loss=0.6944137811660767\n",
            "step 3828: generator_loss=0.7013397216796875, discriminator_loss=0.6935223340988159\n",
            "step 3829: generator_loss=0.6998438835144043, discriminator_loss=0.6935650110244751\n",
            "step 3830: generator_loss=0.6951808929443359, discriminator_loss=0.694281816482544\n",
            "step 3831: generator_loss=0.6918030977249146, discriminator_loss=0.6953993439674377\n",
            "step 3832: generator_loss=0.6952525973320007, discriminator_loss=0.6923682689666748\n",
            "step 3833: generator_loss=0.6974215507507324, discriminator_loss=0.6892399787902832\n",
            "step 3834: generator_loss=0.6988330483436584, discriminator_loss=0.686248779296875\n",
            "step 3835: generator_loss=0.7044078707695007, discriminator_loss=0.6825780868530273\n",
            "step 3836: generator_loss=0.7074072360992432, discriminator_loss=0.6786039471626282\n",
            "step 3837: generator_loss=0.7118721604347229, discriminator_loss=0.6764754056930542\n",
            "step 3838: generator_loss=0.7207606434822083, discriminator_loss=0.6697575449943542\n",
            "step 3839: generator_loss=0.7230938673019409, discriminator_loss=0.6675878763198853\n",
            "step 3840: generator_loss=0.7241889238357544, discriminator_loss=0.6671156883239746\n",
            "step 3841: generator_loss=0.7331531643867493, discriminator_loss=0.6609914898872375\n",
            "step 3842: generator_loss=0.7359871864318848, discriminator_loss=0.6592766046524048\n",
            "step 3843: generator_loss=0.736558198928833, discriminator_loss=0.658352792263031\n",
            "step 3844: generator_loss=0.7411009073257446, discriminator_loss=0.6551766395568848\n",
            "step 3845: generator_loss=0.7430806159973145, discriminator_loss=0.6522533297538757\n",
            "step 3846: generator_loss=0.7419450879096985, discriminator_loss=0.6507128477096558\n",
            "step 3847: generator_loss=0.7419880628585815, discriminator_loss=0.6493297815322876\n",
            "step 3848: generator_loss=0.743120551109314, discriminator_loss=0.6487993001937866\n",
            "step 3849: generator_loss=0.7418320178985596, discriminator_loss=0.6476279497146606\n",
            "step 3850: generator_loss=0.7402013540267944, discriminator_loss=0.6485415101051331\n",
            "step 3851: generator_loss=0.7424860000610352, discriminator_loss=0.6470600962638855\n",
            "step 3852: generator_loss=0.7387470602989197, discriminator_loss=0.6474043130874634\n",
            "step 3853: generator_loss=0.7393568158149719, discriminator_loss=0.6452686786651611\n",
            "step 3854: generator_loss=0.7394822835922241, discriminator_loss=0.6451425552368164\n",
            "step 3855: generator_loss=0.7363245487213135, discriminator_loss=0.6450241208076477\n",
            "step 3856: generator_loss=0.7339904308319092, discriminator_loss=0.6461365222930908\n",
            "step 3857: generator_loss=0.7362151145935059, discriminator_loss=0.6444671154022217\n",
            "step 3858: generator_loss=0.7382547855377197, discriminator_loss=0.6428948044776917\n",
            "step 3859: generator_loss=0.7459007501602173, discriminator_loss=0.6384077072143555\n",
            "step 3860: generator_loss=0.7479456663131714, discriminator_loss=0.6381964683532715\n",
            "step 3861: generator_loss=0.7546316981315613, discriminator_loss=0.6359780430793762\n",
            "step 3862: generator_loss=0.7469382286071777, discriminator_loss=0.6413754224777222\n",
            "step 3863: generator_loss=0.7532941699028015, discriminator_loss=0.6402389407157898\n",
            "step 3864: generator_loss=0.7533261775970459, discriminator_loss=0.6427294015884399\n",
            "step 3865: generator_loss=0.7550889849662781, discriminator_loss=0.6415522694587708\n",
            "step 3866: generator_loss=0.7638772130012512, discriminator_loss=0.6387289762496948\n",
            "step 3867: generator_loss=0.7476749420166016, discriminator_loss=0.6458795666694641\n",
            "step 3868: generator_loss=0.7448824644088745, discriminator_loss=0.6451547145843506\n",
            "step 3869: generator_loss=0.7452428340911865, discriminator_loss=0.6450276374816895\n",
            "step 3870: generator_loss=0.7307195663452148, discriminator_loss=0.6509408950805664\n",
            "step 3871: generator_loss=0.7301532030105591, discriminator_loss=0.6502986550331116\n",
            "step 3872: generator_loss=0.7255620360374451, discriminator_loss=0.6526941061019897\n",
            "step 3873: generator_loss=0.7209765911102295, discriminator_loss=0.6551039218902588\n",
            "step 3874: generator_loss=0.7240734696388245, discriminator_loss=0.6543852686882019\n",
            "step 3875: generator_loss=0.7217982411384583, discriminator_loss=0.6560636162757874\n",
            "step 3876: generator_loss=0.7257088422775269, discriminator_loss=0.6565090417861938\n",
            "step 3877: generator_loss=0.7257423996925354, discriminator_loss=0.6591843366622925\n",
            "step 3878: generator_loss=0.7261456251144409, discriminator_loss=0.6596899032592773\n",
            "step 3879: generator_loss=0.7330982685089111, discriminator_loss=0.6601074934005737\n",
            "step 3880: generator_loss=0.7335004806518555, discriminator_loss=0.6607702970504761\n",
            "step 3881: generator_loss=0.7358428239822388, discriminator_loss=0.6614423394203186\n",
            "step 3882: generator_loss=0.7260684967041016, discriminator_loss=0.6663703918457031\n",
            "step 3883: generator_loss=0.7238170504570007, discriminator_loss=0.6674226522445679\n",
            "step 3884: generator_loss=0.7184584140777588, discriminator_loss=0.6705728769302368\n",
            "step 3885: generator_loss=0.7098598480224609, discriminator_loss=0.6742420196533203\n",
            "step 3886: generator_loss=0.713037371635437, discriminator_loss=0.6732581257820129\n",
            "step 3887: generator_loss=0.7091794013977051, discriminator_loss=0.6747598648071289\n",
            "step 3888: generator_loss=0.7062419652938843, discriminator_loss=0.6755937933921814\n",
            "step 3889: generator_loss=0.7030478119850159, discriminator_loss=0.6777430772781372\n",
            "step 3890: generator_loss=0.706445038318634, discriminator_loss=0.6766206622123718\n",
            "step 3891: generator_loss=0.7145612239837646, discriminator_loss=0.6741858720779419\n",
            "step 3892: generator_loss=0.7083662152290344, discriminator_loss=0.6782379150390625\n",
            "step 3893: generator_loss=0.7103745937347412, discriminator_loss=0.6777138710021973\n",
            "step 3894: generator_loss=0.7120766043663025, discriminator_loss=0.6775088310241699\n",
            "step 3895: generator_loss=0.7149788737297058, discriminator_loss=0.6765236258506775\n",
            "step 3896: generator_loss=0.7162105441093445, discriminator_loss=0.6752280592918396\n",
            "step 3897: generator_loss=0.7222877740859985, discriminator_loss=0.6720162034034729\n",
            "step 3898: generator_loss=0.724681556224823, discriminator_loss=0.6701436042785645\n",
            "step 3899: generator_loss=0.737065315246582, discriminator_loss=0.6620042324066162\n",
            "step 3900: generator_loss=0.7423712015151978, discriminator_loss=0.6607272624969482\n",
            "step 3901: generator_loss=0.7495378255844116, discriminator_loss=0.6581712961196899\n",
            "step 3902: generator_loss=0.752495527267456, discriminator_loss=0.6570900678634644\n",
            "step 3903: generator_loss=0.7555937767028809, discriminator_loss=0.6535916328430176\n",
            "step 3904: generator_loss=0.7425821423530579, discriminator_loss=0.6572768688201904\n",
            "step 3905: generator_loss=0.7471222281455994, discriminator_loss=0.653365969657898\n",
            "step 3906: generator_loss=0.7439364194869995, discriminator_loss=0.6521862149238586\n",
            "step 3907: generator_loss=0.742167592048645, discriminator_loss=0.6505059003829956\n",
            "step 3908: generator_loss=0.7315670251846313, discriminator_loss=0.655602216720581\n",
            "step 3909: generator_loss=0.7367963790893555, discriminator_loss=0.6521567106246948\n",
            "step 3910: generator_loss=0.7320459485054016, discriminator_loss=0.6539433002471924\n",
            "step 3911: generator_loss=0.7282520532608032, discriminator_loss=0.6563769578933716\n",
            "step 3912: generator_loss=0.7264194488525391, discriminator_loss=0.6582000255584717\n",
            "step 3913: generator_loss=0.7218567728996277, discriminator_loss=0.6610926985740662\n",
            "step 3914: generator_loss=0.7236464023590088, discriminator_loss=0.6608791351318359\n",
            "step 3915: generator_loss=0.7200547456741333, discriminator_loss=0.6635964512825012\n",
            "step 3916: generator_loss=0.7206013202667236, discriminator_loss=0.6649683117866516\n",
            "step 3917: generator_loss=0.7198615074157715, discriminator_loss=0.6649270057678223\n",
            "step 3918: generator_loss=0.7235230207443237, discriminator_loss=0.6652508974075317\n",
            "step 3919: generator_loss=0.7184028625488281, discriminator_loss=0.6695619225502014\n",
            "step 3920: generator_loss=0.7170728445053101, discriminator_loss=0.6721655130386353\n",
            "step 3921: generator_loss=0.7208383083343506, discriminator_loss=0.6711746454238892\n",
            "step 3922: generator_loss=0.7154179811477661, discriminator_loss=0.6738402247428894\n",
            "step 3923: generator_loss=0.719819188117981, discriminator_loss=0.6716794967651367\n",
            "step 3924: generator_loss=0.7147558927536011, discriminator_loss=0.6750158667564392\n",
            "step 3925: generator_loss=0.7111091017723083, discriminator_loss=0.6775444746017456\n",
            "step 3926: generator_loss=0.7064336538314819, discriminator_loss=0.6805948615074158\n",
            "step 3927: generator_loss=0.7087734937667847, discriminator_loss=0.6799222230911255\n",
            "step 3928: generator_loss=0.7048143148422241, discriminator_loss=0.681877076625824\n",
            "step 3929: generator_loss=0.707600474357605, discriminator_loss=0.6806763410568237\n",
            "step 3930: generator_loss=0.7013540863990784, discriminator_loss=0.6839981079101562\n",
            "step 3931: generator_loss=0.7036643028259277, discriminator_loss=0.6838042736053467\n",
            "step 3932: generator_loss=0.6976007223129272, discriminator_loss=0.6859375238418579\n",
            "step 3933: generator_loss=0.6974131464958191, discriminator_loss=0.686886191368103\n",
            "step 3934: generator_loss=0.695979654788971, discriminator_loss=0.6883440017700195\n",
            "step 3935: generator_loss=0.7002243995666504, discriminator_loss=0.6861242055892944\n",
            "step 3936: generator_loss=0.7028475999832153, discriminator_loss=0.6893637180328369\n",
            "step 3937: generator_loss=0.7034586668014526, discriminator_loss=0.688836932182312\n",
            "step 3938: generator_loss=0.7001808881759644, discriminator_loss=0.6917663812637329\n",
            "step 3939: generator_loss=0.7000753879547119, discriminator_loss=0.6935474872589111\n",
            "step 3940: generator_loss=0.6974776387214661, discriminator_loss=0.6940654516220093\n",
            "step 3941: generator_loss=0.6976785659790039, discriminator_loss=0.6936696767807007\n",
            "step 3942: generator_loss=0.691871166229248, discriminator_loss=0.696002721786499\n",
            "step 3943: generator_loss=0.6865710616111755, discriminator_loss=0.6963694095611572\n",
            "step 3944: generator_loss=0.6874343156814575, discriminator_loss=0.6968140602111816\n",
            "step 3945: generator_loss=0.6803241968154907, discriminator_loss=0.6990616917610168\n",
            "step 3946: generator_loss=0.6824256181716919, discriminator_loss=0.6992437839508057\n",
            "step 3947: generator_loss=0.6809859275817871, discriminator_loss=0.6995284557342529\n",
            "step 3948: generator_loss=0.6815826892852783, discriminator_loss=0.6991044282913208\n",
            "step 3949: generator_loss=0.684047281742096, discriminator_loss=0.6973487138748169\n",
            "step 3950: generator_loss=0.681938648223877, discriminator_loss=0.6981838345527649\n",
            "step 3951: generator_loss=0.6863499879837036, discriminator_loss=0.6962794065475464\n",
            "step 3952: generator_loss=0.6920422911643982, discriminator_loss=0.693812370300293\n",
            "step 3953: generator_loss=0.6987676620483398, discriminator_loss=0.6904251575469971\n",
            "step 3954: generator_loss=0.7017533779144287, discriminator_loss=0.6872787475585938\n",
            "step 3955: generator_loss=0.7086985111236572, discriminator_loss=0.6827452182769775\n",
            "step 3956: generator_loss=0.7145484685897827, discriminator_loss=0.6783122420310974\n",
            "step 3957: generator_loss=0.721169114112854, discriminator_loss=0.6721534132957458\n",
            "step 3958: generator_loss=0.7272374033927917, discriminator_loss=0.6677864789962769\n",
            "step 3959: generator_loss=0.7335258722305298, discriminator_loss=0.6637893915176392\n",
            "step 3960: generator_loss=0.7378606796264648, discriminator_loss=0.661834716796875\n",
            "step 3961: generator_loss=0.7408604025840759, discriminator_loss=0.6603454947471619\n",
            "step 3962: generator_loss=0.7393921613693237, discriminator_loss=0.6614784598350525\n",
            "step 3963: generator_loss=0.7435141801834106, discriminator_loss=0.6582720279693604\n",
            "step 3964: generator_loss=0.7410753965377808, discriminator_loss=0.6582220196723938\n",
            "step 3965: generator_loss=0.7432681322097778, discriminator_loss=0.655272364616394\n",
            "step 3966: generator_loss=0.7417420744895935, discriminator_loss=0.6563901901245117\n",
            "step 3967: generator_loss=0.7407469749450684, discriminator_loss=0.6552753448486328\n",
            "step 3968: generator_loss=0.7356516718864441, discriminator_loss=0.6563944816589355\n",
            "step 3969: generator_loss=0.7284783124923706, discriminator_loss=0.6588196754455566\n",
            "step 3970: generator_loss=0.7300872206687927, discriminator_loss=0.6564496159553528\n",
            "step 3971: generator_loss=0.7246849536895752, discriminator_loss=0.6573226451873779\n",
            "step 3972: generator_loss=0.7259724140167236, discriminator_loss=0.6567510366439819\n",
            "step 3973: generator_loss=0.7223833799362183, discriminator_loss=0.6578493118286133\n",
            "step 3974: generator_loss=0.7188485860824585, discriminator_loss=0.6590824127197266\n",
            "step 3975: generator_loss=0.7230416536331177, discriminator_loss=0.6591999530792236\n",
            "step 3976: generator_loss=0.7214022278785706, discriminator_loss=0.660001814365387\n",
            "step 3977: generator_loss=0.7219743728637695, discriminator_loss=0.6616582870483398\n",
            "step 3978: generator_loss=0.7270178198814392, discriminator_loss=0.6599435210227966\n",
            "step 3979: generator_loss=0.7287763357162476, discriminator_loss=0.6602885127067566\n",
            "step 3980: generator_loss=0.724168062210083, discriminator_loss=0.6629922986030579\n",
            "step 3981: generator_loss=0.7299747467041016, discriminator_loss=0.6612632274627686\n",
            "step 3982: generator_loss=0.7239271402359009, discriminator_loss=0.6657483577728271\n",
            "step 3983: generator_loss=0.7289320230484009, discriminator_loss=0.6646879315376282\n",
            "step 3984: generator_loss=0.7246683835983276, discriminator_loss=0.6666849851608276\n",
            "step 3985: generator_loss=0.7177334427833557, discriminator_loss=0.6705256700515747\n",
            "step 3986: generator_loss=0.7204757332801819, discriminator_loss=0.6700505018234253\n",
            "step 3987: generator_loss=0.7164984941482544, discriminator_loss=0.6709180474281311\n",
            "step 3988: generator_loss=0.7134064435958862, discriminator_loss=0.672720193862915\n",
            "step 3989: generator_loss=0.7138752937316895, discriminator_loss=0.671736478805542\n",
            "step 3990: generator_loss=0.7112081050872803, discriminator_loss=0.6745494604110718\n",
            "step 3991: generator_loss=0.7087682485580444, discriminator_loss=0.676053524017334\n",
            "step 3992: generator_loss=0.7093617916107178, discriminator_loss=0.6755651235580444\n",
            "step 3993: generator_loss=0.7107222676277161, discriminator_loss=0.6746920943260193\n",
            "step 3994: generator_loss=0.7096235752105713, discriminator_loss=0.676095724105835\n",
            "step 3995: generator_loss=0.7054746150970459, discriminator_loss=0.6791858673095703\n",
            "step 3996: generator_loss=0.7029114961624146, discriminator_loss=0.6821401119232178\n",
            "step 3997: generator_loss=0.7081790566444397, discriminator_loss=0.680012583732605\n",
            "step 3998: generator_loss=0.7064429521560669, discriminator_loss=0.6819653511047363\n",
            "step 3999: generator_loss=0.7060741186141968, discriminator_loss=0.683363676071167\n",
            "step 4000: generator_loss=0.7061269879341125, discriminator_loss=0.6840040683746338\n",
            "step 4001: generator_loss=0.7056251764297485, discriminator_loss=0.686793863773346\n",
            "step 4002: generator_loss=0.7063456773757935, discriminator_loss=0.6859055161476135\n",
            "step 4003: generator_loss=0.704748272895813, discriminator_loss=0.6879271864891052\n",
            "step 4004: generator_loss=0.701724648475647, discriminator_loss=0.688767671585083\n",
            "step 4005: generator_loss=0.6978176832199097, discriminator_loss=0.6894313097000122\n",
            "step 4006: generator_loss=0.6965559720993042, discriminator_loss=0.6903672218322754\n",
            "step 4007: generator_loss=0.6971398591995239, discriminator_loss=0.6897224187850952\n",
            "step 4008: generator_loss=0.6946396231651306, discriminator_loss=0.6902691125869751\n",
            "step 4009: generator_loss=0.6946119070053101, discriminator_loss=0.6906604766845703\n",
            "step 4010: generator_loss=0.6952002048492432, discriminator_loss=0.6903181076049805\n",
            "step 4011: generator_loss=0.6946850419044495, discriminator_loss=0.6920032501220703\n",
            "step 4012: generator_loss=0.6952317953109741, discriminator_loss=0.6922629475593567\n",
            "step 4013: generator_loss=0.6920047998428345, discriminator_loss=0.6941760182380676\n",
            "step 4014: generator_loss=0.6950541734695435, discriminator_loss=0.6932024359703064\n",
            "step 4015: generator_loss=0.6946805119514465, discriminator_loss=0.6945066452026367\n",
            "step 4016: generator_loss=0.6970528364181519, discriminator_loss=0.6918196678161621\n",
            "step 4017: generator_loss=0.6972799301147461, discriminator_loss=0.6929513216018677\n",
            "step 4018: generator_loss=0.6959862112998962, discriminator_loss=0.6922388076782227\n",
            "step 4019: generator_loss=0.698837399482727, discriminator_loss=0.6905553340911865\n",
            "step 4020: generator_loss=0.6984167098999023, discriminator_loss=0.6907081007957458\n",
            "step 4021: generator_loss=0.6994333267211914, discriminator_loss=0.6902203559875488\n",
            "step 4022: generator_loss=0.7009423971176147, discriminator_loss=0.68910813331604\n",
            "step 4023: generator_loss=0.6972417831420898, discriminator_loss=0.692499041557312\n",
            "step 4024: generator_loss=0.7004224061965942, discriminator_loss=0.6901116967201233\n",
            "step 4025: generator_loss=0.7034882307052612, discriminator_loss=0.6885337829589844\n",
            "step 4026: generator_loss=0.7035260200500488, discriminator_loss=0.6883825063705444\n",
            "step 4027: generator_loss=0.7061747312545776, discriminator_loss=0.686028003692627\n",
            "step 4028: generator_loss=0.703811526298523, discriminator_loss=0.6866318583488464\n",
            "step 4029: generator_loss=0.7040929794311523, discriminator_loss=0.686246395111084\n",
            "step 4030: generator_loss=0.7035160660743713, discriminator_loss=0.6861416101455688\n",
            "step 4031: generator_loss=0.7034426927566528, discriminator_loss=0.6856697797775269\n",
            "step 4032: generator_loss=0.6997653245925903, discriminator_loss=0.687951922416687\n",
            "step 4033: generator_loss=0.7000341415405273, discriminator_loss=0.6882426142692566\n",
            "step 4034: generator_loss=0.6970608234405518, discriminator_loss=0.6877580881118774\n",
            "step 4035: generator_loss=0.6960841417312622, discriminator_loss=0.6866781711578369\n",
            "step 4036: generator_loss=0.6911423206329346, discriminator_loss=0.68910813331604\n",
            "step 4037: generator_loss=0.690689742565155, discriminator_loss=0.6886800527572632\n",
            "step 4038: generator_loss=0.6904006004333496, discriminator_loss=0.6885371208190918\n",
            "step 4039: generator_loss=0.6892484426498413, discriminator_loss=0.6888207197189331\n",
            "step 4040: generator_loss=0.6908770799636841, discriminator_loss=0.6888439655303955\n",
            "step 4041: generator_loss=0.690930187702179, discriminator_loss=0.6890038251876831\n",
            "step 4042: generator_loss=0.69447261095047, discriminator_loss=0.6882359981536865\n",
            "step 4043: generator_loss=0.6939787268638611, discriminator_loss=0.689241886138916\n",
            "step 4044: generator_loss=0.6980398893356323, discriminator_loss=0.6882834434509277\n",
            "step 4045: generator_loss=0.699895441532135, discriminator_loss=0.6847424507141113\n",
            "step 4046: generator_loss=0.7100743055343628, discriminator_loss=0.6804853081703186\n",
            "step 4047: generator_loss=0.7066614031791687, discriminator_loss=0.67976975440979\n",
            "step 4048: generator_loss=0.70787513256073, discriminator_loss=0.6770296096801758\n",
            "step 4049: generator_loss=0.7149462103843689, discriminator_loss=0.6720550060272217\n",
            "step 4050: generator_loss=0.7264116406440735, discriminator_loss=0.667293906211853\n",
            "step 4051: generator_loss=0.7268813252449036, discriminator_loss=0.6674234867095947\n",
            "step 4052: generator_loss=0.7266923189163208, discriminator_loss=0.6679617166519165\n",
            "step 4053: generator_loss=0.7280396819114685, discriminator_loss=0.6670201420783997\n",
            "step 4054: generator_loss=0.7386220097541809, discriminator_loss=0.6641937494277954\n",
            "step 4055: generator_loss=0.7297282218933105, discriminator_loss=0.666180431842804\n",
            "step 4056: generator_loss=0.7311875820159912, discriminator_loss=0.6639381647109985\n",
            "step 4057: generator_loss=0.7172068953514099, discriminator_loss=0.6703254580497742\n",
            "step 4058: generator_loss=0.7149246335029602, discriminator_loss=0.672231137752533\n",
            "step 4059: generator_loss=0.7108412981033325, discriminator_loss=0.6729781627655029\n",
            "step 4060: generator_loss=0.7052173614501953, discriminator_loss=0.6768319606781006\n",
            "step 4061: generator_loss=0.7033499479293823, discriminator_loss=0.6759482622146606\n",
            "step 4062: generator_loss=0.708277702331543, discriminator_loss=0.675438404083252\n",
            "step 4063: generator_loss=0.6923090219497681, discriminator_loss=0.6838005781173706\n",
            "step 4064: generator_loss=0.6856026649475098, discriminator_loss=0.688569188117981\n",
            "step 4065: generator_loss=0.6939005851745605, discriminator_loss=0.6847447156906128\n",
            "step 4066: generator_loss=0.6929917931556702, discriminator_loss=0.6880074739456177\n",
            "step 4067: generator_loss=0.6922458410263062, discriminator_loss=0.688957929611206\n",
            "step 4068: generator_loss=0.6853843331336975, discriminator_loss=0.6938382387161255\n",
            "step 4069: generator_loss=0.6846572160720825, discriminator_loss=0.6975386142730713\n",
            "step 4070: generator_loss=0.6814613938331604, discriminator_loss=0.701795220375061\n",
            "step 4071: generator_loss=0.6911294460296631, discriminator_loss=0.6970707178115845\n",
            "step 4072: generator_loss=0.6920047402381897, discriminator_loss=0.6989160776138306\n",
            "step 4073: generator_loss=0.6840658187866211, discriminator_loss=0.7034636735916138\n",
            "step 4074: generator_loss=0.6826286315917969, discriminator_loss=0.7054579854011536\n",
            "step 4075: generator_loss=0.6930230855941772, discriminator_loss=0.6994218826293945\n",
            "step 4076: generator_loss=0.6911424398422241, discriminator_loss=0.6999788284301758\n",
            "step 4077: generator_loss=0.6976907849311829, discriminator_loss=0.6976441740989685\n",
            "step 4078: generator_loss=0.6898012757301331, discriminator_loss=0.698927640914917\n",
            "step 4079: generator_loss=0.6932015419006348, discriminator_loss=0.6962409615516663\n",
            "step 4080: generator_loss=0.691813051700592, discriminator_loss=0.6950615644454956\n",
            "step 4081: generator_loss=0.7005540132522583, discriminator_loss=0.6884596943855286\n",
            "step 4082: generator_loss=0.7019745111465454, discriminator_loss=0.6877239942550659\n",
            "step 4083: generator_loss=0.6943848133087158, discriminator_loss=0.6913669109344482\n",
            "step 4084: generator_loss=0.6985235810279846, discriminator_loss=0.6881513595581055\n",
            "step 4085: generator_loss=0.7019215226173401, discriminator_loss=0.6878489851951599\n",
            "step 4086: generator_loss=0.7034716010093689, discriminator_loss=0.687877893447876\n",
            "step 4087: generator_loss=0.7053996324539185, discriminator_loss=0.6855764389038086\n",
            "step 4088: generator_loss=0.7081019878387451, discriminator_loss=0.6829769015312195\n",
            "step 4089: generator_loss=0.7076402902603149, discriminator_loss=0.6833387017250061\n",
            "step 4090: generator_loss=0.7128757238388062, discriminator_loss=0.681473433971405\n",
            "step 4091: generator_loss=0.7156006097793579, discriminator_loss=0.678325891494751\n",
            "step 4092: generator_loss=0.720501184463501, discriminator_loss=0.6737565994262695\n",
            "step 4093: generator_loss=0.7252870798110962, discriminator_loss=0.671777606010437\n",
            "step 4094: generator_loss=0.7345258593559265, discriminator_loss=0.6636663675308228\n",
            "step 4095: generator_loss=0.7424211502075195, discriminator_loss=0.6573067903518677\n",
            "step 4096: generator_loss=0.7451090812683105, discriminator_loss=0.654484748840332\n",
            "step 4097: generator_loss=0.7486007809638977, discriminator_loss=0.6518529653549194\n",
            "step 4098: generator_loss=0.7571603059768677, discriminator_loss=0.6478155851364136\n",
            "step 4099: generator_loss=0.7609966993331909, discriminator_loss=0.6444391012191772\n",
            "step 4100: generator_loss=0.763501763343811, discriminator_loss=0.6418195962905884\n",
            "step 4101: generator_loss=0.762916624546051, discriminator_loss=0.6413556337356567\n",
            "step 4102: generator_loss=0.761481523513794, discriminator_loss=0.6413393020629883\n",
            "step 4103: generator_loss=0.7595220804214478, discriminator_loss=0.6396823525428772\n",
            "step 4104: generator_loss=0.7520027160644531, discriminator_loss=0.6423273086547852\n",
            "step 4105: generator_loss=0.7531729936599731, discriminator_loss=0.6384400725364685\n",
            "step 4106: generator_loss=0.7405210137367249, discriminator_loss=0.6427760124206543\n",
            "step 4107: generator_loss=0.7399728298187256, discriminator_loss=0.6403884887695312\n",
            "step 4108: generator_loss=0.7403894066810608, discriminator_loss=0.6381524205207825\n",
            "step 4109: generator_loss=0.7357643842697144, discriminator_loss=0.6372523307800293\n",
            "step 4110: generator_loss=0.7402666211128235, discriminator_loss=0.6335695385932922\n",
            "step 4111: generator_loss=0.7375766038894653, discriminator_loss=0.635017991065979\n",
            "step 4112: generator_loss=0.7430422306060791, discriminator_loss=0.632825493812561\n",
            "step 4113: generator_loss=0.754071831703186, discriminator_loss=0.6322153806686401\n",
            "step 4114: generator_loss=0.7586922645568848, discriminator_loss=0.6331305503845215\n",
            "step 4115: generator_loss=0.7605773210525513, discriminator_loss=0.6356273293495178\n",
            "step 4116: generator_loss=0.7646026611328125, discriminator_loss=0.637132465839386\n",
            "step 4117: generator_loss=0.7670459151268005, discriminator_loss=0.6380941867828369\n",
            "step 4118: generator_loss=0.7610824704170227, discriminator_loss=0.6418733596801758\n",
            "step 4119: generator_loss=0.7551156282424927, discriminator_loss=0.6443507671356201\n",
            "step 4120: generator_loss=0.7502211332321167, discriminator_loss=0.6476625800132751\n",
            "step 4121: generator_loss=0.7459684610366821, discriminator_loss=0.647636890411377\n",
            "step 4122: generator_loss=0.73203045129776, discriminator_loss=0.6539086699485779\n",
            "step 4123: generator_loss=0.719118595123291, discriminator_loss=0.6592293977737427\n",
            "step 4124: generator_loss=0.7194263935089111, discriminator_loss=0.6583883762359619\n",
            "step 4125: generator_loss=0.7192384004592896, discriminator_loss=0.658980667591095\n",
            "step 4126: generator_loss=0.705337643623352, discriminator_loss=0.6646960377693176\n",
            "step 4127: generator_loss=0.7083535194396973, discriminator_loss=0.6648803949356079\n",
            "step 4128: generator_loss=0.7029610872268677, discriminator_loss=0.6697509288787842\n",
            "step 4129: generator_loss=0.7099131345748901, discriminator_loss=0.6684476137161255\n",
            "step 4130: generator_loss=0.7152610421180725, discriminator_loss=0.6684301495552063\n",
            "step 4131: generator_loss=0.7103041410446167, discriminator_loss=0.6749110221862793\n",
            "step 4132: generator_loss=0.7107462882995605, discriminator_loss=0.678514301776886\n",
            "step 4133: generator_loss=0.7133243680000305, discriminator_loss=0.6803286671638489\n",
            "step 4134: generator_loss=0.7156158685684204, discriminator_loss=0.681465744972229\n",
            "step 4135: generator_loss=0.7107231616973877, discriminator_loss=0.684529185295105\n",
            "step 4136: generator_loss=0.7072725892066956, discriminator_loss=0.6862046718597412\n",
            "step 4137: generator_loss=0.6998645067214966, discriminator_loss=0.6900812387466431\n",
            "step 4138: generator_loss=0.695094108581543, discriminator_loss=0.6935681104660034\n",
            "step 4139: generator_loss=0.688169002532959, discriminator_loss=0.6978731155395508\n",
            "step 4140: generator_loss=0.6843827366828918, discriminator_loss=0.6990832090377808\n",
            "step 4141: generator_loss=0.6848033666610718, discriminator_loss=0.6991375684738159\n",
            "step 4142: generator_loss=0.6806666254997253, discriminator_loss=0.7017987370491028\n",
            "step 4143: generator_loss=0.6767339110374451, discriminator_loss=0.7052569389343262\n",
            "step 4144: generator_loss=0.6737529039382935, discriminator_loss=0.7051465511322021\n",
            "step 4145: generator_loss=0.6748575568199158, discriminator_loss=0.7064944505691528\n",
            "step 4146: generator_loss=0.6798629760742188, discriminator_loss=0.7036020159721375\n",
            "step 4147: generator_loss=0.6761943697929382, discriminator_loss=0.7061651945114136\n",
            "step 4148: generator_loss=0.6803994178771973, discriminator_loss=0.7069025039672852\n",
            "step 4149: generator_loss=0.6739785075187683, discriminator_loss=0.7124433517456055\n",
            "step 4150: generator_loss=0.6881376504898071, discriminator_loss=0.7052761316299438\n",
            "step 4151: generator_loss=0.6833946704864502, discriminator_loss=0.7093384265899658\n",
            "step 4152: generator_loss=0.6795728802680969, discriminator_loss=0.7142101526260376\n",
            "step 4153: generator_loss=0.6862843036651611, discriminator_loss=0.7105789184570312\n",
            "step 4154: generator_loss=0.681036651134491, discriminator_loss=0.7134302854537964\n",
            "step 4155: generator_loss=0.6725809574127197, discriminator_loss=0.7179207801818848\n",
            "step 4156: generator_loss=0.6703448295593262, discriminator_loss=0.7183979749679565\n",
            "step 4157: generator_loss=0.6698250770568848, discriminator_loss=0.7184686660766602\n",
            "step 4158: generator_loss=0.6711907982826233, discriminator_loss=0.7173541784286499\n",
            "step 4159: generator_loss=0.6672331094741821, discriminator_loss=0.719167947769165\n",
            "step 4160: generator_loss=0.6628121137619019, discriminator_loss=0.7187284231185913\n",
            "step 4161: generator_loss=0.6575809717178345, discriminator_loss=0.7234176397323608\n",
            "step 4162: generator_loss=0.6660447120666504, discriminator_loss=0.7182655334472656\n",
            "step 4163: generator_loss=0.65846848487854, discriminator_loss=0.7230860590934753\n",
            "step 4164: generator_loss=0.655484676361084, discriminator_loss=0.7251744270324707\n",
            "step 4165: generator_loss=0.6498950123786926, discriminator_loss=0.7277790307998657\n",
            "step 4166: generator_loss=0.6488247513771057, discriminator_loss=0.7318751811981201\n",
            "step 4167: generator_loss=0.659799337387085, discriminator_loss=0.7254809141159058\n",
            "step 4168: generator_loss=0.660203218460083, discriminator_loss=0.7259216904640198\n",
            "step 4169: generator_loss=0.6587053537368774, discriminator_loss=0.7263689041137695\n",
            "step 4170: generator_loss=0.6551907658576965, discriminator_loss=0.7301161885261536\n",
            "step 4171: generator_loss=0.6601948142051697, discriminator_loss=0.7255984544754028\n",
            "step 4172: generator_loss=0.6574352979660034, discriminator_loss=0.726473331451416\n",
            "step 4173: generator_loss=0.6551270484924316, discriminator_loss=0.7270069122314453\n",
            "step 4174: generator_loss=0.6664783358573914, discriminator_loss=0.7185665965080261\n",
            "step 4175: generator_loss=0.6628918647766113, discriminator_loss=0.7198135852813721\n",
            "step 4176: generator_loss=0.6663646697998047, discriminator_loss=0.7185428142547607\n",
            "step 4177: generator_loss=0.6717287302017212, discriminator_loss=0.7151377201080322\n",
            "step 4178: generator_loss=0.6696152687072754, discriminator_loss=0.7190099358558655\n",
            "step 4179: generator_loss=0.672193169593811, discriminator_loss=0.720223605632782\n",
            "step 4180: generator_loss=0.676546037197113, discriminator_loss=0.7180447578430176\n",
            "step 4181: generator_loss=0.6740216612815857, discriminator_loss=0.7200318574905396\n",
            "step 4182: generator_loss=0.6667313575744629, discriminator_loss=0.7254258394241333\n",
            "step 4183: generator_loss=0.6716331243515015, discriminator_loss=0.7230164408683777\n",
            "step 4184: generator_loss=0.6721185445785522, discriminator_loss=0.7201615571975708\n",
            "step 4185: generator_loss=0.6664667725563049, discriminator_loss=0.7236655950546265\n",
            "step 4186: generator_loss=0.6649777889251709, discriminator_loss=0.7232397198677063\n",
            "step 4187: generator_loss=0.6606041193008423, discriminator_loss=0.7254666090011597\n",
            "step 4188: generator_loss=0.661945104598999, discriminator_loss=0.7244683504104614\n",
            "step 4189: generator_loss=0.6557101011276245, discriminator_loss=0.7284450531005859\n",
            "step 4190: generator_loss=0.6548570394515991, discriminator_loss=0.7274206876754761\n",
            "step 4191: generator_loss=0.6534497737884521, discriminator_loss=0.7273517847061157\n",
            "step 4192: generator_loss=0.6539257764816284, discriminator_loss=0.7293448448181152\n",
            "step 4193: generator_loss=0.6561097502708435, discriminator_loss=0.7286385893821716\n",
            "step 4194: generator_loss=0.6475068926811218, discriminator_loss=0.7350690960884094\n",
            "step 4195: generator_loss=0.6586104035377502, discriminator_loss=0.7292799949645996\n",
            "step 4196: generator_loss=0.6614127159118652, discriminator_loss=0.728466272354126\n",
            "step 4197: generator_loss=0.6563803553581238, discriminator_loss=0.7321127653121948\n",
            "step 4198: generator_loss=0.655813455581665, discriminator_loss=0.7345763444900513\n",
            "step 4199: generator_loss=0.6608999967575073, discriminator_loss=0.7328578233718872\n",
            "step 4200: generator_loss=0.6635258197784424, discriminator_loss=0.7307459115982056\n",
            "step 4201: generator_loss=0.6607854962348938, discriminator_loss=0.7340003252029419\n",
            "step 4202: generator_loss=0.661778450012207, discriminator_loss=0.7331913709640503\n",
            "step 4203: generator_loss=0.6597205400466919, discriminator_loss=0.7337263226509094\n",
            "step 4204: generator_loss=0.6561430096626282, discriminator_loss=0.7354488372802734\n",
            "step 4205: generator_loss=0.6546505689620972, discriminator_loss=0.7361149787902832\n",
            "step 4206: generator_loss=0.6567357778549194, discriminator_loss=0.7336667776107788\n",
            "step 4207: generator_loss=0.6462453603744507, discriminator_loss=0.7395738363265991\n",
            "step 4208: generator_loss=0.6429712772369385, discriminator_loss=0.7387314438819885\n",
            "step 4209: generator_loss=0.6505832672119141, discriminator_loss=0.7342547178268433\n",
            "step 4210: generator_loss=0.6445660591125488, discriminator_loss=0.7376042008399963\n",
            "step 4211: generator_loss=0.6463572978973389, discriminator_loss=0.7355262041091919\n",
            "step 4212: generator_loss=0.6453012228012085, discriminator_loss=0.7354113459587097\n",
            "step 4213: generator_loss=0.6557168364524841, discriminator_loss=0.7303966879844666\n",
            "step 4214: generator_loss=0.6557134389877319, discriminator_loss=0.7275106906890869\n",
            "step 4215: generator_loss=0.6545150279998779, discriminator_loss=0.7293199300765991\n",
            "step 4216: generator_loss=0.6575009226799011, discriminator_loss=0.7262451648712158\n",
            "step 4217: generator_loss=0.663623034954071, discriminator_loss=0.7239552736282349\n",
            "step 4218: generator_loss=0.6712644100189209, discriminator_loss=0.7209041714668274\n",
            "step 4219: generator_loss=0.6729487180709839, discriminator_loss=0.719192385673523\n",
            "step 4220: generator_loss=0.6782796382904053, discriminator_loss=0.7168446779251099\n",
            "step 4221: generator_loss=0.6814918518066406, discriminator_loss=0.7136276960372925\n",
            "step 4222: generator_loss=0.6914972066879272, discriminator_loss=0.7073469161987305\n",
            "step 4223: generator_loss=0.6955307722091675, discriminator_loss=0.7036257982254028\n",
            "step 4224: generator_loss=0.7016540169715881, discriminator_loss=0.6986348628997803\n",
            "step 4225: generator_loss=0.706954836845398, discriminator_loss=0.6934444904327393\n",
            "step 4226: generator_loss=0.7107858657836914, discriminator_loss=0.6873931884765625\n",
            "step 4227: generator_loss=0.717300534248352, discriminator_loss=0.6815275549888611\n",
            "step 4228: generator_loss=0.7225508093833923, discriminator_loss=0.6766838431358337\n",
            "step 4229: generator_loss=0.7264600992202759, discriminator_loss=0.6715813875198364\n",
            "step 4230: generator_loss=0.7274875044822693, discriminator_loss=0.6700176000595093\n",
            "step 4231: generator_loss=0.7279586791992188, discriminator_loss=0.6658163070678711\n",
            "step 4232: generator_loss=0.729888379573822, discriminator_loss=0.6639392375946045\n",
            "step 4233: generator_loss=0.7213008403778076, discriminator_loss=0.66563880443573\n",
            "step 4234: generator_loss=0.7227033376693726, discriminator_loss=0.6630618572235107\n",
            "step 4235: generator_loss=0.7203418016433716, discriminator_loss=0.6618354916572571\n",
            "step 4236: generator_loss=0.7180554866790771, discriminator_loss=0.6609838008880615\n",
            "step 4237: generator_loss=0.7194896936416626, discriminator_loss=0.6583967208862305\n",
            "step 4238: generator_loss=0.7189149856567383, discriminator_loss=0.6586576700210571\n",
            "step 4239: generator_loss=0.7210603952407837, discriminator_loss=0.6563688516616821\n",
            "step 4240: generator_loss=0.7220068573951721, discriminator_loss=0.6561970114707947\n",
            "step 4241: generator_loss=0.7190837264060974, discriminator_loss=0.6574230194091797\n",
            "step 4242: generator_loss=0.7295584678649902, discriminator_loss=0.6526470184326172\n",
            "step 4243: generator_loss=0.7316039800643921, discriminator_loss=0.6529158353805542\n",
            "step 4244: generator_loss=0.7411800026893616, discriminator_loss=0.6490992307662964\n",
            "step 4245: generator_loss=0.7485306262969971, discriminator_loss=0.6483787894248962\n",
            "step 4246: generator_loss=0.7451777458190918, discriminator_loss=0.65163654088974\n",
            "step 4247: generator_loss=0.7462671399116516, discriminator_loss=0.6530042886734009\n",
            "step 4248: generator_loss=0.7456009387969971, discriminator_loss=0.6538815498352051\n",
            "step 4249: generator_loss=0.7439390420913696, discriminator_loss=0.6560383439064026\n",
            "step 4250: generator_loss=0.7490894198417664, discriminator_loss=0.6511231660842896\n",
            "step 4251: generator_loss=0.7348645925521851, discriminator_loss=0.6566643118858337\n",
            "step 4252: generator_loss=0.7252540588378906, discriminator_loss=0.6591458916664124\n",
            "step 4253: generator_loss=0.7219401001930237, discriminator_loss=0.6598981618881226\n",
            "step 4254: generator_loss=0.7192249894142151, discriminator_loss=0.6603205800056458\n",
            "step 4255: generator_loss=0.7035620212554932, discriminator_loss=0.667709231376648\n",
            "step 4256: generator_loss=0.7120637893676758, discriminator_loss=0.6637604236602783\n",
            "step 4257: generator_loss=0.7125141620635986, discriminator_loss=0.6637204885482788\n",
            "step 4258: generator_loss=0.7082639336585999, discriminator_loss=0.6682186126708984\n",
            "step 4259: generator_loss=0.7058869004249573, discriminator_loss=0.6716355085372925\n",
            "step 4260: generator_loss=0.716940701007843, discriminator_loss=0.6692404747009277\n",
            "step 4261: generator_loss=0.712035059928894, discriminator_loss=0.6733665466308594\n",
            "step 4262: generator_loss=0.7098827362060547, discriminator_loss=0.6764371395111084\n",
            "step 4263: generator_loss=0.704319179058075, discriminator_loss=0.6813868284225464\n",
            "step 4264: generator_loss=0.7137791514396667, discriminator_loss=0.678496241569519\n",
            "step 4265: generator_loss=0.6976127624511719, discriminator_loss=0.6878078579902649\n",
            "step 4266: generator_loss=0.7104377150535583, discriminator_loss=0.6845920085906982\n",
            "step 4267: generator_loss=0.7116892337799072, discriminator_loss=0.6861987709999084\n",
            "step 4268: generator_loss=0.7152372598648071, discriminator_loss=0.6850897073745728\n",
            "step 4269: generator_loss=0.7076300382614136, discriminator_loss=0.6900348663330078\n",
            "step 4270: generator_loss=0.7129031419754028, discriminator_loss=0.6867325305938721\n",
            "step 4271: generator_loss=0.703483521938324, discriminator_loss=0.6909253597259521\n",
            "step 4272: generator_loss=0.6938601136207581, discriminator_loss=0.6949900388717651\n",
            "step 4273: generator_loss=0.6931167840957642, discriminator_loss=0.6935268640518188\n",
            "step 4274: generator_loss=0.6818609237670898, discriminator_loss=0.6984678506851196\n",
            "step 4275: generator_loss=0.6817009449005127, discriminator_loss=0.6984043121337891\n",
            "step 4276: generator_loss=0.6834965944290161, discriminator_loss=0.6969485878944397\n",
            "step 4277: generator_loss=0.6890699863433838, discriminator_loss=0.6930356025695801\n",
            "step 4278: generator_loss=0.6757400035858154, discriminator_loss=0.7022752165794373\n",
            "step 4279: generator_loss=0.6845651865005493, discriminator_loss=0.6971358060836792\n",
            "step 4280: generator_loss=0.683556079864502, discriminator_loss=0.6985005140304565\n",
            "step 4281: generator_loss=0.6818085312843323, discriminator_loss=0.7019387483596802\n",
            "step 4282: generator_loss=0.6819000244140625, discriminator_loss=0.7027937173843384\n",
            "step 4283: generator_loss=0.6933795213699341, discriminator_loss=0.6967825293540955\n",
            "step 4284: generator_loss=0.686419665813446, discriminator_loss=0.7000311613082886\n",
            "step 4285: generator_loss=0.6902099847793579, discriminator_loss=0.695799708366394\n",
            "step 4286: generator_loss=0.688666582107544, discriminator_loss=0.6962695121765137\n",
            "step 4287: generator_loss=0.6876860857009888, discriminator_loss=0.694988489151001\n",
            "step 4288: generator_loss=0.6933596134185791, discriminator_loss=0.6913508176803589\n",
            "step 4289: generator_loss=0.7011542320251465, discriminator_loss=0.6869403123855591\n",
            "step 4290: generator_loss=0.7038606405258179, discriminator_loss=0.6840947866439819\n",
            "step 4291: generator_loss=0.7065836787223816, discriminator_loss=0.6829584836959839\n",
            "step 4292: generator_loss=0.7085000872612, discriminator_loss=0.6811685562133789\n",
            "step 4293: generator_loss=0.718480110168457, discriminator_loss=0.6756001710891724\n",
            "step 4294: generator_loss=0.7263424396514893, discriminator_loss=0.669426441192627\n",
            "step 4295: generator_loss=0.7268695831298828, discriminator_loss=0.667153537273407\n",
            "step 4296: generator_loss=0.7371425628662109, discriminator_loss=0.6611859202384949\n",
            "step 4297: generator_loss=0.7344607710838318, discriminator_loss=0.6615475416183472\n",
            "step 4298: generator_loss=0.7411455512046814, discriminator_loss=0.6567537784576416\n",
            "step 4299: generator_loss=0.7374050617218018, discriminator_loss=0.6580029726028442\n",
            "step 4300: generator_loss=0.7393054962158203, discriminator_loss=0.657035768032074\n",
            "step 4301: generator_loss=0.7421391010284424, discriminator_loss=0.6540855765342712\n",
            "step 4302: generator_loss=0.7399487495422363, discriminator_loss=0.652860164642334\n",
            "step 4303: generator_loss=0.7403113842010498, discriminator_loss=0.6522946357727051\n",
            "step 4304: generator_loss=0.7409613728523254, discriminator_loss=0.6493916511535645\n",
            "step 4305: generator_loss=0.7408427000045776, discriminator_loss=0.649125337600708\n",
            "step 4306: generator_loss=0.7408698201179504, discriminator_loss=0.6478654146194458\n",
            "step 4307: generator_loss=0.7411753535270691, discriminator_loss=0.648800790309906\n",
            "step 4308: generator_loss=0.7392327189445496, discriminator_loss=0.6490215063095093\n",
            "step 4309: generator_loss=0.7375845909118652, discriminator_loss=0.6507502794265747\n",
            "step 4310: generator_loss=0.7401623725891113, discriminator_loss=0.6498990654945374\n",
            "step 4311: generator_loss=0.7353445887565613, discriminator_loss=0.651919960975647\n",
            "step 4312: generator_loss=0.7321106195449829, discriminator_loss=0.6534088850021362\n",
            "step 4313: generator_loss=0.735996425151825, discriminator_loss=0.6510083079338074\n",
            "step 4314: generator_loss=0.7297546863555908, discriminator_loss=0.6549442410469055\n",
            "step 4315: generator_loss=0.7277337312698364, discriminator_loss=0.6562857031822205\n",
            "step 4316: generator_loss=0.7251043319702148, discriminator_loss=0.657599925994873\n",
            "step 4317: generator_loss=0.7301647663116455, discriminator_loss=0.6557785868644714\n",
            "step 4318: generator_loss=0.7279147505760193, discriminator_loss=0.6566480398178101\n",
            "step 4319: generator_loss=0.7225512266159058, discriminator_loss=0.6610392332077026\n",
            "step 4320: generator_loss=0.7302488088607788, discriminator_loss=0.6573236584663391\n",
            "step 4321: generator_loss=0.7328829169273376, discriminator_loss=0.657174825668335\n",
            "step 4322: generator_loss=0.729017436504364, discriminator_loss=0.6595744490623474\n",
            "step 4323: generator_loss=0.730632483959198, discriminator_loss=0.6580167412757874\n",
            "step 4324: generator_loss=0.7350406050682068, discriminator_loss=0.6564247012138367\n",
            "step 4325: generator_loss=0.7316193580627441, discriminator_loss=0.6581439971923828\n",
            "step 4326: generator_loss=0.7338191270828247, discriminator_loss=0.6566240191459656\n",
            "step 4327: generator_loss=0.7389290928840637, discriminator_loss=0.6519641876220703\n",
            "step 4328: generator_loss=0.7434655427932739, discriminator_loss=0.6502200365066528\n",
            "step 4329: generator_loss=0.7362483739852905, discriminator_loss=0.6526133418083191\n",
            "step 4330: generator_loss=0.7396862506866455, discriminator_loss=0.6498641967773438\n",
            "step 4331: generator_loss=0.7397311925888062, discriminator_loss=0.6495624780654907\n",
            "step 4332: generator_loss=0.7364338040351868, discriminator_loss=0.6515429615974426\n",
            "step 4333: generator_loss=0.7341157793998718, discriminator_loss=0.6513451337814331\n",
            "step 4334: generator_loss=0.7333171963691711, discriminator_loss=0.6504616737365723\n",
            "step 4335: generator_loss=0.736233115196228, discriminator_loss=0.6468803882598877\n",
            "step 4336: generator_loss=0.7327459454536438, discriminator_loss=0.6496074199676514\n",
            "step 4337: generator_loss=0.7336593866348267, discriminator_loss=0.648451566696167\n",
            "step 4338: generator_loss=0.7328219413757324, discriminator_loss=0.648555338382721\n",
            "step 4339: generator_loss=0.7352728247642517, discriminator_loss=0.6494869589805603\n",
            "step 4340: generator_loss=0.7385945916175842, discriminator_loss=0.6493504047393799\n",
            "step 4341: generator_loss=0.7344744801521301, discriminator_loss=0.6521968245506287\n",
            "step 4342: generator_loss=0.7341163754463196, discriminator_loss=0.6536999940872192\n",
            "step 4343: generator_loss=0.7352660894393921, discriminator_loss=0.654116153717041\n",
            "step 4344: generator_loss=0.7331898808479309, discriminator_loss=0.6542263627052307\n",
            "step 4345: generator_loss=0.734619140625, discriminator_loss=0.6537122130393982\n",
            "step 4346: generator_loss=0.735193133354187, discriminator_loss=0.6543333530426025\n",
            "step 4347: generator_loss=0.7314660549163818, discriminator_loss=0.6564618349075317\n",
            "step 4348: generator_loss=0.7296909689903259, discriminator_loss=0.6573333740234375\n",
            "step 4349: generator_loss=0.7286196947097778, discriminator_loss=0.6587238311767578\n",
            "step 4350: generator_loss=0.7226829528808594, discriminator_loss=0.6606848239898682\n",
            "step 4351: generator_loss=0.7174365520477295, discriminator_loss=0.6631202697753906\n",
            "step 4352: generator_loss=0.7178244590759277, discriminator_loss=0.6632058620452881\n",
            "step 4353: generator_loss=0.71580970287323, discriminator_loss=0.6646502614021301\n",
            "step 4354: generator_loss=0.7141431570053101, discriminator_loss=0.6659921407699585\n",
            "step 4355: generator_loss=0.7117108702659607, discriminator_loss=0.668976366519928\n",
            "step 4356: generator_loss=0.7148374915122986, discriminator_loss=0.669525146484375\n",
            "step 4357: generator_loss=0.7132377624511719, discriminator_loss=0.6714577674865723\n",
            "step 4358: generator_loss=0.7150163054466248, discriminator_loss=0.6707513332366943\n",
            "step 4359: generator_loss=0.7139647006988525, discriminator_loss=0.670586347579956\n",
            "step 4360: generator_loss=0.7159769535064697, discriminator_loss=0.6702927947044373\n",
            "step 4361: generator_loss=0.7146521806716919, discriminator_loss=0.673189640045166\n",
            "step 4362: generator_loss=0.7152204513549805, discriminator_loss=0.6735703349113464\n",
            "step 4363: generator_loss=0.7101205587387085, discriminator_loss=0.6777644157409668\n",
            "step 4364: generator_loss=0.7097957134246826, discriminator_loss=0.677435040473938\n",
            "step 4365: generator_loss=0.7091565132141113, discriminator_loss=0.6778016090393066\n",
            "step 4366: generator_loss=0.7138540744781494, discriminator_loss=0.6752192378044128\n",
            "step 4367: generator_loss=0.7131608724594116, discriminator_loss=0.6766368746757507\n",
            "step 4368: generator_loss=0.7132378220558167, discriminator_loss=0.6741837859153748\n",
            "step 4369: generator_loss=0.7079459428787231, discriminator_loss=0.677453875541687\n",
            "step 4370: generator_loss=0.7210313081741333, discriminator_loss=0.6687483787536621\n",
            "step 4371: generator_loss=0.713871955871582, discriminator_loss=0.6728979349136353\n",
            "step 4372: generator_loss=0.715461015701294, discriminator_loss=0.6723822951316833\n",
            "step 4373: generator_loss=0.7140131592750549, discriminator_loss=0.6752792596817017\n",
            "step 4374: generator_loss=0.7112511396408081, discriminator_loss=0.6779661178588867\n",
            "step 4375: generator_loss=0.7144122123718262, discriminator_loss=0.6771036386489868\n",
            "step 4376: generator_loss=0.7186498641967773, discriminator_loss=0.6749434471130371\n",
            "step 4377: generator_loss=0.7210732698440552, discriminator_loss=0.6740254759788513\n",
            "step 4378: generator_loss=0.7109720706939697, discriminator_loss=0.681763231754303\n",
            "step 4379: generator_loss=0.7132874727249146, discriminator_loss=0.6812506914138794\n",
            "step 4380: generator_loss=0.7100751399993896, discriminator_loss=0.6830043792724609\n",
            "step 4381: generator_loss=0.6993394494056702, discriminator_loss=0.6918362975120544\n",
            "step 4382: generator_loss=0.7024905681610107, discriminator_loss=0.6885230541229248\n",
            "step 4383: generator_loss=0.6872373819351196, discriminator_loss=0.6970887184143066\n",
            "step 4384: generator_loss=0.6996333599090576, discriminator_loss=0.6888580322265625\n",
            "step 4385: generator_loss=0.6660383939743042, discriminator_loss=0.706939697265625\n",
            "step 4386: generator_loss=0.6778587102890015, discriminator_loss=0.7001303434371948\n",
            "step 4387: generator_loss=0.686852216720581, discriminator_loss=0.6947624087333679\n",
            "step 4388: generator_loss=0.6809452176094055, discriminator_loss=0.6999484896659851\n",
            "step 4389: generator_loss=0.6745128035545349, discriminator_loss=0.707061767578125\n",
            "step 4390: generator_loss=0.6795880794525146, discriminator_loss=0.7051624059677124\n",
            "step 4391: generator_loss=0.6735302209854126, discriminator_loss=0.7153420448303223\n",
            "step 4392: generator_loss=0.6818133592605591, discriminator_loss=0.7131824493408203\n",
            "step 4393: generator_loss=0.6775007247924805, discriminator_loss=0.7180461883544922\n",
            "step 4394: generator_loss=0.683651328086853, discriminator_loss=0.7175825834274292\n",
            "step 4395: generator_loss=0.6583598852157593, discriminator_loss=0.7351365089416504\n",
            "step 4396: generator_loss=0.660881757736206, discriminator_loss=0.7361917495727539\n",
            "step 4397: generator_loss=0.6420371532440186, discriminator_loss=0.7503384947776794\n",
            "step 4398: generator_loss=0.6732422709465027, discriminator_loss=0.7322064638137817\n",
            "step 4399: generator_loss=0.6466927528381348, discriminator_loss=0.7446123361587524\n",
            "step 4400: generator_loss=0.6470391750335693, discriminator_loss=0.7440764904022217\n",
            "step 4401: generator_loss=0.6518433094024658, discriminator_loss=0.7384313941001892\n",
            "step 4402: generator_loss=0.6317495703697205, discriminator_loss=0.7507964372634888\n",
            "step 4403: generator_loss=0.639141857624054, discriminator_loss=0.7425482273101807\n",
            "step 4404: generator_loss=0.6351494789123535, discriminator_loss=0.7423895001411438\n",
            "step 4405: generator_loss=0.6391699314117432, discriminator_loss=0.7396607398986816\n",
            "step 4406: generator_loss=0.6344387531280518, discriminator_loss=0.7395102977752686\n",
            "step 4407: generator_loss=0.6467810869216919, discriminator_loss=0.7280632257461548\n",
            "step 4408: generator_loss=0.6538434028625488, discriminator_loss=0.7224823832511902\n",
            "step 4409: generator_loss=0.664861798286438, discriminator_loss=0.712954044342041\n",
            "step 4410: generator_loss=0.6622604131698608, discriminator_loss=0.7147584557533264\n",
            "step 4411: generator_loss=0.6957146525382996, discriminator_loss=0.6959691047668457\n",
            "step 4412: generator_loss=0.6935220956802368, discriminator_loss=0.6981669664382935\n",
            "step 4413: generator_loss=0.69475919008255, discriminator_loss=0.6992238163948059\n",
            "step 4414: generator_loss=0.701602041721344, discriminator_loss=0.6983602643013\n",
            "step 4415: generator_loss=0.712084174156189, discriminator_loss=0.6927781105041504\n",
            "step 4416: generator_loss=0.7126494646072388, discriminator_loss=0.6911775469779968\n",
            "step 4417: generator_loss=0.7198617458343506, discriminator_loss=0.6860914826393127\n",
            "step 4418: generator_loss=0.7113305330276489, discriminator_loss=0.6877107620239258\n",
            "step 4419: generator_loss=0.7166475057601929, discriminator_loss=0.6831673979759216\n",
            "step 4420: generator_loss=0.7231578230857849, discriminator_loss=0.6755564212799072\n",
            "step 4421: generator_loss=0.7173172235488892, discriminator_loss=0.6751264333724976\n",
            "step 4422: generator_loss=0.7177619934082031, discriminator_loss=0.6720625162124634\n",
            "step 4423: generator_loss=0.7163103818893433, discriminator_loss=0.6690343618392944\n",
            "step 4424: generator_loss=0.7147819995880127, discriminator_loss=0.6673765182495117\n",
            "step 4425: generator_loss=0.722495436668396, discriminator_loss=0.6613640189170837\n",
            "step 4426: generator_loss=0.7307763695716858, discriminator_loss=0.6550418138504028\n",
            "step 4427: generator_loss=0.7384395599365234, discriminator_loss=0.6505899429321289\n",
            "step 4428: generator_loss=0.7377934455871582, discriminator_loss=0.6511000394821167\n",
            "step 4429: generator_loss=0.7456942796707153, discriminator_loss=0.6474055051803589\n",
            "step 4430: generator_loss=0.7538636326789856, discriminator_loss=0.6433347463607788\n",
            "step 4431: generator_loss=0.7588343620300293, discriminator_loss=0.639870822429657\n",
            "step 4432: generator_loss=0.7629486322402954, discriminator_loss=0.6369693875312805\n",
            "step 4433: generator_loss=0.7710571885108948, discriminator_loss=0.632847785949707\n",
            "step 4434: generator_loss=0.7767385244369507, discriminator_loss=0.6281060576438904\n",
            "step 4435: generator_loss=0.7839339375495911, discriminator_loss=0.624958872795105\n",
            "step 4436: generator_loss=0.7814741730690002, discriminator_loss=0.6278649568557739\n",
            "step 4437: generator_loss=0.7845733165740967, discriminator_loss=0.6255874633789062\n",
            "step 4438: generator_loss=0.7794936895370483, discriminator_loss=0.626470685005188\n",
            "step 4439: generator_loss=0.7689282894134521, discriminator_loss=0.6299147605895996\n",
            "step 4440: generator_loss=0.7680214047431946, discriminator_loss=0.6283723711967468\n",
            "step 4441: generator_loss=0.7683377861976624, discriminator_loss=0.6262526512145996\n",
            "step 4442: generator_loss=0.7616410255432129, discriminator_loss=0.6262296438217163\n",
            "step 4443: generator_loss=0.7530407309532166, discriminator_loss=0.6280363202095032\n",
            "step 4444: generator_loss=0.7533471584320068, discriminator_loss=0.6270512342453003\n",
            "step 4445: generator_loss=0.7510533332824707, discriminator_loss=0.6274318695068359\n",
            "step 4446: generator_loss=0.7452150583267212, discriminator_loss=0.6305262446403503\n",
            "step 4447: generator_loss=0.7489620447158813, discriminator_loss=0.6289544701576233\n",
            "step 4448: generator_loss=0.7474056482315063, discriminator_loss=0.6310351490974426\n",
            "step 4449: generator_loss=0.7497389316558838, discriminator_loss=0.631855845451355\n",
            "step 4450: generator_loss=0.7534878849983215, discriminator_loss=0.633644700050354\n",
            "step 4451: generator_loss=0.7531594634056091, discriminator_loss=0.6364539861679077\n",
            "step 4452: generator_loss=0.7580392360687256, discriminator_loss=0.6367805004119873\n",
            "step 4453: generator_loss=0.7561264038085938, discriminator_loss=0.6400587558746338\n",
            "step 4454: generator_loss=0.7586283683776855, discriminator_loss=0.6388493180274963\n",
            "step 4455: generator_loss=0.7544470429420471, discriminator_loss=0.6430844068527222\n",
            "step 4456: generator_loss=0.7421879768371582, discriminator_loss=0.6485419273376465\n",
            "step 4457: generator_loss=0.7425736784934998, discriminator_loss=0.6495956182479858\n",
            "step 4458: generator_loss=0.7454574704170227, discriminator_loss=0.6481842994689941\n",
            "step 4459: generator_loss=0.7472725510597229, discriminator_loss=0.647886335849762\n",
            "step 4460: generator_loss=0.7322889566421509, discriminator_loss=0.6536593437194824\n",
            "step 4461: generator_loss=0.721405029296875, discriminator_loss=0.6582666635513306\n",
            "step 4462: generator_loss=0.7218360900878906, discriminator_loss=0.6601108908653259\n",
            "step 4463: generator_loss=0.7195168733596802, discriminator_loss=0.6611249446868896\n",
            "step 4464: generator_loss=0.7190605401992798, discriminator_loss=0.6616592407226562\n",
            "step 4465: generator_loss=0.7210049629211426, discriminator_loss=0.6608849167823792\n",
            "step 4466: generator_loss=0.7137058973312378, discriminator_loss=0.6670854091644287\n",
            "step 4467: generator_loss=0.6991198062896729, discriminator_loss=0.675306499004364\n",
            "step 4468: generator_loss=0.7170134782791138, discriminator_loss=0.667471170425415\n",
            "step 4469: generator_loss=0.7183587551116943, discriminator_loss=0.6694326400756836\n",
            "step 4470: generator_loss=0.7182367444038391, discriminator_loss=0.6727240681648254\n",
            "step 4471: generator_loss=0.7294221520423889, discriminator_loss=0.6704401969909668\n",
            "step 4472: generator_loss=0.7287760972976685, discriminator_loss=0.6723746061325073\n",
            "step 4473: generator_loss=0.7260441780090332, discriminator_loss=0.6764771938323975\n",
            "step 4474: generator_loss=0.7286695241928101, discriminator_loss=0.6746220588684082\n",
            "step 4475: generator_loss=0.7351343631744385, discriminator_loss=0.6725077629089355\n",
            "step 4476: generator_loss=0.706932783126831, discriminator_loss=0.6850959062576294\n",
            "step 4477: generator_loss=0.7221522331237793, discriminator_loss=0.67864990234375\n",
            "step 4478: generator_loss=0.7213466763496399, discriminator_loss=0.6779299974441528\n",
            "step 4479: generator_loss=0.7181742191314697, discriminator_loss=0.6782580018043518\n",
            "step 4480: generator_loss=0.7126184701919556, discriminator_loss=0.6793142557144165\n",
            "step 4481: generator_loss=0.7086787819862366, discriminator_loss=0.6785060167312622\n",
            "step 4482: generator_loss=0.7168898582458496, discriminator_loss=0.6732450723648071\n",
            "step 4483: generator_loss=0.7173628211021423, discriminator_loss=0.6722702980041504\n",
            "step 4484: generator_loss=0.7221298217773438, discriminator_loss=0.6682782173156738\n",
            "step 4485: generator_loss=0.7181655168533325, discriminator_loss=0.6694583892822266\n",
            "step 4486: generator_loss=0.7198237180709839, discriminator_loss=0.6675374507904053\n",
            "step 4487: generator_loss=0.7140072584152222, discriminator_loss=0.669620931148529\n",
            "step 4488: generator_loss=0.7205803990364075, discriminator_loss=0.6671320199966431\n",
            "step 4489: generator_loss=0.7193381786346436, discriminator_loss=0.6659987568855286\n",
            "step 4490: generator_loss=0.7355668544769287, discriminator_loss=0.6602109670639038\n",
            "step 4491: generator_loss=0.735999584197998, discriminator_loss=0.6592923998832703\n",
            "step 4492: generator_loss=0.7330690622329712, discriminator_loss=0.6615868806838989\n",
            "step 4493: generator_loss=0.7377215623855591, discriminator_loss=0.660186767578125\n",
            "step 4494: generator_loss=0.7422023415565491, discriminator_loss=0.6572864651679993\n",
            "step 4495: generator_loss=0.7339945435523987, discriminator_loss=0.6607879996299744\n",
            "step 4496: generator_loss=0.7424120903015137, discriminator_loss=0.6564687490463257\n",
            "step 4497: generator_loss=0.726648211479187, discriminator_loss=0.6627769470214844\n",
            "step 4498: generator_loss=0.7278531789779663, discriminator_loss=0.6594724655151367\n",
            "step 4499: generator_loss=0.7303944230079651, discriminator_loss=0.658734142780304\n",
            "step 4500: generator_loss=0.7195404171943665, discriminator_loss=0.6609135866165161\n",
            "step 4501: generator_loss=0.7277712821960449, discriminator_loss=0.6561193466186523\n",
            "step 4502: generator_loss=0.717825174331665, discriminator_loss=0.6608387231826782\n",
            "step 4503: generator_loss=0.7163285613059998, discriminator_loss=0.6616568565368652\n",
            "step 4504: generator_loss=0.7278223037719727, discriminator_loss=0.6563129425048828\n",
            "step 4505: generator_loss=0.7237887382507324, discriminator_loss=0.6589632034301758\n",
            "step 4506: generator_loss=0.7268346548080444, discriminator_loss=0.6581823825836182\n",
            "step 4507: generator_loss=0.7446603178977966, discriminator_loss=0.6513330936431885\n",
            "step 4508: generator_loss=0.7435330152511597, discriminator_loss=0.6517311334609985\n",
            "step 4509: generator_loss=0.7593443393707275, discriminator_loss=0.646844744682312\n",
            "step 4510: generator_loss=0.7627866268157959, discriminator_loss=0.6477833986282349\n",
            "step 4511: generator_loss=0.765906572341919, discriminator_loss=0.6479865312576294\n",
            "step 4512: generator_loss=0.7735735177993774, discriminator_loss=0.6461896896362305\n",
            "step 4513: generator_loss=0.766193687915802, discriminator_loss=0.6486003994941711\n",
            "step 4514: generator_loss=0.7734522223472595, discriminator_loss=0.643511176109314\n",
            "step 4515: generator_loss=0.7603735327720642, discriminator_loss=0.6461993455886841\n",
            "step 4516: generator_loss=0.7782056331634521, discriminator_loss=0.6366740465164185\n",
            "step 4517: generator_loss=0.7694394588470459, discriminator_loss=0.6358495950698853\n",
            "step 4518: generator_loss=0.7569806575775146, discriminator_loss=0.6391959190368652\n",
            "step 4519: generator_loss=0.7494690418243408, discriminator_loss=0.6409170031547546\n",
            "step 4520: generator_loss=0.7469937801361084, discriminator_loss=0.639925479888916\n",
            "step 4521: generator_loss=0.7507588863372803, discriminator_loss=0.6376038789749146\n",
            "step 4522: generator_loss=0.7380703687667847, discriminator_loss=0.641507625579834\n",
            "step 4523: generator_loss=0.7385644316673279, discriminator_loss=0.6422964334487915\n",
            "step 4524: generator_loss=0.740256130695343, discriminator_loss=0.6431041955947876\n",
            "step 4525: generator_loss=0.7405463457107544, discriminator_loss=0.6426960825920105\n",
            "step 4526: generator_loss=0.7508800029754639, discriminator_loss=0.6385766267776489\n",
            "step 4527: generator_loss=0.7441928386688232, discriminator_loss=0.6432040929794312\n",
            "step 4528: generator_loss=0.753264844417572, discriminator_loss=0.6382962465286255\n",
            "step 4529: generator_loss=0.7565528154373169, discriminator_loss=0.6362303495407104\n",
            "step 4530: generator_loss=0.7598351240158081, discriminator_loss=0.6355190277099609\n",
            "step 4531: generator_loss=0.7621877193450928, discriminator_loss=0.6354238986968994\n",
            "step 4532: generator_loss=0.7687702178955078, discriminator_loss=0.6331645250320435\n",
            "step 4533: generator_loss=0.771152138710022, discriminator_loss=0.6326444149017334\n",
            "step 4534: generator_loss=0.7660667896270752, discriminator_loss=0.6351509690284729\n",
            "step 4535: generator_loss=0.768666684627533, discriminator_loss=0.6337931156158447\n",
            "step 4536: generator_loss=0.7698647379875183, discriminator_loss=0.6323832273483276\n",
            "step 4537: generator_loss=0.7731176614761353, discriminator_loss=0.6301305890083313\n",
            "step 4538: generator_loss=0.7627825140953064, discriminator_loss=0.6338311433792114\n",
            "step 4539: generator_loss=0.7647948265075684, discriminator_loss=0.6302025318145752\n",
            "step 4540: generator_loss=0.7622597217559814, discriminator_loss=0.6294097304344177\n",
            "step 4541: generator_loss=0.7628540992736816, discriminator_loss=0.6296459436416626\n",
            "step 4542: generator_loss=0.7594643235206604, discriminator_loss=0.6310750246047974\n",
            "step 4543: generator_loss=0.7559299468994141, discriminator_loss=0.6333609819412231\n",
            "step 4544: generator_loss=0.7589207887649536, discriminator_loss=0.6333596706390381\n",
            "step 4545: generator_loss=0.7571045160293579, discriminator_loss=0.635249674320221\n",
            "step 4546: generator_loss=0.7642085552215576, discriminator_loss=0.6331776976585388\n",
            "step 4547: generator_loss=0.7628133893013, discriminator_loss=0.6354542970657349\n",
            "step 4548: generator_loss=0.7572958469390869, discriminator_loss=0.6381101608276367\n",
            "step 4549: generator_loss=0.7513709664344788, discriminator_loss=0.6420698165893555\n",
            "step 4550: generator_loss=0.7500529289245605, discriminator_loss=0.6418333649635315\n",
            "step 4551: generator_loss=0.7444376349449158, discriminator_loss=0.6439462900161743\n",
            "step 4552: generator_loss=0.7370272278785706, discriminator_loss=0.6468096971511841\n",
            "step 4553: generator_loss=0.7357372045516968, discriminator_loss=0.6477651596069336\n",
            "step 4554: generator_loss=0.7255817651748657, discriminator_loss=0.6506475806236267\n",
            "step 4555: generator_loss=0.7259960174560547, discriminator_loss=0.6510635614395142\n",
            "step 4556: generator_loss=0.7236020565032959, discriminator_loss=0.6531009078025818\n",
            "step 4557: generator_loss=0.7328572273254395, discriminator_loss=0.6507755517959595\n",
            "step 4558: generator_loss=0.7265743017196655, discriminator_loss=0.6546698808670044\n",
            "step 4559: generator_loss=0.7229934930801392, discriminator_loss=0.657893180847168\n",
            "step 4560: generator_loss=0.7228215932846069, discriminator_loss=0.6597146987915039\n",
            "step 4561: generator_loss=0.7262345552444458, discriminator_loss=0.6599826216697693\n",
            "step 4562: generator_loss=0.721193790435791, discriminator_loss=0.6643432974815369\n",
            "step 4563: generator_loss=0.7211215496063232, discriminator_loss=0.6637794375419617\n",
            "step 4564: generator_loss=0.7171590328216553, discriminator_loss=0.6677875518798828\n",
            "step 4565: generator_loss=0.7150310277938843, discriminator_loss=0.6678823828697205\n",
            "step 4566: generator_loss=0.7131057977676392, discriminator_loss=0.6686812043190002\n",
            "step 4567: generator_loss=0.713866114616394, discriminator_loss=0.6703439950942993\n",
            "step 4568: generator_loss=0.713503897190094, discriminator_loss=0.6707451343536377\n",
            "step 4569: generator_loss=0.719476580619812, discriminator_loss=0.6679445505142212\n",
            "step 4570: generator_loss=0.7148767709732056, discriminator_loss=0.671444296836853\n",
            "step 4571: generator_loss=0.7208143472671509, discriminator_loss=0.6709684133529663\n",
            "step 4572: generator_loss=0.7188992500305176, discriminator_loss=0.674657940864563\n",
            "step 4573: generator_loss=0.7145625352859497, discriminator_loss=0.676963210105896\n",
            "step 4574: generator_loss=0.7188928127288818, discriminator_loss=0.6756703853607178\n",
            "step 4575: generator_loss=0.719867467880249, discriminator_loss=0.6756628751754761\n",
            "step 4576: generator_loss=0.7074645757675171, discriminator_loss=0.6824250221252441\n",
            "step 4577: generator_loss=0.7017675042152405, discriminator_loss=0.6832462549209595\n",
            "step 4578: generator_loss=0.6974918842315674, discriminator_loss=0.6861342191696167\n",
            "step 4579: generator_loss=0.6918026208877563, discriminator_loss=0.6886674165725708\n",
            "step 4580: generator_loss=0.682131826877594, discriminator_loss=0.6950472593307495\n",
            "step 4581: generator_loss=0.6865071058273315, discriminator_loss=0.6925079226493835\n",
            "step 4582: generator_loss=0.6774625778198242, discriminator_loss=0.6987050771713257\n",
            "step 4583: generator_loss=0.6823164224624634, discriminator_loss=0.6966409683227539\n",
            "step 4584: generator_loss=0.6720094680786133, discriminator_loss=0.7040228247642517\n",
            "step 4585: generator_loss=0.6664100289344788, discriminator_loss=0.7093808650970459\n",
            "step 4586: generator_loss=0.6634077429771423, discriminator_loss=0.7130718231201172\n",
            "step 4587: generator_loss=0.6764308214187622, discriminator_loss=0.7093944549560547\n",
            "step 4588: generator_loss=0.6705760955810547, discriminator_loss=0.7157052755355835\n",
            "step 4589: generator_loss=0.6628921031951904, discriminator_loss=0.7215136289596558\n",
            "step 4590: generator_loss=0.6671856641769409, discriminator_loss=0.7203147411346436\n",
            "step 4591: generator_loss=0.6679952144622803, discriminator_loss=0.722294270992279\n",
            "step 4592: generator_loss=0.6583794355392456, discriminator_loss=0.7267983555793762\n",
            "step 4593: generator_loss=0.6662518978118896, discriminator_loss=0.7229644060134888\n",
            "step 4594: generator_loss=0.6525973081588745, discriminator_loss=0.7307947874069214\n",
            "step 4595: generator_loss=0.6615743637084961, discriminator_loss=0.7264893651008606\n",
            "step 4596: generator_loss=0.6531511545181274, discriminator_loss=0.7292020916938782\n",
            "step 4597: generator_loss=0.6573625802993774, discriminator_loss=0.7268325090408325\n",
            "step 4598: generator_loss=0.6518611907958984, discriminator_loss=0.7312924265861511\n",
            "step 4599: generator_loss=0.6485610008239746, discriminator_loss=0.7347842454910278\n",
            "step 4600: generator_loss=0.6586072444915771, discriminator_loss=0.7309079170227051\n",
            "step 4601: generator_loss=0.659437894821167, discriminator_loss=0.7314742803573608\n",
            "step 4602: generator_loss=0.656072735786438, discriminator_loss=0.7329323887825012\n",
            "step 4603: generator_loss=0.6614258289337158, discriminator_loss=0.7304842472076416\n",
            "step 4604: generator_loss=0.6633638143539429, discriminator_loss=0.730291485786438\n",
            "step 4605: generator_loss=0.6589412093162537, discriminator_loss=0.7302085161209106\n",
            "step 4606: generator_loss=0.6556528210639954, discriminator_loss=0.7321776151657104\n",
            "step 4607: generator_loss=0.6593865156173706, discriminator_loss=0.7294813394546509\n",
            "step 4608: generator_loss=0.6609362959861755, discriminator_loss=0.7291514873504639\n",
            "step 4609: generator_loss=0.6606128215789795, discriminator_loss=0.728226900100708\n",
            "step 4610: generator_loss=0.6565715074539185, discriminator_loss=0.7278735637664795\n",
            "step 4611: generator_loss=0.6548341512680054, discriminator_loss=0.7276160717010498\n",
            "step 4612: generator_loss=0.6561222076416016, discriminator_loss=0.7228057384490967\n",
            "step 4613: generator_loss=0.6579657793045044, discriminator_loss=0.7205949425697327\n",
            "step 4614: generator_loss=0.6645310521125793, discriminator_loss=0.7175241112709045\n",
            "step 4615: generator_loss=0.6641479134559631, discriminator_loss=0.7180877923965454\n",
            "step 4616: generator_loss=0.6632364988327026, discriminator_loss=0.718317985534668\n",
            "step 4617: generator_loss=0.6692402362823486, discriminator_loss=0.714603841304779\n",
            "step 4618: generator_loss=0.6736096143722534, discriminator_loss=0.7147866487503052\n",
            "step 4619: generator_loss=0.6819642186164856, discriminator_loss=0.711377739906311\n",
            "step 4620: generator_loss=0.6808881759643555, discriminator_loss=0.7127026319503784\n",
            "step 4621: generator_loss=0.7034533023834229, discriminator_loss=0.7034096717834473\n",
            "step 4622: generator_loss=0.6991146802902222, discriminator_loss=0.7061741352081299\n",
            "step 4623: generator_loss=0.699791669845581, discriminator_loss=0.7060227394104004\n",
            "step 4624: generator_loss=0.7061261534690857, discriminator_loss=0.7028849720954895\n",
            "step 4625: generator_loss=0.7120193243026733, discriminator_loss=0.6981518268585205\n",
            "step 4626: generator_loss=0.6949633359909058, discriminator_loss=0.7033450603485107\n",
            "step 4627: generator_loss=0.6883205771446228, discriminator_loss=0.7048234343528748\n",
            "step 4628: generator_loss=0.6892765164375305, discriminator_loss=0.7002434730529785\n",
            "step 4629: generator_loss=0.6774529218673706, discriminator_loss=0.7021961212158203\n",
            "step 4630: generator_loss=0.6769107580184937, discriminator_loss=0.6983292698860168\n",
            "step 4631: generator_loss=0.6767210364341736, discriminator_loss=0.6951549053192139\n",
            "step 4632: generator_loss=0.6796244978904724, discriminator_loss=0.6904264688491821\n",
            "step 4633: generator_loss=0.6797171831130981, discriminator_loss=0.6876269578933716\n",
            "step 4634: generator_loss=0.6881735324859619, discriminator_loss=0.6815147399902344\n",
            "step 4635: generator_loss=0.6893244981765747, discriminator_loss=0.678094744682312\n",
            "step 4636: generator_loss=0.7077771425247192, discriminator_loss=0.6674697399139404\n",
            "step 4637: generator_loss=0.718938946723938, discriminator_loss=0.6630213260650635\n",
            "step 4638: generator_loss=0.7259912490844727, discriminator_loss=0.6615818738937378\n",
            "step 4639: generator_loss=0.7450743913650513, discriminator_loss=0.6568887233734131\n",
            "step 4640: generator_loss=0.7567040920257568, discriminator_loss=0.6544588804244995\n",
            "step 4641: generator_loss=0.7548180818557739, discriminator_loss=0.657371997833252\n",
            "step 4642: generator_loss=0.7633628249168396, discriminator_loss=0.6522646546363831\n",
            "step 4643: generator_loss=0.7667579650878906, discriminator_loss=0.6507709622383118\n",
            "step 4644: generator_loss=0.7625166177749634, discriminator_loss=0.6497352123260498\n",
            "step 4645: generator_loss=0.7633287906646729, discriminator_loss=0.6473664045333862\n",
            "step 4646: generator_loss=0.7558952569961548, discriminator_loss=0.6472526788711548\n",
            "step 4647: generator_loss=0.7555359601974487, discriminator_loss=0.6454406976699829\n",
            "step 4648: generator_loss=0.7432726621627808, discriminator_loss=0.6482011079788208\n",
            "step 4649: generator_loss=0.7390322685241699, discriminator_loss=0.6481609344482422\n",
            "step 4650: generator_loss=0.7329625487327576, discriminator_loss=0.6500657796859741\n",
            "step 4651: generator_loss=0.734476625919342, discriminator_loss=0.6473481059074402\n",
            "step 4652: generator_loss=0.7353502511978149, discriminator_loss=0.6465262174606323\n",
            "step 4653: generator_loss=0.729800820350647, discriminator_loss=0.6492577791213989\n",
            "step 4654: generator_loss=0.734938383102417, discriminator_loss=0.6461501121520996\n",
            "step 4655: generator_loss=0.7310341000556946, discriminator_loss=0.64898681640625\n",
            "step 4656: generator_loss=0.7371820211410522, discriminator_loss=0.6461581587791443\n",
            "step 4657: generator_loss=0.7412069439888, discriminator_loss=0.6458982229232788\n",
            "step 4658: generator_loss=0.7466906309127808, discriminator_loss=0.646481454372406\n",
            "step 4659: generator_loss=0.7555640935897827, discriminator_loss=0.6455297470092773\n",
            "step 4660: generator_loss=0.7587012648582458, discriminator_loss=0.6465773582458496\n",
            "step 4661: generator_loss=0.7571271061897278, discriminator_loss=0.6492685079574585\n",
            "step 4662: generator_loss=0.7608442902565002, discriminator_loss=0.6491811275482178\n",
            "step 4663: generator_loss=0.7535959482192993, discriminator_loss=0.6519583463668823\n",
            "step 4664: generator_loss=0.7513413429260254, discriminator_loss=0.651835024356842\n",
            "step 4665: generator_loss=0.7476845383644104, discriminator_loss=0.6504908800125122\n",
            "step 4666: generator_loss=0.7369225025177002, discriminator_loss=0.6546685695648193\n",
            "step 4667: generator_loss=0.7259985208511353, discriminator_loss=0.657170295715332\n",
            "step 4668: generator_loss=0.7247651815414429, discriminator_loss=0.656531810760498\n",
            "step 4669: generator_loss=0.7206815481185913, discriminator_loss=0.6573175191879272\n",
            "step 4670: generator_loss=0.717119574546814, discriminator_loss=0.65779048204422\n",
            "step 4671: generator_loss=0.7128604650497437, discriminator_loss=0.6603381633758545\n",
            "step 4672: generator_loss=0.714921236038208, discriminator_loss=0.6591205596923828\n",
            "step 4673: generator_loss=0.7115713357925415, discriminator_loss=0.6624342203140259\n",
            "step 4674: generator_loss=0.7141945362091064, discriminator_loss=0.6622141003608704\n",
            "step 4675: generator_loss=0.722929835319519, discriminator_loss=0.6587623357772827\n",
            "step 4676: generator_loss=0.7224464416503906, discriminator_loss=0.661624550819397\n",
            "step 4677: generator_loss=0.7237347960472107, discriminator_loss=0.6622330546379089\n",
            "step 4678: generator_loss=0.7261969447135925, discriminator_loss=0.661086916923523\n",
            "step 4679: generator_loss=0.7269078493118286, discriminator_loss=0.6614459156990051\n",
            "step 4680: generator_loss=0.7272576093673706, discriminator_loss=0.6639968752861023\n",
            "step 4681: generator_loss=0.7208589315414429, discriminator_loss=0.6674129366874695\n",
            "step 4682: generator_loss=0.7273097038269043, discriminator_loss=0.6660842895507812\n",
            "step 4683: generator_loss=0.7205761671066284, discriminator_loss=0.6710164546966553\n",
            "step 4684: generator_loss=0.7178456783294678, discriminator_loss=0.6726300716400146\n",
            "step 4685: generator_loss=0.7145780324935913, discriminator_loss=0.6754822731018066\n",
            "step 4686: generator_loss=0.7066593170166016, discriminator_loss=0.6787537336349487\n",
            "step 4687: generator_loss=0.6982041597366333, discriminator_loss=0.682914137840271\n",
            "step 4688: generator_loss=0.6919666528701782, discriminator_loss=0.6867823600769043\n",
            "step 4689: generator_loss=0.6831828355789185, discriminator_loss=0.6923888921737671\n",
            "step 4690: generator_loss=0.6882773637771606, discriminator_loss=0.6911368370056152\n",
            "step 4691: generator_loss=0.683922529220581, discriminator_loss=0.6954962015151978\n",
            "step 4692: generator_loss=0.6862658858299255, discriminator_loss=0.6947764158248901\n",
            "step 4693: generator_loss=0.6857566833496094, discriminator_loss=0.6975660920143127\n",
            "step 4694: generator_loss=0.6733317375183105, discriminator_loss=0.7059566378593445\n",
            "step 4695: generator_loss=0.665302574634552, discriminator_loss=0.7123247385025024\n",
            "step 4696: generator_loss=0.6685960292816162, discriminator_loss=0.7127489447593689\n",
            "step 4697: generator_loss=0.6640019416809082, discriminator_loss=0.7181469202041626\n",
            "step 4698: generator_loss=0.6703259944915771, discriminator_loss=0.7172147035598755\n",
            "step 4699: generator_loss=0.663811206817627, discriminator_loss=0.7225489616394043\n",
            "step 4700: generator_loss=0.6616120338439941, discriminator_loss=0.7260774970054626\n",
            "step 4701: generator_loss=0.6594454050064087, discriminator_loss=0.7273135185241699\n",
            "step 4702: generator_loss=0.6585520505905151, discriminator_loss=0.7284358739852905\n",
            "step 4703: generator_loss=0.6491497755050659, discriminator_loss=0.7348915338516235\n",
            "step 4704: generator_loss=0.6539783477783203, discriminator_loss=0.7342735528945923\n",
            "step 4705: generator_loss=0.6451826691627502, discriminator_loss=0.7395319938659668\n",
            "step 4706: generator_loss=0.6426293253898621, discriminator_loss=0.7413791418075562\n",
            "step 4707: generator_loss=0.6588164567947388, discriminator_loss=0.7331829071044922\n",
            "step 4708: generator_loss=0.6635977029800415, discriminator_loss=0.7323613166809082\n",
            "step 4709: generator_loss=0.6420425176620483, discriminator_loss=0.7446288466453552\n",
            "step 4710: generator_loss=0.666667640209198, discriminator_loss=0.7294211983680725\n",
            "step 4711: generator_loss=0.6572341322898865, discriminator_loss=0.7353149652481079\n",
            "step 4712: generator_loss=0.660494863986969, discriminator_loss=0.7317473292350769\n",
            "step 4713: generator_loss=0.6612859964370728, discriminator_loss=0.7298215627670288\n",
            "step 4714: generator_loss=0.6567917466163635, discriminator_loss=0.7298320531845093\n",
            "step 4715: generator_loss=0.6565849781036377, discriminator_loss=0.7258719205856323\n",
            "step 4716: generator_loss=0.6592172384262085, discriminator_loss=0.7208184599876404\n",
            "step 4717: generator_loss=0.6550425887107849, discriminator_loss=0.7189202308654785\n",
            "step 4718: generator_loss=0.6617096662521362, discriminator_loss=0.712691068649292\n",
            "step 4719: generator_loss=0.6649097204208374, discriminator_loss=0.709082305431366\n",
            "step 4720: generator_loss=0.6726956367492676, discriminator_loss=0.7024235725402832\n",
            "step 4721: generator_loss=0.6718904972076416, discriminator_loss=0.701856255531311\n",
            "step 4722: generator_loss=0.6846824884414673, discriminator_loss=0.6933954954147339\n",
            "step 4723: generator_loss=0.7030171155929565, discriminator_loss=0.6831173300743103\n",
            "step 4724: generator_loss=0.7056243419647217, discriminator_loss=0.6809561252593994\n",
            "step 4725: generator_loss=0.7084336280822754, discriminator_loss=0.6785960793495178\n",
            "step 4726: generator_loss=0.7270137071609497, discriminator_loss=0.6710716485977173\n",
            "step 4727: generator_loss=0.7290101051330566, discriminator_loss=0.6704775094985962\n",
            "step 4728: generator_loss=0.7394931316375732, discriminator_loss=0.6658862233161926\n",
            "step 4729: generator_loss=0.7384955286979675, discriminator_loss=0.6645396947860718\n",
            "step 4730: generator_loss=0.7335216403007507, discriminator_loss=0.664124608039856\n",
            "step 4731: generator_loss=0.7316509485244751, discriminator_loss=0.662084698677063\n",
            "step 4732: generator_loss=0.7276924848556519, discriminator_loss=0.6606035828590393\n",
            "step 4733: generator_loss=0.7290961146354675, discriminator_loss=0.657332181930542\n",
            "step 4734: generator_loss=0.7271696925163269, discriminator_loss=0.6559706926345825\n",
            "step 4735: generator_loss=0.7312939763069153, discriminator_loss=0.6511303186416626\n",
            "step 4736: generator_loss=0.7311238050460815, discriminator_loss=0.6507242918014526\n",
            "step 4737: generator_loss=0.728619396686554, discriminator_loss=0.6491682529449463\n",
            "step 4738: generator_loss=0.7302332520484924, discriminator_loss=0.6480861902236938\n",
            "step 4739: generator_loss=0.7387746572494507, discriminator_loss=0.6422881484031677\n",
            "step 4740: generator_loss=0.7505499720573425, discriminator_loss=0.6370248794555664\n",
            "step 4741: generator_loss=0.7665499448776245, discriminator_loss=0.6302429437637329\n",
            "step 4742: generator_loss=0.7731271982192993, discriminator_loss=0.6310312747955322\n",
            "step 4743: generator_loss=0.7838680744171143, discriminator_loss=0.6299994587898254\n",
            "step 4744: generator_loss=0.7861585021018982, discriminator_loss=0.6329811811447144\n",
            "step 4745: generator_loss=0.7928308248519897, discriminator_loss=0.6319595575332642\n",
            "step 4746: generator_loss=0.7958061695098877, discriminator_loss=0.632088303565979\n",
            "step 4747: generator_loss=0.79050612449646, discriminator_loss=0.6324871778488159\n",
            "step 4748: generator_loss=0.7875644564628601, discriminator_loss=0.6317923665046692\n",
            "step 4749: generator_loss=0.7791011929512024, discriminator_loss=0.6307344436645508\n",
            "step 4750: generator_loss=0.7773983478546143, discriminator_loss=0.6283147931098938\n",
            "step 4751: generator_loss=0.7801846265792847, discriminator_loss=0.6252849698066711\n",
            "step 4752: generator_loss=0.7677878141403198, discriminator_loss=0.6301518082618713\n",
            "step 4753: generator_loss=0.7560420632362366, discriminator_loss=0.6345494985580444\n",
            "step 4754: generator_loss=0.7511060237884521, discriminator_loss=0.6377096176147461\n",
            "step 4755: generator_loss=0.7420983910560608, discriminator_loss=0.6403455138206482\n",
            "step 4756: generator_loss=0.7465455532073975, discriminator_loss=0.6389954090118408\n",
            "step 4757: generator_loss=0.7472559213638306, discriminator_loss=0.6386921405792236\n",
            "step 4758: generator_loss=0.7345879673957825, discriminator_loss=0.6437609195709229\n",
            "step 4759: generator_loss=0.7331689596176147, discriminator_loss=0.6466381549835205\n",
            "step 4760: generator_loss=0.7263155579566956, discriminator_loss=0.6506127119064331\n",
            "step 4761: generator_loss=0.7318739891052246, discriminator_loss=0.6499516367912292\n",
            "step 4762: generator_loss=0.7411926984786987, discriminator_loss=0.6480687856674194\n",
            "step 4763: generator_loss=0.7291563153266907, discriminator_loss=0.654839038848877\n",
            "step 4764: generator_loss=0.7329497337341309, discriminator_loss=0.6550546884536743\n",
            "step 4765: generator_loss=0.7386820912361145, discriminator_loss=0.6523996591567993\n",
            "step 4766: generator_loss=0.7332701683044434, discriminator_loss=0.6573337316513062\n",
            "step 4767: generator_loss=0.727468729019165, discriminator_loss=0.6605571508407593\n",
            "step 4768: generator_loss=0.7275058627128601, discriminator_loss=0.6628063917160034\n",
            "step 4769: generator_loss=0.724033772945404, discriminator_loss=0.6670128107070923\n",
            "step 4770: generator_loss=0.7227128744125366, discriminator_loss=0.6682147979736328\n",
            "step 4771: generator_loss=0.7179164886474609, discriminator_loss=0.6716021299362183\n",
            "step 4772: generator_loss=0.7226361036300659, discriminator_loss=0.6685900092124939\n",
            "step 4773: generator_loss=0.721342921257019, discriminator_loss=0.6710150241851807\n",
            "step 4774: generator_loss=0.7146083116531372, discriminator_loss=0.6736727952957153\n",
            "step 4775: generator_loss=0.7017920017242432, discriminator_loss=0.6797211766242981\n",
            "step 4776: generator_loss=0.7040596008300781, discriminator_loss=0.6797553300857544\n",
            "step 4777: generator_loss=0.7023300528526306, discriminator_loss=0.6801445484161377\n",
            "step 4778: generator_loss=0.7000548243522644, discriminator_loss=0.6825583577156067\n",
            "step 4779: generator_loss=0.6982415914535522, discriminator_loss=0.6833491921424866\n",
            "step 4780: generator_loss=0.6955130100250244, discriminator_loss=0.6867114305496216\n",
            "step 4781: generator_loss=0.6999769806861877, discriminator_loss=0.6868680715560913\n",
            "step 4782: generator_loss=0.7014474272727966, discriminator_loss=0.687462568283081\n",
            "step 4783: generator_loss=0.7005943655967712, discriminator_loss=0.6886552572250366\n",
            "step 4784: generator_loss=0.7037848234176636, discriminator_loss=0.6887379884719849\n",
            "step 4785: generator_loss=0.6978036761283875, discriminator_loss=0.6929055452346802\n",
            "step 4786: generator_loss=0.6996055841445923, discriminator_loss=0.6927120685577393\n",
            "step 4787: generator_loss=0.6948495507240295, discriminator_loss=0.6953828930854797\n",
            "step 4788: generator_loss=0.6916137933731079, discriminator_loss=0.6973270773887634\n",
            "step 4789: generator_loss=0.6920121312141418, discriminator_loss=0.6967734694480896\n",
            "step 4790: generator_loss=0.6898725032806396, discriminator_loss=0.6968519687652588\n",
            "step 4791: generator_loss=0.6842695474624634, discriminator_loss=0.7006613612174988\n",
            "step 4792: generator_loss=0.6850923895835876, discriminator_loss=0.7015290260314941\n",
            "step 4793: generator_loss=0.6871382594108582, discriminator_loss=0.7013825178146362\n",
            "step 4794: generator_loss=0.6934570074081421, discriminator_loss=0.6995002031326294\n",
            "step 4795: generator_loss=0.6913992166519165, discriminator_loss=0.7017590999603271\n",
            "step 4796: generator_loss=0.6902947425842285, discriminator_loss=0.7017918825149536\n",
            "step 4797: generator_loss=0.6882961392402649, discriminator_loss=0.7038320302963257\n",
            "step 4798: generator_loss=0.6916614770889282, discriminator_loss=0.7017749547958374\n",
            "step 4799: generator_loss=0.6974742412567139, discriminator_loss=0.6989282369613647\n",
            "step 4800: generator_loss=0.6941986680030823, discriminator_loss=0.7016886472702026\n",
            "step 4801: generator_loss=0.69469153881073, discriminator_loss=0.7035163640975952\n",
            "step 4802: generator_loss=0.6926695108413696, discriminator_loss=0.7036458849906921\n",
            "step 4803: generator_loss=0.6981624364852905, discriminator_loss=0.7015221118927002\n",
            "step 4804: generator_loss=0.6875325441360474, discriminator_loss=0.7068669199943542\n",
            "step 4805: generator_loss=0.6907971501350403, discriminator_loss=0.7051067352294922\n",
            "step 4806: generator_loss=0.6874935626983643, discriminator_loss=0.7057839632034302\n",
            "step 4807: generator_loss=0.6751145720481873, discriminator_loss=0.7096364498138428\n",
            "step 4808: generator_loss=0.6747689247131348, discriminator_loss=0.7090446352958679\n",
            "step 4809: generator_loss=0.672480046749115, discriminator_loss=0.7096061706542969\n",
            "step 4810: generator_loss=0.6717989444732666, discriminator_loss=0.7107019424438477\n",
            "step 4811: generator_loss=0.6751313209533691, discriminator_loss=0.7114278078079224\n",
            "step 4812: generator_loss=0.6701655387878418, discriminator_loss=0.7143222093582153\n",
            "step 4813: generator_loss=0.6757625937461853, discriminator_loss=0.7136145234107971\n",
            "step 4814: generator_loss=0.6725516319274902, discriminator_loss=0.7163466215133667\n",
            "step 4815: generator_loss=0.6805310249328613, discriminator_loss=0.7126147747039795\n",
            "step 4816: generator_loss=0.6781115531921387, discriminator_loss=0.7133297324180603\n",
            "step 4817: generator_loss=0.6751565337181091, discriminator_loss=0.7150442600250244\n",
            "step 4818: generator_loss=0.675783634185791, discriminator_loss=0.7153488397598267\n",
            "step 4819: generator_loss=0.6747088432312012, discriminator_loss=0.7152983546257019\n",
            "step 4820: generator_loss=0.6726779937744141, discriminator_loss=0.7172400951385498\n",
            "step 4821: generator_loss=0.6708906888961792, discriminator_loss=0.7175952196121216\n",
            "step 4822: generator_loss=0.6693355441093445, discriminator_loss=0.7158098220825195\n",
            "step 4823: generator_loss=0.6638088226318359, discriminator_loss=0.7169625759124756\n",
            "step 4824: generator_loss=0.6634632349014282, discriminator_loss=0.7174917459487915\n",
            "step 4825: generator_loss=0.6616785526275635, discriminator_loss=0.7180277109146118\n",
            "step 4826: generator_loss=0.6661175489425659, discriminator_loss=0.7156301736831665\n",
            "step 4827: generator_loss=0.6675518155097961, discriminator_loss=0.7123241424560547\n",
            "step 4828: generator_loss=0.6634401082992554, discriminator_loss=0.7142783403396606\n",
            "step 4829: generator_loss=0.6747689247131348, discriminator_loss=0.7081785202026367\n",
            "step 4830: generator_loss=0.6681128144264221, discriminator_loss=0.7150821685791016\n",
            "step 4831: generator_loss=0.6758098602294922, discriminator_loss=0.713161051273346\n",
            "step 4832: generator_loss=0.6760879755020142, discriminator_loss=0.7133831977844238\n",
            "step 4833: generator_loss=0.6782786846160889, discriminator_loss=0.7116950750350952\n",
            "step 4834: generator_loss=0.6769787073135376, discriminator_loss=0.7131575345993042\n",
            "step 4835: generator_loss=0.6711050271987915, discriminator_loss=0.7150453925132751\n",
            "step 4836: generator_loss=0.6765221357345581, discriminator_loss=0.710472822189331\n",
            "step 4837: generator_loss=0.6808238625526428, discriminator_loss=0.7061969041824341\n",
            "step 4838: generator_loss=0.6793328523635864, discriminator_loss=0.7059376239776611\n",
            "step 4839: generator_loss=0.6842848062515259, discriminator_loss=0.7026612758636475\n",
            "step 4840: generator_loss=0.6902735233306885, discriminator_loss=0.6987038254737854\n",
            "step 4841: generator_loss=0.6911517381668091, discriminator_loss=0.6981039047241211\n",
            "step 4842: generator_loss=0.6919439435005188, discriminator_loss=0.6987512111663818\n",
            "step 4843: generator_loss=0.7005089521408081, discriminator_loss=0.6955814361572266\n",
            "step 4844: generator_loss=0.7002649307250977, discriminator_loss=0.6964379549026489\n",
            "step 4845: generator_loss=0.6977891325950623, discriminator_loss=0.6991576552391052\n",
            "step 4846: generator_loss=0.7046355605125427, discriminator_loss=0.6941638588905334\n",
            "step 4847: generator_loss=0.706970751285553, discriminator_loss=0.6924179196357727\n",
            "step 4848: generator_loss=0.6993730664253235, discriminator_loss=0.6955941915512085\n",
            "step 4849: generator_loss=0.6979348659515381, discriminator_loss=0.6934473514556885\n",
            "step 4850: generator_loss=0.704243540763855, discriminator_loss=0.6894546747207642\n",
            "step 4851: generator_loss=0.6962019205093384, discriminator_loss=0.6905936598777771\n",
            "step 4852: generator_loss=0.7031333446502686, discriminator_loss=0.6865765452384949\n",
            "step 4853: generator_loss=0.7024925351142883, discriminator_loss=0.6858659386634827\n",
            "step 4854: generator_loss=0.7089022994041443, discriminator_loss=0.6829083561897278\n",
            "step 4855: generator_loss=0.7039746046066284, discriminator_loss=0.6859292984008789\n",
            "step 4856: generator_loss=0.7143025398254395, discriminator_loss=0.6821401119232178\n",
            "step 4857: generator_loss=0.7151796817779541, discriminator_loss=0.6799466609954834\n",
            "step 4858: generator_loss=0.7141093611717224, discriminator_loss=0.6800280213356018\n",
            "step 4859: generator_loss=0.7134546041488647, discriminator_loss=0.680371880531311\n",
            "step 4860: generator_loss=0.708736777305603, discriminator_loss=0.6805905103683472\n",
            "step 4861: generator_loss=0.7089035511016846, discriminator_loss=0.680664598941803\n",
            "step 4862: generator_loss=0.7145119905471802, discriminator_loss=0.6778684854507446\n",
            "step 4863: generator_loss=0.7093312740325928, discriminator_loss=0.6790773868560791\n",
            "step 4864: generator_loss=0.7072262763977051, discriminator_loss=0.6788263320922852\n",
            "step 4865: generator_loss=0.7035574316978455, discriminator_loss=0.6801344156265259\n",
            "step 4866: generator_loss=0.7035102248191833, discriminator_loss=0.6805234551429749\n",
            "step 4867: generator_loss=0.7086041569709778, discriminator_loss=0.6786407232284546\n",
            "step 4868: generator_loss=0.7050892114639282, discriminator_loss=0.6793752908706665\n",
            "step 4869: generator_loss=0.7010469436645508, discriminator_loss=0.6815138459205627\n",
            "step 4870: generator_loss=0.7042146921157837, discriminator_loss=0.6808582544326782\n",
            "step 4871: generator_loss=0.7018767595291138, discriminator_loss=0.6818435192108154\n",
            "step 4872: generator_loss=0.7002816200256348, discriminator_loss=0.6803755164146423\n",
            "step 4873: generator_loss=0.7014305591583252, discriminator_loss=0.6802130341529846\n",
            "step 4874: generator_loss=0.703758180141449, discriminator_loss=0.6787861585617065\n",
            "step 4875: generator_loss=0.7026538848876953, discriminator_loss=0.6785718202590942\n",
            "step 4876: generator_loss=0.7051711678504944, discriminator_loss=0.6784332394599915\n",
            "step 4877: generator_loss=0.7001249194145203, discriminator_loss=0.6822289824485779\n",
            "step 4878: generator_loss=0.7042311429977417, discriminator_loss=0.681025505065918\n",
            "step 4879: generator_loss=0.701809287071228, discriminator_loss=0.6844529509544373\n",
            "step 4880: generator_loss=0.705560564994812, discriminator_loss=0.6818951368331909\n",
            "step 4881: generator_loss=0.7010319232940674, discriminator_loss=0.6834575533866882\n",
            "step 4882: generator_loss=0.7027343511581421, discriminator_loss=0.6843164563179016\n",
            "step 4883: generator_loss=0.7061642408370972, discriminator_loss=0.6811037063598633\n",
            "step 4884: generator_loss=0.7022640705108643, discriminator_loss=0.6820911765098572\n",
            "step 4885: generator_loss=0.7010979652404785, discriminator_loss=0.681839108467102\n",
            "step 4886: generator_loss=0.7008326053619385, discriminator_loss=0.6815809011459351\n",
            "step 4887: generator_loss=0.7027713656425476, discriminator_loss=0.6813653111457825\n",
            "step 4888: generator_loss=0.703352689743042, discriminator_loss=0.681816577911377\n",
            "step 4889: generator_loss=0.702305257320404, discriminator_loss=0.6832314729690552\n",
            "step 4890: generator_loss=0.7021862268447876, discriminator_loss=0.6841166019439697\n",
            "step 4891: generator_loss=0.7013429999351501, discriminator_loss=0.6857391595840454\n",
            "step 4892: generator_loss=0.7073605060577393, discriminator_loss=0.6832430958747864\n",
            "step 4893: generator_loss=0.7035677433013916, discriminator_loss=0.685548722743988\n",
            "step 4894: generator_loss=0.7026146650314331, discriminator_loss=0.6858733892440796\n",
            "step 4895: generator_loss=0.699567437171936, discriminator_loss=0.6886106729507446\n",
            "step 4896: generator_loss=0.6982909440994263, discriminator_loss=0.6876787543296814\n",
            "step 4897: generator_loss=0.697501540184021, discriminator_loss=0.6893523931503296\n",
            "step 4898: generator_loss=0.6907320022583008, discriminator_loss=0.6933508515357971\n",
            "step 4899: generator_loss=0.6921725869178772, discriminator_loss=0.6918487548828125\n",
            "step 4900: generator_loss=0.6941587924957275, discriminator_loss=0.6915572881698608\n",
            "step 4901: generator_loss=0.6923642158508301, discriminator_loss=0.6907386183738708\n",
            "step 4902: generator_loss=0.6926889419555664, discriminator_loss=0.6916601657867432\n",
            "step 4903: generator_loss=0.6903375387191772, discriminator_loss=0.6931152939796448\n",
            "step 4904: generator_loss=0.6903409957885742, discriminator_loss=0.694260835647583\n",
            "step 4905: generator_loss=0.6934151649475098, discriminator_loss=0.6930739283561707\n",
            "step 4906: generator_loss=0.6895705461502075, discriminator_loss=0.6945275068283081\n",
            "step 4907: generator_loss=0.6908080577850342, discriminator_loss=0.6950733661651611\n",
            "step 4908: generator_loss=0.6942873001098633, discriminator_loss=0.6932336688041687\n",
            "step 4909: generator_loss=0.6976327300071716, discriminator_loss=0.6919198036193848\n",
            "step 4910: generator_loss=0.6970741748809814, discriminator_loss=0.6938700675964355\n",
            "step 4911: generator_loss=0.6969122290611267, discriminator_loss=0.6943423748016357\n",
            "step 4912: generator_loss=0.6962081789970398, discriminator_loss=0.6943410038948059\n",
            "step 4913: generator_loss=0.6922365427017212, discriminator_loss=0.6960592269897461\n",
            "step 4914: generator_loss=0.6923111081123352, discriminator_loss=0.695404052734375\n",
            "step 4915: generator_loss=0.6907021999359131, discriminator_loss=0.6944112777709961\n",
            "step 4916: generator_loss=0.6891324520111084, discriminator_loss=0.6939738988876343\n",
            "step 4917: generator_loss=0.6885323524475098, discriminator_loss=0.6930180191993713\n",
            "step 4918: generator_loss=0.6901264190673828, discriminator_loss=0.6916263699531555\n",
            "step 4919: generator_loss=0.6871525645256042, discriminator_loss=0.6921340227127075\n",
            "step 4920: generator_loss=0.6879640817642212, discriminator_loss=0.6926448941230774\n",
            "step 4921: generator_loss=0.6901556253433228, discriminator_loss=0.6907922029495239\n",
            "step 4922: generator_loss=0.6917696595191956, discriminator_loss=0.6904500722885132\n",
            "step 4923: generator_loss=0.6947901844978333, discriminator_loss=0.6888570189476013\n",
            "step 4924: generator_loss=0.6983591318130493, discriminator_loss=0.6879798173904419\n",
            "step 4925: generator_loss=0.7070605754852295, discriminator_loss=0.6843867301940918\n",
            "step 4926: generator_loss=0.7053965330123901, discriminator_loss=0.6849849820137024\n",
            "step 4927: generator_loss=0.7069805860519409, discriminator_loss=0.6863785982131958\n",
            "step 4928: generator_loss=0.7113154530525208, discriminator_loss=0.6843041181564331\n",
            "step 4929: generator_loss=0.7065305709838867, discriminator_loss=0.687659740447998\n",
            "step 4930: generator_loss=0.7077387571334839, discriminator_loss=0.6879806518554688\n",
            "step 4931: generator_loss=0.7067438960075378, discriminator_loss=0.6875485181808472\n",
            "step 4932: generator_loss=0.6994258761405945, discriminator_loss=0.6910540461540222\n",
            "step 4933: generator_loss=0.6983008980751038, discriminator_loss=0.6891070604324341\n",
            "step 4934: generator_loss=0.6936297416687012, discriminator_loss=0.6911106109619141\n",
            "step 4935: generator_loss=0.6862413287162781, discriminator_loss=0.6934748888015747\n",
            "step 4936: generator_loss=0.6816821098327637, discriminator_loss=0.6950048208236694\n",
            "step 4937: generator_loss=0.6847696900367737, discriminator_loss=0.6932792067527771\n",
            "step 4938: generator_loss=0.6872812509536743, discriminator_loss=0.6925550699234009\n",
            "step 4939: generator_loss=0.6834042072296143, discriminator_loss=0.6939129829406738\n",
            "step 4940: generator_loss=0.6817665100097656, discriminator_loss=0.6961643695831299\n",
            "step 4941: generator_loss=0.6860272288322449, discriminator_loss=0.6963894367218018\n",
            "step 4942: generator_loss=0.689494788646698, discriminator_loss=0.6950821876525879\n",
            "step 4943: generator_loss=0.6840628385543823, discriminator_loss=0.6976882219314575\n",
            "step 4944: generator_loss=0.6864334344863892, discriminator_loss=0.6991214752197266\n",
            "step 4945: generator_loss=0.6894893646240234, discriminator_loss=0.6989580988883972\n",
            "step 4946: generator_loss=0.6928032636642456, discriminator_loss=0.6985554695129395\n",
            "step 4947: generator_loss=0.688259482383728, discriminator_loss=0.7006586790084839\n",
            "step 4948: generator_loss=0.6897816061973572, discriminator_loss=0.7009045481681824\n",
            "step 4949: generator_loss=0.6895531415939331, discriminator_loss=0.7011020183563232\n",
            "step 4950: generator_loss=0.6926614046096802, discriminator_loss=0.6987448930740356\n",
            "step 4951: generator_loss=0.688016951084137, discriminator_loss=0.701457142829895\n",
            "step 4952: generator_loss=0.6837253570556641, discriminator_loss=0.7035258412361145\n",
            "step 4953: generator_loss=0.6835619211196899, discriminator_loss=0.702634334564209\n",
            "step 4954: generator_loss=0.682567298412323, discriminator_loss=0.7024379968643188\n",
            "step 4955: generator_loss=0.6844263076782227, discriminator_loss=0.7011920213699341\n",
            "step 4956: generator_loss=0.6862789392471313, discriminator_loss=0.699954628944397\n",
            "step 4957: generator_loss=0.6860601902008057, discriminator_loss=0.7002120018005371\n",
            "step 4958: generator_loss=0.6879743337631226, discriminator_loss=0.6982240676879883\n",
            "step 4959: generator_loss=0.6907742619514465, discriminator_loss=0.6973782777786255\n",
            "step 4960: generator_loss=0.6853134632110596, discriminator_loss=0.7019115686416626\n",
            "step 4961: generator_loss=0.6897294521331787, discriminator_loss=0.6984997987747192\n",
            "step 4962: generator_loss=0.6891405582427979, discriminator_loss=0.6999702453613281\n",
            "step 4963: generator_loss=0.6919279098510742, discriminator_loss=0.6981871128082275\n",
            "step 4964: generator_loss=0.6852157115936279, discriminator_loss=0.6998257040977478\n",
            "step 4965: generator_loss=0.6893552541732788, discriminator_loss=0.6962481141090393\n",
            "step 4966: generator_loss=0.6880406141281128, discriminator_loss=0.6961972713470459\n",
            "step 4967: generator_loss=0.6873531341552734, discriminator_loss=0.6962494850158691\n",
            "step 4968: generator_loss=0.6896383762359619, discriminator_loss=0.6939855813980103\n",
            "step 4969: generator_loss=0.6866276264190674, discriminator_loss=0.6960650682449341\n",
            "step 4970: generator_loss=0.688625693321228, discriminator_loss=0.6959025859832764\n",
            "step 4971: generator_loss=0.6903097033500671, discriminator_loss=0.6965650916099548\n",
            "step 4972: generator_loss=0.6914001107215881, discriminator_loss=0.6963624954223633\n",
            "step 4973: generator_loss=0.690930187702179, discriminator_loss=0.6965062618255615\n",
            "step 4974: generator_loss=0.6904885768890381, discriminator_loss=0.6960957050323486\n",
            "step 4975: generator_loss=0.6919382214546204, discriminator_loss=0.6965025663375854\n",
            "step 4976: generator_loss=0.6903015375137329, discriminator_loss=0.6966264247894287\n",
            "step 4977: generator_loss=0.688656210899353, discriminator_loss=0.6974621415138245\n",
            "step 4978: generator_loss=0.6895641088485718, discriminator_loss=0.6975743174552917\n",
            "step 4979: generator_loss=0.6825611591339111, discriminator_loss=0.7000366449356079\n",
            "step 4980: generator_loss=0.6833467483520508, discriminator_loss=0.7005141377449036\n",
            "step 4981: generator_loss=0.6829818487167358, discriminator_loss=0.7002912759780884\n",
            "step 4982: generator_loss=0.6837554574012756, discriminator_loss=0.7006638646125793\n",
            "step 4983: generator_loss=0.6810311079025269, discriminator_loss=0.7011222243309021\n",
            "step 4984: generator_loss=0.6809161305427551, discriminator_loss=0.7037771940231323\n",
            "step 4985: generator_loss=0.6782598495483398, discriminator_loss=0.7033597230911255\n",
            "step 4986: generator_loss=0.6781435012817383, discriminator_loss=0.7050042152404785\n",
            "step 4987: generator_loss=0.6797586679458618, discriminator_loss=0.7049016356468201\n",
            "step 4988: generator_loss=0.6855842471122742, discriminator_loss=0.7015836238861084\n",
            "step 4989: generator_loss=0.6741807460784912, discriminator_loss=0.7085394263267517\n",
            "step 4990: generator_loss=0.6764468550682068, discriminator_loss=0.706743597984314\n",
            "step 4991: generator_loss=0.6800205707550049, discriminator_loss=0.7073338627815247\n",
            "step 4992: generator_loss=0.6824235916137695, discriminator_loss=0.7055691480636597\n",
            "step 4993: generator_loss=0.6845575571060181, discriminator_loss=0.7042700052261353\n",
            "step 4994: generator_loss=0.6762908697128296, discriminator_loss=0.7084273099899292\n",
            "step 4995: generator_loss=0.6815916895866394, discriminator_loss=0.7048203945159912\n",
            "step 4996: generator_loss=0.6942842602729797, discriminator_loss=0.6982046961784363\n",
            "step 4997: generator_loss=0.6907510757446289, discriminator_loss=0.6990134716033936\n",
            "step 4998: generator_loss=0.6885294914245605, discriminator_loss=0.7005527019500732\n",
            "step 4999: generator_loss=0.6921775341033936, discriminator_loss=0.6985079050064087\n",
            "step 5000: generator_loss=0.6881103515625, discriminator_loss=0.700470507144928\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "n_steps = 5_000\n",
        "steps_per_save = 250\n",
        "seed = 0\n",
        "key = jax.random.key(seed)\n",
        "batch_size = 128\n",
        "latent_dim = 64\n",
        "loss_type = \"nonsaturating\"\n",
        "\n",
        "# Train GAN and save checkpoints\n",
        "generator_training_state, discriminator_training_state, key = train_gan(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    n_steps=n_steps,\n",
        "    steps_per_save=steps_per_save,\n",
        "    key=key,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "184e87a8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_real_vs_generated_histogram_from_last_checkpoint(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    y_max: float = 0.02,\n",
        "):\n",
        "    \"\"\"Plot one real histogram and one generated histogram from last checkpoint.\n",
        "\n",
        "    Restores the generator from the latest checkpoint for this constraints\n",
        "    experiment and plots a single real vs generated histogram.\n",
        "    \"\"\"\n",
        "    # Restore generator from the last checkpoint for this constraints experiment\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    steps = sorted(ckpt_manager.all_steps())\n",
        "    if not steps:\n",
        "        raise ValueError(\n",
        "            f\"No checkpoints found for experiment_name={experiment_name!r}.\"\n",
        "        )\n",
        "\n",
        "    # Template states for restore (we only actually use the generator here)\n",
        "    gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=jax.random.key(0),\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "    restored = ckpt_manager.restore(\n",
        "        steps[-1],\n",
        "        args=ocp.args.StandardRestore(\n",
        "            item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "        ),\n",
        "    )\n",
        "    generator_for_plot = restored[\"generator\"]\n",
        "\n",
        "    # Local PRNG keys for this plotting run\n",
        "    key = jax.random.key(0)\n",
        "    key, z_key, real_key = jax.random.split(key, 3)\n",
        "\n",
        "    # Sample a latent vector and generate one fake histogram\n",
        "    z = jax.random.normal(z_key, (1, latent_dim))\n",
        "    fake_hist = generator_for_plot.apply_fn(\n",
        "        {\"params\": generator_for_plot.params},\n",
        "        z,\n",
        "    )[0]\n",
        "\n",
        "    # Sample one real histogram from the training set\n",
        "    real_idx = jax.random.randint(\n",
        "        real_key, shape=(), minval=0, maxval=train_data.shape[0]\n",
        "    )\n",
        "    real_hist = train_data[real_idx]\n",
        "\n",
        "    # Ensure both are non-negative and normalized to sum to 1\n",
        "    fake_hist = jnp.maximum(fake_hist, 0.0)\n",
        "    fake_hist = fake_hist / fake_hist.sum()\n",
        "    real_hist = real_hist / real_hist.sum()\n",
        "\n",
        "    # Convert to NumPy for plotting\n",
        "    fake_hist_np = np.array(fake_hist)\n",
        "    real_hist_np = np.array(real_hist)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(real_hist_np, label=\"Real histogram\")\n",
        "    plt.plot(fake_hist_np, label=\"Generated histogram\")\n",
        "    plt.xlabel(\"Bin index\")\n",
        "    plt.ylabel(\"Probability (normalized)\")\n",
        "    plt.ylim(top=y_max)\n",
        "    plt.title(\"Real vs Generated Histogram\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot using the latest checkpoint available (e.g. after first 5000 steps)\n",
        "plot_real_vs_generated_histogram_from_last_checkpoint(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        "    y_max=0.02,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb76bbaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_experiment_for_loss_type(\n",
        "#     loss_type: str,\n",
        "#     train_data,\n",
        "#     learning_rate: float = 1e-3,\n",
        "#     batch_size: int = 128,\n",
        "#     latent_dim: int = 64,\n",
        "#     n_steps: int = 5_000,\n",
        "#     steps_per_save: int = 250,\n",
        "#     seed: int = 0,\n",
        "# ):\n",
        "#     \"\"\"Train GAN and save checkpoints for a given loss_type.\n",
        "\n",
        "#     This function returns training results; plotting is handled separately.\n",
        "\n",
        "#     loss_type: \"nonsaturating\" or \"saturating\".\n",
        "#     \"\"\"\n",
        "#     print(f\"=== Running experiment with loss_type={loss_type} ===\")\n",
        "\n",
        "#     # Optimizer and PRNG key\n",
        "#     optimizer = optax.adam(learning_rate)\n",
        "#     key = jax.random.key(seed)\n",
        "\n",
        "#     # Train GAN and save checkpoints\n",
        "#     generator_training_state, discriminator_training_state, key = train_gan(\n",
        "#         train_data=train_data,\n",
        "#         optimizer=optimizer,\n",
        "#         n_steps=n_steps,\n",
        "#         steps_per_save=steps_per_save,\n",
        "#         key=key,\n",
        "#         batch_size=batch_size,\n",
        "#         latent_dim=latent_dim,\n",
        "#         loss_type=loss_type,\n",
        "#     )\n",
        "\n",
        "#     # --- Loss curves over checkpoints for this loss_type ---\n",
        "#     gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "#         optimizer,\n",
        "#         key=jax.random.key(0),\n",
        "#         latent_dim=latent_dim,\n",
        "#     )\n",
        "#     experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "#     ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "#     steps = sorted(ckpt_manager.all_steps())\n",
        "\n",
        "#     g_losses, d_losses = [], []\n",
        "#     real_images = train_data[\"image\"]\n",
        "\n",
        "#     for step in steps:\n",
        "#         restored = ckpt_manager.restore(\n",
        "#             step,\n",
        "#             args=ocp.args.StandardRestore(\n",
        "#                 item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "#             ),\n",
        "#         )\n",
        "#         gen_state = restored[\"generator\"]\n",
        "#         disc_state = restored[\"discriminator\"]\n",
        "\n",
        "#         key = jax.random.key(0)\n",
        "#         key, z_key = jax.random.split(key)\n",
        "#         z_vectors = jax.random.normal(\n",
        "#             z_key,\n",
        "#             (real_images.shape[0], latent_dim),\n",
        "#         )\n",
        "\n",
        "#         g_loss = calculate_generator_loss(\n",
        "#             gen_state.params,\n",
        "#             disc_state.params,\n",
        "#             gen_state.apply_fn,\n",
        "#             disc_state.apply_fn,\n",
        "#             z_vectors,\n",
        "#             loss_type=loss_type,\n",
        "#         )\n",
        "#         d_loss = calculate_discriminator_loss(\n",
        "#             disc_state.params,\n",
        "#             gen_state.params,\n",
        "#             gen_state.apply_fn,\n",
        "#             disc_state.apply_fn,\n",
        "#             z_vectors,\n",
        "#             real_images,\n",
        "#         )\n",
        "\n",
        "#         g_losses.append(float(g_loss))\n",
        "#         d_losses.append(float(d_loss))\n",
        "\n",
        "#     # Collect results (plotting handled separately)\n",
        "#     return {\n",
        "#         \"generator_state\": generator_training_state,\n",
        "#         \"discriminator_state\": discriminator_training_state,\n",
        "#         \"steps\": steps,\n",
        "#         \"g_losses\": g_losses,\n",
        "#         \"d_losses\": d_losses,\n",
        "#         \"loss_type\": loss_type,\n",
        "#         \"latent_dim\": latent_dim,\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda4c1e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_experiment_for_loss_type(results, train_data):\n",
        "#     \"\"\"Plot sample images and loss dynamics for a single loss_type run.\n",
        "\n",
        "#     Expects the dict returned by `run_experiment_for_loss_type` and the\n",
        "#     corresponding `train_data` used for training.\n",
        "#     \"\"\"\n",
        "#     loss_type = results[\"loss_type\"]\n",
        "#     generator_training_state = results[\"generator_state\"]\n",
        "#     steps = results[\"steps\"]\n",
        "#     g_losses = results[\"g_losses\"]\n",
        "#     d_losses = results[\"d_losses\"]\n",
        "#     latent_dim = results.get(\"latent_dim\", 64)\n",
        "\n",
        "#     # --- Plot a sample real vs fake image ---\n",
        "#     key = jax.random.key(0)\n",
        "#     key, z_key, real_image_key = jax.random.split(key, 3)\n",
        "#     z_vector = jax.random.normal(z_key, (1, latent_dim))\n",
        "#     real_idx = jax.random.randint(\n",
        "#         real_image_key, shape=(), minval=0, maxval=train_data.shape[0]\n",
        "#     )\n",
        "\n",
        "#     fake_image_flat = generator_training_state.apply_fn(\n",
        "#         {\"params\": generator_training_state.params},\n",
        "#         z_vector,\n",
        "#     )\n",
        "#     fake_image = fake_image_flat[0].reshape(28, 28)\n",
        "\n",
        "#     real_image_flat = train_data[real_idx]\n",
        "#     real_image = real_image_flat.reshape(28, 28)\n",
        "\n",
        "#     fig, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
        "#     axes[0].imshow(real_image, cmap=\"gray\"); axes[0].set_title(f\"Real ({loss_type})\"); axes[0].axis(\"off\")\n",
        "#     axes[1].imshow(fake_image, cmap=\"gray\"); axes[1].set_title(f\"Fake ({loss_type})\"); axes[1].axis(\"off\")\n",
        "#     plt.tight_layout(); plt.show()\n",
        "\n",
        "#     # --- Plot loss curves over checkpoints for this loss_type ---\n",
        "#     plt.figure()\n",
        "#     plt.plot(steps, g_losses, label=f\"generator ({loss_type})\")\n",
        "#     plt.plot(steps, d_losses, label=f\"discriminator ({loss_type})\")\n",
        "#     plt.xlabel(\"training step\")\n",
        "#     plt.ylabel(\"loss\")\n",
        "#     plt.title(f\"GAN losses ({loss_type})\")\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n",
        "\n",
        "#     # --- Plot training dynamics in (g, d) loss space ---\n",
        "#     g = np.array(g_losses)\n",
        "#     d = np.array(d_losses)\n",
        "\n",
        "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
        "#     points = np.column_stack([g, d])\n",
        "#     segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "#     t = np.linspace(0.0, 1.0, len(points) - 1)\n",
        "#     colors = cm.coolwarm(t)\n",
        "\n",
        "#     lc = LineCollection(segments, colors=colors, linewidths=1.5)\n",
        "#     ax.add_collection(lc)\n",
        "\n",
        "#     ax.scatter(g, d, c=t.tolist() + [1.0], cmap=\"coolwarm\", s=8, alpha=0.7)\n",
        "#     ax.scatter(g[0], d[0], color=\"blue\", s=50, label=\"start\")\n",
        "#     ax.scatter(g[-1], d[-1], color=\"red\", s=50, label=\"end\")\n",
        "\n",
        "#     ax.set_xlabel(\"Generator loss\")\n",
        "#     ax.set_ylabel(\"Discriminator loss\")\n",
        "#     ax.set_title(f\"GAN training dynamics in loss space ({loss_type})\")\n",
        "#     ax.grid(True)\n",
        "#     ax.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6610b8cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Convenience: run the full experiment for both loss types\n",
        "\n",
        "# results_by_loss_type = {}\n",
        "# for lt in [\"saturating\", \"nonsaturating\"]:\n",
        "#     results_by_loss_type[lt] = run_experiment_for_loss_type(\n",
        "#         loss_type=lt,\n",
        "#         train_data=train_data,\n",
        "#         learning_rate=1e-3,\n",
        "#         batch_size=128,\n",
        "#         latent_dim=64,\n",
        "#         n_steps=5_000,\n",
        "#         steps_per_save=250,\n",
        "#         seed=0,\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a819429",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot_experiment_for_loss_type(results_by_loss_type[\"saturating\"], train_data)\n",
        "# plot_experiment_for_loss_type(results_by_loss_type[\"nonsaturating\"], train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c19808",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# out_dir = Path(\"../../lab-notes/2025-12-17_gan-nonsaturating_loss\")\n",
        "# out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# for i, num in enumerate(sorted(plt.get_fignums()), start=1):\n",
        "#     fig = plt.figure(num)\n",
        "#     fig.savefig(\n",
        "#         out_dir / f\"2025-12-17_gan-nonsaturating_loss_{i}.png\",\n",
        "#         dpi=200,\n",
        "#         bbox_inches=\"tight\",\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "b932c49e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    batch_size: int,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "):\n",
        "    \"\"\"Reload checkpoints for this notebook's run and plot D vs G loss.\n",
        "\n",
        "    Uses the same naming convention as `train_gan` and evaluates losses\n",
        "    at each saved checkpoint on a fresh mini-batch from `train_data`.\n",
        "    \"\"\"\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    steps = sorted(ckpt_manager.all_steps())\n",
        "\n",
        "    if not steps:\n",
        "        print(f\"No checkpoints found for experiment_name={experiment_name!r}.\")\n",
        "        return\n",
        "\n",
        "    # Template states for restoring checkpoints\n",
        "    gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=jax.random.key(0),\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    for step in steps:\n",
        "        restored = ckpt_manager.restore(\n",
        "            step,\n",
        "            args=ocp.args.StandardRestore(\n",
        "                item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "            ),\n",
        "        )\n",
        "        gen_state = restored[\"generator\"]\n",
        "        disc_state = restored[\"discriminator\"]\n",
        "\n",
        "        # Use a fresh key for evaluation at each checkpoint\n",
        "        key_eval = jax.random.key(0)\n",
        "        key_eval, key_z, key_real = jax.random.split(key_eval, 3)\n",
        "\n",
        "        z_vectors = jax.random.normal(key_z, (batch_size, latent_dim))\n",
        "        real_images_batch = subsample_images_for_batch(key_real, train_data, batch_size)\n",
        "\n",
        "        g_loss = calculate_generator_loss(\n",
        "            gen_state.params,\n",
        "            disc_state.params,\n",
        "            gen_state.apply_fn,\n",
        "            disc_state.apply_fn,\n",
        "            z_vectors,\n",
        "            loss_type=loss_type,\n",
        "        )\n",
        "        d_loss = calculate_discriminator_loss(\n",
        "            disc_state.params,\n",
        "            gen_state.params,\n",
        "            gen_state.apply_fn,\n",
        "            disc_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "\n",
        "        g_losses.append(float(g_loss))\n",
        "        d_losses.append(float(d_loss))\n",
        "\n",
        "    # Convert to NumPy arrays for plotting\n",
        "    g = np.array(g_losses)\n",
        "    d = np.array(d_losses)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    points = np.column_stack([d, g])  # x: discriminator, y: generator\n",
        "\n",
        "    if len(points) > 1:\n",
        "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "        t = np.linspace(0.0, 1.0, len(points) - 1)\n",
        "        colors = cm.coolwarm(t)\n",
        "\n",
        "        lc = LineCollection(segments, colors=colors, linewidths=1.5)\n",
        "        ax.add_collection(lc)\n",
        "\n",
        "    # Scatter the points (including first/last)\n",
        "    ax.scatter(points[:, 0], points[:, 1], c=np.linspace(0.0, 1.0, len(points)), cmap=\"coolwarm\", s=12, alpha=0.8)\n",
        "\n",
        "    if len(points) > 0:\n",
        "        ax.scatter(points[0, 0], points[0, 1], color=\"blue\", s=60, label=\"start\")\n",
        "        ax.scatter(points[-1, 0], points[-1, 1], color=\"red\", s=60, label=\"end\")\n",
        "\n",
        "    ax.set_xlabel(\"Discriminator loss\")\n",
        "    ax.set_ylabel(\"Generator loss\")\n",
        "    ax.set_title(f\"GAN training dynamics in loss space ({loss_type})\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "e52d1762",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6bfb59f",
      "metadata": {},
      "source": [
        "Extended training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b792d56f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming training from global step 5000 (experiment_name='gan_collapse_GradientTransformationExtraArgs_nonsaturating')\n",
            "step 5001: generator_loss=0.6918826699256897, discriminator_loss=0.6978166103363037\n",
            "step 5002: generator_loss=0.6950002312660217, discriminator_loss=0.6969693303108215\n",
            "step 5003: generator_loss=0.686896026134491, discriminator_loss=0.6997767686843872\n",
            "step 5004: generator_loss=0.6931746006011963, discriminator_loss=0.6966055035591125\n",
            "step 5005: generator_loss=0.6839315891265869, discriminator_loss=0.7021609544754028\n",
            "step 5006: generator_loss=0.6849571466445923, discriminator_loss=0.7006086707115173\n",
            "step 5007: generator_loss=0.691627025604248, discriminator_loss=0.6959927082061768\n",
            "step 5008: generator_loss=0.6815807819366455, discriminator_loss=0.6999855041503906\n",
            "step 5009: generator_loss=0.685611367225647, discriminator_loss=0.6969600915908813\n",
            "step 5010: generator_loss=0.6832938194274902, discriminator_loss=0.6970372200012207\n",
            "step 5011: generator_loss=0.679381251335144, discriminator_loss=0.6982468962669373\n",
            "step 5012: generator_loss=0.6818717122077942, discriminator_loss=0.69642573595047\n",
            "step 5013: generator_loss=0.6668511629104614, discriminator_loss=0.7050496339797974\n",
            "step 5014: generator_loss=0.6930282115936279, discriminator_loss=0.6926136612892151\n",
            "step 5015: generator_loss=0.6966074705123901, discriminator_loss=0.6914460062980652\n",
            "step 5016: generator_loss=0.7130393981933594, discriminator_loss=0.6819183826446533\n",
            "step 5017: generator_loss=0.7033851146697998, discriminator_loss=0.6875371336936951\n",
            "step 5018: generator_loss=0.7192754149436951, discriminator_loss=0.6790988445281982\n",
            "step 5019: generator_loss=0.7224588394165039, discriminator_loss=0.6768664121627808\n",
            "step 5020: generator_loss=0.7311389446258545, discriminator_loss=0.6741209626197815\n",
            "step 5021: generator_loss=0.7390012741088867, discriminator_loss=0.6692385673522949\n",
            "step 5022: generator_loss=0.7534909248352051, discriminator_loss=0.6623666286468506\n",
            "step 5023: generator_loss=0.7570658922195435, discriminator_loss=0.6605319976806641\n",
            "step 5024: generator_loss=0.7726030349731445, discriminator_loss=0.6530547142028809\n",
            "step 5025: generator_loss=0.785195529460907, discriminator_loss=0.6470067501068115\n",
            "step 5026: generator_loss=0.7712042331695557, discriminator_loss=0.6518500447273254\n",
            "step 5027: generator_loss=0.7725620865821838, discriminator_loss=0.6468229293823242\n",
            "step 5028: generator_loss=0.7644153833389282, discriminator_loss=0.6460122466087341\n",
            "step 5029: generator_loss=0.7526232600212097, discriminator_loss=0.6472684144973755\n",
            "step 5030: generator_loss=0.7486534714698792, discriminator_loss=0.6437248587608337\n",
            "step 5031: generator_loss=0.7281837463378906, discriminator_loss=0.6495717763900757\n",
            "step 5032: generator_loss=0.7393918037414551, discriminator_loss=0.6432697772979736\n",
            "step 5033: generator_loss=0.7291247844696045, discriminator_loss=0.6475892066955566\n",
            "step 5034: generator_loss=0.7102154493331909, discriminator_loss=0.6560304164886475\n",
            "step 5035: generator_loss=0.7245768308639526, discriminator_loss=0.6498640775680542\n",
            "step 5036: generator_loss=0.7361167669296265, discriminator_loss=0.6475667953491211\n",
            "step 5037: generator_loss=0.7362644672393799, discriminator_loss=0.6480250358581543\n",
            "step 5038: generator_loss=0.7326012849807739, discriminator_loss=0.654260516166687\n",
            "step 5039: generator_loss=0.7368950843811035, discriminator_loss=0.6568583250045776\n",
            "step 5040: generator_loss=0.7224769592285156, discriminator_loss=0.665808916091919\n",
            "step 5041: generator_loss=0.7381277680397034, discriminator_loss=0.6603734493255615\n",
            "step 5042: generator_loss=0.7447012662887573, discriminator_loss=0.6581789255142212\n",
            "step 5043: generator_loss=0.7436632513999939, discriminator_loss=0.6582582592964172\n",
            "step 5044: generator_loss=0.7401254177093506, discriminator_loss=0.6614837646484375\n",
            "step 5045: generator_loss=0.7206796407699585, discriminator_loss=0.6686069965362549\n",
            "step 5046: generator_loss=0.726830005645752, discriminator_loss=0.6634342074394226\n",
            "step 5047: generator_loss=0.7343515157699585, discriminator_loss=0.6586076021194458\n",
            "step 5048: generator_loss=0.7200344800949097, discriminator_loss=0.6642801761627197\n",
            "step 5049: generator_loss=0.7128667831420898, discriminator_loss=0.6665366888046265\n",
            "step 5050: generator_loss=0.7119515538215637, discriminator_loss=0.6663281321525574\n",
            "step 5051: generator_loss=0.6951032280921936, discriminator_loss=0.6735972762107849\n",
            "step 5052: generator_loss=0.7066360116004944, discriminator_loss=0.6678646802902222\n",
            "step 5053: generator_loss=0.705777645111084, discriminator_loss=0.6668692827224731\n",
            "step 5054: generator_loss=0.7047476768493652, discriminator_loss=0.6670730113983154\n",
            "step 5055: generator_loss=0.7112558484077454, discriminator_loss=0.6663406491279602\n",
            "step 5056: generator_loss=0.7056575417518616, discriminator_loss=0.6668704152107239\n",
            "step 5057: generator_loss=0.7011985778808594, discriminator_loss=0.672330379486084\n",
            "step 5058: generator_loss=0.7088053226470947, discriminator_loss=0.6704083681106567\n",
            "step 5059: generator_loss=0.7168760895729065, discriminator_loss=0.6677207946777344\n",
            "step 5060: generator_loss=0.7213845252990723, discriminator_loss=0.667494535446167\n",
            "step 5061: generator_loss=0.7286276817321777, discriminator_loss=0.6643563508987427\n",
            "step 5062: generator_loss=0.7256711721420288, discriminator_loss=0.6671940088272095\n",
            "step 5063: generator_loss=0.7370131611824036, discriminator_loss=0.6607884168624878\n",
            "step 5064: generator_loss=0.7299371957778931, discriminator_loss=0.6615285873413086\n",
            "step 5065: generator_loss=0.7360156774520874, discriminator_loss=0.6561542749404907\n",
            "step 5066: generator_loss=0.738389790058136, discriminator_loss=0.653019905090332\n",
            "step 5067: generator_loss=0.7351632714271545, discriminator_loss=0.65269935131073\n",
            "step 5068: generator_loss=0.7403017282485962, discriminator_loss=0.6494219303131104\n",
            "step 5069: generator_loss=0.7454530000686646, discriminator_loss=0.6469595432281494\n",
            "step 5070: generator_loss=0.7437251210212708, discriminator_loss=0.6472044587135315\n",
            "step 5071: generator_loss=0.7406673431396484, discriminator_loss=0.6483345627784729\n",
            "step 5072: generator_loss=0.7507216930389404, discriminator_loss=0.6427968144416809\n",
            "step 5073: generator_loss=0.7412412166595459, discriminator_loss=0.6470811367034912\n",
            "step 5074: generator_loss=0.7489762306213379, discriminator_loss=0.6442527770996094\n",
            "step 5075: generator_loss=0.7594262361526489, discriminator_loss=0.6397973299026489\n",
            "step 5076: generator_loss=0.738621711730957, discriminator_loss=0.6463543176651001\n",
            "step 5077: generator_loss=0.7588937282562256, discriminator_loss=0.6380233764648438\n",
            "step 5078: generator_loss=0.7532548308372498, discriminator_loss=0.6404930949211121\n",
            "step 5079: generator_loss=0.7471498250961304, discriminator_loss=0.6427149772644043\n",
            "step 5080: generator_loss=0.7600058317184448, discriminator_loss=0.6396303772926331\n",
            "step 5081: generator_loss=0.7519886493682861, discriminator_loss=0.6407306790351868\n",
            "step 5082: generator_loss=0.7575927972793579, discriminator_loss=0.6398277282714844\n",
            "step 5083: generator_loss=0.7463324069976807, discriminator_loss=0.6462147235870361\n",
            "step 5084: generator_loss=0.7432107925415039, discriminator_loss=0.6472001075744629\n",
            "step 5085: generator_loss=0.7485436201095581, discriminator_loss=0.6457343101501465\n",
            "step 5086: generator_loss=0.7466903328895569, discriminator_loss=0.6484161615371704\n",
            "step 5087: generator_loss=0.7469544410705566, discriminator_loss=0.6471052169799805\n",
            "step 5088: generator_loss=0.7380572557449341, discriminator_loss=0.6512818336486816\n",
            "step 5089: generator_loss=0.7402436137199402, discriminator_loss=0.6503129005432129\n",
            "step 5090: generator_loss=0.7283371090888977, discriminator_loss=0.6558300256729126\n",
            "step 5091: generator_loss=0.7396611571311951, discriminator_loss=0.6513985395431519\n",
            "step 5092: generator_loss=0.7375626564025879, discriminator_loss=0.6545342206954956\n",
            "step 5093: generator_loss=0.7426241636276245, discriminator_loss=0.6534543633460999\n",
            "step 5094: generator_loss=0.7524553537368774, discriminator_loss=0.6504597663879395\n",
            "step 5095: generator_loss=0.7521822452545166, discriminator_loss=0.6502468585968018\n",
            "step 5096: generator_loss=0.7366283535957336, discriminator_loss=0.6566842198371887\n",
            "step 5097: generator_loss=0.7452759742736816, discriminator_loss=0.6539552211761475\n",
            "step 5098: generator_loss=0.7298725247383118, discriminator_loss=0.660179078578949\n",
            "step 5099: generator_loss=0.7327584028244019, discriminator_loss=0.6599315404891968\n",
            "step 5100: generator_loss=0.7296634316444397, discriminator_loss=0.6614030599594116\n",
            "step 5101: generator_loss=0.7290967702865601, discriminator_loss=0.6608032584190369\n",
            "step 5102: generator_loss=0.7225766777992249, discriminator_loss=0.6640134453773499\n",
            "step 5103: generator_loss=0.7234485149383545, discriminator_loss=0.6644060015678406\n",
            "step 5104: generator_loss=0.7218637466430664, discriminator_loss=0.6647992730140686\n",
            "step 5105: generator_loss=0.707518458366394, discriminator_loss=0.6724708080291748\n",
            "step 5106: generator_loss=0.7173550128936768, discriminator_loss=0.6682003736495972\n",
            "step 5107: generator_loss=0.7179778814315796, discriminator_loss=0.668978214263916\n",
            "step 5108: generator_loss=0.7065349817276001, discriminator_loss=0.6735149621963501\n",
            "step 5109: generator_loss=0.7063601016998291, discriminator_loss=0.6751677989959717\n",
            "step 5110: generator_loss=0.7055709362030029, discriminator_loss=0.6774305105209351\n",
            "step 5111: generator_loss=0.7074808478355408, discriminator_loss=0.6790194511413574\n",
            "step 5112: generator_loss=0.7056354284286499, discriminator_loss=0.6801636219024658\n",
            "step 5113: generator_loss=0.7053621411323547, discriminator_loss=0.682750940322876\n",
            "step 5114: generator_loss=0.7039204239845276, discriminator_loss=0.6835630536079407\n",
            "step 5115: generator_loss=0.7086923122406006, discriminator_loss=0.6827036142349243\n",
            "step 5116: generator_loss=0.699925422668457, discriminator_loss=0.6875312328338623\n",
            "step 5117: generator_loss=0.6953328251838684, discriminator_loss=0.689642608165741\n",
            "step 5118: generator_loss=0.6939356923103333, discriminator_loss=0.6933261752128601\n",
            "step 5119: generator_loss=0.6987977027893066, discriminator_loss=0.6913841366767883\n",
            "step 5120: generator_loss=0.69720458984375, discriminator_loss=0.6926319599151611\n",
            "step 5121: generator_loss=0.6979039311408997, discriminator_loss=0.6920862793922424\n",
            "step 5122: generator_loss=0.6892405152320862, discriminator_loss=0.6959723234176636\n",
            "step 5123: generator_loss=0.6843224167823792, discriminator_loss=0.6994483470916748\n",
            "step 5124: generator_loss=0.6871963143348694, discriminator_loss=0.6992769241333008\n",
            "step 5125: generator_loss=0.6834550499916077, discriminator_loss=0.700942873954773\n",
            "step 5126: generator_loss=0.6875768899917603, discriminator_loss=0.7001544237136841\n",
            "step 5127: generator_loss=0.6821029186248779, discriminator_loss=0.7035809755325317\n",
            "step 5128: generator_loss=0.6837761402130127, discriminator_loss=0.7027517557144165\n",
            "step 5129: generator_loss=0.6818980574607849, discriminator_loss=0.7044169306755066\n",
            "step 5130: generator_loss=0.6819902062416077, discriminator_loss=0.7046896815299988\n",
            "step 5131: generator_loss=0.6840847730636597, discriminator_loss=0.7041983008384705\n",
            "step 5132: generator_loss=0.6836560368537903, discriminator_loss=0.7054861783981323\n",
            "step 5133: generator_loss=0.6851963996887207, discriminator_loss=0.705310583114624\n",
            "step 5134: generator_loss=0.6848057508468628, discriminator_loss=0.7049741744995117\n",
            "step 5135: generator_loss=0.683673083782196, discriminator_loss=0.7047570943832397\n",
            "step 5136: generator_loss=0.6807734966278076, discriminator_loss=0.7072742581367493\n",
            "step 5137: generator_loss=0.683184027671814, discriminator_loss=0.706200122833252\n",
            "step 5138: generator_loss=0.6851542592048645, discriminator_loss=0.7057750225067139\n",
            "step 5139: generator_loss=0.6879997253417969, discriminator_loss=0.7040867209434509\n",
            "step 5140: generator_loss=0.6866239309310913, discriminator_loss=0.703909158706665\n",
            "step 5141: generator_loss=0.687220573425293, discriminator_loss=0.7044337391853333\n",
            "step 5142: generator_loss=0.6865450143814087, discriminator_loss=0.7053054571151733\n",
            "step 5143: generator_loss=0.6816457509994507, discriminator_loss=0.7086136341094971\n",
            "step 5144: generator_loss=0.6842776536941528, discriminator_loss=0.7075953483581543\n",
            "step 5145: generator_loss=0.6842069029808044, discriminator_loss=0.7071443796157837\n",
            "step 5146: generator_loss=0.6765910983085632, discriminator_loss=0.7112435102462769\n",
            "step 5147: generator_loss=0.6794551610946655, discriminator_loss=0.709595799446106\n",
            "step 5148: generator_loss=0.6729416847229004, discriminator_loss=0.7132391929626465\n",
            "step 5149: generator_loss=0.6732503175735474, discriminator_loss=0.7123473882675171\n",
            "step 5150: generator_loss=0.671178936958313, discriminator_loss=0.7128355503082275\n",
            "step 5151: generator_loss=0.6691166162490845, discriminator_loss=0.7131950259208679\n",
            "step 5152: generator_loss=0.6680880188941956, discriminator_loss=0.7139366865158081\n",
            "step 5153: generator_loss=0.6664140224456787, discriminator_loss=0.7152608633041382\n",
            "step 5154: generator_loss=0.6682107448577881, discriminator_loss=0.7148352861404419\n",
            "step 5155: generator_loss=0.6695702075958252, discriminator_loss=0.7146823406219482\n",
            "step 5156: generator_loss=0.6707663536071777, discriminator_loss=0.7151975631713867\n",
            "step 5157: generator_loss=0.6680226922035217, discriminator_loss=0.7167962789535522\n",
            "step 5158: generator_loss=0.671208918094635, discriminator_loss=0.7151793241500854\n",
            "step 5159: generator_loss=0.6710723042488098, discriminator_loss=0.7160341739654541\n",
            "step 5160: generator_loss=0.671142578125, discriminator_loss=0.7166520357131958\n",
            "step 5161: generator_loss=0.6675717234611511, discriminator_loss=0.7191464900970459\n",
            "step 5162: generator_loss=0.6739459037780762, discriminator_loss=0.7174469828605652\n",
            "step 5163: generator_loss=0.6701803207397461, discriminator_loss=0.7195550799369812\n",
            "step 5164: generator_loss=0.6674556732177734, discriminator_loss=0.7206926345825195\n",
            "step 5165: generator_loss=0.6690382361412048, discriminator_loss=0.7205438613891602\n",
            "step 5166: generator_loss=0.665084183216095, discriminator_loss=0.7235034108161926\n",
            "step 5167: generator_loss=0.667601466178894, discriminator_loss=0.7211209535598755\n",
            "step 5168: generator_loss=0.6641844511032104, discriminator_loss=0.7241588830947876\n",
            "step 5169: generator_loss=0.6639093160629272, discriminator_loss=0.7229207754135132\n",
            "step 5170: generator_loss=0.6590456962585449, discriminator_loss=0.7278324961662292\n",
            "step 5171: generator_loss=0.6561908721923828, discriminator_loss=0.7268669009208679\n",
            "step 5172: generator_loss=0.6568322777748108, discriminator_loss=0.7269448041915894\n",
            "step 5173: generator_loss=0.6605467796325684, discriminator_loss=0.7251721620559692\n",
            "step 5174: generator_loss=0.6544639468193054, discriminator_loss=0.7276760935783386\n",
            "step 5175: generator_loss=0.6544692516326904, discriminator_loss=0.7283270359039307\n",
            "step 5176: generator_loss=0.6553522944450378, discriminator_loss=0.7290292978286743\n",
            "step 5177: generator_loss=0.6548277735710144, discriminator_loss=0.7295824885368347\n",
            "step 5178: generator_loss=0.6574349403381348, discriminator_loss=0.727110743522644\n",
            "step 5179: generator_loss=0.6575794816017151, discriminator_loss=0.7283898591995239\n",
            "step 5180: generator_loss=0.6579355001449585, discriminator_loss=0.7275081872940063\n",
            "step 5181: generator_loss=0.6580853462219238, discriminator_loss=0.7272181510925293\n",
            "step 5182: generator_loss=0.6619566679000854, discriminator_loss=0.7263321876525879\n",
            "step 5183: generator_loss=0.6640268564224243, discriminator_loss=0.724122941493988\n",
            "step 5184: generator_loss=0.6638641357421875, discriminator_loss=0.7246689200401306\n",
            "step 5185: generator_loss=0.6647592782974243, discriminator_loss=0.7228419184684753\n",
            "step 5186: generator_loss=0.6693012714385986, discriminator_loss=0.7201316952705383\n",
            "step 5187: generator_loss=0.6718314290046692, discriminator_loss=0.7177698612213135\n",
            "step 5188: generator_loss=0.674584686756134, discriminator_loss=0.7156568765640259\n",
            "step 5189: generator_loss=0.6770232915878296, discriminator_loss=0.7131816148757935\n",
            "step 5190: generator_loss=0.679338812828064, discriminator_loss=0.710608184337616\n",
            "step 5191: generator_loss=0.6841144561767578, discriminator_loss=0.7057549953460693\n",
            "step 5192: generator_loss=0.6848094463348389, discriminator_loss=0.7057521343231201\n",
            "step 5193: generator_loss=0.6905742287635803, discriminator_loss=0.7006060481071472\n",
            "step 5194: generator_loss=0.6933795213699341, discriminator_loss=0.6973356604576111\n",
            "step 5195: generator_loss=0.6963626146316528, discriminator_loss=0.6956101655960083\n",
            "step 5196: generator_loss=0.7032688856124878, discriminator_loss=0.6909031867980957\n",
            "step 5197: generator_loss=0.7085521221160889, discriminator_loss=0.6886804103851318\n",
            "step 5198: generator_loss=0.7184662222862244, discriminator_loss=0.6833359599113464\n",
            "step 5199: generator_loss=0.7250732183456421, discriminator_loss=0.6809651255607605\n",
            "step 5200: generator_loss=0.7317683696746826, discriminator_loss=0.6782558560371399\n",
            "step 5201: generator_loss=0.7312970161437988, discriminator_loss=0.6814170479774475\n",
            "step 5202: generator_loss=0.7313923835754395, discriminator_loss=0.6809695959091187\n",
            "step 5203: generator_loss=0.7288018465042114, discriminator_loss=0.681881308555603\n",
            "step 5204: generator_loss=0.7229229807853699, discriminator_loss=0.6828287839889526\n",
            "step 5205: generator_loss=0.7163764238357544, discriminator_loss=0.6822323799133301\n",
            "step 5206: generator_loss=0.708499014377594, discriminator_loss=0.6828328967094421\n",
            "step 5207: generator_loss=0.7011516094207764, discriminator_loss=0.6849514245986938\n",
            "step 5208: generator_loss=0.6950639486312866, discriminator_loss=0.6833593845367432\n",
            "step 5209: generator_loss=0.6876218318939209, discriminator_loss=0.6860223412513733\n",
            "step 5210: generator_loss=0.6843166351318359, discriminator_loss=0.6872317790985107\n",
            "step 5211: generator_loss=0.6846060752868652, discriminator_loss=0.6874748468399048\n",
            "step 5212: generator_loss=0.6829692125320435, discriminator_loss=0.6893250942230225\n",
            "step 5213: generator_loss=0.6858291625976562, discriminator_loss=0.6901625990867615\n",
            "step 5214: generator_loss=0.6848447322845459, discriminator_loss=0.6944190263748169\n",
            "step 5215: generator_loss=0.6899338960647583, discriminator_loss=0.6948508024215698\n",
            "step 5216: generator_loss=0.6932328939437866, discriminator_loss=0.6968138217926025\n",
            "step 5217: generator_loss=0.6925151944160461, discriminator_loss=0.6984232664108276\n",
            "step 5218: generator_loss=0.6900985836982727, discriminator_loss=0.701596200466156\n",
            "step 5219: generator_loss=0.6892285346984863, discriminator_loss=0.7027257084846497\n",
            "step 5220: generator_loss=0.6873548626899719, discriminator_loss=0.7072662711143494\n",
            "step 5221: generator_loss=0.6854546070098877, discriminator_loss=0.7064052224159241\n",
            "step 5222: generator_loss=0.683335542678833, discriminator_loss=0.7080922722816467\n",
            "step 5223: generator_loss=0.6818388104438782, discriminator_loss=0.7085360884666443\n",
            "step 5224: generator_loss=0.6812417507171631, discriminator_loss=0.7084395289421082\n",
            "step 5225: generator_loss=0.6750001311302185, discriminator_loss=0.7107515335083008\n",
            "step 5226: generator_loss=0.6783817410469055, discriminator_loss=0.7086372375488281\n",
            "step 5227: generator_loss=0.677858829498291, discriminator_loss=0.7078675627708435\n",
            "step 5228: generator_loss=0.682144820690155, discriminator_loss=0.7069324254989624\n",
            "step 5229: generator_loss=0.6838908791542053, discriminator_loss=0.7044042348861694\n",
            "step 5230: generator_loss=0.6837998628616333, discriminator_loss=0.7043936252593994\n",
            "step 5231: generator_loss=0.685549259185791, discriminator_loss=0.7053284645080566\n",
            "step 5232: generator_loss=0.6908903121948242, discriminator_loss=0.7026241421699524\n",
            "step 5233: generator_loss=0.6935771703720093, discriminator_loss=0.7018800973892212\n",
            "step 5234: generator_loss=0.6949582695960999, discriminator_loss=0.7025384306907654\n",
            "step 5235: generator_loss=0.6938990354537964, discriminator_loss=0.7028504610061646\n",
            "step 5236: generator_loss=0.6968256235122681, discriminator_loss=0.7018458843231201\n",
            "step 5237: generator_loss=0.6994204521179199, discriminator_loss=0.6991589069366455\n",
            "step 5238: generator_loss=0.6966910362243652, discriminator_loss=0.698874831199646\n",
            "step 5239: generator_loss=0.6941496133804321, discriminator_loss=0.6986154317855835\n",
            "step 5240: generator_loss=0.6938967704772949, discriminator_loss=0.6974704265594482\n",
            "step 5241: generator_loss=0.692926287651062, discriminator_loss=0.6970618367195129\n",
            "step 5242: generator_loss=0.693734884262085, discriminator_loss=0.6945764422416687\n",
            "step 5243: generator_loss=0.6959127187728882, discriminator_loss=0.6918989419937134\n",
            "step 5244: generator_loss=0.6995987892150879, discriminator_loss=0.6878545880317688\n",
            "step 5245: generator_loss=0.7049176692962646, discriminator_loss=0.6832437515258789\n",
            "step 5246: generator_loss=0.708268404006958, discriminator_loss=0.6801494359970093\n",
            "step 5247: generator_loss=0.7170601487159729, discriminator_loss=0.6759076118469238\n",
            "step 5248: generator_loss=0.7288947701454163, discriminator_loss=0.6703540086746216\n",
            "step 5249: generator_loss=0.7361598014831543, discriminator_loss=0.6677024960517883\n",
            "step 5250: generator_loss=0.7387912273406982, discriminator_loss=0.6683317422866821\n",
            "step 5251: generator_loss=0.7400654554367065, discriminator_loss=0.6689891815185547\n",
            "step 5252: generator_loss=0.745303750038147, discriminator_loss=0.667217493057251\n",
            "step 5253: generator_loss=0.7421213984489441, discriminator_loss=0.6668097376823425\n",
            "step 5254: generator_loss=0.7372862696647644, discriminator_loss=0.6672248244285583\n",
            "step 5255: generator_loss=0.726760745048523, discriminator_loss=0.6691288948059082\n",
            "step 5256: generator_loss=0.7247762680053711, discriminator_loss=0.666121244430542\n",
            "step 5257: generator_loss=0.7165291905403137, discriminator_loss=0.6681482791900635\n",
            "step 5258: generator_loss=0.712973952293396, discriminator_loss=0.6674941182136536\n",
            "step 5259: generator_loss=0.7077161073684692, discriminator_loss=0.6683306694030762\n",
            "step 5260: generator_loss=0.7048513889312744, discriminator_loss=0.6679332256317139\n",
            "step 5261: generator_loss=0.7029414176940918, discriminator_loss=0.6691678762435913\n",
            "step 5262: generator_loss=0.7041987776756287, discriminator_loss=0.6689682602882385\n",
            "step 5263: generator_loss=0.70840984582901, discriminator_loss=0.6680869460105896\n",
            "step 5264: generator_loss=0.7077600359916687, discriminator_loss=0.6703848838806152\n",
            "step 5265: generator_loss=0.7161539793014526, discriminator_loss=0.6678358912467957\n",
            "step 5266: generator_loss=0.7205361127853394, discriminator_loss=0.6682665944099426\n",
            "step 5267: generator_loss=0.7213258743286133, discriminator_loss=0.6695748567581177\n",
            "step 5268: generator_loss=0.7251604795455933, discriminator_loss=0.6685361862182617\n",
            "step 5269: generator_loss=0.7262450456619263, discriminator_loss=0.6684346199035645\n",
            "step 5270: generator_loss=0.733298659324646, discriminator_loss=0.6648851633071899\n",
            "step 5271: generator_loss=0.7241588234901428, discriminator_loss=0.6679530143737793\n",
            "step 5272: generator_loss=0.7337806224822998, discriminator_loss=0.6639962792396545\n",
            "step 5273: generator_loss=0.7294212579727173, discriminator_loss=0.663558840751648\n",
            "step 5274: generator_loss=0.728218138217926, discriminator_loss=0.6640987396240234\n",
            "step 5275: generator_loss=0.7239569425582886, discriminator_loss=0.665870189666748\n",
            "step 5276: generator_loss=0.7199108004570007, discriminator_loss=0.6673193573951721\n",
            "step 5277: generator_loss=0.7168580293655396, discriminator_loss=0.6680026650428772\n",
            "step 5278: generator_loss=0.714729368686676, discriminator_loss=0.6671632528305054\n",
            "step 5279: generator_loss=0.7096830010414124, discriminator_loss=0.6674767136573792\n",
            "step 5280: generator_loss=0.7089293599128723, discriminator_loss=0.667692244052887\n",
            "step 5281: generator_loss=0.709191620349884, discriminator_loss=0.6665465831756592\n",
            "step 5282: generator_loss=0.7072832584381104, discriminator_loss=0.668357253074646\n",
            "step 5283: generator_loss=0.7059277296066284, discriminator_loss=0.6695739030838013\n",
            "step 5284: generator_loss=0.7058594226837158, discriminator_loss=0.6705280542373657\n",
            "step 5285: generator_loss=0.7127671241760254, discriminator_loss=0.6685815453529358\n",
            "step 5286: generator_loss=0.7120088934898376, discriminator_loss=0.6700282096862793\n",
            "step 5287: generator_loss=0.7084372043609619, discriminator_loss=0.6731186509132385\n",
            "step 5288: generator_loss=0.7140111923217773, discriminator_loss=0.6721537113189697\n",
            "step 5289: generator_loss=0.7165600061416626, discriminator_loss=0.6723531484603882\n",
            "step 5290: generator_loss=0.7177812457084656, discriminator_loss=0.67049241065979\n",
            "step 5291: generator_loss=0.7196213603019714, discriminator_loss=0.6701427698135376\n",
            "step 5292: generator_loss=0.716392993927002, discriminator_loss=0.6721657514572144\n",
            "step 5293: generator_loss=0.7147454023361206, discriminator_loss=0.6713771224021912\n",
            "step 5294: generator_loss=0.7156962156295776, discriminator_loss=0.670139491558075\n",
            "step 5295: generator_loss=0.7130992412567139, discriminator_loss=0.6735590696334839\n",
            "step 5296: generator_loss=0.7117289304733276, discriminator_loss=0.6711646318435669\n",
            "step 5297: generator_loss=0.7104637622833252, discriminator_loss=0.6728127002716064\n",
            "step 5298: generator_loss=0.7097501158714294, discriminator_loss=0.6742420196533203\n",
            "step 5299: generator_loss=0.7065352201461792, discriminator_loss=0.6760361790657043\n",
            "step 5300: generator_loss=0.7035655379295349, discriminator_loss=0.6776793003082275\n",
            "step 5301: generator_loss=0.7033747434616089, discriminator_loss=0.6783539652824402\n",
            "step 5302: generator_loss=0.7011726498603821, discriminator_loss=0.6818684935569763\n",
            "step 5303: generator_loss=0.6995476484298706, discriminator_loss=0.6826691627502441\n",
            "step 5304: generator_loss=0.701208233833313, discriminator_loss=0.6828581094741821\n",
            "step 5305: generator_loss=0.6975242495536804, discriminator_loss=0.6855261325836182\n",
            "step 5306: generator_loss=0.6966288685798645, discriminator_loss=0.6865254640579224\n",
            "step 5307: generator_loss=0.6985767483711243, discriminator_loss=0.6861886978149414\n",
            "step 5308: generator_loss=0.6961621046066284, discriminator_loss=0.6880397796630859\n",
            "step 5309: generator_loss=0.6946630477905273, discriminator_loss=0.6890062689781189\n",
            "step 5310: generator_loss=0.6895800828933716, discriminator_loss=0.6912163496017456\n",
            "step 5311: generator_loss=0.6928806304931641, discriminator_loss=0.6886581778526306\n",
            "step 5312: generator_loss=0.688819408416748, discriminator_loss=0.6936084032058716\n",
            "step 5313: generator_loss=0.6904662847518921, discriminator_loss=0.6901345252990723\n",
            "step 5314: generator_loss=0.6908395886421204, discriminator_loss=0.688769519329071\n",
            "step 5315: generator_loss=0.6939979791641235, discriminator_loss=0.6877017021179199\n",
            "step 5316: generator_loss=0.694983720779419, discriminator_loss=0.6850428581237793\n",
            "step 5317: generator_loss=0.6955926418304443, discriminator_loss=0.6847370266914368\n",
            "step 5318: generator_loss=0.6972384452819824, discriminator_loss=0.6835479736328125\n",
            "step 5319: generator_loss=0.7006681561470032, discriminator_loss=0.6836203932762146\n",
            "step 5320: generator_loss=0.7052425146102905, discriminator_loss=0.682152271270752\n",
            "step 5321: generator_loss=0.7094101905822754, discriminator_loss=0.6798325181007385\n",
            "step 5322: generator_loss=0.7134659886360168, discriminator_loss=0.6771422624588013\n",
            "step 5323: generator_loss=0.717105507850647, discriminator_loss=0.6761347651481628\n",
            "step 5324: generator_loss=0.7230706214904785, discriminator_loss=0.6708922386169434\n",
            "step 5325: generator_loss=0.7293832898139954, discriminator_loss=0.6682462692260742\n",
            "step 5326: generator_loss=0.7343586683273315, discriminator_loss=0.663373589515686\n",
            "step 5327: generator_loss=0.739875078201294, discriminator_loss=0.6594567894935608\n",
            "step 5328: generator_loss=0.744297981262207, discriminator_loss=0.6558260917663574\n",
            "step 5329: generator_loss=0.7498176097869873, discriminator_loss=0.6522635221481323\n",
            "step 5330: generator_loss=0.7511245608329773, discriminator_loss=0.6502339243888855\n",
            "step 5331: generator_loss=0.7527245879173279, discriminator_loss=0.6499114036560059\n",
            "step 5332: generator_loss=0.7518532276153564, discriminator_loss=0.6494273543357849\n",
            "step 5333: generator_loss=0.7465908527374268, discriminator_loss=0.6505852341651917\n",
            "step 5334: generator_loss=0.7422433495521545, discriminator_loss=0.650322437286377\n",
            "step 5335: generator_loss=0.7362895011901855, discriminator_loss=0.6507954001426697\n",
            "step 5336: generator_loss=0.730485200881958, discriminator_loss=0.6505388617515564\n",
            "step 5337: generator_loss=0.7286096811294556, discriminator_loss=0.6484590768814087\n",
            "step 5338: generator_loss=0.7255831956863403, discriminator_loss=0.6484189629554749\n",
            "step 5339: generator_loss=0.7250341773033142, discriminator_loss=0.6468707323074341\n",
            "step 5340: generator_loss=0.7238317131996155, discriminator_loss=0.648949921131134\n",
            "step 5341: generator_loss=0.7284220457077026, discriminator_loss=0.6475374698638916\n",
            "step 5342: generator_loss=0.7299946546554565, discriminator_loss=0.6502172946929932\n",
            "step 5343: generator_loss=0.7327263355255127, discriminator_loss=0.6501480937004089\n",
            "step 5344: generator_loss=0.7389215230941772, discriminator_loss=0.6514832973480225\n",
            "step 5345: generator_loss=0.7403936386108398, discriminator_loss=0.6503242254257202\n",
            "step 5346: generator_loss=0.7374354600906372, discriminator_loss=0.654682993888855\n",
            "step 5347: generator_loss=0.7424701452255249, discriminator_loss=0.6521705985069275\n",
            "step 5348: generator_loss=0.7362654209136963, discriminator_loss=0.6562695503234863\n",
            "step 5349: generator_loss=0.7350557446479797, discriminator_loss=0.6568214893341064\n",
            "step 5350: generator_loss=0.7325055003166199, discriminator_loss=0.6566462516784668\n",
            "step 5351: generator_loss=0.7296806573867798, discriminator_loss=0.657322108745575\n",
            "step 5352: generator_loss=0.72415691614151, discriminator_loss=0.6597514748573303\n",
            "step 5353: generator_loss=0.7192692756652832, discriminator_loss=0.6609916687011719\n",
            "step 5354: generator_loss=0.7164617776870728, discriminator_loss=0.6634982824325562\n",
            "step 5355: generator_loss=0.7183457612991333, discriminator_loss=0.6619688868522644\n",
            "step 5356: generator_loss=0.7129999399185181, discriminator_loss=0.6648035049438477\n",
            "step 5357: generator_loss=0.7110284566879272, discriminator_loss=0.6668040156364441\n",
            "step 5358: generator_loss=0.7117387056350708, discriminator_loss=0.6666290163993835\n",
            "step 5359: generator_loss=0.7126044034957886, discriminator_loss=0.6674832105636597\n",
            "step 5360: generator_loss=0.7081295251846313, discriminator_loss=0.6720027923583984\n",
            "step 5361: generator_loss=0.7079096436500549, discriminator_loss=0.6727707386016846\n",
            "step 5362: generator_loss=0.7050734758377075, discriminator_loss=0.6759381294250488\n",
            "step 5363: generator_loss=0.7113010883331299, discriminator_loss=0.6741335391998291\n",
            "step 5364: generator_loss=0.7093428373336792, discriminator_loss=0.6757640838623047\n",
            "step 5365: generator_loss=0.711811363697052, discriminator_loss=0.6755355596542358\n",
            "step 5366: generator_loss=0.7103030681610107, discriminator_loss=0.6775747537612915\n",
            "step 5367: generator_loss=0.710997462272644, discriminator_loss=0.676125705242157\n",
            "step 5368: generator_loss=0.7185612320899963, discriminator_loss=0.6718452572822571\n",
            "step 5369: generator_loss=0.7170507907867432, discriminator_loss=0.6727039813995361\n",
            "step 5370: generator_loss=0.716483473777771, discriminator_loss=0.6726861596107483\n",
            "step 5371: generator_loss=0.7167496681213379, discriminator_loss=0.6749134063720703\n",
            "step 5372: generator_loss=0.721291720867157, discriminator_loss=0.6743012070655823\n",
            "step 5373: generator_loss=0.717171311378479, discriminator_loss=0.6783741116523743\n",
            "step 5374: generator_loss=0.7165170907974243, discriminator_loss=0.6798460483551025\n",
            "step 5375: generator_loss=0.711794912815094, discriminator_loss=0.6799455881118774\n",
            "step 5376: generator_loss=0.7039334177970886, discriminator_loss=0.6844757795333862\n",
            "step 5377: generator_loss=0.7027090787887573, discriminator_loss=0.6832470893859863\n",
            "step 5378: generator_loss=0.7001599669456482, discriminator_loss=0.6836065053939819\n",
            "step 5379: generator_loss=0.6959530115127563, discriminator_loss=0.6850310564041138\n",
            "step 5380: generator_loss=0.6913526058197021, discriminator_loss=0.687978208065033\n",
            "step 5381: generator_loss=0.6886910200119019, discriminator_loss=0.690392792224884\n",
            "step 5382: generator_loss=0.6906321048736572, discriminator_loss=0.6913716793060303\n",
            "step 5383: generator_loss=0.6880632638931274, discriminator_loss=0.6948261260986328\n",
            "step 5384: generator_loss=0.6863544583320618, discriminator_loss=0.6954853534698486\n",
            "step 5385: generator_loss=0.6909235715866089, discriminator_loss=0.6942111849784851\n",
            "step 5386: generator_loss=0.6906616687774658, discriminator_loss=0.6943965554237366\n",
            "step 5387: generator_loss=0.6802659034729004, discriminator_loss=0.6999865770339966\n",
            "step 5388: generator_loss=0.6874969601631165, discriminator_loss=0.6947094798088074\n",
            "step 5389: generator_loss=0.6847068667411804, discriminator_loss=0.6955466270446777\n",
            "step 5390: generator_loss=0.6879445314407349, discriminator_loss=0.6933019161224365\n",
            "step 5391: generator_loss=0.7026892900466919, discriminator_loss=0.6849831938743591\n",
            "step 5392: generator_loss=0.6920353174209595, discriminator_loss=0.6885489225387573\n",
            "step 5393: generator_loss=0.7036470174789429, discriminator_loss=0.6834093332290649\n",
            "step 5394: generator_loss=0.7122631072998047, discriminator_loss=0.6832928657531738\n",
            "step 5395: generator_loss=0.7207012176513672, discriminator_loss=0.680611252784729\n",
            "step 5396: generator_loss=0.7229814529418945, discriminator_loss=0.6803873181343079\n",
            "step 5397: generator_loss=0.7228013277053833, discriminator_loss=0.6807734370231628\n",
            "step 5398: generator_loss=0.7141772508621216, discriminator_loss=0.6851240396499634\n",
            "step 5399: generator_loss=0.7037349343299866, discriminator_loss=0.6892194747924805\n",
            "step 5400: generator_loss=0.6981581449508667, discriminator_loss=0.6910601258277893\n",
            "step 5401: generator_loss=0.6941824555397034, discriminator_loss=0.6906864047050476\n",
            "step 5402: generator_loss=0.6949606537818909, discriminator_loss=0.6880061626434326\n",
            "step 5403: generator_loss=0.6847573518753052, discriminator_loss=0.69144606590271\n",
            "step 5404: generator_loss=0.6701099872589111, discriminator_loss=0.7005038261413574\n",
            "step 5405: generator_loss=0.6821461915969849, discriminator_loss=0.6937034726142883\n",
            "step 5406: generator_loss=0.6798471212387085, discriminator_loss=0.6949928998947144\n",
            "step 5407: generator_loss=0.6759288907051086, discriminator_loss=0.7004733085632324\n",
            "step 5408: generator_loss=0.6753116846084595, discriminator_loss=0.701346218585968\n",
            "step 5409: generator_loss=0.6815543174743652, discriminator_loss=0.7005631327629089\n",
            "step 5410: generator_loss=0.6778041124343872, discriminator_loss=0.7043395638465881\n",
            "step 5411: generator_loss=0.6802803874015808, discriminator_loss=0.7040212154388428\n",
            "step 5412: generator_loss=0.693412184715271, discriminator_loss=0.6989468336105347\n",
            "step 5413: generator_loss=0.6776361465454102, discriminator_loss=0.7077598571777344\n",
            "step 5414: generator_loss=0.6871365308761597, discriminator_loss=0.7054555416107178\n",
            "step 5415: generator_loss=0.6970228552818298, discriminator_loss=0.7007097601890564\n",
            "step 5416: generator_loss=0.701840877532959, discriminator_loss=0.699756383895874\n",
            "step 5417: generator_loss=0.6928212642669678, discriminator_loss=0.7065882086753845\n",
            "step 5418: generator_loss=0.6879235506057739, discriminator_loss=0.709869384765625\n",
            "step 5419: generator_loss=0.6969932317733765, discriminator_loss=0.7044869065284729\n",
            "step 5420: generator_loss=0.6864172220230103, discriminator_loss=0.7099775671958923\n",
            "step 5421: generator_loss=0.6754552125930786, discriminator_loss=0.7136862277984619\n",
            "step 5422: generator_loss=0.6654661893844604, discriminator_loss=0.7188463807106018\n",
            "step 5423: generator_loss=0.6595701575279236, discriminator_loss=0.7215824723243713\n",
            "step 5424: generator_loss=0.6685434579849243, discriminator_loss=0.7136441469192505\n",
            "step 5425: generator_loss=0.6551446914672852, discriminator_loss=0.7188754081726074\n",
            "step 5426: generator_loss=0.6654456853866577, discriminator_loss=0.7122759819030762\n",
            "step 5427: generator_loss=0.6626290082931519, discriminator_loss=0.7150373458862305\n",
            "step 5428: generator_loss=0.6716154217720032, discriminator_loss=0.7115786075592041\n",
            "step 5429: generator_loss=0.6648960113525391, discriminator_loss=0.717437207698822\n",
            "step 5430: generator_loss=0.6750068664550781, discriminator_loss=0.7141407132148743\n",
            "step 5431: generator_loss=0.6770391464233398, discriminator_loss=0.7159742116928101\n",
            "step 5432: generator_loss=0.6805934906005859, discriminator_loss=0.7134015560150146\n",
            "step 5433: generator_loss=0.673263430595398, discriminator_loss=0.7172929048538208\n",
            "step 5434: generator_loss=0.686774492263794, discriminator_loss=0.708598256111145\n",
            "step 5435: generator_loss=0.6875292062759399, discriminator_loss=0.7042052149772644\n",
            "step 5436: generator_loss=0.6839874982833862, discriminator_loss=0.7034568190574646\n",
            "step 5437: generator_loss=0.6882058382034302, discriminator_loss=0.6986951231956482\n",
            "step 5438: generator_loss=0.6887530088424683, discriminator_loss=0.6965908408164978\n",
            "step 5439: generator_loss=0.6995373964309692, discriminator_loss=0.6898186802864075\n",
            "step 5440: generator_loss=0.7022256851196289, discriminator_loss=0.6880848407745361\n",
            "step 5441: generator_loss=0.7081327438354492, discriminator_loss=0.6832777261734009\n",
            "step 5442: generator_loss=0.7150568962097168, discriminator_loss=0.6782239675521851\n",
            "step 5443: generator_loss=0.7189728617668152, discriminator_loss=0.6741540431976318\n",
            "step 5444: generator_loss=0.7221802473068237, discriminator_loss=0.6693732142448425\n",
            "step 5445: generator_loss=0.7327138185501099, discriminator_loss=0.6620392799377441\n",
            "step 5446: generator_loss=0.7324175834655762, discriminator_loss=0.6594569683074951\n",
            "step 5447: generator_loss=0.739033579826355, discriminator_loss=0.6542648077011108\n",
            "step 5448: generator_loss=0.7477402687072754, discriminator_loss=0.6492220759391785\n",
            "step 5449: generator_loss=0.7546221017837524, discriminator_loss=0.6458757519721985\n",
            "step 5450: generator_loss=0.7593870759010315, discriminator_loss=0.6424443125724792\n",
            "step 5451: generator_loss=0.7627063989639282, discriminator_loss=0.6409173607826233\n",
            "step 5452: generator_loss=0.7615958452224731, discriminator_loss=0.6421006917953491\n",
            "step 5453: generator_loss=0.7707101702690125, discriminator_loss=0.6387057304382324\n",
            "step 5454: generator_loss=0.7691630125045776, discriminator_loss=0.6405371427536011\n",
            "step 5455: generator_loss=0.771033525466919, discriminator_loss=0.6382797956466675\n",
            "step 5456: generator_loss=0.7700836062431335, discriminator_loss=0.636751651763916\n",
            "step 5457: generator_loss=0.7605568170547485, discriminator_loss=0.6389809846878052\n",
            "step 5458: generator_loss=0.7575777769088745, discriminator_loss=0.6375848054885864\n",
            "step 5459: generator_loss=0.7503605484962463, discriminator_loss=0.6385515332221985\n",
            "step 5460: generator_loss=0.7407005429267883, discriminator_loss=0.6410003900527954\n",
            "step 5461: generator_loss=0.7455776929855347, discriminator_loss=0.6374614834785461\n",
            "step 5462: generator_loss=0.7360777854919434, discriminator_loss=0.6399118900299072\n",
            "step 5463: generator_loss=0.7397067546844482, discriminator_loss=0.6374940276145935\n",
            "step 5464: generator_loss=0.7427287101745605, discriminator_loss=0.6361409425735474\n",
            "step 5465: generator_loss=0.7503502368927002, discriminator_loss=0.6330502033233643\n",
            "step 5466: generator_loss=0.7567033767700195, discriminator_loss=0.6306149363517761\n",
            "step 5467: generator_loss=0.7580159902572632, discriminator_loss=0.6319677829742432\n",
            "step 5468: generator_loss=0.768496572971344, discriminator_loss=0.6297311782836914\n",
            "step 5469: generator_loss=0.7764827013015747, discriminator_loss=0.6291759610176086\n",
            "step 5470: generator_loss=0.7754115462303162, discriminator_loss=0.631932258605957\n",
            "step 5471: generator_loss=0.776409924030304, discriminator_loss=0.6335833668708801\n",
            "step 5472: generator_loss=0.779039740562439, discriminator_loss=0.6322795748710632\n",
            "step 5473: generator_loss=0.7723060846328735, discriminator_loss=0.6346962451934814\n",
            "step 5474: generator_loss=0.7703983187675476, discriminator_loss=0.6340644955635071\n",
            "step 5475: generator_loss=0.7608475089073181, discriminator_loss=0.6367894411087036\n",
            "step 5476: generator_loss=0.7554129362106323, discriminator_loss=0.6370478868484497\n",
            "step 5477: generator_loss=0.7503478527069092, discriminator_loss=0.6371092796325684\n",
            "step 5478: generator_loss=0.7438421249389648, discriminator_loss=0.6382688283920288\n",
            "step 5479: generator_loss=0.7388569712638855, discriminator_loss=0.6398250460624695\n",
            "step 5480: generator_loss=0.738480806350708, discriminator_loss=0.638749897480011\n",
            "step 5481: generator_loss=0.7338345646858215, discriminator_loss=0.6405519247055054\n",
            "step 5482: generator_loss=0.7288464307785034, discriminator_loss=0.6435337662696838\n",
            "step 5483: generator_loss=0.7312455177307129, discriminator_loss=0.6442255973815918\n",
            "step 5484: generator_loss=0.7349154949188232, discriminator_loss=0.6435880661010742\n",
            "step 5485: generator_loss=0.7363970279693604, discriminator_loss=0.6444529294967651\n",
            "step 5486: generator_loss=0.7373061776161194, discriminator_loss=0.6459586024284363\n",
            "step 5487: generator_loss=0.7450164556503296, discriminator_loss=0.6436577439308167\n",
            "step 5488: generator_loss=0.7393646836280823, discriminator_loss=0.6486009359359741\n",
            "step 5489: generator_loss=0.7420226335525513, discriminator_loss=0.6480283141136169\n",
            "step 5490: generator_loss=0.7436712980270386, discriminator_loss=0.6497295498847961\n",
            "step 5491: generator_loss=0.7394886016845703, discriminator_loss=0.6516880393028259\n",
            "step 5492: generator_loss=0.7430191040039062, discriminator_loss=0.6499704122543335\n",
            "step 5493: generator_loss=0.7429505586624146, discriminator_loss=0.650688886642456\n",
            "step 5494: generator_loss=0.7431479692459106, discriminator_loss=0.6500549912452698\n",
            "step 5495: generator_loss=0.751914381980896, discriminator_loss=0.6463944911956787\n",
            "step 5496: generator_loss=0.7460891008377075, discriminator_loss=0.6486368179321289\n",
            "step 5497: generator_loss=0.7415854930877686, discriminator_loss=0.6515223979949951\n",
            "step 5498: generator_loss=0.7432031631469727, discriminator_loss=0.6513323783874512\n",
            "step 5499: generator_loss=0.7389894127845764, discriminator_loss=0.6529821157455444\n",
            "step 5500: generator_loss=0.7396637797355652, discriminator_loss=0.6527413725852966\n",
            "step 5501: generator_loss=0.7367480993270874, discriminator_loss=0.6530094146728516\n",
            "step 5502: generator_loss=0.7360924482345581, discriminator_loss=0.6533923149108887\n",
            "step 5503: generator_loss=0.7290480136871338, discriminator_loss=0.6560511589050293\n",
            "step 5504: generator_loss=0.7298494577407837, discriminator_loss=0.6548486948013306\n",
            "step 5505: generator_loss=0.7269880771636963, discriminator_loss=0.6560157537460327\n",
            "step 5506: generator_loss=0.7253059148788452, discriminator_loss=0.6578396558761597\n",
            "step 5507: generator_loss=0.721223771572113, discriminator_loss=0.659823477268219\n",
            "step 5508: generator_loss=0.7251530885696411, discriminator_loss=0.6594094634056091\n",
            "step 5509: generator_loss=0.7208214998245239, discriminator_loss=0.660290539264679\n",
            "step 5510: generator_loss=0.7162795066833496, discriminator_loss=0.6644850373268127\n",
            "step 5511: generator_loss=0.7186538577079773, discriminator_loss=0.6639061570167542\n",
            "step 5512: generator_loss=0.7175367474555969, discriminator_loss=0.6651585102081299\n",
            "step 5513: generator_loss=0.7156918048858643, discriminator_loss=0.6685876250267029\n",
            "step 5514: generator_loss=0.7199977040290833, discriminator_loss=0.6685711145401001\n",
            "step 5515: generator_loss=0.7196147441864014, discriminator_loss=0.6685540080070496\n",
            "step 5516: generator_loss=0.7219042778015137, discriminator_loss=0.6693340539932251\n",
            "step 5517: generator_loss=0.7158776521682739, discriminator_loss=0.6718840599060059\n",
            "step 5518: generator_loss=0.7195749878883362, discriminator_loss=0.670989990234375\n",
            "step 5519: generator_loss=0.7230109572410583, discriminator_loss=0.6698329448699951\n",
            "step 5520: generator_loss=0.7210044860839844, discriminator_loss=0.6720837354660034\n",
            "step 5521: generator_loss=0.7254704833030701, discriminator_loss=0.6708584427833557\n",
            "step 5522: generator_loss=0.7139296531677246, discriminator_loss=0.6768025755882263\n",
            "step 5523: generator_loss=0.7121917009353638, discriminator_loss=0.6786463260650635\n",
            "step 5524: generator_loss=0.7149988412857056, discriminator_loss=0.6775906085968018\n",
            "step 5525: generator_loss=0.7067413330078125, discriminator_loss=0.6808441877365112\n",
            "step 5526: generator_loss=0.7035079002380371, discriminator_loss=0.6824434995651245\n",
            "step 5527: generator_loss=0.6999301910400391, discriminator_loss=0.6839730739593506\n",
            "step 5528: generator_loss=0.701425313949585, discriminator_loss=0.6834974884986877\n",
            "step 5529: generator_loss=0.6945219039916992, discriminator_loss=0.6877776384353638\n",
            "step 5530: generator_loss=0.6958620548248291, discriminator_loss=0.6869879961013794\n",
            "step 5531: generator_loss=0.6835411190986633, discriminator_loss=0.6927747130393982\n",
            "step 5532: generator_loss=0.685478687286377, discriminator_loss=0.6922345161437988\n",
            "step 5533: generator_loss=0.682289719581604, discriminator_loss=0.6970431208610535\n",
            "step 5534: generator_loss=0.6830674409866333, discriminator_loss=0.6967785358428955\n",
            "step 5535: generator_loss=0.6819820404052734, discriminator_loss=0.6993361711502075\n",
            "step 5536: generator_loss=0.6792757511138916, discriminator_loss=0.7033916711807251\n",
            "step 5537: generator_loss=0.6810197234153748, discriminator_loss=0.704892635345459\n",
            "step 5538: generator_loss=0.6821548938751221, discriminator_loss=0.7050495147705078\n",
            "step 5539: generator_loss=0.6832699775695801, discriminator_loss=0.7060106992721558\n",
            "step 5540: generator_loss=0.6829329133033752, discriminator_loss=0.7065742015838623\n",
            "step 5541: generator_loss=0.6853188872337341, discriminator_loss=0.7070186734199524\n",
            "step 5542: generator_loss=0.6797789335250854, discriminator_loss=0.7096878886222839\n",
            "step 5543: generator_loss=0.6788852214813232, discriminator_loss=0.7099621295928955\n",
            "step 5544: generator_loss=0.6766537427902222, discriminator_loss=0.7115370035171509\n",
            "step 5545: generator_loss=0.6792980432510376, discriminator_loss=0.7089555859565735\n",
            "step 5546: generator_loss=0.679896354675293, discriminator_loss=0.7083514332771301\n",
            "step 5547: generator_loss=0.6800066828727722, discriminator_loss=0.7056452631950378\n",
            "step 5548: generator_loss=0.6804314255714417, discriminator_loss=0.7060999870300293\n",
            "step 5549: generator_loss=0.6841949224472046, discriminator_loss=0.7051366567611694\n",
            "step 5550: generator_loss=0.6892476677894592, discriminator_loss=0.7031084895133972\n",
            "step 5551: generator_loss=0.6862338185310364, discriminator_loss=0.7044314742088318\n",
            "step 5552: generator_loss=0.6874462366104126, discriminator_loss=0.7032954096794128\n",
            "step 5553: generator_loss=0.6807552576065063, discriminator_loss=0.7071524858474731\n",
            "step 5554: generator_loss=0.6839098930358887, discriminator_loss=0.7050356268882751\n",
            "step 5555: generator_loss=0.6786174774169922, discriminator_loss=0.7070397138595581\n",
            "step 5556: generator_loss=0.6763986349105835, discriminator_loss=0.7073035836219788\n",
            "step 5557: generator_loss=0.6694707870483398, discriminator_loss=0.7100101709365845\n",
            "step 5558: generator_loss=0.672500729560852, discriminator_loss=0.7068728804588318\n",
            "step 5559: generator_loss=0.6697813272476196, discriminator_loss=0.7074846029281616\n",
            "step 5560: generator_loss=0.6722185611724854, discriminator_loss=0.706527829170227\n",
            "step 5561: generator_loss=0.6747088432312012, discriminator_loss=0.7041107416152954\n",
            "step 5562: generator_loss=0.6796669960021973, discriminator_loss=0.7015801668167114\n",
            "step 5563: generator_loss=0.6832841038703918, discriminator_loss=0.7015173435211182\n",
            "step 5564: generator_loss=0.6929249167442322, discriminator_loss=0.6970194578170776\n",
            "step 5565: generator_loss=0.6891744136810303, discriminator_loss=0.700614869594574\n",
            "step 5566: generator_loss=0.6983959078788757, discriminator_loss=0.6986055374145508\n",
            "step 5567: generator_loss=0.6988117694854736, discriminator_loss=0.7009638547897339\n",
            "step 5568: generator_loss=0.6974649429321289, discriminator_loss=0.7018136382102966\n",
            "step 5569: generator_loss=0.7003448009490967, discriminator_loss=0.7009546756744385\n",
            "step 5570: generator_loss=0.6913784742355347, discriminator_loss=0.7063572406768799\n",
            "step 5571: generator_loss=0.6990604996681213, discriminator_loss=0.7007216215133667\n",
            "step 5572: generator_loss=0.6904340982437134, discriminator_loss=0.7048198580741882\n",
            "step 5573: generator_loss=0.6782418489456177, discriminator_loss=0.7099136114120483\n",
            "step 5574: generator_loss=0.6699204444885254, discriminator_loss=0.7139406204223633\n",
            "step 5575: generator_loss=0.6714892387390137, discriminator_loss=0.7117406129837036\n",
            "step 5576: generator_loss=0.659832239151001, discriminator_loss=0.7154779434204102\n",
            "step 5577: generator_loss=0.662312388420105, discriminator_loss=0.7150964736938477\n",
            "step 5578: generator_loss=0.6477011442184448, discriminator_loss=0.7230387926101685\n",
            "step 5579: generator_loss=0.6428498029708862, discriminator_loss=0.7277041673660278\n",
            "step 5580: generator_loss=0.6440162658691406, discriminator_loss=0.7294410467147827\n",
            "step 5581: generator_loss=0.65265953540802, discriminator_loss=0.725167989730835\n",
            "step 5582: generator_loss=0.6588677167892456, discriminator_loss=0.7252388596534729\n",
            "step 5583: generator_loss=0.6616883277893066, discriminator_loss=0.7247753739356995\n",
            "step 5584: generator_loss=0.6606199145317078, discriminator_loss=0.7276365160942078\n",
            "step 5585: generator_loss=0.6605151891708374, discriminator_loss=0.72999507188797\n",
            "step 5586: generator_loss=0.6562137603759766, discriminator_loss=0.734846830368042\n",
            "step 5587: generator_loss=0.6646209359169006, discriminator_loss=0.7341923117637634\n",
            "step 5588: generator_loss=0.6553255915641785, discriminator_loss=0.7405474185943604\n",
            "step 5589: generator_loss=0.6565964221954346, discriminator_loss=0.7406806945800781\n",
            "step 5590: generator_loss=0.6511805057525635, discriminator_loss=0.7430623769760132\n",
            "step 5591: generator_loss=0.664602518081665, discriminator_loss=0.7334269285202026\n",
            "step 5592: generator_loss=0.6499277353286743, discriminator_loss=0.7418699264526367\n",
            "step 5593: generator_loss=0.6540320515632629, discriminator_loss=0.7374303340911865\n",
            "step 5594: generator_loss=0.6522107124328613, discriminator_loss=0.739341676235199\n",
            "step 5595: generator_loss=0.6554007530212402, discriminator_loss=0.7370724081993103\n",
            "step 5596: generator_loss=0.6519651412963867, discriminator_loss=0.7372273802757263\n",
            "step 5597: generator_loss=0.672581672668457, discriminator_loss=0.725165069103241\n",
            "step 5598: generator_loss=0.6672727465629578, discriminator_loss=0.7266668081283569\n",
            "step 5599: generator_loss=0.6644225120544434, discriminator_loss=0.7277642488479614\n",
            "step 5600: generator_loss=0.6786222457885742, discriminator_loss=0.7228543162345886\n",
            "step 5601: generator_loss=0.6781099438667297, discriminator_loss=0.7234541177749634\n",
            "step 5602: generator_loss=0.6867120862007141, discriminator_loss=0.718862771987915\n",
            "step 5603: generator_loss=0.6866683959960938, discriminator_loss=0.7181745171546936\n",
            "step 5604: generator_loss=0.6713019609451294, discriminator_loss=0.7276167869567871\n",
            "step 5605: generator_loss=0.6858678460121155, discriminator_loss=0.7180095911026001\n",
            "step 5606: generator_loss=0.6926664113998413, discriminator_loss=0.7119752168655396\n",
            "step 5607: generator_loss=0.6888686418533325, discriminator_loss=0.7094228267669678\n",
            "step 5608: generator_loss=0.6703579425811768, discriminator_loss=0.714457631111145\n",
            "step 5609: generator_loss=0.6553409099578857, discriminator_loss=0.716855525970459\n",
            "step 5610: generator_loss=0.6657826900482178, discriminator_loss=0.7057833671569824\n",
            "step 5611: generator_loss=0.6596512794494629, discriminator_loss=0.7038253545761108\n",
            "step 5612: generator_loss=0.6718581318855286, discriminator_loss=0.6936899423599243\n",
            "step 5613: generator_loss=0.6662795543670654, discriminator_loss=0.6934909224510193\n",
            "step 5614: generator_loss=0.6772081851959229, discriminator_loss=0.6873425245285034\n",
            "step 5615: generator_loss=0.6777263283729553, discriminator_loss=0.689336895942688\n",
            "step 5616: generator_loss=0.6890840530395508, discriminator_loss=0.6849562525749207\n",
            "step 5617: generator_loss=0.6995941400527954, discriminator_loss=0.6809718012809753\n",
            "step 5618: generator_loss=0.7145895957946777, discriminator_loss=0.6758434176445007\n",
            "step 5619: generator_loss=0.7189726233482361, discriminator_loss=0.6748363375663757\n",
            "step 5620: generator_loss=0.7339787483215332, discriminator_loss=0.6683751344680786\n",
            "step 5621: generator_loss=0.7447850704193115, discriminator_loss=0.6636085510253906\n",
            "step 5622: generator_loss=0.7453246116638184, discriminator_loss=0.6618403792381287\n",
            "step 5623: generator_loss=0.7576810121536255, discriminator_loss=0.6542345881462097\n",
            "step 5624: generator_loss=0.7577012777328491, discriminator_loss=0.6513360738754272\n",
            "step 5625: generator_loss=0.764106273651123, discriminator_loss=0.6440118551254272\n",
            "step 5626: generator_loss=0.7714170217514038, discriminator_loss=0.6391360759735107\n",
            "step 5627: generator_loss=0.7712550163269043, discriminator_loss=0.6356591582298279\n",
            "step 5628: generator_loss=0.7851390838623047, discriminator_loss=0.6271761655807495\n",
            "step 5629: generator_loss=0.7754503488540649, discriminator_loss=0.6288629174232483\n",
            "step 5630: generator_loss=0.7892192602157593, discriminator_loss=0.6216490864753723\n",
            "step 5631: generator_loss=0.7950561046600342, discriminator_loss=0.618576169013977\n",
            "step 5632: generator_loss=0.7905232310295105, discriminator_loss=0.6208406090736389\n",
            "step 5633: generator_loss=0.7949619293212891, discriminator_loss=0.62114417552948\n",
            "step 5634: generator_loss=0.7855783700942993, discriminator_loss=0.625415027141571\n",
            "step 5635: generator_loss=0.7985184192657471, discriminator_loss=0.6222070455551147\n",
            "step 5636: generator_loss=0.7901760339736938, discriminator_loss=0.6255906224250793\n",
            "step 5637: generator_loss=0.7785173654556274, discriminator_loss=0.6287720203399658\n",
            "step 5638: generator_loss=0.7823969721794128, discriminator_loss=0.6251385807991028\n",
            "step 5639: generator_loss=0.7797799706459045, discriminator_loss=0.624351978302002\n",
            "step 5640: generator_loss=0.7684424519538879, discriminator_loss=0.627468466758728\n",
            "step 5641: generator_loss=0.7647038698196411, discriminator_loss=0.6262915134429932\n",
            "step 5642: generator_loss=0.7452651262283325, discriminator_loss=0.6332074999809265\n",
            "step 5643: generator_loss=0.7532951831817627, discriminator_loss=0.628314733505249\n",
            "step 5644: generator_loss=0.7449429035186768, discriminator_loss=0.632992684841156\n",
            "step 5645: generator_loss=0.7493603825569153, discriminator_loss=0.6314816474914551\n",
            "step 5646: generator_loss=0.7428684234619141, discriminator_loss=0.6355150938034058\n",
            "step 5647: generator_loss=0.7494876384735107, discriminator_loss=0.633895993232727\n",
            "step 5648: generator_loss=0.7464867234230042, discriminator_loss=0.6368527412414551\n",
            "step 5649: generator_loss=0.7523155212402344, discriminator_loss=0.6374568939208984\n",
            "step 5650: generator_loss=0.7493960857391357, discriminator_loss=0.6424301266670227\n",
            "step 5651: generator_loss=0.7593504786491394, discriminator_loss=0.6415905356407166\n",
            "step 5652: generator_loss=0.7550699710845947, discriminator_loss=0.6441551446914673\n",
            "step 5653: generator_loss=0.7648206949234009, discriminator_loss=0.6422824263572693\n",
            "step 5654: generator_loss=0.7612258791923523, discriminator_loss=0.64409339427948\n",
            "step 5655: generator_loss=0.7520606517791748, discriminator_loss=0.6479671001434326\n",
            "step 5656: generator_loss=0.7409775257110596, discriminator_loss=0.6521989107131958\n",
            "step 5657: generator_loss=0.7350631356239319, discriminator_loss=0.6548923850059509\n",
            "step 5658: generator_loss=0.7425463795661926, discriminator_loss=0.6483817100524902\n",
            "step 5659: generator_loss=0.7182186841964722, discriminator_loss=0.657629668712616\n",
            "step 5660: generator_loss=0.7206979990005493, discriminator_loss=0.6576405763626099\n",
            "step 5661: generator_loss=0.7166756987571716, discriminator_loss=0.6583890914916992\n",
            "step 5662: generator_loss=0.7096552848815918, discriminator_loss=0.6628634333610535\n",
            "step 5663: generator_loss=0.7070446014404297, discriminator_loss=0.6645941138267517\n",
            "step 5664: generator_loss=0.7063500881195068, discriminator_loss=0.6675337553024292\n",
            "step 5665: generator_loss=0.7077841758728027, discriminator_loss=0.6693615317344666\n",
            "step 5666: generator_loss=0.7101554870605469, discriminator_loss=0.6718440055847168\n",
            "step 5667: generator_loss=0.7093616724014282, discriminator_loss=0.6738765239715576\n",
            "step 5668: generator_loss=0.708524763584137, discriminator_loss=0.6768622994422913\n",
            "step 5669: generator_loss=0.703660249710083, discriminator_loss=0.6798747777938843\n",
            "step 5670: generator_loss=0.7042633295059204, discriminator_loss=0.6817790269851685\n",
            "step 5671: generator_loss=0.7000478506088257, discriminator_loss=0.6846790313720703\n",
            "step 5672: generator_loss=0.6989390850067139, discriminator_loss=0.6854928135871887\n",
            "step 5673: generator_loss=0.6943813562393188, discriminator_loss=0.6893255710601807\n",
            "step 5674: generator_loss=0.693524181842804, discriminator_loss=0.6917346119880676\n",
            "step 5675: generator_loss=0.6869869232177734, discriminator_loss=0.6938890218734741\n",
            "step 5676: generator_loss=0.6853294968605042, discriminator_loss=0.6948205232620239\n",
            "step 5677: generator_loss=0.6811773777008057, discriminator_loss=0.6966272592544556\n",
            "step 5678: generator_loss=0.6805814504623413, discriminator_loss=0.6959177255630493\n",
            "step 5679: generator_loss=0.6723370552062988, discriminator_loss=0.7011926174163818\n",
            "step 5680: generator_loss=0.676031768321991, discriminator_loss=0.6992183923721313\n",
            "step 5681: generator_loss=0.6770973205566406, discriminator_loss=0.7003256678581238\n",
            "step 5682: generator_loss=0.6690738797187805, discriminator_loss=0.7050421833992004\n",
            "step 5683: generator_loss=0.6693345308303833, discriminator_loss=0.7062039375305176\n",
            "step 5684: generator_loss=0.6654955148696899, discriminator_loss=0.7074369788169861\n",
            "step 5685: generator_loss=0.6646082401275635, discriminator_loss=0.708685576915741\n",
            "step 5686: generator_loss=0.6684183478355408, discriminator_loss=0.7056576013565063\n",
            "step 5687: generator_loss=0.6625552177429199, discriminator_loss=0.7093541622161865\n",
            "step 5688: generator_loss=0.669898509979248, discriminator_loss=0.703361988067627\n",
            "step 5689: generator_loss=0.6682443618774414, discriminator_loss=0.7044281959533691\n",
            "step 5690: generator_loss=0.6710681319236755, discriminator_loss=0.70155268907547\n",
            "step 5691: generator_loss=0.6764556765556335, discriminator_loss=0.699762225151062\n",
            "step 5692: generator_loss=0.679576575756073, discriminator_loss=0.6979770660400391\n",
            "step 5693: generator_loss=0.6771649122238159, discriminator_loss=0.7001120448112488\n",
            "step 5694: generator_loss=0.6826719045639038, discriminator_loss=0.6997067928314209\n",
            "step 5695: generator_loss=0.6916789412498474, discriminator_loss=0.6948661804199219\n",
            "step 5696: generator_loss=0.6917881965637207, discriminator_loss=0.6964332461357117\n",
            "step 5697: generator_loss=0.6929940581321716, discriminator_loss=0.6974602937698364\n",
            "step 5698: generator_loss=0.6962754726409912, discriminator_loss=0.6983392238616943\n",
            "step 5699: generator_loss=0.7030303478240967, discriminator_loss=0.6960206031799316\n",
            "step 5700: generator_loss=0.6903033256530762, discriminator_loss=0.7029118537902832\n",
            "step 5701: generator_loss=0.6976133584976196, discriminator_loss=0.6994308233261108\n",
            "step 5702: generator_loss=0.6992676258087158, discriminator_loss=0.6981405019760132\n",
            "step 5703: generator_loss=0.7004457712173462, discriminator_loss=0.6975772976875305\n",
            "step 5704: generator_loss=0.6881799101829529, discriminator_loss=0.7065650820732117\n",
            "step 5705: generator_loss=0.676183819770813, discriminator_loss=0.7139307856559753\n",
            "step 5706: generator_loss=0.6792535781860352, discriminator_loss=0.7120751738548279\n",
            "step 5707: generator_loss=0.6626306772232056, discriminator_loss=0.7233017683029175\n",
            "step 5708: generator_loss=0.6636632680892944, discriminator_loss=0.7231079339981079\n",
            "step 5709: generator_loss=0.6615915894508362, discriminator_loss=0.7233784198760986\n",
            "step 5710: generator_loss=0.6497273445129395, discriminator_loss=0.7307989597320557\n",
            "step 5711: generator_loss=0.647305965423584, discriminator_loss=0.7338699102401733\n",
            "step 5712: generator_loss=0.6433680057525635, discriminator_loss=0.7366480231285095\n",
            "step 5713: generator_loss=0.6470955610275269, discriminator_loss=0.7348008155822754\n",
            "step 5714: generator_loss=0.6521378755569458, discriminator_loss=0.7344181537628174\n",
            "step 5715: generator_loss=0.6429232358932495, discriminator_loss=0.7405312061309814\n",
            "step 5716: generator_loss=0.6411100625991821, discriminator_loss=0.7423578500747681\n",
            "step 5717: generator_loss=0.6443338394165039, discriminator_loss=0.7426400780677795\n",
            "step 5718: generator_loss=0.6555639505386353, discriminator_loss=0.738671600818634\n",
            "step 5719: generator_loss=0.6451461911201477, discriminator_loss=0.7462356686592102\n",
            "step 5720: generator_loss=0.6501414775848389, discriminator_loss=0.7436726093292236\n",
            "step 5721: generator_loss=0.6513374447822571, discriminator_loss=0.7409534454345703\n",
            "step 5722: generator_loss=0.65203857421875, discriminator_loss=0.7391055822372437\n",
            "step 5723: generator_loss=0.6439456939697266, discriminator_loss=0.739371657371521\n",
            "step 5724: generator_loss=0.6428155899047852, discriminator_loss=0.7356429100036621\n",
            "step 5725: generator_loss=0.6358225345611572, discriminator_loss=0.7347497940063477\n",
            "step 5726: generator_loss=0.6445261836051941, discriminator_loss=0.7260441184043884\n",
            "step 5727: generator_loss=0.6606705188751221, discriminator_loss=0.7156341671943665\n",
            "step 5728: generator_loss=0.6630736589431763, discriminator_loss=0.7129144072532654\n",
            "step 5729: generator_loss=0.6791703701019287, discriminator_loss=0.7039430141448975\n",
            "step 5730: generator_loss=0.6836283206939697, discriminator_loss=0.6996725797653198\n",
            "step 5731: generator_loss=0.6898469924926758, discriminator_loss=0.6956071853637695\n",
            "step 5732: generator_loss=0.6959234476089478, discriminator_loss=0.6908277869224548\n",
            "step 5733: generator_loss=0.7140490412712097, discriminator_loss=0.6826539635658264\n",
            "step 5734: generator_loss=0.7145398259162903, discriminator_loss=0.6835507154464722\n",
            "step 5735: generator_loss=0.7277845740318298, discriminator_loss=0.6787515878677368\n",
            "step 5736: generator_loss=0.7409960627555847, discriminator_loss=0.6744259595870972\n",
            "step 5737: generator_loss=0.744598388671875, discriminator_loss=0.6738333702087402\n",
            "step 5738: generator_loss=0.7419259548187256, discriminator_loss=0.6750577092170715\n",
            "step 5739: generator_loss=0.7419466972351074, discriminator_loss=0.6739488244056702\n",
            "step 5740: generator_loss=0.7403132319450378, discriminator_loss=0.6736809015274048\n",
            "step 5741: generator_loss=0.7425971031188965, discriminator_loss=0.6699874401092529\n",
            "step 5742: generator_loss=0.7375195622444153, discriminator_loss=0.6685426831245422\n",
            "step 5743: generator_loss=0.7408773899078369, discriminator_loss=0.6642673015594482\n",
            "step 5744: generator_loss=0.7334954738616943, discriminator_loss=0.6660201549530029\n",
            "step 5745: generator_loss=0.7257113456726074, discriminator_loss=0.6674050092697144\n",
            "step 5746: generator_loss=0.7257269620895386, discriminator_loss=0.6661903262138367\n",
            "step 5747: generator_loss=0.7175137996673584, discriminator_loss=0.6669209003448486\n",
            "step 5748: generator_loss=0.7141532301902771, discriminator_loss=0.6672683358192444\n",
            "step 5749: generator_loss=0.7044076919555664, discriminator_loss=0.6709454655647278\n",
            "step 5750: generator_loss=0.7062406539916992, discriminator_loss=0.6692886352539062\n",
            "step 5751: generator_loss=0.699260950088501, discriminator_loss=0.6721667051315308\n",
            "step 5752: generator_loss=0.7013911604881287, discriminator_loss=0.6721458435058594\n",
            "step 5753: generator_loss=0.7064892649650574, discriminator_loss=0.6699506044387817\n",
            "step 5754: generator_loss=0.7089903950691223, discriminator_loss=0.6690527200698853\n",
            "step 5755: generator_loss=0.7120739221572876, discriminator_loss=0.6682326793670654\n",
            "step 5756: generator_loss=0.7156510353088379, discriminator_loss=0.668452262878418\n",
            "step 5757: generator_loss=0.7166024446487427, discriminator_loss=0.6683627963066101\n",
            "step 5758: generator_loss=0.717262864112854, discriminator_loss=0.6688523292541504\n",
            "step 5759: generator_loss=0.7242145538330078, discriminator_loss=0.6669455766677856\n",
            "step 5760: generator_loss=0.7299900054931641, discriminator_loss=0.6666093468666077\n",
            "step 5761: generator_loss=0.7271723747253418, discriminator_loss=0.6674624681472778\n",
            "step 5762: generator_loss=0.7261590957641602, discriminator_loss=0.667827844619751\n",
            "step 5763: generator_loss=0.7250233292579651, discriminator_loss=0.6672801971435547\n",
            "step 5764: generator_loss=0.7344236969947815, discriminator_loss=0.6633194088935852\n",
            "step 5765: generator_loss=0.7275708913803101, discriminator_loss=0.6646698713302612\n",
            "step 5766: generator_loss=0.7368263006210327, discriminator_loss=0.6593554019927979\n",
            "step 5767: generator_loss=0.7304462194442749, discriminator_loss=0.6624945998191833\n",
            "step 5768: generator_loss=0.7439316511154175, discriminator_loss=0.6581621766090393\n",
            "step 5769: generator_loss=0.740074634552002, discriminator_loss=0.6582150459289551\n",
            "step 5770: generator_loss=0.7353835701942444, discriminator_loss=0.6593459248542786\n",
            "step 5771: generator_loss=0.7355155944824219, discriminator_loss=0.6589046716690063\n",
            "step 5772: generator_loss=0.7375364899635315, discriminator_loss=0.6578725576400757\n",
            "step 5773: generator_loss=0.7534118890762329, discriminator_loss=0.6490114331245422\n",
            "step 5774: generator_loss=0.7596601843833923, discriminator_loss=0.644443690776825\n",
            "step 5775: generator_loss=0.7433908581733704, discriminator_loss=0.6508839130401611\n",
            "step 5776: generator_loss=0.7520111799240112, discriminator_loss=0.6481513381004333\n",
            "step 5777: generator_loss=0.7466135025024414, discriminator_loss=0.651411235332489\n",
            "step 5778: generator_loss=0.7498193979263306, discriminator_loss=0.6507799625396729\n",
            "step 5779: generator_loss=0.7427918910980225, discriminator_loss=0.6539632081985474\n",
            "step 5780: generator_loss=0.747855544090271, discriminator_loss=0.6513007283210754\n",
            "step 5781: generator_loss=0.7423315048217773, discriminator_loss=0.6536027193069458\n",
            "step 5782: generator_loss=0.7379202842712402, discriminator_loss=0.6551342010498047\n",
            "step 5783: generator_loss=0.736923336982727, discriminator_loss=0.65395188331604\n",
            "step 5784: generator_loss=0.7354631423950195, discriminator_loss=0.6536570191383362\n",
            "step 5785: generator_loss=0.7229139804840088, discriminator_loss=0.6606078147888184\n",
            "step 5786: generator_loss=0.7282307147979736, discriminator_loss=0.6579685211181641\n",
            "step 5787: generator_loss=0.7320894598960876, discriminator_loss=0.6574939489364624\n",
            "step 5788: generator_loss=0.7409521341323853, discriminator_loss=0.6536794900894165\n",
            "step 5789: generator_loss=0.7235333919525146, discriminator_loss=0.6627547144889832\n",
            "step 5790: generator_loss=0.7337667942047119, discriminator_loss=0.6599867343902588\n",
            "step 5791: generator_loss=0.7334460020065308, discriminator_loss=0.662239670753479\n",
            "step 5792: generator_loss=0.7359460592269897, discriminator_loss=0.6635749340057373\n",
            "step 5793: generator_loss=0.7276918888092041, discriminator_loss=0.6685351133346558\n",
            "step 5794: generator_loss=0.7253408432006836, discriminator_loss=0.6702547073364258\n",
            "step 5795: generator_loss=0.7203336954116821, discriminator_loss=0.6735914945602417\n",
            "step 5796: generator_loss=0.7251383066177368, discriminator_loss=0.6718415021896362\n",
            "step 5797: generator_loss=0.7148250341415405, discriminator_loss=0.6777231097221375\n",
            "step 5798: generator_loss=0.7082590460777283, discriminator_loss=0.6850278377532959\n",
            "step 5799: generator_loss=0.7150771617889404, discriminator_loss=0.6817130446434021\n",
            "step 5800: generator_loss=0.7051772475242615, discriminator_loss=0.689391016960144\n",
            "step 5801: generator_loss=0.700063943862915, discriminator_loss=0.6923868656158447\n",
            "step 5802: generator_loss=0.6882085800170898, discriminator_loss=0.6994799375534058\n",
            "step 5803: generator_loss=0.6806479096412659, discriminator_loss=0.7014607787132263\n",
            "step 5804: generator_loss=0.6793931722640991, discriminator_loss=0.7038938999176025\n",
            "step 5805: generator_loss=0.6611511707305908, discriminator_loss=0.7138493061065674\n",
            "step 5806: generator_loss=0.668864369392395, discriminator_loss=0.7107596397399902\n",
            "step 5807: generator_loss=0.6577357053756714, discriminator_loss=0.7181837558746338\n",
            "step 5808: generator_loss=0.6468446254730225, discriminator_loss=0.7254058122634888\n",
            "step 5809: generator_loss=0.6530775427818298, discriminator_loss=0.7232889533042908\n",
            "step 5810: generator_loss=0.6538583040237427, discriminator_loss=0.7249883413314819\n",
            "step 5811: generator_loss=0.6632684469223022, discriminator_loss=0.7202367782592773\n",
            "step 5812: generator_loss=0.6629548072814941, discriminator_loss=0.7230156660079956\n",
            "step 5813: generator_loss=0.6657432317733765, discriminator_loss=0.7239320278167725\n",
            "step 5814: generator_loss=0.6636537313461304, discriminator_loss=0.7288787961006165\n",
            "step 5815: generator_loss=0.6902851462364197, discriminator_loss=0.7162512540817261\n",
            "step 5816: generator_loss=0.6712461709976196, discriminator_loss=0.7278801202774048\n",
            "step 5817: generator_loss=0.6803677082061768, discriminator_loss=0.7233541011810303\n",
            "step 5818: generator_loss=0.6797606348991394, discriminator_loss=0.7237011194229126\n",
            "step 5819: generator_loss=0.6740270853042603, discriminator_loss=0.7253751158714294\n",
            "step 5820: generator_loss=0.6790145039558411, discriminator_loss=0.7176162004470825\n",
            "step 5821: generator_loss=0.6776646375656128, discriminator_loss=0.7141210436820984\n",
            "step 5822: generator_loss=0.6759209632873535, discriminator_loss=0.7111258506774902\n",
            "step 5823: generator_loss=0.671393096446991, discriminator_loss=0.7111396789550781\n",
            "step 5824: generator_loss=0.6737065315246582, discriminator_loss=0.7082489132881165\n",
            "step 5825: generator_loss=0.6868822574615479, discriminator_loss=0.7006466388702393\n",
            "step 5826: generator_loss=0.6772466897964478, discriminator_loss=0.706218421459198\n",
            "step 5827: generator_loss=0.6735498309135437, discriminator_loss=0.7084916830062866\n",
            "step 5828: generator_loss=0.6811461448669434, discriminator_loss=0.7038979530334473\n",
            "step 5829: generator_loss=0.6748108863830566, discriminator_loss=0.7066593170166016\n",
            "step 5830: generator_loss=0.6721065044403076, discriminator_loss=0.7063033580780029\n",
            "step 5831: generator_loss=0.6808052062988281, discriminator_loss=0.699609637260437\n",
            "step 5832: generator_loss=0.6828054189682007, discriminator_loss=0.6976888179779053\n",
            "step 5833: generator_loss=0.6916963458061218, discriminator_loss=0.6929418444633484\n",
            "step 5834: generator_loss=0.6970281600952148, discriminator_loss=0.6901800632476807\n",
            "step 5835: generator_loss=0.6991849541664124, discriminator_loss=0.6885505318641663\n",
            "step 5836: generator_loss=0.7050716876983643, discriminator_loss=0.6888551712036133\n",
            "step 5837: generator_loss=0.7153297662734985, discriminator_loss=0.6859303712844849\n",
            "step 5838: generator_loss=0.7141767740249634, discriminator_loss=0.6883439421653748\n",
            "step 5839: generator_loss=0.7211039066314697, discriminator_loss=0.6876134872436523\n",
            "step 5840: generator_loss=0.7205537557601929, discriminator_loss=0.6903285980224609\n",
            "step 5841: generator_loss=0.7204733490943909, discriminator_loss=0.6892417073249817\n",
            "step 5842: generator_loss=0.7184600830078125, discriminator_loss=0.6873044967651367\n",
            "step 5843: generator_loss=0.708622395992279, discriminator_loss=0.6902173757553101\n",
            "step 5844: generator_loss=0.7094389796257019, discriminator_loss=0.6859722137451172\n",
            "step 5845: generator_loss=0.6987079381942749, discriminator_loss=0.6876136064529419\n",
            "step 5846: generator_loss=0.7042772769927979, discriminator_loss=0.6797891855239868\n",
            "step 5847: generator_loss=0.7023172378540039, discriminator_loss=0.678119421005249\n",
            "step 5848: generator_loss=0.6932318210601807, discriminator_loss=0.6804726719856262\n",
            "step 5849: generator_loss=0.7028801441192627, discriminator_loss=0.6745904684066772\n",
            "step 5850: generator_loss=0.7004585862159729, discriminator_loss=0.6773804426193237\n",
            "step 5851: generator_loss=0.7095568776130676, discriminator_loss=0.6743422746658325\n",
            "step 5852: generator_loss=0.7141923904418945, discriminator_loss=0.6746832132339478\n",
            "step 5853: generator_loss=0.7215520143508911, discriminator_loss=0.6731996536254883\n",
            "step 5854: generator_loss=0.7247586846351624, discriminator_loss=0.6743719577789307\n",
            "step 5855: generator_loss=0.7314808368682861, discriminator_loss=0.6713759303092957\n",
            "step 5856: generator_loss=0.7290109992027283, discriminator_loss=0.6736961007118225\n",
            "step 5857: generator_loss=0.7373157739639282, discriminator_loss=0.6680868864059448\n",
            "step 5858: generator_loss=0.7318679690361023, discriminator_loss=0.6717860102653503\n",
            "step 5859: generator_loss=0.7302993535995483, discriminator_loss=0.6719276905059814\n",
            "step 5860: generator_loss=0.7235679626464844, discriminator_loss=0.6738864183425903\n",
            "step 5861: generator_loss=0.7144750952720642, discriminator_loss=0.6756501793861389\n",
            "step 5862: generator_loss=0.7131031155586243, discriminator_loss=0.674983024597168\n",
            "step 5863: generator_loss=0.7077257633209229, discriminator_loss=0.675903856754303\n",
            "step 5864: generator_loss=0.7006981372833252, discriminator_loss=0.6778397560119629\n",
            "step 5865: generator_loss=0.702681839466095, discriminator_loss=0.6764986515045166\n",
            "step 5866: generator_loss=0.7033971548080444, discriminator_loss=0.6751534938812256\n",
            "step 5867: generator_loss=0.7100813388824463, discriminator_loss=0.6725814342498779\n",
            "step 5868: generator_loss=0.7221550941467285, discriminator_loss=0.6672337055206299\n",
            "step 5869: generator_loss=0.7192234992980957, discriminator_loss=0.6690057516098022\n",
            "step 5870: generator_loss=0.7171338200569153, discriminator_loss=0.6709847450256348\n",
            "step 5871: generator_loss=0.7262183427810669, discriminator_loss=0.6691445112228394\n",
            "step 5872: generator_loss=0.7268369197845459, discriminator_loss=0.6703476905822754\n",
            "step 5873: generator_loss=0.7200859189033508, discriminator_loss=0.6730269193649292\n",
            "step 5874: generator_loss=0.7118496894836426, discriminator_loss=0.6774371862411499\n",
            "step 5875: generator_loss=0.719146728515625, discriminator_loss=0.6750603914260864\n",
            "step 5876: generator_loss=0.713229775428772, discriminator_loss=0.6785272359848022\n",
            "step 5877: generator_loss=0.7143987417221069, discriminator_loss=0.676569938659668\n",
            "step 5878: generator_loss=0.7086466550827026, discriminator_loss=0.6780454516410828\n",
            "step 5879: generator_loss=0.708796501159668, discriminator_loss=0.6777169704437256\n",
            "step 5880: generator_loss=0.7088285684585571, discriminator_loss=0.6767425537109375\n",
            "step 5881: generator_loss=0.7047877311706543, discriminator_loss=0.6795063018798828\n",
            "step 5882: generator_loss=0.7070424556732178, discriminator_loss=0.6784866452217102\n",
            "step 5883: generator_loss=0.7101175785064697, discriminator_loss=0.6773830056190491\n",
            "step 5884: generator_loss=0.6980095505714417, discriminator_loss=0.6840654611587524\n",
            "step 5885: generator_loss=0.7051571607589722, discriminator_loss=0.6817401051521301\n",
            "step 5886: generator_loss=0.702704906463623, discriminator_loss=0.6820574998855591\n",
            "step 5887: generator_loss=0.6990604400634766, discriminator_loss=0.6838672161102295\n",
            "step 5888: generator_loss=0.6985312700271606, discriminator_loss=0.6836875677108765\n",
            "step 5889: generator_loss=0.6962810158729553, discriminator_loss=0.6845045685768127\n",
            "step 5890: generator_loss=0.6912107467651367, discriminator_loss=0.6853340268135071\n",
            "step 5891: generator_loss=0.6880934238433838, discriminator_loss=0.6875219345092773\n",
            "step 5892: generator_loss=0.6879764199256897, discriminator_loss=0.689586877822876\n",
            "step 5893: generator_loss=0.6947513818740845, discriminator_loss=0.688808262348175\n",
            "step 5894: generator_loss=0.6903462409973145, discriminator_loss=0.6939404606819153\n",
            "step 5895: generator_loss=0.6853489875793457, discriminator_loss=0.6986492872238159\n",
            "step 5896: generator_loss=0.6914520859718323, discriminator_loss=0.6961860656738281\n",
            "step 5897: generator_loss=0.683711051940918, discriminator_loss=0.7007983922958374\n",
            "step 5898: generator_loss=0.6872695684432983, discriminator_loss=0.6997496485710144\n",
            "step 5899: generator_loss=0.6801305413246155, discriminator_loss=0.7050567865371704\n",
            "step 5900: generator_loss=0.6807654500007629, discriminator_loss=0.7054234743118286\n",
            "step 5901: generator_loss=0.6690167188644409, discriminator_loss=0.71150803565979\n",
            "step 5902: generator_loss=0.6673152446746826, discriminator_loss=0.712593674659729\n",
            "step 5903: generator_loss=0.6675945520401001, discriminator_loss=0.7125229835510254\n",
            "step 5904: generator_loss=0.6576067805290222, discriminator_loss=0.718650221824646\n",
            "step 5905: generator_loss=0.666225016117096, discriminator_loss=0.7158955335617065\n",
            "step 5906: generator_loss=0.6622165441513062, discriminator_loss=0.7185442447662354\n",
            "step 5907: generator_loss=0.6624467968940735, discriminator_loss=0.7192590236663818\n",
            "step 5908: generator_loss=0.6663620471954346, discriminator_loss=0.7175009250640869\n",
            "step 5909: generator_loss=0.6760982275009155, discriminator_loss=0.715168833732605\n",
            "step 5910: generator_loss=0.68172687292099, discriminator_loss=0.7136334180831909\n",
            "step 5911: generator_loss=0.6855026483535767, discriminator_loss=0.7148903012275696\n",
            "step 5912: generator_loss=0.6883572936058044, discriminator_loss=0.7142508625984192\n",
            "step 5913: generator_loss=0.6744080781936646, discriminator_loss=0.7204568386077881\n",
            "step 5914: generator_loss=0.6817352771759033, discriminator_loss=0.7156901359558105\n",
            "step 5915: generator_loss=0.6595199704170227, discriminator_loss=0.7254724502563477\n",
            "step 5916: generator_loss=0.6726573705673218, discriminator_loss=0.7176888585090637\n",
            "step 5917: generator_loss=0.6640967726707458, discriminator_loss=0.7199701070785522\n",
            "step 5918: generator_loss=0.6594525575637817, discriminator_loss=0.7224483489990234\n",
            "step 5919: generator_loss=0.6545147895812988, discriminator_loss=0.7243903875350952\n",
            "step 5920: generator_loss=0.6721068620681763, discriminator_loss=0.7137978076934814\n",
            "step 5921: generator_loss=0.6767721176147461, discriminator_loss=0.7101874351501465\n",
            "step 5922: generator_loss=0.6597576141357422, discriminator_loss=0.7216947078704834\n",
            "step 5923: generator_loss=0.6745282411575317, discriminator_loss=0.7139307856559753\n",
            "step 5924: generator_loss=0.6824946999549866, discriminator_loss=0.7105982303619385\n",
            "step 5925: generator_loss=0.6781103014945984, discriminator_loss=0.7128516435623169\n",
            "step 5926: generator_loss=0.6890523433685303, discriminator_loss=0.7069775462150574\n",
            "step 5927: generator_loss=0.6794722080230713, discriminator_loss=0.7099964022636414\n",
            "step 5928: generator_loss=0.6705378890037537, discriminator_loss=0.7119571566581726\n",
            "step 5929: generator_loss=0.6674826741218567, discriminator_loss=0.7123769521713257\n",
            "step 5930: generator_loss=0.6766332387924194, discriminator_loss=0.706652820110321\n",
            "step 5931: generator_loss=0.6754978895187378, discriminator_loss=0.7056431770324707\n",
            "step 5932: generator_loss=0.6773001551628113, discriminator_loss=0.7052757740020752\n",
            "step 5933: generator_loss=0.6895236968994141, discriminator_loss=0.6991965174674988\n",
            "step 5934: generator_loss=0.6921737790107727, discriminator_loss=0.6983364820480347\n",
            "step 5935: generator_loss=0.7015330791473389, discriminator_loss=0.6919726133346558\n",
            "step 5936: generator_loss=0.7085464000701904, discriminator_loss=0.6903047561645508\n",
            "step 5937: generator_loss=0.7027415037155151, discriminator_loss=0.6931597590446472\n",
            "step 5938: generator_loss=0.7048766016960144, discriminator_loss=0.6916587352752686\n",
            "step 5939: generator_loss=0.7091741561889648, discriminator_loss=0.6876379251480103\n",
            "step 5940: generator_loss=0.7079730033874512, discriminator_loss=0.6871799230575562\n",
            "step 5941: generator_loss=0.7065571546554565, discriminator_loss=0.6866210699081421\n",
            "step 5942: generator_loss=0.7038708925247192, discriminator_loss=0.684746265411377\n",
            "step 5943: generator_loss=0.7049943208694458, discriminator_loss=0.6812093257904053\n",
            "step 5944: generator_loss=0.7050464153289795, discriminator_loss=0.6804794073104858\n",
            "step 5945: generator_loss=0.7195632457733154, discriminator_loss=0.671231746673584\n",
            "step 5946: generator_loss=0.7172404527664185, discriminator_loss=0.6690729856491089\n",
            "step 5947: generator_loss=0.722321629524231, discriminator_loss=0.6669046878814697\n",
            "step 5948: generator_loss=0.7379770278930664, discriminator_loss=0.6614660024642944\n",
            "step 5949: generator_loss=0.7357392311096191, discriminator_loss=0.6629887819290161\n",
            "step 5950: generator_loss=0.7491357922554016, discriminator_loss=0.6569280028343201\n",
            "step 5951: generator_loss=0.7537469863891602, discriminator_loss=0.6560293436050415\n",
            "step 5952: generator_loss=0.7473673820495605, discriminator_loss=0.6577101945877075\n",
            "step 5953: generator_loss=0.7526858448982239, discriminator_loss=0.6532864570617676\n",
            "step 5954: generator_loss=0.7469077706336975, discriminator_loss=0.6540064215660095\n",
            "step 5955: generator_loss=0.746016800403595, discriminator_loss=0.6527919769287109\n",
            "step 5956: generator_loss=0.7536734938621521, discriminator_loss=0.6481977701187134\n",
            "step 5957: generator_loss=0.7511301040649414, discriminator_loss=0.6471883058547974\n",
            "step 5958: generator_loss=0.7479064464569092, discriminator_loss=0.6488398313522339\n",
            "step 5959: generator_loss=0.7374576330184937, discriminator_loss=0.6521025896072388\n",
            "step 5960: generator_loss=0.73972088098526, discriminator_loss=0.6492000818252563\n",
            "step 5961: generator_loss=0.7291595935821533, discriminator_loss=0.6528317332267761\n",
            "step 5962: generator_loss=0.7302954196929932, discriminator_loss=0.6501696109771729\n",
            "step 5963: generator_loss=0.716442346572876, discriminator_loss=0.6559699177742004\n",
            "step 5964: generator_loss=0.7131962776184082, discriminator_loss=0.6540467143058777\n",
            "step 5965: generator_loss=0.7236909866333008, discriminator_loss=0.6511157751083374\n",
            "step 5966: generator_loss=0.7256397008895874, discriminator_loss=0.6489920616149902\n",
            "step 5967: generator_loss=0.7179306745529175, discriminator_loss=0.650724470615387\n",
            "step 5968: generator_loss=0.7422279119491577, discriminator_loss=0.6421411037445068\n",
            "step 5969: generator_loss=0.743186354637146, discriminator_loss=0.6434162855148315\n",
            "step 5970: generator_loss=0.7466542720794678, discriminator_loss=0.6448495388031006\n",
            "step 5971: generator_loss=0.7608411312103271, discriminator_loss=0.6435815095901489\n",
            "step 5972: generator_loss=0.7649904489517212, discriminator_loss=0.645910382270813\n",
            "step 5973: generator_loss=0.7813193798065186, discriminator_loss=0.6418250799179077\n",
            "step 5974: generator_loss=0.7850525379180908, discriminator_loss=0.6420058012008667\n",
            "step 5975: generator_loss=0.7911980152130127, discriminator_loss=0.6408247947692871\n",
            "step 5976: generator_loss=0.7805570363998413, discriminator_loss=0.644142746925354\n",
            "step 5977: generator_loss=0.7699286341667175, discriminator_loss=0.6457774043083191\n",
            "step 5978: generator_loss=0.7481725215911865, discriminator_loss=0.6525280475616455\n",
            "step 5979: generator_loss=0.7402401566505432, discriminator_loss=0.6543923616409302\n",
            "step 5980: generator_loss=0.7286163568496704, discriminator_loss=0.6552178859710693\n",
            "step 5981: generator_loss=0.7125446796417236, discriminator_loss=0.6609396934509277\n",
            "step 5982: generator_loss=0.7108391523361206, discriminator_loss=0.6620240211486816\n",
            "step 5983: generator_loss=0.6983031034469604, discriminator_loss=0.6660898327827454\n",
            "step 5984: generator_loss=0.6938839554786682, discriminator_loss=0.6691250205039978\n",
            "step 5985: generator_loss=0.6926419138908386, discriminator_loss=0.6720284223556519\n",
            "step 5986: generator_loss=0.6936467885971069, discriminator_loss=0.6730926036834717\n",
            "step 5987: generator_loss=0.6929957866668701, discriminator_loss=0.6770287156105042\n",
            "step 5988: generator_loss=0.7004868984222412, discriminator_loss=0.6800395250320435\n",
            "step 5989: generator_loss=0.6979674100875854, discriminator_loss=0.6836628913879395\n",
            "step 5990: generator_loss=0.7053983211517334, discriminator_loss=0.6844083666801453\n",
            "step 5991: generator_loss=0.6967289447784424, discriminator_loss=0.6910518407821655\n",
            "step 5992: generator_loss=0.7093243598937988, discriminator_loss=0.6898657083511353\n",
            "step 5993: generator_loss=0.7018065452575684, discriminator_loss=0.6948186159133911\n",
            "step 5994: generator_loss=0.6877559423446655, discriminator_loss=0.7036135792732239\n",
            "step 5995: generator_loss=0.6892914772033691, discriminator_loss=0.70436692237854\n",
            "step 5996: generator_loss=0.6890512704849243, discriminator_loss=0.7045377492904663\n",
            "step 5997: generator_loss=0.6765494346618652, discriminator_loss=0.712370753288269\n",
            "step 5998: generator_loss=0.6625987887382507, discriminator_loss=0.7190414667129517\n",
            "step 5999: generator_loss=0.6691727638244629, discriminator_loss=0.7165814638137817\n",
            "step 6000: generator_loss=0.6520720720291138, discriminator_loss=0.7251095771789551\n",
            "step 6001: generator_loss=0.6552852988243103, discriminator_loss=0.7257752418518066\n",
            "step 6002: generator_loss=0.652071475982666, discriminator_loss=0.724984884262085\n",
            "step 6003: generator_loss=0.636614203453064, discriminator_loss=0.7357847094535828\n",
            "step 6004: generator_loss=0.6367648839950562, discriminator_loss=0.7396227121353149\n",
            "step 6005: generator_loss=0.6430264711380005, discriminator_loss=0.7357357740402222\n",
            "step 6006: generator_loss=0.6439417600631714, discriminator_loss=0.7357200384140015\n",
            "step 6007: generator_loss=0.6353877782821655, discriminator_loss=0.7431752681732178\n",
            "step 6008: generator_loss=0.6545969247817993, discriminator_loss=0.7352418899536133\n",
            "step 6009: generator_loss=0.6399093866348267, discriminator_loss=0.7454615831375122\n",
            "step 6010: generator_loss=0.6511930227279663, discriminator_loss=0.7438638806343079\n",
            "step 6011: generator_loss=0.6565903425216675, discriminator_loss=0.7418447136878967\n",
            "step 6012: generator_loss=0.6393157243728638, discriminator_loss=0.7532456517219543\n",
            "step 6013: generator_loss=0.6554698944091797, discriminator_loss=0.7434306740760803\n",
            "step 6014: generator_loss=0.6466819047927856, discriminator_loss=0.7459275126457214\n",
            "step 6015: generator_loss=0.6623630523681641, discriminator_loss=0.7353953123092651\n",
            "step 6016: generator_loss=0.6465182304382324, discriminator_loss=0.7431727051734924\n",
            "step 6017: generator_loss=0.648259162902832, discriminator_loss=0.7388391494750977\n",
            "step 6018: generator_loss=0.6451285481452942, discriminator_loss=0.7406151294708252\n",
            "step 6019: generator_loss=0.6493008136749268, discriminator_loss=0.7366025447845459\n",
            "step 6020: generator_loss=0.6572695970535278, discriminator_loss=0.728416919708252\n",
            "step 6021: generator_loss=0.6660580635070801, discriminator_loss=0.7206320762634277\n",
            "step 6022: generator_loss=0.6612564325332642, discriminator_loss=0.7215790152549744\n",
            "step 6023: generator_loss=0.6709895133972168, discriminator_loss=0.7145116329193115\n",
            "step 6024: generator_loss=0.6662816405296326, discriminator_loss=0.718603253364563\n",
            "step 6025: generator_loss=0.6752039194107056, discriminator_loss=0.7120721340179443\n",
            "step 6026: generator_loss=0.6750187873840332, discriminator_loss=0.7116186022758484\n",
            "step 6027: generator_loss=0.6781026124954224, discriminator_loss=0.710106611251831\n",
            "step 6028: generator_loss=0.689295768737793, discriminator_loss=0.7029888033866882\n",
            "step 6029: generator_loss=0.6945068836212158, discriminator_loss=0.6987477540969849\n",
            "step 6030: generator_loss=0.711453914642334, discriminator_loss=0.6899634003639221\n",
            "step 6031: generator_loss=0.7067159414291382, discriminator_loss=0.6897020936012268\n",
            "step 6032: generator_loss=0.7137731313705444, discriminator_loss=0.685791552066803\n",
            "step 6033: generator_loss=0.7154849767684937, discriminator_loss=0.6840759515762329\n",
            "step 6034: generator_loss=0.714258074760437, discriminator_loss=0.6828716397285461\n",
            "step 6035: generator_loss=0.7188823223114014, discriminator_loss=0.6783131957054138\n",
            "step 6036: generator_loss=0.7184921503067017, discriminator_loss=0.6754235625267029\n",
            "step 6037: generator_loss=0.7197028398513794, discriminator_loss=0.6726160049438477\n",
            "step 6038: generator_loss=0.7211697101593018, discriminator_loss=0.6711012125015259\n",
            "step 6039: generator_loss=0.7201525568962097, discriminator_loss=0.6724914312362671\n",
            "step 6040: generator_loss=0.7364715933799744, discriminator_loss=0.665634036064148\n",
            "step 6041: generator_loss=0.7322094440460205, discriminator_loss=0.667300283908844\n",
            "step 6042: generator_loss=0.74005126953125, discriminator_loss=0.6647107601165771\n",
            "step 6043: generator_loss=0.7462997436523438, discriminator_loss=0.6622187495231628\n",
            "step 6044: generator_loss=0.7419594526290894, discriminator_loss=0.6634567975997925\n",
            "step 6045: generator_loss=0.7366621494293213, discriminator_loss=0.6665496826171875\n",
            "step 6046: generator_loss=0.7355027198791504, discriminator_loss=0.6675253510475159\n",
            "step 6047: generator_loss=0.7362641096115112, discriminator_loss=0.6645004749298096\n",
            "step 6048: generator_loss=0.7222145199775696, discriminator_loss=0.6681661605834961\n",
            "step 6049: generator_loss=0.7191029191017151, discriminator_loss=0.6658358573913574\n",
            "step 6050: generator_loss=0.7141364216804504, discriminator_loss=0.6664584875106812\n",
            "step 6051: generator_loss=0.7056570053100586, discriminator_loss=0.6695947647094727\n",
            "step 6052: generator_loss=0.7077134847640991, discriminator_loss=0.6666209697723389\n",
            "step 6053: generator_loss=0.7012341022491455, discriminator_loss=0.6717912554740906\n",
            "step 6054: generator_loss=0.7016968727111816, discriminator_loss=0.6714180111885071\n",
            "step 6055: generator_loss=0.7086602449417114, discriminator_loss=0.6684015989303589\n",
            "step 6056: generator_loss=0.7075423002243042, discriminator_loss=0.6690713167190552\n",
            "step 6057: generator_loss=0.705021321773529, discriminator_loss=0.6716611981391907\n",
            "step 6058: generator_loss=0.7103723287582397, discriminator_loss=0.6703282594680786\n",
            "step 6059: generator_loss=0.707604706287384, discriminator_loss=0.6742942929267883\n",
            "step 6060: generator_loss=0.7095482349395752, discriminator_loss=0.6762475371360779\n",
            "step 6061: generator_loss=0.7122336030006409, discriminator_loss=0.676111102104187\n",
            "step 6062: generator_loss=0.7139549255371094, discriminator_loss=0.6763625144958496\n",
            "step 6063: generator_loss=0.7109737396240234, discriminator_loss=0.679466724395752\n",
            "step 6064: generator_loss=0.7071367502212524, discriminator_loss=0.6811139583587646\n",
            "step 6065: generator_loss=0.7031456232070923, discriminator_loss=0.6834623217582703\n",
            "step 6066: generator_loss=0.7027691006660461, discriminator_loss=0.6816948652267456\n",
            "step 6067: generator_loss=0.704023003578186, discriminator_loss=0.6811562776565552\n",
            "step 6068: generator_loss=0.7016565799713135, discriminator_loss=0.68220055103302\n",
            "step 6069: generator_loss=0.6950904726982117, discriminator_loss=0.6842669248580933\n",
            "step 6070: generator_loss=0.6965421438217163, discriminator_loss=0.6829179525375366\n",
            "step 6071: generator_loss=0.6948152780532837, discriminator_loss=0.6851431727409363\n",
            "step 6072: generator_loss=0.6918078660964966, discriminator_loss=0.6878976821899414\n",
            "step 6073: generator_loss=0.693092942237854, discriminator_loss=0.686692476272583\n",
            "step 6074: generator_loss=0.6914446949958801, discriminator_loss=0.6891446709632874\n",
            "step 6075: generator_loss=0.6924793124198914, discriminator_loss=0.68962562084198\n",
            "step 6076: generator_loss=0.6920360326766968, discriminator_loss=0.6908689737319946\n",
            "step 6077: generator_loss=0.6943359375, discriminator_loss=0.689045250415802\n",
            "step 6078: generator_loss=0.6945673823356628, discriminator_loss=0.6904826164245605\n",
            "step 6079: generator_loss=0.6954820156097412, discriminator_loss=0.6902163624763489\n",
            "step 6080: generator_loss=0.6964226365089417, discriminator_loss=0.691322922706604\n",
            "step 6081: generator_loss=0.6996821165084839, discriminator_loss=0.6907418966293335\n",
            "step 6082: generator_loss=0.70014488697052, discriminator_loss=0.6900959014892578\n",
            "step 6083: generator_loss=0.7026330828666687, discriminator_loss=0.6890833973884583\n",
            "step 6084: generator_loss=0.7047044634819031, discriminator_loss=0.6899296641349792\n",
            "step 6085: generator_loss=0.7044488787651062, discriminator_loss=0.688966691493988\n",
            "step 6086: generator_loss=0.699331521987915, discriminator_loss=0.691543698310852\n",
            "step 6087: generator_loss=0.6976231336593628, discriminator_loss=0.6924504637718201\n",
            "step 6088: generator_loss=0.6987063884735107, discriminator_loss=0.6906665563583374\n",
            "step 6089: generator_loss=0.6931651830673218, discriminator_loss=0.6925954222679138\n",
            "step 6090: generator_loss=0.6878959536552429, discriminator_loss=0.6943967342376709\n",
            "step 6091: generator_loss=0.6876338720321655, discriminator_loss=0.6936087608337402\n",
            "step 6092: generator_loss=0.6877058744430542, discriminator_loss=0.6934468746185303\n",
            "step 6093: generator_loss=0.691608190536499, discriminator_loss=0.6899117231369019\n",
            "step 6094: generator_loss=0.6915126442909241, discriminator_loss=0.6876236796379089\n",
            "step 6095: generator_loss=0.6911213397979736, discriminator_loss=0.6876610517501831\n",
            "step 6096: generator_loss=0.6993388533592224, discriminator_loss=0.6818766593933105\n",
            "step 6097: generator_loss=0.7024314403533936, discriminator_loss=0.6795142292976379\n",
            "step 6098: generator_loss=0.7091219425201416, discriminator_loss=0.6781302690505981\n",
            "step 6099: generator_loss=0.7182797193527222, discriminator_loss=0.6744669675827026\n",
            "step 6100: generator_loss=0.7228015661239624, discriminator_loss=0.6755304336547852\n",
            "step 6101: generator_loss=0.7260957956314087, discriminator_loss=0.6778241395950317\n",
            "step 6102: generator_loss=0.7264655828475952, discriminator_loss=0.6787304878234863\n",
            "step 6103: generator_loss=0.7233501076698303, discriminator_loss=0.6816656589508057\n",
            "step 6104: generator_loss=0.7236449718475342, discriminator_loss=0.680672824382782\n",
            "step 6105: generator_loss=0.7175397276878357, discriminator_loss=0.6817086935043335\n",
            "step 6106: generator_loss=0.7142534255981445, discriminator_loss=0.6810952425003052\n",
            "step 6107: generator_loss=0.7079779505729675, discriminator_loss=0.6815729141235352\n",
            "step 6108: generator_loss=0.6999077796936035, discriminator_loss=0.6839470863342285\n",
            "step 6109: generator_loss=0.7009806632995605, discriminator_loss=0.681668758392334\n",
            "step 6110: generator_loss=0.6948710680007935, discriminator_loss=0.6857953071594238\n",
            "step 6111: generator_loss=0.6915313005447388, discriminator_loss=0.6867310404777527\n",
            "step 6112: generator_loss=0.6864449977874756, discriminator_loss=0.6897625923156738\n",
            "step 6113: generator_loss=0.689128041267395, discriminator_loss=0.6883928179740906\n",
            "step 6114: generator_loss=0.6850270628929138, discriminator_loss=0.6923233270645142\n",
            "step 6115: generator_loss=0.6862196326255798, discriminator_loss=0.6928870677947998\n",
            "step 6116: generator_loss=0.6842978000640869, discriminator_loss=0.6953334808349609\n",
            "step 6117: generator_loss=0.6888715028762817, discriminator_loss=0.6952751874923706\n",
            "step 6118: generator_loss=0.6893601417541504, discriminator_loss=0.6960266828536987\n",
            "step 6119: generator_loss=0.6893274784088135, discriminator_loss=0.6977311968803406\n",
            "step 6120: generator_loss=0.6896650195121765, discriminator_loss=0.699472188949585\n",
            "step 6121: generator_loss=0.6876615285873413, discriminator_loss=0.7011170983314514\n",
            "step 6122: generator_loss=0.6873490810394287, discriminator_loss=0.7036689519882202\n",
            "step 6123: generator_loss=0.6839429140090942, discriminator_loss=0.7044404745101929\n",
            "step 6124: generator_loss=0.684370219707489, discriminator_loss=0.704987645149231\n",
            "step 6125: generator_loss=0.682017982006073, discriminator_loss=0.7060585021972656\n",
            "step 6126: generator_loss=0.6814675331115723, discriminator_loss=0.7065181732177734\n",
            "step 6127: generator_loss=0.6768890619277954, discriminator_loss=0.709187388420105\n",
            "step 6128: generator_loss=0.6743475198745728, discriminator_loss=0.709663987159729\n",
            "step 6129: generator_loss=0.6731395721435547, discriminator_loss=0.710905909538269\n",
            "step 6130: generator_loss=0.6718682050704956, discriminator_loss=0.7120786905288696\n",
            "step 6131: generator_loss=0.6674574017524719, discriminator_loss=0.7156558036804199\n",
            "step 6132: generator_loss=0.668550968170166, discriminator_loss=0.7138078212738037\n",
            "step 6133: generator_loss=0.6669403910636902, discriminator_loss=0.7144781351089478\n",
            "step 6134: generator_loss=0.6635537147521973, discriminator_loss=0.7186880707740784\n",
            "step 6135: generator_loss=0.6644858121871948, discriminator_loss=0.7178164124488831\n",
            "step 6136: generator_loss=0.6651339530944824, discriminator_loss=0.7187145948410034\n",
            "step 6137: generator_loss=0.6642590761184692, discriminator_loss=0.7213062047958374\n",
            "step 6138: generator_loss=0.6660353541374207, discriminator_loss=0.7208438515663147\n",
            "step 6139: generator_loss=0.667683482170105, discriminator_loss=0.7182745933532715\n",
            "step 6140: generator_loss=0.6642558574676514, discriminator_loss=0.7208983898162842\n",
            "step 6141: generator_loss=0.6623828411102295, discriminator_loss=0.7248952388763428\n",
            "step 6142: generator_loss=0.6640712022781372, discriminator_loss=0.7230411767959595\n",
            "step 6143: generator_loss=0.6660901308059692, discriminator_loss=0.7219483852386475\n",
            "step 6144: generator_loss=0.6627101898193359, discriminator_loss=0.7263501882553101\n",
            "step 6145: generator_loss=0.6636870503425598, discriminator_loss=0.725730836391449\n",
            "step 6146: generator_loss=0.6610143780708313, discriminator_loss=0.727933406829834\n",
            "step 6147: generator_loss=0.6636030673980713, discriminator_loss=0.7278960943222046\n",
            "step 6148: generator_loss=0.6614363193511963, discriminator_loss=0.7276429533958435\n",
            "step 6149: generator_loss=0.6593728065490723, discriminator_loss=0.7298967838287354\n",
            "step 6150: generator_loss=0.6570101380348206, discriminator_loss=0.7312082052230835\n",
            "step 6151: generator_loss=0.6543290615081787, discriminator_loss=0.7331233024597168\n",
            "step 6152: generator_loss=0.6498672962188721, discriminator_loss=0.7359048128128052\n",
            "step 6153: generator_loss=0.6521965265274048, discriminator_loss=0.7355376482009888\n",
            "step 6154: generator_loss=0.6508675813674927, discriminator_loss=0.7348836660385132\n",
            "step 6155: generator_loss=0.6463356614112854, discriminator_loss=0.7394658327102661\n",
            "step 6156: generator_loss=0.6460036039352417, discriminator_loss=0.7386879324913025\n",
            "step 6157: generator_loss=0.6494072675704956, discriminator_loss=0.7382330298423767\n",
            "step 6158: generator_loss=0.6443592309951782, discriminator_loss=0.7420175671577454\n",
            "step 6159: generator_loss=0.6477923393249512, discriminator_loss=0.7405432462692261\n",
            "step 6160: generator_loss=0.6473382115364075, discriminator_loss=0.740256130695343\n",
            "step 6161: generator_loss=0.6459935307502747, discriminator_loss=0.7420529723167419\n",
            "step 6162: generator_loss=0.64593106508255, discriminator_loss=0.7433445453643799\n",
            "step 6163: generator_loss=0.644622802734375, discriminator_loss=0.7455018758773804\n",
            "step 6164: generator_loss=0.6427785754203796, discriminator_loss=0.7455636858940125\n",
            "step 6165: generator_loss=0.6475390195846558, discriminator_loss=0.7441638708114624\n",
            "step 6166: generator_loss=0.6465399265289307, discriminator_loss=0.7451410293579102\n",
            "step 6167: generator_loss=0.6472834944725037, discriminator_loss=0.7449236512184143\n",
            "step 6168: generator_loss=0.64955735206604, discriminator_loss=0.7440434098243713\n",
            "step 6169: generator_loss=0.6493061780929565, discriminator_loss=0.743796706199646\n",
            "step 6170: generator_loss=0.649378776550293, discriminator_loss=0.7442149519920349\n",
            "step 6171: generator_loss=0.6525396108627319, discriminator_loss=0.7432671785354614\n",
            "step 6172: generator_loss=0.6527385711669922, discriminator_loss=0.7434144020080566\n",
            "step 6173: generator_loss=0.6510168313980103, discriminator_loss=0.7444462776184082\n",
            "step 6174: generator_loss=0.6513315439224243, discriminator_loss=0.7436599135398865\n",
            "step 6175: generator_loss=0.6474108099937439, discriminator_loss=0.7461017370223999\n",
            "step 6176: generator_loss=0.6492635011672974, discriminator_loss=0.7442961931228638\n",
            "step 6177: generator_loss=0.6501680612564087, discriminator_loss=0.7421963214874268\n",
            "step 6178: generator_loss=0.6509634256362915, discriminator_loss=0.7411372661590576\n",
            "step 6179: generator_loss=0.6513936519622803, discriminator_loss=0.7397056818008423\n",
            "step 6180: generator_loss=0.6560561060905457, discriminator_loss=0.7364459037780762\n",
            "step 6181: generator_loss=0.6595951318740845, discriminator_loss=0.7335360646247864\n",
            "step 6182: generator_loss=0.6665441989898682, discriminator_loss=0.7280166149139404\n",
            "step 6183: generator_loss=0.6706355810165405, discriminator_loss=0.7254036664962769\n",
            "step 6184: generator_loss=0.6765480041503906, discriminator_loss=0.7222356200218201\n",
            "step 6185: generator_loss=0.6805399656295776, discriminator_loss=0.7200381755828857\n",
            "step 6186: generator_loss=0.6808657646179199, discriminator_loss=0.7218412756919861\n",
            "step 6187: generator_loss=0.6851539611816406, discriminator_loss=0.7184204459190369\n",
            "step 6188: generator_loss=0.6845682859420776, discriminator_loss=0.7181546092033386\n",
            "step 6189: generator_loss=0.6845757961273193, discriminator_loss=0.7176815271377563\n",
            "step 6190: generator_loss=0.6830867528915405, discriminator_loss=0.7166363000869751\n",
            "step 6191: generator_loss=0.6799435615539551, discriminator_loss=0.7154504060745239\n",
            "step 6192: generator_loss=0.6778889894485474, discriminator_loss=0.7147512435913086\n",
            "step 6193: generator_loss=0.6773425936698914, discriminator_loss=0.7122805714607239\n",
            "step 6194: generator_loss=0.6748909950256348, discriminator_loss=0.7115564346313477\n",
            "step 6195: generator_loss=0.6740959882736206, discriminator_loss=0.7089023590087891\n",
            "step 6196: generator_loss=0.6740447282791138, discriminator_loss=0.7067914009094238\n",
            "step 6197: generator_loss=0.6780685186386108, discriminator_loss=0.7028603553771973\n",
            "step 6198: generator_loss=0.6770432591438293, discriminator_loss=0.7023882865905762\n",
            "step 6199: generator_loss=0.6803140044212341, discriminator_loss=0.6997923851013184\n",
            "step 6200: generator_loss=0.6844924092292786, discriminator_loss=0.697199821472168\n",
            "step 6201: generator_loss=0.6901724338531494, discriminator_loss=0.6939034461975098\n",
            "step 6202: generator_loss=0.6969772577285767, discriminator_loss=0.69206702709198\n",
            "step 6203: generator_loss=0.7054382562637329, discriminator_loss=0.6879794001579285\n",
            "step 6204: generator_loss=0.7163428068161011, discriminator_loss=0.6827090978622437\n",
            "step 6205: generator_loss=0.7216329574584961, discriminator_loss=0.6787140369415283\n",
            "step 6206: generator_loss=0.726474940776825, discriminator_loss=0.67487633228302\n",
            "step 6207: generator_loss=0.7272107601165771, discriminator_loss=0.6731280088424683\n",
            "step 6208: generator_loss=0.7237251996994019, discriminator_loss=0.6741979122161865\n",
            "step 6209: generator_loss=0.7311230301856995, discriminator_loss=0.6703577637672424\n",
            "step 6210: generator_loss=0.714836597442627, discriminator_loss=0.6770520210266113\n",
            "step 6211: generator_loss=0.7252852916717529, discriminator_loss=0.671960711479187\n",
            "step 6212: generator_loss=0.7271891832351685, discriminator_loss=0.6713510155677795\n",
            "step 6213: generator_loss=0.7187304496765137, discriminator_loss=0.6741251349449158\n",
            "step 6214: generator_loss=0.7149274349212646, discriminator_loss=0.6739831566810608\n",
            "step 6215: generator_loss=0.7193087339401245, discriminator_loss=0.6706459522247314\n",
            "step 6216: generator_loss=0.7034769058227539, discriminator_loss=0.677270770072937\n",
            "step 6217: generator_loss=0.7103042602539062, discriminator_loss=0.6719487905502319\n",
            "step 6218: generator_loss=0.6947190761566162, discriminator_loss=0.679502010345459\n",
            "step 6219: generator_loss=0.6885243654251099, discriminator_loss=0.6852142810821533\n",
            "step 6220: generator_loss=0.690019965171814, discriminator_loss=0.6847665905952454\n",
            "step 6221: generator_loss=0.6892037391662598, discriminator_loss=0.6854740381240845\n",
            "step 6222: generator_loss=0.6837388873100281, discriminator_loss=0.6905155777931213\n",
            "step 6223: generator_loss=0.6939254999160767, discriminator_loss=0.6859925985336304\n",
            "step 6224: generator_loss=0.6967882513999939, discriminator_loss=0.6866719722747803\n",
            "step 6225: generator_loss=0.7005800008773804, discriminator_loss=0.6849147081375122\n",
            "step 6226: generator_loss=0.6998531818389893, discriminator_loss=0.6843449473381042\n",
            "step 6227: generator_loss=0.7065972685813904, discriminator_loss=0.6811424493789673\n",
            "step 6228: generator_loss=0.7138911485671997, discriminator_loss=0.6757161617279053\n",
            "step 6229: generator_loss=0.7211533784866333, discriminator_loss=0.670444667339325\n",
            "step 6230: generator_loss=0.7212313413619995, discriminator_loss=0.6695611476898193\n",
            "step 6231: generator_loss=0.7283058166503906, discriminator_loss=0.6672767996788025\n",
            "step 6232: generator_loss=0.7312542796134949, discriminator_loss=0.6654834151268005\n",
            "step 6233: generator_loss=0.7371227145195007, discriminator_loss=0.6619430184364319\n",
            "step 6234: generator_loss=0.7422748804092407, discriminator_loss=0.6588558554649353\n",
            "step 6235: generator_loss=0.7477694749832153, discriminator_loss=0.6555414199829102\n",
            "step 6236: generator_loss=0.7479360103607178, discriminator_loss=0.6543269753456116\n",
            "step 6237: generator_loss=0.7455217838287354, discriminator_loss=0.654376208782196\n",
            "step 6238: generator_loss=0.7553383708000183, discriminator_loss=0.6463045477867126\n",
            "step 6239: generator_loss=0.740079402923584, discriminator_loss=0.6504331827163696\n",
            "step 6240: generator_loss=0.7417508363723755, discriminator_loss=0.6473914384841919\n",
            "step 6241: generator_loss=0.7537246942520142, discriminator_loss=0.63957679271698\n",
            "step 6242: generator_loss=0.7442530989646912, discriminator_loss=0.6417062282562256\n",
            "step 6243: generator_loss=0.7510172128677368, discriminator_loss=0.6416106820106506\n",
            "step 6244: generator_loss=0.7379372119903564, discriminator_loss=0.6481713056564331\n",
            "step 6245: generator_loss=0.7473011612892151, discriminator_loss=0.6435936093330383\n",
            "step 6246: generator_loss=0.7407860159873962, discriminator_loss=0.6458804607391357\n",
            "step 6247: generator_loss=0.7336421012878418, discriminator_loss=0.6500409245491028\n",
            "step 6248: generator_loss=0.7281495332717896, discriminator_loss=0.6514577269554138\n",
            "step 6249: generator_loss=0.7256516814231873, discriminator_loss=0.6533107757568359\n",
            "step 6250: generator_loss=0.718929648399353, discriminator_loss=0.6562822461128235\n",
            "step 6251: generator_loss=0.7076349854469299, discriminator_loss=0.6582604050636292\n",
            "step 6252: generator_loss=0.7104510068893433, discriminator_loss=0.6576948165893555\n",
            "step 6253: generator_loss=0.712937593460083, discriminator_loss=0.6561000347137451\n",
            "step 6254: generator_loss=0.7105624675750732, discriminator_loss=0.6581063866615295\n",
            "step 6255: generator_loss=0.7075730562210083, discriminator_loss=0.6599262952804565\n",
            "step 6256: generator_loss=0.710762619972229, discriminator_loss=0.659946620464325\n",
            "step 6257: generator_loss=0.7090506553649902, discriminator_loss=0.6621524095535278\n",
            "step 6258: generator_loss=0.7149168848991394, discriminator_loss=0.6633394360542297\n",
            "step 6259: generator_loss=0.7180805206298828, discriminator_loss=0.6619703769683838\n",
            "step 6260: generator_loss=0.7244372367858887, discriminator_loss=0.6637775897979736\n",
            "step 6261: generator_loss=0.7263723611831665, discriminator_loss=0.6652136445045471\n",
            "step 6262: generator_loss=0.7245450019836426, discriminator_loss=0.667439341545105\n",
            "step 6263: generator_loss=0.7205248475074768, discriminator_loss=0.6699836850166321\n",
            "step 6264: generator_loss=0.7186616063117981, discriminator_loss=0.6712146997451782\n",
            "step 6265: generator_loss=0.7076478004455566, discriminator_loss=0.67561936378479\n",
            "step 6266: generator_loss=0.711783766746521, discriminator_loss=0.674571692943573\n",
            "step 6267: generator_loss=0.7139732241630554, discriminator_loss=0.6732502579689026\n",
            "step 6268: generator_loss=0.7127825021743774, discriminator_loss=0.6735156774520874\n",
            "step 6269: generator_loss=0.7103325128555298, discriminator_loss=0.6758067607879639\n",
            "step 6270: generator_loss=0.711645781993866, discriminator_loss=0.6749557852745056\n",
            "step 6271: generator_loss=0.6979212164878845, discriminator_loss=0.683916449546814\n",
            "step 6272: generator_loss=0.6978881359100342, discriminator_loss=0.6841657757759094\n",
            "step 6273: generator_loss=0.6957225799560547, discriminator_loss=0.684772253036499\n",
            "step 6274: generator_loss=0.692939281463623, discriminator_loss=0.6874698400497437\n",
            "step 6275: generator_loss=0.6993381977081299, discriminator_loss=0.6843219995498657\n",
            "step 6276: generator_loss=0.6868365406990051, discriminator_loss=0.6895561218261719\n",
            "step 6277: generator_loss=0.6890778541564941, discriminator_loss=0.6891929507255554\n",
            "step 6278: generator_loss=0.6857155561447144, discriminator_loss=0.6924452781677246\n",
            "step 6279: generator_loss=0.6819061636924744, discriminator_loss=0.6966682076454163\n",
            "step 6280: generator_loss=0.6917034387588501, discriminator_loss=0.692899763584137\n",
            "step 6281: generator_loss=0.6842191219329834, discriminator_loss=0.7001900672912598\n",
            "step 6282: generator_loss=0.6996810436248779, discriminator_loss=0.6918785572052002\n",
            "step 6283: generator_loss=0.6838841438293457, discriminator_loss=0.7004296779632568\n",
            "step 6284: generator_loss=0.6902262568473816, discriminator_loss=0.6974997520446777\n",
            "step 6285: generator_loss=0.6942943334579468, discriminator_loss=0.6944268941879272\n",
            "step 6286: generator_loss=0.6883649230003357, discriminator_loss=0.6986944675445557\n",
            "step 6287: generator_loss=0.6811559200286865, discriminator_loss=0.7013729810714722\n",
            "step 6288: generator_loss=0.686499834060669, discriminator_loss=0.6988098621368408\n",
            "step 6289: generator_loss=0.6878489255905151, discriminator_loss=0.6959995031356812\n",
            "step 6290: generator_loss=0.6899863481521606, discriminator_loss=0.6930454969406128\n",
            "step 6291: generator_loss=0.704616367816925, discriminator_loss=0.6862745881080627\n",
            "step 6292: generator_loss=0.6886202096939087, discriminator_loss=0.6960697174072266\n",
            "step 6293: generator_loss=0.6952478885650635, discriminator_loss=0.6947818994522095\n",
            "step 6294: generator_loss=0.7026801109313965, discriminator_loss=0.6922048330307007\n",
            "step 6295: generator_loss=0.6845241785049438, discriminator_loss=0.70229572057724\n",
            "step 6296: generator_loss=0.7045156359672546, discriminator_loss=0.691741406917572\n",
            "step 6297: generator_loss=0.7037352323532104, discriminator_loss=0.6916244626045227\n",
            "step 6298: generator_loss=0.7024642825126648, discriminator_loss=0.6938539743423462\n",
            "step 6299: generator_loss=0.7081125974655151, discriminator_loss=0.691579282283783\n",
            "step 6300: generator_loss=0.6997987627983093, discriminator_loss=0.6947455406188965\n",
            "step 6301: generator_loss=0.6903579831123352, discriminator_loss=0.6987091898918152\n",
            "step 6302: generator_loss=0.693484902381897, discriminator_loss=0.6949813365936279\n",
            "step 6303: generator_loss=0.7038353085517883, discriminator_loss=0.6874150633811951\n",
            "step 6304: generator_loss=0.690305233001709, discriminator_loss=0.6921288967132568\n",
            "step 6305: generator_loss=0.7068776488304138, discriminator_loss=0.682758092880249\n",
            "step 6306: generator_loss=0.6832179427146912, discriminator_loss=0.6918015480041504\n",
            "step 6307: generator_loss=0.6990979909896851, discriminator_loss=0.6818053722381592\n",
            "step 6308: generator_loss=0.70646071434021, discriminator_loss=0.6775034070014954\n",
            "step 6309: generator_loss=0.7044403553009033, discriminator_loss=0.6783630847930908\n",
            "step 6310: generator_loss=0.6961054801940918, discriminator_loss=0.6829673051834106\n",
            "step 6311: generator_loss=0.7128357887268066, discriminator_loss=0.6764696836471558\n",
            "step 6312: generator_loss=0.7243261337280273, discriminator_loss=0.6718083024024963\n",
            "step 6313: generator_loss=0.7355912923812866, discriminator_loss=0.6658061146736145\n",
            "step 6314: generator_loss=0.739228367805481, discriminator_loss=0.6613876223564148\n",
            "step 6315: generator_loss=0.7256641983985901, discriminator_loss=0.6678401231765747\n",
            "step 6316: generator_loss=0.7430920600891113, discriminator_loss=0.6581220626831055\n",
            "step 6317: generator_loss=0.7353758215904236, discriminator_loss=0.6610670685768127\n",
            "step 6318: generator_loss=0.7406389713287354, discriminator_loss=0.6564617156982422\n",
            "step 6319: generator_loss=0.7409049868583679, discriminator_loss=0.6550965309143066\n",
            "step 6320: generator_loss=0.7589061260223389, discriminator_loss=0.6453543901443481\n",
            "step 6321: generator_loss=0.7313250303268433, discriminator_loss=0.6543596386909485\n",
            "step 6322: generator_loss=0.7433865070343018, discriminator_loss=0.6484066247940063\n",
            "step 6323: generator_loss=0.7433881759643555, discriminator_loss=0.6449757814407349\n",
            "step 6324: generator_loss=0.7416508197784424, discriminator_loss=0.6449286937713623\n",
            "step 6325: generator_loss=0.7474074363708496, discriminator_loss=0.6427865028381348\n",
            "step 6326: generator_loss=0.7618841528892517, discriminator_loss=0.639024019241333\n",
            "step 6327: generator_loss=0.7740274667739868, discriminator_loss=0.6351121068000793\n",
            "step 6328: generator_loss=0.7694703340530396, discriminator_loss=0.6362020969390869\n",
            "step 6329: generator_loss=0.7622624635696411, discriminator_loss=0.6397354602813721\n",
            "step 6330: generator_loss=0.7881276607513428, discriminator_loss=0.6288708448410034\n",
            "step 6331: generator_loss=0.7875803112983704, discriminator_loss=0.627505898475647\n",
            "step 6332: generator_loss=0.7666062116622925, discriminator_loss=0.6326857805252075\n",
            "step 6333: generator_loss=0.7848001718521118, discriminator_loss=0.6256582140922546\n",
            "step 6334: generator_loss=0.7670369148254395, discriminator_loss=0.6318208575248718\n",
            "step 6335: generator_loss=0.7616672515869141, discriminator_loss=0.6354164481163025\n",
            "step 6336: generator_loss=0.767684817314148, discriminator_loss=0.6322267651557922\n",
            "step 6337: generator_loss=0.7590094804763794, discriminator_loss=0.6368441581726074\n",
            "step 6338: generator_loss=0.7579869031906128, discriminator_loss=0.637816846370697\n",
            "step 6339: generator_loss=0.7655332088470459, discriminator_loss=0.635682225227356\n",
            "step 6340: generator_loss=0.7524241209030151, discriminator_loss=0.641932487487793\n",
            "step 6341: generator_loss=0.7504291534423828, discriminator_loss=0.6426894664764404\n",
            "step 6342: generator_loss=0.7473328113555908, discriminator_loss=0.6455553770065308\n",
            "step 6343: generator_loss=0.738520085811615, discriminator_loss=0.6488697528839111\n",
            "step 6344: generator_loss=0.7415335178375244, discriminator_loss=0.6492884159088135\n",
            "step 6345: generator_loss=0.7402266263961792, discriminator_loss=0.649666428565979\n",
            "step 6346: generator_loss=0.7318463325500488, discriminator_loss=0.6536463499069214\n",
            "step 6347: generator_loss=0.7298086285591125, discriminator_loss=0.6563974618911743\n",
            "step 6348: generator_loss=0.73106849193573, discriminator_loss=0.6562900543212891\n",
            "step 6349: generator_loss=0.7339046001434326, discriminator_loss=0.6564710140228271\n",
            "step 6350: generator_loss=0.7345389723777771, discriminator_loss=0.6566861867904663\n",
            "step 6351: generator_loss=0.7304450273513794, discriminator_loss=0.6612825989723206\n",
            "step 6352: generator_loss=0.7337459325790405, discriminator_loss=0.6608039140701294\n",
            "step 6353: generator_loss=0.7264514565467834, discriminator_loss=0.6660826802253723\n",
            "step 6354: generator_loss=0.7263123989105225, discriminator_loss=0.667504072189331\n",
            "step 6355: generator_loss=0.7228716611862183, discriminator_loss=0.6701149940490723\n",
            "step 6356: generator_loss=0.7182952761650085, discriminator_loss=0.6723793745040894\n",
            "step 6357: generator_loss=0.7086977362632751, discriminator_loss=0.6766006350517273\n",
            "step 6358: generator_loss=0.7044427990913391, discriminator_loss=0.6786901950836182\n",
            "step 6359: generator_loss=0.7002216577529907, discriminator_loss=0.6810042858123779\n",
            "step 6360: generator_loss=0.697404146194458, discriminator_loss=0.6830813884735107\n",
            "step 6361: generator_loss=0.6928139328956604, discriminator_loss=0.6851738095283508\n",
            "step 6362: generator_loss=0.6899687051773071, discriminator_loss=0.6861469745635986\n",
            "step 6363: generator_loss=0.6850247383117676, discriminator_loss=0.6911864876747131\n",
            "step 6364: generator_loss=0.6842925548553467, discriminator_loss=0.6921563148498535\n",
            "step 6365: generator_loss=0.683568000793457, discriminator_loss=0.6959577798843384\n",
            "step 6366: generator_loss=0.6814274191856384, discriminator_loss=0.698851466178894\n",
            "step 6367: generator_loss=0.682888388633728, discriminator_loss=0.7016931772232056\n",
            "step 6368: generator_loss=0.683279275894165, discriminator_loss=0.7032085657119751\n",
            "step 6369: generator_loss=0.683010458946228, discriminator_loss=0.7049458622932434\n",
            "step 6370: generator_loss=0.6816648840904236, discriminator_loss=0.7089300155639648\n",
            "step 6371: generator_loss=0.6795555949211121, discriminator_loss=0.7125958204269409\n",
            "step 6372: generator_loss=0.678094744682312, discriminator_loss=0.7118059396743774\n",
            "step 6373: generator_loss=0.6750065088272095, discriminator_loss=0.7147229909896851\n",
            "step 6374: generator_loss=0.6740127801895142, discriminator_loss=0.715148389339447\n",
            "step 6375: generator_loss=0.6710161566734314, discriminator_loss=0.7166686058044434\n",
            "step 6376: generator_loss=0.670244574546814, discriminator_loss=0.7179955244064331\n",
            "step 6377: generator_loss=0.6679143905639648, discriminator_loss=0.7173539400100708\n",
            "step 6378: generator_loss=0.6670083403587341, discriminator_loss=0.721116304397583\n",
            "step 6379: generator_loss=0.664182186126709, discriminator_loss=0.7221709489822388\n",
            "step 6380: generator_loss=0.6634209752082825, discriminator_loss=0.7236426472663879\n",
            "step 6381: generator_loss=0.6604889631271362, discriminator_loss=0.7272752523422241\n",
            "step 6382: generator_loss=0.6594751477241516, discriminator_loss=0.7288640737533569\n",
            "step 6383: generator_loss=0.6558342576026917, discriminator_loss=0.7312400937080383\n",
            "step 6384: generator_loss=0.6530699729919434, discriminator_loss=0.7331113815307617\n",
            "step 6385: generator_loss=0.6508010625839233, discriminator_loss=0.735073447227478\n",
            "step 6386: generator_loss=0.6471388339996338, discriminator_loss=0.7377090454101562\n",
            "step 6387: generator_loss=0.6452594995498657, discriminator_loss=0.739192545413971\n",
            "step 6388: generator_loss=0.6469464898109436, discriminator_loss=0.7411595582962036\n",
            "step 6389: generator_loss=0.6463630199432373, discriminator_loss=0.7402443885803223\n",
            "step 6390: generator_loss=0.6501083374023438, discriminator_loss=0.7388852834701538\n",
            "step 6391: generator_loss=0.6526698470115662, discriminator_loss=0.7388678789138794\n",
            "step 6392: generator_loss=0.6525264978408813, discriminator_loss=0.7415133118629456\n",
            "step 6393: generator_loss=0.6536954641342163, discriminator_loss=0.7425161600112915\n",
            "step 6394: generator_loss=0.6583555936813354, discriminator_loss=0.7425364255905151\n",
            "step 6395: generator_loss=0.6503438353538513, discriminator_loss=0.7465778589248657\n",
            "step 6396: generator_loss=0.6457374095916748, discriminator_loss=0.7502260208129883\n",
            "step 6397: generator_loss=0.6411488056182861, discriminator_loss=0.753547191619873\n",
            "step 6398: generator_loss=0.6367815136909485, discriminator_loss=0.7533431649208069\n",
            "step 6399: generator_loss=0.6256449818611145, discriminator_loss=0.7601480484008789\n",
            "step 6400: generator_loss=0.6276910901069641, discriminator_loss=0.7572938799858093\n",
            "step 6401: generator_loss=0.6213341355323792, discriminator_loss=0.7592611312866211\n",
            "step 6402: generator_loss=0.6160469651222229, discriminator_loss=0.7608004212379456\n",
            "step 6403: generator_loss=0.6177187561988831, discriminator_loss=0.7593063116073608\n",
            "step 6404: generator_loss=0.6196645498275757, discriminator_loss=0.75767982006073\n",
            "step 6405: generator_loss=0.6206299066543579, discriminator_loss=0.7560354471206665\n",
            "step 6406: generator_loss=0.6188980937004089, discriminator_loss=0.7562881708145142\n",
            "step 6407: generator_loss=0.628077507019043, discriminator_loss=0.750027596950531\n",
            "step 6408: generator_loss=0.6351797580718994, discriminator_loss=0.7459384799003601\n",
            "step 6409: generator_loss=0.6405479907989502, discriminator_loss=0.7408812046051025\n",
            "step 6410: generator_loss=0.643751859664917, discriminator_loss=0.7393619418144226\n",
            "step 6411: generator_loss=0.6583001017570496, discriminator_loss=0.7334833145141602\n",
            "step 6412: generator_loss=0.6681389808654785, discriminator_loss=0.7295581102371216\n",
            "step 6413: generator_loss=0.6751439571380615, discriminator_loss=0.727185070514679\n",
            "step 6414: generator_loss=0.671892523765564, discriminator_loss=0.7307920455932617\n",
            "step 6415: generator_loss=0.6769753694534302, discriminator_loss=0.7277873754501343\n",
            "step 6416: generator_loss=0.6804579496383667, discriminator_loss=0.7243771553039551\n",
            "step 6417: generator_loss=0.6835453510284424, discriminator_loss=0.7218355536460876\n",
            "step 6418: generator_loss=0.6781584620475769, discriminator_loss=0.7227241396903992\n",
            "step 6419: generator_loss=0.6744873523712158, discriminator_loss=0.720900297164917\n",
            "step 6420: generator_loss=0.6783308386802673, discriminator_loss=0.7150602340698242\n",
            "step 6421: generator_loss=0.6770155429840088, discriminator_loss=0.7130507230758667\n",
            "step 6422: generator_loss=0.6673293709754944, discriminator_loss=0.7147773504257202\n",
            "step 6423: generator_loss=0.6691173315048218, discriminator_loss=0.7126486301422119\n",
            "step 6424: generator_loss=0.6701271533966064, discriminator_loss=0.7121845483779907\n",
            "step 6425: generator_loss=0.6701467037200928, discriminator_loss=0.7119907736778259\n",
            "step 6426: generator_loss=0.6707830429077148, discriminator_loss=0.7099772095680237\n",
            "step 6427: generator_loss=0.6686055064201355, discriminator_loss=0.7124648094177246\n",
            "step 6428: generator_loss=0.6787885427474976, discriminator_loss=0.7079042196273804\n",
            "step 6429: generator_loss=0.6817247271537781, discriminator_loss=0.7057459950447083\n",
            "step 6430: generator_loss=0.6821251511573792, discriminator_loss=0.7048103213310242\n",
            "step 6431: generator_loss=0.6803441047668457, discriminator_loss=0.7044380903244019\n",
            "step 6432: generator_loss=0.6769506931304932, discriminator_loss=0.7058537006378174\n",
            "step 6433: generator_loss=0.6850671172142029, discriminator_loss=0.7008066177368164\n",
            "step 6434: generator_loss=0.6937698721885681, discriminator_loss=0.6977855563163757\n",
            "step 6435: generator_loss=0.687053918838501, discriminator_loss=0.7018551230430603\n",
            "step 6436: generator_loss=0.6911207437515259, discriminator_loss=0.6986808180809021\n",
            "step 6437: generator_loss=0.6927211284637451, discriminator_loss=0.6973578929901123\n",
            "step 6438: generator_loss=0.6947839856147766, discriminator_loss=0.695163369178772\n",
            "step 6439: generator_loss=0.7020774483680725, discriminator_loss=0.6900783777236938\n",
            "step 6440: generator_loss=0.7145544290542603, discriminator_loss=0.6841895580291748\n",
            "step 6441: generator_loss=0.7241241335868835, discriminator_loss=0.6795622110366821\n",
            "step 6442: generator_loss=0.7373141050338745, discriminator_loss=0.6727939248085022\n",
            "step 6443: generator_loss=0.7449312210083008, discriminator_loss=0.6692342162132263\n",
            "step 6444: generator_loss=0.7558976411819458, discriminator_loss=0.6652401089668274\n",
            "step 6445: generator_loss=0.7596597671508789, discriminator_loss=0.6625173091888428\n",
            "step 6446: generator_loss=0.7624185681343079, discriminator_loss=0.6594114899635315\n",
            "step 6447: generator_loss=0.7580034732818604, discriminator_loss=0.6609727740287781\n",
            "step 6448: generator_loss=0.7540959119796753, discriminator_loss=0.6609343886375427\n",
            "step 6449: generator_loss=0.7465137839317322, discriminator_loss=0.6595582365989685\n",
            "step 6450: generator_loss=0.7399593591690063, discriminator_loss=0.6585183143615723\n",
            "step 6451: generator_loss=0.7272505760192871, discriminator_loss=0.659740686416626\n",
            "step 6452: generator_loss=0.7196345925331116, discriminator_loss=0.6605573296546936\n",
            "step 6453: generator_loss=0.7114502191543579, discriminator_loss=0.6589405536651611\n",
            "step 6454: generator_loss=0.7059051394462585, discriminator_loss=0.6612301468849182\n",
            "step 6455: generator_loss=0.7053234577178955, discriminator_loss=0.6592426300048828\n",
            "step 6456: generator_loss=0.7084610462188721, discriminator_loss=0.6574941277503967\n",
            "step 6457: generator_loss=0.7105658650398254, discriminator_loss=0.6584627628326416\n",
            "step 6458: generator_loss=0.7188356518745422, discriminator_loss=0.6563510894775391\n",
            "step 6459: generator_loss=0.7269902229309082, discriminator_loss=0.6569148302078247\n",
            "step 6460: generator_loss=0.7359287738800049, discriminator_loss=0.6564937233924866\n",
            "step 6461: generator_loss=0.7454228401184082, discriminator_loss=0.6563589572906494\n",
            "step 6462: generator_loss=0.7513190507888794, discriminator_loss=0.6569322943687439\n",
            "step 6463: generator_loss=0.7588516473770142, discriminator_loss=0.6520260572433472\n",
            "step 6464: generator_loss=0.7616422176361084, discriminator_loss=0.6504034399986267\n",
            "step 6465: generator_loss=0.7638890147209167, discriminator_loss=0.6491341590881348\n",
            "step 6466: generator_loss=0.7667360305786133, discriminator_loss=0.6464422941207886\n",
            "step 6467: generator_loss=0.7680917978286743, discriminator_loss=0.644300639629364\n",
            "step 6468: generator_loss=0.7648581266403198, discriminator_loss=0.6415671110153198\n",
            "step 6469: generator_loss=0.7607334852218628, discriminator_loss=0.6398510932922363\n",
            "step 6470: generator_loss=0.7523871660232544, discriminator_loss=0.6391148567199707\n",
            "step 6471: generator_loss=0.7613953351974487, discriminator_loss=0.6318454146385193\n",
            "step 6472: generator_loss=0.7536834478378296, discriminator_loss=0.6341172456741333\n",
            "step 6473: generator_loss=0.7517274618148804, discriminator_loss=0.6354905366897583\n",
            "step 6474: generator_loss=0.7489117383956909, discriminator_loss=0.6375873684883118\n",
            "step 6475: generator_loss=0.7413291335105896, discriminator_loss=0.6399100422859192\n",
            "step 6476: generator_loss=0.7453762888908386, discriminator_loss=0.6397449970245361\n",
            "step 6477: generator_loss=0.7466248273849487, discriminator_loss=0.6399397850036621\n",
            "step 6478: generator_loss=0.7478066682815552, discriminator_loss=0.6392490863800049\n",
            "step 6479: generator_loss=0.7478675842285156, discriminator_loss=0.639072060585022\n",
            "step 6480: generator_loss=0.7615466117858887, discriminator_loss=0.6336163878440857\n",
            "step 6481: generator_loss=0.7539156675338745, discriminator_loss=0.6380535364151001\n",
            "step 6482: generator_loss=0.7508478164672852, discriminator_loss=0.635261058807373\n",
            "step 6483: generator_loss=0.7511000633239746, discriminator_loss=0.6398730874061584\n",
            "step 6484: generator_loss=0.7542017102241516, discriminator_loss=0.6364471316337585\n",
            "step 6485: generator_loss=0.7582598924636841, discriminator_loss=0.6370705366134644\n",
            "step 6486: generator_loss=0.7588579058647156, discriminator_loss=0.6382929086685181\n",
            "step 6487: generator_loss=0.7532535195350647, discriminator_loss=0.6396796703338623\n",
            "step 6488: generator_loss=0.7547547817230225, discriminator_loss=0.6401966214179993\n",
            "step 6489: generator_loss=0.7528861165046692, discriminator_loss=0.6426663398742676\n",
            "step 6490: generator_loss=0.748088002204895, discriminator_loss=0.6441465616226196\n",
            "step 6491: generator_loss=0.746446967124939, discriminator_loss=0.6445010304450989\n",
            "step 6492: generator_loss=0.7371969223022461, discriminator_loss=0.6505772471427917\n",
            "step 6493: generator_loss=0.738893985748291, discriminator_loss=0.6503603458404541\n",
            "step 6494: generator_loss=0.7371671199798584, discriminator_loss=0.6504116654396057\n",
            "step 6495: generator_loss=0.7317140698432922, discriminator_loss=0.6543155312538147\n",
            "step 6496: generator_loss=0.7281540036201477, discriminator_loss=0.6569570302963257\n",
            "step 6497: generator_loss=0.7220256328582764, discriminator_loss=0.6614401936531067\n",
            "step 6498: generator_loss=0.7257819175720215, discriminator_loss=0.6620246767997742\n",
            "step 6499: generator_loss=0.7146228551864624, discriminator_loss=0.6690196990966797\n",
            "step 6500: generator_loss=0.7118619680404663, discriminator_loss=0.6739684343338013\n",
            "step 6501: generator_loss=0.7089656591415405, discriminator_loss=0.6738765239715576\n",
            "step 6502: generator_loss=0.7072415351867676, discriminator_loss=0.6763705015182495\n",
            "step 6503: generator_loss=0.6988621950149536, discriminator_loss=0.6824535131454468\n",
            "step 6504: generator_loss=0.6972392201423645, discriminator_loss=0.684357762336731\n",
            "step 6505: generator_loss=0.6957786083221436, discriminator_loss=0.6866079568862915\n",
            "step 6506: generator_loss=0.6868276596069336, discriminator_loss=0.6939220428466797\n",
            "step 6507: generator_loss=0.6838729381561279, discriminator_loss=0.6983441710472107\n",
            "step 6508: generator_loss=0.6884775161743164, discriminator_loss=0.6979053020477295\n",
            "step 6509: generator_loss=0.682938277721405, discriminator_loss=0.7028045654296875\n",
            "step 6510: generator_loss=0.6741293668746948, discriminator_loss=0.7082945704460144\n",
            "step 6511: generator_loss=0.6818655133247375, discriminator_loss=0.7037733793258667\n",
            "step 6512: generator_loss=0.6755420565605164, discriminator_loss=0.7099027633666992\n",
            "step 6513: generator_loss=0.6787530779838562, discriminator_loss=0.710762619972229\n",
            "step 6514: generator_loss=0.6761716604232788, discriminator_loss=0.7131404876708984\n",
            "step 6515: generator_loss=0.6731736660003662, discriminator_loss=0.7158175110816956\n",
            "step 6516: generator_loss=0.6662743091583252, discriminator_loss=0.7222039103507996\n",
            "step 6517: generator_loss=0.669647753238678, discriminator_loss=0.7198232412338257\n",
            "step 6518: generator_loss=0.658146321773529, discriminator_loss=0.7265473008155823\n",
            "step 6519: generator_loss=0.6616906523704529, discriminator_loss=0.7254824638366699\n",
            "step 6520: generator_loss=0.6495633125305176, discriminator_loss=0.7311010360717773\n",
            "step 6521: generator_loss=0.6497795581817627, discriminator_loss=0.7324330806732178\n",
            "step 6522: generator_loss=0.6379836797714233, discriminator_loss=0.7359911203384399\n",
            "step 6523: generator_loss=0.6383081674575806, discriminator_loss=0.7368645668029785\n",
            "step 6524: generator_loss=0.6367945671081543, discriminator_loss=0.7401288747787476\n",
            "step 6525: generator_loss=0.6376799941062927, discriminator_loss=0.7382441759109497\n",
            "step 6526: generator_loss=0.6355576515197754, discriminator_loss=0.7420141696929932\n",
            "step 6527: generator_loss=0.6411327719688416, discriminator_loss=0.741584837436676\n",
            "step 6528: generator_loss=0.6423046588897705, discriminator_loss=0.743929386138916\n",
            "step 6529: generator_loss=0.6502206325531006, discriminator_loss=0.7405273914337158\n",
            "step 6530: generator_loss=0.6536840200424194, discriminator_loss=0.7405332326889038\n",
            "step 6531: generator_loss=0.6527595520019531, discriminator_loss=0.7398515939712524\n",
            "step 6532: generator_loss=0.6488103270530701, discriminator_loss=0.7397369146347046\n",
            "step 6533: generator_loss=0.6411257386207581, discriminator_loss=0.741729736328125\n",
            "step 6534: generator_loss=0.646399736404419, discriminator_loss=0.7341147065162659\n",
            "step 6535: generator_loss=0.6477792263031006, discriminator_loss=0.7281680107116699\n",
            "step 6536: generator_loss=0.642951250076294, discriminator_loss=0.724929928779602\n",
            "step 6537: generator_loss=0.6512612104415894, discriminator_loss=0.7148473858833313\n",
            "step 6538: generator_loss=0.6570405960083008, discriminator_loss=0.7087479829788208\n",
            "step 6539: generator_loss=0.6663892269134521, discriminator_loss=0.7008200883865356\n",
            "step 6540: generator_loss=0.6768370866775513, discriminator_loss=0.6976356506347656\n",
            "step 6541: generator_loss=0.685971200466156, discriminator_loss=0.6954367756843567\n",
            "step 6542: generator_loss=0.7052575349807739, discriminator_loss=0.6900601387023926\n",
            "step 6543: generator_loss=0.7098820805549622, discriminator_loss=0.6896302700042725\n",
            "step 6544: generator_loss=0.7147635221481323, discriminator_loss=0.6895250678062439\n",
            "step 6545: generator_loss=0.7175229787826538, discriminator_loss=0.686696469783783\n",
            "step 6546: generator_loss=0.721320390701294, discriminator_loss=0.6822024583816528\n",
            "step 6547: generator_loss=0.7169657945632935, discriminator_loss=0.6822599172592163\n",
            "step 6548: generator_loss=0.7129787802696228, discriminator_loss=0.6809557676315308\n",
            "step 6549: generator_loss=0.711066722869873, discriminator_loss=0.6788004636764526\n",
            "step 6550: generator_loss=0.7014098167419434, discriminator_loss=0.6796523332595825\n",
            "step 6551: generator_loss=0.7018160820007324, discriminator_loss=0.6751596927642822\n",
            "step 6552: generator_loss=0.6971482634544373, discriminator_loss=0.6757891178131104\n",
            "step 6553: generator_loss=0.694311797618866, discriminator_loss=0.6766386032104492\n",
            "step 6554: generator_loss=0.6967409253120422, discriminator_loss=0.6760632991790771\n",
            "step 6555: generator_loss=0.7022086381912231, discriminator_loss=0.6738322973251343\n",
            "step 6556: generator_loss=0.6957676410675049, discriminator_loss=0.6773150563240051\n",
            "step 6557: generator_loss=0.7090136408805847, discriminator_loss=0.6706741452217102\n",
            "step 6558: generator_loss=0.7071045637130737, discriminator_loss=0.6717060804367065\n",
            "step 6559: generator_loss=0.7165688872337341, discriminator_loss=0.6664927005767822\n",
            "step 6560: generator_loss=0.7217885255813599, discriminator_loss=0.6647836565971375\n",
            "step 6561: generator_loss=0.7279199361801147, discriminator_loss=0.662688672542572\n",
            "step 6562: generator_loss=0.7297521829605103, discriminator_loss=0.6636977195739746\n",
            "step 6563: generator_loss=0.7356771230697632, discriminator_loss=0.6619153022766113\n",
            "step 6564: generator_loss=0.7378263473510742, discriminator_loss=0.661891520023346\n",
            "step 6565: generator_loss=0.745156466960907, discriminator_loss=0.6595555543899536\n",
            "step 6566: generator_loss=0.7393592596054077, discriminator_loss=0.6608258485794067\n",
            "step 6567: generator_loss=0.7462546229362488, discriminator_loss=0.6582098007202148\n",
            "step 6568: generator_loss=0.7434611320495605, discriminator_loss=0.6572758555412292\n",
            "step 6569: generator_loss=0.736793041229248, discriminator_loss=0.6599841117858887\n",
            "step 6570: generator_loss=0.7307969927787781, discriminator_loss=0.6601836085319519\n",
            "step 6571: generator_loss=0.7282322645187378, discriminator_loss=0.6597669124603271\n",
            "step 6572: generator_loss=0.7242676019668579, discriminator_loss=0.6610756516456604\n",
            "step 6573: generator_loss=0.7098449468612671, discriminator_loss=0.6674639582633972\n",
            "step 6574: generator_loss=0.7129941582679749, discriminator_loss=0.6651086211204529\n",
            "step 6575: generator_loss=0.7190727591514587, discriminator_loss=0.6609944701194763\n",
            "step 6576: generator_loss=0.7165895104408264, discriminator_loss=0.6633725166320801\n",
            "step 6577: generator_loss=0.713822603225708, discriminator_loss=0.664628803730011\n",
            "step 6578: generator_loss=0.7136873006820679, discriminator_loss=0.66554194688797\n",
            "step 6579: generator_loss=0.7061347365379333, discriminator_loss=0.6700617074966431\n",
            "step 6580: generator_loss=0.7053560018539429, discriminator_loss=0.6723947525024414\n",
            "step 6581: generator_loss=0.7139948010444641, discriminator_loss=0.6690501570701599\n",
            "step 6582: generator_loss=0.7185842990875244, discriminator_loss=0.668544352054596\n",
            "step 6583: generator_loss=0.7251570820808411, discriminator_loss=0.6651808023452759\n",
            "step 6584: generator_loss=0.7183116674423218, discriminator_loss=0.6703627109527588\n",
            "step 6585: generator_loss=0.715248703956604, discriminator_loss=0.6729327440261841\n",
            "step 6586: generator_loss=0.7164876461029053, discriminator_loss=0.6723647117614746\n",
            "step 6587: generator_loss=0.7259182929992676, discriminator_loss=0.6684157252311707\n",
            "step 6588: generator_loss=0.7247881889343262, discriminator_loss=0.6688710451126099\n",
            "step 6589: generator_loss=0.7225796580314636, discriminator_loss=0.6694788932800293\n",
            "step 6590: generator_loss=0.7196410298347473, discriminator_loss=0.6706745028495789\n",
            "step 6591: generator_loss=0.7159221768379211, discriminator_loss=0.6715373396873474\n",
            "step 6592: generator_loss=0.7160619497299194, discriminator_loss=0.6722166538238525\n",
            "step 6593: generator_loss=0.7143182158470154, discriminator_loss=0.673203706741333\n",
            "step 6594: generator_loss=0.7188730239868164, discriminator_loss=0.6701382398605347\n",
            "step 6595: generator_loss=0.710029125213623, discriminator_loss=0.6742056608200073\n",
            "step 6596: generator_loss=0.7079074382781982, discriminator_loss=0.6754406690597534\n",
            "step 6597: generator_loss=0.7043274641036987, discriminator_loss=0.675879955291748\n",
            "step 6598: generator_loss=0.7039036154747009, discriminator_loss=0.6758641600608826\n",
            "step 6599: generator_loss=0.7027815580368042, discriminator_loss=0.6764809489250183\n",
            "step 6600: generator_loss=0.7034534215927124, discriminator_loss=0.6759665012359619\n",
            "step 6601: generator_loss=0.7006613612174988, discriminator_loss=0.6758723258972168\n",
            "step 6602: generator_loss=0.6992276906967163, discriminator_loss=0.6779440641403198\n",
            "step 6603: generator_loss=0.7070626616477966, discriminator_loss=0.6750288009643555\n",
            "step 6604: generator_loss=0.7088227272033691, discriminator_loss=0.6756899356842041\n",
            "step 6605: generator_loss=0.717365026473999, discriminator_loss=0.6724327802658081\n",
            "step 6606: generator_loss=0.7197891473770142, discriminator_loss=0.6740361452102661\n",
            "step 6607: generator_loss=0.7214558124542236, discriminator_loss=0.6741559505462646\n",
            "step 6608: generator_loss=0.725677490234375, discriminator_loss=0.6717590093612671\n",
            "step 6609: generator_loss=0.72395920753479, discriminator_loss=0.6714003682136536\n",
            "step 6610: generator_loss=0.7170456647872925, discriminator_loss=0.6726689338684082\n",
            "step 6611: generator_loss=0.7158617377281189, discriminator_loss=0.6710116863250732\n",
            "step 6612: generator_loss=0.7088177800178528, discriminator_loss=0.6710246801376343\n",
            "step 6613: generator_loss=0.7059822082519531, discriminator_loss=0.6723394393920898\n",
            "step 6614: generator_loss=0.7050609588623047, discriminator_loss=0.6728720664978027\n",
            "step 6615: generator_loss=0.7001000046730042, discriminator_loss=0.6738104224205017\n",
            "step 6616: generator_loss=0.7028954029083252, discriminator_loss=0.6730778813362122\n",
            "step 6617: generator_loss=0.7045606970787048, discriminator_loss=0.6726217269897461\n",
            "step 6618: generator_loss=0.7053508758544922, discriminator_loss=0.6728622913360596\n",
            "step 6619: generator_loss=0.7090756297111511, discriminator_loss=0.6716502904891968\n",
            "step 6620: generator_loss=0.7110825181007385, discriminator_loss=0.6697062253952026\n",
            "step 6621: generator_loss=0.7139350771903992, discriminator_loss=0.6715843677520752\n",
            "step 6622: generator_loss=0.7171480655670166, discriminator_loss=0.6713699102401733\n",
            "step 6623: generator_loss=0.7193713784217834, discriminator_loss=0.6704540252685547\n",
            "step 6624: generator_loss=0.718407154083252, discriminator_loss=0.6713330745697021\n",
            "step 6625: generator_loss=0.7169188261032104, discriminator_loss=0.6727889776229858\n",
            "step 6626: generator_loss=0.7114741802215576, discriminator_loss=0.6756660342216492\n",
            "step 6627: generator_loss=0.7165742516517639, discriminator_loss=0.6716063022613525\n",
            "step 6628: generator_loss=0.7110624313354492, discriminator_loss=0.6755553483963013\n",
            "step 6629: generator_loss=0.7121375799179077, discriminator_loss=0.6765521764755249\n",
            "step 6630: generator_loss=0.7070082426071167, discriminator_loss=0.6779953837394714\n",
            "step 6631: generator_loss=0.7043317556381226, discriminator_loss=0.6783878207206726\n",
            "step 6632: generator_loss=0.6999050378799438, discriminator_loss=0.6808350682258606\n",
            "step 6633: generator_loss=0.7027891278266907, discriminator_loss=0.6809566020965576\n",
            "step 6634: generator_loss=0.6992846131324768, discriminator_loss=0.682349681854248\n",
            "step 6635: generator_loss=0.7030819058418274, discriminator_loss=0.6802593469619751\n",
            "step 6636: generator_loss=0.6984611749649048, discriminator_loss=0.6849098801612854\n",
            "step 6637: generator_loss=0.7028511166572571, discriminator_loss=0.685032069683075\n",
            "step 6638: generator_loss=0.702775239944458, discriminator_loss=0.686780571937561\n",
            "step 6639: generator_loss=0.7031607627868652, discriminator_loss=0.685631513595581\n",
            "step 6640: generator_loss=0.7038865089416504, discriminator_loss=0.6870865821838379\n",
            "step 6641: generator_loss=0.7105627059936523, discriminator_loss=0.6841115951538086\n",
            "step 6642: generator_loss=0.707405686378479, discriminator_loss=0.6862058639526367\n",
            "step 6643: generator_loss=0.7116646766662598, discriminator_loss=0.6844285726547241\n",
            "step 6644: generator_loss=0.7127634286880493, discriminator_loss=0.6836011409759521\n",
            "step 6645: generator_loss=0.7088115811347961, discriminator_loss=0.6860607266426086\n",
            "step 6646: generator_loss=0.705764651298523, discriminator_loss=0.6877921223640442\n",
            "step 6647: generator_loss=0.7072548866271973, discriminator_loss=0.6860392093658447\n",
            "step 6648: generator_loss=0.701189398765564, discriminator_loss=0.6874549388885498\n",
            "step 6649: generator_loss=0.697725772857666, discriminator_loss=0.6900156736373901\n",
            "step 6650: generator_loss=0.694736123085022, discriminator_loss=0.6904130578041077\n",
            "step 6651: generator_loss=0.6896533966064453, discriminator_loss=0.6931201219558716\n",
            "step 6652: generator_loss=0.6866539716720581, discriminator_loss=0.6939955949783325\n",
            "step 6653: generator_loss=0.6835584044456482, discriminator_loss=0.694571316242218\n",
            "step 6654: generator_loss=0.683649480342865, discriminator_loss=0.6963642835617065\n",
            "step 6655: generator_loss=0.6838057637214661, discriminator_loss=0.696163535118103\n",
            "step 6656: generator_loss=0.6846550703048706, discriminator_loss=0.697648286819458\n",
            "step 6657: generator_loss=0.6855272054672241, discriminator_loss=0.6988579034805298\n",
            "step 6658: generator_loss=0.6814717650413513, discriminator_loss=0.70374596118927\n",
            "step 6659: generator_loss=0.684441328048706, discriminator_loss=0.7022944688796997\n",
            "step 6660: generator_loss=0.6855974197387695, discriminator_loss=0.7036744356155396\n",
            "step 6661: generator_loss=0.6875790953636169, discriminator_loss=0.703265905380249\n",
            "step 6662: generator_loss=0.6858516335487366, discriminator_loss=0.704221248626709\n",
            "step 6663: generator_loss=0.6857872009277344, discriminator_loss=0.7049261331558228\n",
            "step 6664: generator_loss=0.684482991695404, discriminator_loss=0.70671147108078\n",
            "step 6665: generator_loss=0.6855778694152832, discriminator_loss=0.704927921295166\n",
            "step 6666: generator_loss=0.684772253036499, discriminator_loss=0.7037615776062012\n",
            "step 6667: generator_loss=0.6863616108894348, discriminator_loss=0.7030339241027832\n",
            "step 6668: generator_loss=0.6860240697860718, discriminator_loss=0.7019767761230469\n",
            "step 6669: generator_loss=0.6867893934249878, discriminator_loss=0.7024655342102051\n",
            "step 6670: generator_loss=0.68680739402771, discriminator_loss=0.7042601108551025\n",
            "step 6671: generator_loss=0.6870138645172119, discriminator_loss=0.7036938667297363\n",
            "step 6672: generator_loss=0.686897873878479, discriminator_loss=0.7039276957511902\n",
            "step 6673: generator_loss=0.6867752075195312, discriminator_loss=0.7029026746749878\n",
            "step 6674: generator_loss=0.6860997676849365, discriminator_loss=0.7048235535621643\n",
            "step 6675: generator_loss=0.6875700354576111, discriminator_loss=0.7010787725448608\n",
            "step 6676: generator_loss=0.6865646839141846, discriminator_loss=0.7016223669052124\n",
            "step 6677: generator_loss=0.686809778213501, discriminator_loss=0.700445830821991\n",
            "step 6678: generator_loss=0.686734676361084, discriminator_loss=0.6984611749649048\n",
            "step 6679: generator_loss=0.687932014465332, discriminator_loss=0.6993333697319031\n",
            "step 6680: generator_loss=0.6905349493026733, discriminator_loss=0.6971850395202637\n",
            "step 6681: generator_loss=0.690655529499054, discriminator_loss=0.6988520622253418\n",
            "step 6682: generator_loss=0.688032865524292, discriminator_loss=0.7024708390235901\n",
            "step 6683: generator_loss=0.6880812644958496, discriminator_loss=0.7023488283157349\n",
            "step 6684: generator_loss=0.6862692832946777, discriminator_loss=0.7028416991233826\n",
            "step 6685: generator_loss=0.6818119883537292, discriminator_loss=0.7059788703918457\n",
            "step 6686: generator_loss=0.6790329217910767, discriminator_loss=0.7078182697296143\n",
            "step 6687: generator_loss=0.6782901883125305, discriminator_loss=0.7077983617782593\n",
            "step 6688: generator_loss=0.674646258354187, discriminator_loss=0.7091310620307922\n",
            "step 6689: generator_loss=0.6761530637741089, discriminator_loss=0.7092334628105164\n",
            "step 6690: generator_loss=0.6702851057052612, discriminator_loss=0.7117226123809814\n",
            "step 6691: generator_loss=0.670360803604126, discriminator_loss=0.7132325172424316\n",
            "step 6692: generator_loss=0.6707199215888977, discriminator_loss=0.7137556076049805\n",
            "step 6693: generator_loss=0.6675224900245667, discriminator_loss=0.7169641256332397\n",
            "step 6694: generator_loss=0.6715216636657715, discriminator_loss=0.7141149044036865\n",
            "step 6695: generator_loss=0.6669549345970154, discriminator_loss=0.7182028293609619\n",
            "step 6696: generator_loss=0.6669555306434631, discriminator_loss=0.7196415662765503\n",
            "step 6697: generator_loss=0.6644008159637451, discriminator_loss=0.7218972444534302\n",
            "step 6698: generator_loss=0.6610077619552612, discriminator_loss=0.7237349152565002\n",
            "step 6699: generator_loss=0.6606923937797546, discriminator_loss=0.725434422492981\n",
            "step 6700: generator_loss=0.6633429527282715, discriminator_loss=0.7244160175323486\n",
            "step 6701: generator_loss=0.6661750078201294, discriminator_loss=0.7244720458984375\n",
            "step 6702: generator_loss=0.6561163067817688, discriminator_loss=0.7302013039588928\n",
            "step 6703: generator_loss=0.6581482887268066, discriminator_loss=0.7303566336631775\n",
            "step 6704: generator_loss=0.6586745381355286, discriminator_loss=0.7319949865341187\n",
            "step 6705: generator_loss=0.6521490812301636, discriminator_loss=0.7348906993865967\n",
            "step 6706: generator_loss=0.6578295230865479, discriminator_loss=0.7321336269378662\n",
            "step 6707: generator_loss=0.6531734466552734, discriminator_loss=0.7345463633537292\n",
            "step 6708: generator_loss=0.6585334539413452, discriminator_loss=0.7308458089828491\n",
            "step 6709: generator_loss=0.6542604565620422, discriminator_loss=0.7344520688056946\n",
            "step 6710: generator_loss=0.6452236175537109, discriminator_loss=0.7399157285690308\n",
            "step 6711: generator_loss=0.6439203023910522, discriminator_loss=0.7422419786453247\n",
            "step 6712: generator_loss=0.6479750871658325, discriminator_loss=0.7401292324066162\n",
            "step 6713: generator_loss=0.6470364928245544, discriminator_loss=0.7413111925125122\n",
            "step 6714: generator_loss=0.6473729610443115, discriminator_loss=0.7414094805717468\n",
            "step 6715: generator_loss=0.6410542130470276, discriminator_loss=0.7454662322998047\n",
            "step 6716: generator_loss=0.6488513946533203, discriminator_loss=0.7400177121162415\n",
            "step 6717: generator_loss=0.6408277750015259, discriminator_loss=0.7461510896682739\n",
            "step 6718: generator_loss=0.6349671483039856, discriminator_loss=0.7504663467407227\n",
            "step 6719: generator_loss=0.632576584815979, discriminator_loss=0.7518756985664368\n",
            "step 6720: generator_loss=0.6350491046905518, discriminator_loss=0.750667929649353\n",
            "step 6721: generator_loss=0.6305203437805176, discriminator_loss=0.7547957897186279\n",
            "step 6722: generator_loss=0.6401317715644836, discriminator_loss=0.7504482865333557\n",
            "step 6723: generator_loss=0.6404858231544495, discriminator_loss=0.7517499327659607\n",
            "step 6724: generator_loss=0.6424251794815063, discriminator_loss=0.7508448362350464\n",
            "step 6725: generator_loss=0.6350541114807129, discriminator_loss=0.7549904584884644\n",
            "step 6726: generator_loss=0.6413865089416504, discriminator_loss=0.7518829107284546\n",
            "step 6727: generator_loss=0.6423429250717163, discriminator_loss=0.7507153153419495\n",
            "step 6728: generator_loss=0.6491521000862122, discriminator_loss=0.7478811740875244\n",
            "step 6729: generator_loss=0.641465425491333, discriminator_loss=0.7513688802719116\n",
            "step 6730: generator_loss=0.637031078338623, discriminator_loss=0.7523021697998047\n",
            "step 6731: generator_loss=0.6433480978012085, discriminator_loss=0.7467104196548462\n",
            "step 6732: generator_loss=0.6436703205108643, discriminator_loss=0.7459771633148193\n",
            "step 6733: generator_loss=0.6414715051651001, discriminator_loss=0.7453047633171082\n",
            "step 6734: generator_loss=0.6516616344451904, discriminator_loss=0.7389158010482788\n",
            "step 6735: generator_loss=0.6552194356918335, discriminator_loss=0.7368277907371521\n",
            "step 6736: generator_loss=0.6618313193321228, discriminator_loss=0.7299399375915527\n",
            "step 6737: generator_loss=0.6690852046012878, discriminator_loss=0.7261753082275391\n",
            "step 6738: generator_loss=0.6710156202316284, discriminator_loss=0.7258684635162354\n",
            "step 6739: generator_loss=0.6793855428695679, discriminator_loss=0.7197737693786621\n",
            "step 6740: generator_loss=0.6869935989379883, discriminator_loss=0.7151674628257751\n",
            "step 6741: generator_loss=0.6907366514205933, discriminator_loss=0.7120563387870789\n",
            "step 6742: generator_loss=0.6926433444023132, discriminator_loss=0.70941162109375\n",
            "step 6743: generator_loss=0.6986805200576782, discriminator_loss=0.7025139331817627\n",
            "step 6744: generator_loss=0.6976211667060852, discriminator_loss=0.6999839544296265\n",
            "step 6745: generator_loss=0.7003751397132874, discriminator_loss=0.6957499980926514\n",
            "step 6746: generator_loss=0.7046606540679932, discriminator_loss=0.6918110251426697\n",
            "step 6747: generator_loss=0.7007037997245789, discriminator_loss=0.6908168792724609\n",
            "step 6748: generator_loss=0.6988222599029541, discriminator_loss=0.6893212199211121\n",
            "step 6749: generator_loss=0.6922180652618408, discriminator_loss=0.6900424361228943\n",
            "step 6750: generator_loss=0.6977025270462036, discriminator_loss=0.6858159303665161\n",
            "step 6751: generator_loss=0.6924957633018494, discriminator_loss=0.6863522529602051\n",
            "step 6752: generator_loss=0.6947991847991943, discriminator_loss=0.6817554235458374\n",
            "step 6753: generator_loss=0.7023398876190186, discriminator_loss=0.6784043312072754\n",
            "step 6754: generator_loss=0.7012770771980286, discriminator_loss=0.6775139570236206\n",
            "step 6755: generator_loss=0.7010602355003357, discriminator_loss=0.6778687238693237\n",
            "step 6756: generator_loss=0.7115693092346191, discriminator_loss=0.6729949712753296\n",
            "step 6757: generator_loss=0.7188892364501953, discriminator_loss=0.6700218915939331\n",
            "step 6758: generator_loss=0.7302571535110474, discriminator_loss=0.6643914580345154\n",
            "step 6759: generator_loss=0.7394087314605713, discriminator_loss=0.6589555144309998\n",
            "step 6760: generator_loss=0.7473284006118774, discriminator_loss=0.6561821699142456\n",
            "step 6761: generator_loss=0.746514081954956, discriminator_loss=0.6576212644577026\n",
            "step 6762: generator_loss=0.7545413970947266, discriminator_loss=0.6539114713668823\n",
            "step 6763: generator_loss=0.7610801458358765, discriminator_loss=0.6517357230186462\n",
            "step 6764: generator_loss=0.7687177062034607, discriminator_loss=0.6470661163330078\n",
            "step 6765: generator_loss=0.7671283483505249, discriminator_loss=0.6452326774597168\n",
            "step 6766: generator_loss=0.7585611343383789, discriminator_loss=0.6468255519866943\n",
            "step 6767: generator_loss=0.7546913027763367, discriminator_loss=0.6448339223861694\n",
            "step 6768: generator_loss=0.7458274364471436, discriminator_loss=0.6474168300628662\n",
            "step 6769: generator_loss=0.7429689168930054, discriminator_loss=0.6443294882774353\n",
            "step 6770: generator_loss=0.7390795946121216, discriminator_loss=0.6441511511802673\n",
            "step 6771: generator_loss=0.7298667430877686, discriminator_loss=0.6485977172851562\n",
            "step 6772: generator_loss=0.7278598546981812, discriminator_loss=0.6496139764785767\n",
            "step 6773: generator_loss=0.7244852781295776, discriminator_loss=0.6517945528030396\n",
            "step 6774: generator_loss=0.7294719219207764, discriminator_loss=0.6518676280975342\n",
            "step 6775: generator_loss=0.7212879061698914, discriminator_loss=0.657127320766449\n",
            "step 6776: generator_loss=0.7332348823547363, discriminator_loss=0.6534363627433777\n",
            "step 6777: generator_loss=0.7275708317756653, discriminator_loss=0.6601513624191284\n",
            "step 6778: generator_loss=0.7293948531150818, discriminator_loss=0.6601166725158691\n",
            "step 6779: generator_loss=0.7301155924797058, discriminator_loss=0.6626632213592529\n",
            "step 6780: generator_loss=0.729117214679718, discriminator_loss=0.6637524366378784\n",
            "step 6781: generator_loss=0.7197554707527161, discriminator_loss=0.6719040870666504\n",
            "step 6782: generator_loss=0.7265752553939819, discriminator_loss=0.6698638200759888\n",
            "step 6783: generator_loss=0.7165697813034058, discriminator_loss=0.6770762205123901\n",
            "step 6784: generator_loss=0.716620922088623, discriminator_loss=0.6770776510238647\n",
            "step 6785: generator_loss=0.7173096537590027, discriminator_loss=0.6790558099746704\n",
            "step 6786: generator_loss=0.6998993158340454, discriminator_loss=0.6895725727081299\n",
            "step 6787: generator_loss=0.6934353113174438, discriminator_loss=0.6931061744689941\n",
            "step 6788: generator_loss=0.6959961652755737, discriminator_loss=0.6904497146606445\n",
            "step 6789: generator_loss=0.6920305490493774, discriminator_loss=0.6949359178543091\n",
            "step 6790: generator_loss=0.6877089738845825, discriminator_loss=0.6950196027755737\n",
            "step 6791: generator_loss=0.6752932071685791, discriminator_loss=0.7012149095535278\n",
            "step 6792: generator_loss=0.6906174421310425, discriminator_loss=0.6952126026153564\n",
            "step 6793: generator_loss=0.673941433429718, discriminator_loss=0.7015644907951355\n",
            "step 6794: generator_loss=0.6809633374214172, discriminator_loss=0.6990671157836914\n",
            "step 6795: generator_loss=0.6777361631393433, discriminator_loss=0.7004252672195435\n",
            "step 6796: generator_loss=0.6826320886611938, discriminator_loss=0.6973251700401306\n",
            "step 6797: generator_loss=0.6850436329841614, discriminator_loss=0.6963478326797485\n",
            "step 6798: generator_loss=0.6881940960884094, discriminator_loss=0.6962989568710327\n",
            "step 6799: generator_loss=0.6872357726097107, discriminator_loss=0.6977531909942627\n",
            "step 6800: generator_loss=0.7039530277252197, discriminator_loss=0.6883556842803955\n",
            "step 6801: generator_loss=0.7094284296035767, discriminator_loss=0.6897090077400208\n",
            "step 6802: generator_loss=0.7222597599029541, discriminator_loss=0.684494137763977\n",
            "step 6803: generator_loss=0.7209653854370117, discriminator_loss=0.6870585680007935\n",
            "step 6804: generator_loss=0.7007983326911926, discriminator_loss=0.6972067952156067\n",
            "step 6805: generator_loss=0.7246364951133728, discriminator_loss=0.6832668781280518\n",
            "step 6806: generator_loss=0.7059447169303894, discriminator_loss=0.690711259841919\n",
            "step 6807: generator_loss=0.7147698402404785, discriminator_loss=0.6844411492347717\n",
            "step 6808: generator_loss=0.7091015577316284, discriminator_loss=0.6811524033546448\n",
            "step 6809: generator_loss=0.7051469087600708, discriminator_loss=0.6797676086425781\n",
            "step 6810: generator_loss=0.6969486474990845, discriminator_loss=0.6816049814224243\n",
            "step 6811: generator_loss=0.7128320932388306, discriminator_loss=0.670885443687439\n",
            "step 6812: generator_loss=0.6984635591506958, discriminator_loss=0.6781644225120544\n",
            "step 6813: generator_loss=0.7113407850265503, discriminator_loss=0.6698262095451355\n",
            "step 6814: generator_loss=0.7181317806243896, discriminator_loss=0.6686733961105347\n",
            "step 6815: generator_loss=0.7111862301826477, discriminator_loss=0.6732363700866699\n",
            "step 6816: generator_loss=0.7243978381156921, discriminator_loss=0.6692138910293579\n",
            "step 6817: generator_loss=0.7330082654953003, discriminator_loss=0.6685896515846252\n",
            "step 6818: generator_loss=0.7399884462356567, discriminator_loss=0.6666566133499146\n",
            "step 6819: generator_loss=0.7422934770584106, discriminator_loss=0.6672604084014893\n",
            "step 6820: generator_loss=0.7340707182884216, discriminator_loss=0.6719898581504822\n",
            "step 6821: generator_loss=0.7332595586776733, discriminator_loss=0.6724714636802673\n",
            "step 6822: generator_loss=0.7212404608726501, discriminator_loss=0.6744660139083862\n",
            "step 6823: generator_loss=0.7107869982719421, discriminator_loss=0.6784393787384033\n",
            "step 6824: generator_loss=0.7088069915771484, discriminator_loss=0.6763778924942017\n",
            "step 6825: generator_loss=0.7176128625869751, discriminator_loss=0.668354332447052\n",
            "step 6826: generator_loss=0.7064410448074341, discriminator_loss=0.6694649457931519\n",
            "step 6827: generator_loss=0.691358745098114, discriminator_loss=0.676336407661438\n",
            "step 6828: generator_loss=0.68973708152771, discriminator_loss=0.6748344898223877\n",
            "step 6829: generator_loss=0.6925935745239258, discriminator_loss=0.6719788908958435\n",
            "step 6830: generator_loss=0.6959546208381653, discriminator_loss=0.6702403426170349\n",
            "step 6831: generator_loss=0.6934165954589844, discriminator_loss=0.6687659025192261\n",
            "step 6832: generator_loss=0.7040705680847168, discriminator_loss=0.666157603263855\n",
            "step 6833: generator_loss=0.7122237682342529, discriminator_loss=0.6612921953201294\n",
            "step 6834: generator_loss=0.7200053334236145, discriminator_loss=0.6606240272521973\n",
            "step 6835: generator_loss=0.7309083342552185, discriminator_loss=0.6576474905014038\n",
            "step 6836: generator_loss=0.7393083572387695, discriminator_loss=0.6566627025604248\n",
            "step 6837: generator_loss=0.7475330829620361, discriminator_loss=0.6558096408843994\n",
            "step 6838: generator_loss=0.7552003860473633, discriminator_loss=0.6545872688293457\n",
            "step 6839: generator_loss=0.7635916471481323, discriminator_loss=0.6511671543121338\n",
            "step 6840: generator_loss=0.7619530558586121, discriminator_loss=0.6526301503181458\n",
            "step 6841: generator_loss=0.7657439708709717, discriminator_loss=0.6512480974197388\n",
            "step 6842: generator_loss=0.7630378007888794, discriminator_loss=0.6494029760360718\n",
            "step 6843: generator_loss=0.7599479556083679, discriminator_loss=0.6475956439971924\n",
            "step 6844: generator_loss=0.7537832260131836, discriminator_loss=0.6468958854675293\n",
            "step 6845: generator_loss=0.7484412789344788, discriminator_loss=0.6475685834884644\n",
            "step 6846: generator_loss=0.7426919937133789, discriminator_loss=0.6466304659843445\n",
            "step 6847: generator_loss=0.7398400902748108, discriminator_loss=0.6469340324401855\n",
            "step 6848: generator_loss=0.7337517738342285, discriminator_loss=0.6474927663803101\n",
            "step 6849: generator_loss=0.7306534051895142, discriminator_loss=0.6475080251693726\n",
            "step 6850: generator_loss=0.726806640625, discriminator_loss=0.649233341217041\n",
            "step 6851: generator_loss=0.7256592512130737, discriminator_loss=0.650517463684082\n",
            "step 6852: generator_loss=0.7263254523277283, discriminator_loss=0.6501777172088623\n",
            "step 6853: generator_loss=0.7233442664146423, discriminator_loss=0.6536573171615601\n",
            "step 6854: generator_loss=0.7256003022193909, discriminator_loss=0.6523608565330505\n",
            "step 6855: generator_loss=0.7288985252380371, discriminator_loss=0.6520096063613892\n",
            "step 6856: generator_loss=0.7305397987365723, discriminator_loss=0.6538411378860474\n",
            "step 6857: generator_loss=0.7274423241615295, discriminator_loss=0.6568963527679443\n",
            "step 6858: generator_loss=0.7276650071144104, discriminator_loss=0.6566579937934875\n",
            "step 6859: generator_loss=0.7302654981613159, discriminator_loss=0.6558891534805298\n",
            "step 6860: generator_loss=0.7302363514900208, discriminator_loss=0.6580209732055664\n",
            "step 6861: generator_loss=0.732123851776123, discriminator_loss=0.6563707590103149\n",
            "step 6862: generator_loss=0.7314858436584473, discriminator_loss=0.6582412719726562\n",
            "step 6863: generator_loss=0.7341713905334473, discriminator_loss=0.6563758850097656\n",
            "step 6864: generator_loss=0.7322776317596436, discriminator_loss=0.6591016054153442\n",
            "step 6865: generator_loss=0.7286317944526672, discriminator_loss=0.6601585149765015\n",
            "step 6866: generator_loss=0.7321499586105347, discriminator_loss=0.6599242687225342\n",
            "step 6867: generator_loss=0.7274768352508545, discriminator_loss=0.6609632968902588\n",
            "step 6868: generator_loss=0.7227814793586731, discriminator_loss=0.6624377965927124\n",
            "step 6869: generator_loss=0.7213466763496399, discriminator_loss=0.6624452471733093\n",
            "step 6870: generator_loss=0.7191423177719116, discriminator_loss=0.6640945672988892\n",
            "step 6871: generator_loss=0.7141876220703125, discriminator_loss=0.6664102673530579\n",
            "step 6872: generator_loss=0.7147407531738281, discriminator_loss=0.6671393513679504\n",
            "step 6873: generator_loss=0.7125146389007568, discriminator_loss=0.668980062007904\n",
            "step 6874: generator_loss=0.7141215801239014, discriminator_loss=0.6670816540718079\n",
            "step 6875: generator_loss=0.7141588926315308, discriminator_loss=0.668621301651001\n",
            "step 6876: generator_loss=0.7147769927978516, discriminator_loss=0.6679544448852539\n",
            "step 6877: generator_loss=0.715161144733429, discriminator_loss=0.6676461100578308\n",
            "step 6878: generator_loss=0.7149037718772888, discriminator_loss=0.6683566570281982\n",
            "step 6879: generator_loss=0.7142142653465271, discriminator_loss=0.6698673963546753\n",
            "step 6880: generator_loss=0.7177354097366333, discriminator_loss=0.6691689491271973\n",
            "step 6881: generator_loss=0.7258233428001404, discriminator_loss=0.667576253414154\n",
            "step 6882: generator_loss=0.726006031036377, discriminator_loss=0.6679843664169312\n",
            "step 6883: generator_loss=0.7306709289550781, discriminator_loss=0.6675240397453308\n",
            "step 6884: generator_loss=0.7344454526901245, discriminator_loss=0.6667337417602539\n",
            "step 6885: generator_loss=0.7355040311813354, discriminator_loss=0.6674892902374268\n",
            "step 6886: generator_loss=0.73299241065979, discriminator_loss=0.6688469052314758\n",
            "step 6887: generator_loss=0.7318740487098694, discriminator_loss=0.6679686903953552\n",
            "step 6888: generator_loss=0.730665922164917, discriminator_loss=0.6674116849899292\n",
            "step 6889: generator_loss=0.7310346364974976, discriminator_loss=0.6656043529510498\n",
            "step 6890: generator_loss=0.7232538461685181, discriminator_loss=0.6658011674880981\n",
            "step 6891: generator_loss=0.7138949632644653, discriminator_loss=0.6687173843383789\n",
            "step 6892: generator_loss=0.7099735736846924, discriminator_loss=0.6699852347373962\n",
            "step 6893: generator_loss=0.7095527648925781, discriminator_loss=0.6676554679870605\n",
            "step 6894: generator_loss=0.7080836892127991, discriminator_loss=0.6675728559494019\n",
            "step 6895: generator_loss=0.7118363380432129, discriminator_loss=0.6645896434783936\n",
            "step 6896: generator_loss=0.7131447196006775, discriminator_loss=0.6644951701164246\n",
            "step 6897: generator_loss=0.7099231481552124, discriminator_loss=0.6669321060180664\n",
            "step 6898: generator_loss=0.7209887504577637, discriminator_loss=0.6621978282928467\n",
            "step 6899: generator_loss=0.7159496545791626, discriminator_loss=0.6669248342514038\n",
            "step 6900: generator_loss=0.7319391965866089, discriminator_loss=0.6602989435195923\n",
            "step 6901: generator_loss=0.7269008159637451, discriminator_loss=0.6627119779586792\n",
            "step 6902: generator_loss=0.7311710715293884, discriminator_loss=0.6614588499069214\n",
            "step 6903: generator_loss=0.7162478566169739, discriminator_loss=0.6686371564865112\n",
            "step 6904: generator_loss=0.7261918783187866, discriminator_loss=0.6649907231330872\n",
            "step 6905: generator_loss=0.7319379448890686, discriminator_loss=0.6634611487388611\n",
            "step 6906: generator_loss=0.7219294905662537, discriminator_loss=0.669923722743988\n",
            "step 6907: generator_loss=0.7292158603668213, discriminator_loss=0.6658809185028076\n",
            "step 6908: generator_loss=0.726641058921814, discriminator_loss=0.666700005531311\n",
            "step 6909: generator_loss=0.7189452052116394, discriminator_loss=0.6683340072631836\n",
            "step 6910: generator_loss=0.7168393135070801, discriminator_loss=0.6707077026367188\n",
            "step 6911: generator_loss=0.7287895083427429, discriminator_loss=0.6663670539855957\n",
            "step 6912: generator_loss=0.7357449531555176, discriminator_loss=0.6622312068939209\n",
            "step 6913: generator_loss=0.7411868572235107, discriminator_loss=0.6611076593399048\n",
            "step 6914: generator_loss=0.7394353747367859, discriminator_loss=0.6616098880767822\n",
            "step 6915: generator_loss=0.7441450357437134, discriminator_loss=0.6606173515319824\n",
            "step 6916: generator_loss=0.745600700378418, discriminator_loss=0.6601242423057556\n",
            "step 6917: generator_loss=0.7436572313308716, discriminator_loss=0.6603842973709106\n",
            "step 6918: generator_loss=0.7427026033401489, discriminator_loss=0.658936619758606\n",
            "step 6919: generator_loss=0.7420997619628906, discriminator_loss=0.6587839126586914\n",
            "step 6920: generator_loss=0.7351576089859009, discriminator_loss=0.6616805195808411\n",
            "step 6921: generator_loss=0.7329899072647095, discriminator_loss=0.662443995475769\n",
            "step 6922: generator_loss=0.740105152130127, discriminator_loss=0.6588083505630493\n",
            "step 6923: generator_loss=0.7164385914802551, discriminator_loss=0.668660044670105\n",
            "step 6924: generator_loss=0.7118781208992004, discriminator_loss=0.6689692735671997\n",
            "step 6925: generator_loss=0.7094748020172119, discriminator_loss=0.6702718138694763\n",
            "step 6926: generator_loss=0.7115836143493652, discriminator_loss=0.6700364947319031\n",
            "step 6927: generator_loss=0.7129596471786499, discriminator_loss=0.6694328188896179\n",
            "step 6928: generator_loss=0.710281252861023, discriminator_loss=0.6693994998931885\n",
            "step 6929: generator_loss=0.6958397626876831, discriminator_loss=0.6797090768814087\n",
            "step 6930: generator_loss=0.7011280655860901, discriminator_loss=0.6751495599746704\n",
            "step 6931: generator_loss=0.6967257261276245, discriminator_loss=0.6781794428825378\n",
            "step 6932: generator_loss=0.6931617259979248, discriminator_loss=0.6843394041061401\n",
            "step 6933: generator_loss=0.6995865702629089, discriminator_loss=0.6826754212379456\n",
            "step 6934: generator_loss=0.7018349766731262, discriminator_loss=0.6838169097900391\n",
            "step 6935: generator_loss=0.699940025806427, discriminator_loss=0.6852437257766724\n",
            "step 6936: generator_loss=0.7080597877502441, discriminator_loss=0.6839900612831116\n",
            "step 6937: generator_loss=0.7059086561203003, discriminator_loss=0.6875068545341492\n",
            "step 6938: generator_loss=0.7076348066329956, discriminator_loss=0.6877894401550293\n",
            "step 6939: generator_loss=0.6987686157226562, discriminator_loss=0.693761944770813\n",
            "step 6940: generator_loss=0.6977031826972961, discriminator_loss=0.696037769317627\n",
            "step 6941: generator_loss=0.6927213668823242, discriminator_loss=0.696965217590332\n",
            "step 6942: generator_loss=0.687512993812561, discriminator_loss=0.7015827298164368\n",
            "step 6943: generator_loss=0.6913738250732422, discriminator_loss=0.699771523475647\n",
            "step 6944: generator_loss=0.681614875793457, discriminator_loss=0.7060530185699463\n",
            "step 6945: generator_loss=0.6770343780517578, discriminator_loss=0.7090495824813843\n",
            "step 6946: generator_loss=0.6824636459350586, discriminator_loss=0.7069617509841919\n",
            "step 6947: generator_loss=0.6813287734985352, discriminator_loss=0.706857442855835\n",
            "step 6948: generator_loss=0.6887046694755554, discriminator_loss=0.7040708661079407\n",
            "step 6949: generator_loss=0.688467264175415, discriminator_loss=0.7037525177001953\n",
            "step 6950: generator_loss=0.7004993557929993, discriminator_loss=0.6954952478408813\n",
            "step 6951: generator_loss=0.6921706199645996, discriminator_loss=0.7001118063926697\n",
            "step 6952: generator_loss=0.6909749507904053, discriminator_loss=0.7027920484542847\n",
            "step 6953: generator_loss=0.6865454912185669, discriminator_loss=0.7067511081695557\n",
            "step 6954: generator_loss=0.6997963190078735, discriminator_loss=0.7003456354141235\n",
            "step 6955: generator_loss=0.6875177621841431, discriminator_loss=0.707791805267334\n",
            "step 6956: generator_loss=0.6868846416473389, discriminator_loss=0.7057855129241943\n",
            "step 6957: generator_loss=0.6796014904975891, discriminator_loss=0.7091931104660034\n",
            "step 6958: generator_loss=0.6778581142425537, discriminator_loss=0.7089964151382446\n",
            "step 6959: generator_loss=0.6784310936927795, discriminator_loss=0.7081206440925598\n",
            "step 6960: generator_loss=0.6734696626663208, discriminator_loss=0.7113605737686157\n",
            "step 6961: generator_loss=0.6623978614807129, discriminator_loss=0.7167870402336121\n",
            "step 6962: generator_loss=0.6691195368766785, discriminator_loss=0.7134249806404114\n",
            "step 6963: generator_loss=0.6761738061904907, discriminator_loss=0.7101250290870667\n",
            "step 6964: generator_loss=0.6666820645332336, discriminator_loss=0.7165706157684326\n",
            "step 6965: generator_loss=0.6739242076873779, discriminator_loss=0.7128185629844666\n",
            "step 6966: generator_loss=0.6729648113250732, discriminator_loss=0.7153807878494263\n",
            "step 6967: generator_loss=0.6682576537132263, discriminator_loss=0.7182162404060364\n",
            "step 6968: generator_loss=0.6710947751998901, discriminator_loss=0.7162663340568542\n",
            "step 6969: generator_loss=0.6815659999847412, discriminator_loss=0.7113215923309326\n",
            "step 6970: generator_loss=0.6720522046089172, discriminator_loss=0.716606855392456\n",
            "step 6971: generator_loss=0.6776894330978394, discriminator_loss=0.7134405374526978\n",
            "step 6972: generator_loss=0.6904423832893372, discriminator_loss=0.7050526738166809\n",
            "step 6973: generator_loss=0.6825670003890991, discriminator_loss=0.7084201574325562\n",
            "step 6974: generator_loss=0.7092714309692383, discriminator_loss=0.6985214352607727\n",
            "step 6975: generator_loss=0.7195008993148804, discriminator_loss=0.6922205090522766\n",
            "step 6976: generator_loss=0.7076706886291504, discriminator_loss=0.7008216977119446\n",
            "step 6977: generator_loss=0.7083078026771545, discriminator_loss=0.7002391815185547\n",
            "step 6978: generator_loss=0.6988805532455444, discriminator_loss=0.7057888507843018\n",
            "step 6979: generator_loss=0.7038871049880981, discriminator_loss=0.7012497782707214\n",
            "step 6980: generator_loss=0.6986279487609863, discriminator_loss=0.7023622393608093\n",
            "step 6981: generator_loss=0.6885002851486206, discriminator_loss=0.7039986252784729\n",
            "step 6982: generator_loss=0.6813303828239441, discriminator_loss=0.70718914270401\n",
            "step 6983: generator_loss=0.6583733558654785, discriminator_loss=0.7185482978820801\n",
            "step 6984: generator_loss=0.6784635782241821, discriminator_loss=0.7070556879043579\n",
            "step 6985: generator_loss=0.6892017126083374, discriminator_loss=0.7006287574768066\n",
            "step 6986: generator_loss=0.6793795824050903, discriminator_loss=0.7038426995277405\n",
            "step 6987: generator_loss=0.6696340441703796, discriminator_loss=0.709059476852417\n",
            "step 6988: generator_loss=0.6766167283058167, discriminator_loss=0.7065029740333557\n",
            "step 6989: generator_loss=0.6785104274749756, discriminator_loss=0.7046989798545837\n",
            "step 6990: generator_loss=0.6708073019981384, discriminator_loss=0.7098740339279175\n",
            "step 6991: generator_loss=0.6663618087768555, discriminator_loss=0.7125575542449951\n",
            "step 6992: generator_loss=0.6751914620399475, discriminator_loss=0.7068122029304504\n",
            "step 6993: generator_loss=0.6717923283576965, discriminator_loss=0.7084787487983704\n",
            "step 6994: generator_loss=0.6791030168533325, discriminator_loss=0.7043041586875916\n",
            "step 6995: generator_loss=0.6921370029449463, discriminator_loss=0.6984813213348389\n",
            "step 6996: generator_loss=0.679856538772583, discriminator_loss=0.7059224843978882\n",
            "step 6997: generator_loss=0.6881356239318848, discriminator_loss=0.7033650875091553\n",
            "step 6998: generator_loss=0.6903728246688843, discriminator_loss=0.7027461528778076\n",
            "step 6999: generator_loss=0.6975560188293457, discriminator_loss=0.6980568170547485\n",
            "step 7000: generator_loss=0.6935179233551025, discriminator_loss=0.6985150575637817\n",
            "step 7001: generator_loss=0.6877857446670532, discriminator_loss=0.6994335651397705\n",
            "step 7002: generator_loss=0.6987062692642212, discriminator_loss=0.691940426826477\n",
            "step 7003: generator_loss=0.6982117891311646, discriminator_loss=0.6918647289276123\n",
            "step 7004: generator_loss=0.7029004096984863, discriminator_loss=0.6892313957214355\n",
            "step 7005: generator_loss=0.7051854133605957, discriminator_loss=0.6867626905441284\n",
            "step 7006: generator_loss=0.7027037739753723, discriminator_loss=0.6866815090179443\n",
            "step 7007: generator_loss=0.7096705436706543, discriminator_loss=0.6836377382278442\n",
            "step 7008: generator_loss=0.7069025039672852, discriminator_loss=0.6847751140594482\n",
            "step 7009: generator_loss=0.7108538150787354, discriminator_loss=0.6820763945579529\n",
            "step 7010: generator_loss=0.7058424353599548, discriminator_loss=0.6831396818161011\n",
            "step 7011: generator_loss=0.7100238800048828, discriminator_loss=0.67948979139328\n",
            "step 7012: generator_loss=0.7014415264129639, discriminator_loss=0.6807466149330139\n",
            "step 7013: generator_loss=0.7005960941314697, discriminator_loss=0.6807982325553894\n",
            "step 7014: generator_loss=0.7138850688934326, discriminator_loss=0.6737948656082153\n",
            "step 7015: generator_loss=0.7147202491760254, discriminator_loss=0.6734310388565063\n",
            "step 7016: generator_loss=0.7198301553726196, discriminator_loss=0.6699634194374084\n",
            "step 7017: generator_loss=0.7207615375518799, discriminator_loss=0.6699384450912476\n",
            "step 7018: generator_loss=0.711146354675293, discriminator_loss=0.6740027666091919\n",
            "step 7019: generator_loss=0.720089316368103, discriminator_loss=0.6711028814315796\n",
            "step 7020: generator_loss=0.734991192817688, discriminator_loss=0.6641809940338135\n",
            "step 7021: generator_loss=0.7279165983200073, discriminator_loss=0.6669315695762634\n",
            "step 7022: generator_loss=0.7262301445007324, discriminator_loss=0.668247401714325\n",
            "step 7023: generator_loss=0.7309679985046387, discriminator_loss=0.6647683382034302\n",
            "step 7024: generator_loss=0.7400153875350952, discriminator_loss=0.6597970724105835\n",
            "step 7025: generator_loss=0.7354968786239624, discriminator_loss=0.6588781476020813\n",
            "step 7026: generator_loss=0.7297094464302063, discriminator_loss=0.6602685451507568\n",
            "step 7027: generator_loss=0.7561203837394714, discriminator_loss=0.6508832573890686\n",
            "step 7028: generator_loss=0.7469691038131714, discriminator_loss=0.6546474695205688\n",
            "step 7029: generator_loss=0.7632597088813782, discriminator_loss=0.6482973694801331\n",
            "step 7030: generator_loss=0.7595094442367554, discriminator_loss=0.6495468616485596\n",
            "step 7031: generator_loss=0.7457977533340454, discriminator_loss=0.6548099517822266\n",
            "step 7032: generator_loss=0.743088960647583, discriminator_loss=0.6547942757606506\n",
            "step 7033: generator_loss=0.7415409088134766, discriminator_loss=0.6529318690299988\n",
            "step 7034: generator_loss=0.737720251083374, discriminator_loss=0.6538443565368652\n",
            "step 7035: generator_loss=0.7352569103240967, discriminator_loss=0.6532307267189026\n",
            "step 7036: generator_loss=0.7307920455932617, discriminator_loss=0.6551221013069153\n",
            "step 7037: generator_loss=0.7341599464416504, discriminator_loss=0.6536080241203308\n",
            "step 7038: generator_loss=0.7554973363876343, discriminator_loss=0.6453246474266052\n",
            "step 7039: generator_loss=0.7292522192001343, discriminator_loss=0.657066822052002\n",
            "step 7040: generator_loss=0.7510977983474731, discriminator_loss=0.648901104927063\n",
            "step 7041: generator_loss=0.759465217590332, discriminator_loss=0.6459330916404724\n",
            "step 7042: generator_loss=0.7400518655776978, discriminator_loss=0.6538851261138916\n",
            "step 7043: generator_loss=0.7407203912734985, discriminator_loss=0.6554311513900757\n",
            "step 7044: generator_loss=0.7364626526832581, discriminator_loss=0.6572350263595581\n",
            "step 7045: generator_loss=0.7489869594573975, discriminator_loss=0.6519848704338074\n",
            "step 7046: generator_loss=0.7472031116485596, discriminator_loss=0.6541285514831543\n",
            "step 7047: generator_loss=0.7472527027130127, discriminator_loss=0.6536206603050232\n",
            "step 7048: generator_loss=0.7300441265106201, discriminator_loss=0.6605094075202942\n",
            "step 7049: generator_loss=0.7348393201828003, discriminator_loss=0.6576155424118042\n",
            "step 7050: generator_loss=0.7350426912307739, discriminator_loss=0.6571823358535767\n",
            "step 7051: generator_loss=0.7288224697113037, discriminator_loss=0.6599396467208862\n",
            "step 7052: generator_loss=0.7163769006729126, discriminator_loss=0.6650133728981018\n",
            "step 7053: generator_loss=0.7236202955245972, discriminator_loss=0.6610299944877625\n",
            "step 7054: generator_loss=0.7202176451683044, discriminator_loss=0.6625397205352783\n",
            "step 7055: generator_loss=0.7113687992095947, discriminator_loss=0.6669724583625793\n",
            "step 7056: generator_loss=0.713468611240387, discriminator_loss=0.6652876138687134\n",
            "step 7057: generator_loss=0.716404139995575, discriminator_loss=0.6662133932113647\n",
            "step 7058: generator_loss=0.7406966090202332, discriminator_loss=0.6576553583145142\n",
            "step 7059: generator_loss=0.7270525097846985, discriminator_loss=0.6658438444137573\n",
            "step 7060: generator_loss=0.738160252571106, discriminator_loss=0.6624845266342163\n",
            "step 7061: generator_loss=0.7220470309257507, discriminator_loss=0.6689668893814087\n",
            "step 7062: generator_loss=0.72412109375, discriminator_loss=0.6692359447479248\n",
            "step 7063: generator_loss=0.7352722883224487, discriminator_loss=0.6644464731216431\n",
            "step 7064: generator_loss=0.7406080961227417, discriminator_loss=0.6625059247016907\n",
            "step 7065: generator_loss=0.7385818362236023, discriminator_loss=0.663276731967926\n",
            "step 7066: generator_loss=0.7303280234336853, discriminator_loss=0.6668785810470581\n",
            "step 7067: generator_loss=0.7349948883056641, discriminator_loss=0.6621261239051819\n",
            "step 7068: generator_loss=0.7343623638153076, discriminator_loss=0.6624586582183838\n",
            "step 7069: generator_loss=0.7312445640563965, discriminator_loss=0.66193026304245\n",
            "step 7070: generator_loss=0.7292298078536987, discriminator_loss=0.6624433994293213\n",
            "step 7071: generator_loss=0.7385465502738953, discriminator_loss=0.6584683656692505\n",
            "step 7072: generator_loss=0.7270169258117676, discriminator_loss=0.6642011404037476\n",
            "step 7073: generator_loss=0.7259780168533325, discriminator_loss=0.664874792098999\n",
            "step 7074: generator_loss=0.727375864982605, discriminator_loss=0.6655819416046143\n",
            "step 7075: generator_loss=0.7291542291641235, discriminator_loss=0.6638128161430359\n",
            "step 7076: generator_loss=0.7259995937347412, discriminator_loss=0.666252076625824\n",
            "step 7077: generator_loss=0.7135297656059265, discriminator_loss=0.6711461544036865\n",
            "step 7078: generator_loss=0.7077438235282898, discriminator_loss=0.6746948957443237\n",
            "step 7079: generator_loss=0.7095626592636108, discriminator_loss=0.6743794679641724\n",
            "step 7080: generator_loss=0.7058228254318237, discriminator_loss=0.679054319858551\n",
            "step 7081: generator_loss=0.7096555233001709, discriminator_loss=0.6764280796051025\n",
            "step 7082: generator_loss=0.7038789391517639, discriminator_loss=0.6812244653701782\n",
            "step 7083: generator_loss=0.7134870290756226, discriminator_loss=0.6772481799125671\n",
            "step 7084: generator_loss=0.719003438949585, discriminator_loss=0.6756123900413513\n",
            "step 7085: generator_loss=0.7250193357467651, discriminator_loss=0.6744635105133057\n",
            "step 7086: generator_loss=0.713609516620636, discriminator_loss=0.6801470518112183\n",
            "step 7087: generator_loss=0.710712730884552, discriminator_loss=0.6822912096977234\n",
            "step 7088: generator_loss=0.7057456374168396, discriminator_loss=0.6865312457084656\n",
            "step 7089: generator_loss=0.7053507566452026, discriminator_loss=0.6862934827804565\n",
            "step 7090: generator_loss=0.6976298689842224, discriminator_loss=0.6894257664680481\n",
            "step 7091: generator_loss=0.6924940943717957, discriminator_loss=0.6911498308181763\n",
            "step 7092: generator_loss=0.6916160583496094, discriminator_loss=0.6894158720970154\n",
            "step 7093: generator_loss=0.6832240223884583, discriminator_loss=0.6930190324783325\n",
            "step 7094: generator_loss=0.6801705956459045, discriminator_loss=0.693602442741394\n",
            "step 7095: generator_loss=0.6793204545974731, discriminator_loss=0.6956237554550171\n",
            "step 7096: generator_loss=0.6846305131912231, discriminator_loss=0.6939725875854492\n",
            "step 7097: generator_loss=0.6803643703460693, discriminator_loss=0.6974962949752808\n",
            "step 7098: generator_loss=0.6808896064758301, discriminator_loss=0.6992052793502808\n",
            "step 7099: generator_loss=0.6885252594947815, discriminator_loss=0.6986019611358643\n",
            "step 7100: generator_loss=0.6776842474937439, discriminator_loss=0.7084349393844604\n",
            "step 7101: generator_loss=0.6846571564674377, discriminator_loss=0.7059837579727173\n",
            "step 7102: generator_loss=0.6719525456428528, discriminator_loss=0.7174488306045532\n",
            "step 7103: generator_loss=0.671941876411438, discriminator_loss=0.719856858253479\n",
            "step 7104: generator_loss=0.6790777444839478, discriminator_loss=0.7181462049484253\n",
            "step 7105: generator_loss=0.6796200275421143, discriminator_loss=0.7196304202079773\n",
            "step 7106: generator_loss=0.6674091815948486, discriminator_loss=0.7272552251815796\n",
            "step 7107: generator_loss=0.6555089950561523, discriminator_loss=0.7328461408615112\n",
            "step 7108: generator_loss=0.6561118364334106, discriminator_loss=0.7315033674240112\n",
            "step 7109: generator_loss=0.657595157623291, discriminator_loss=0.7307161092758179\n",
            "step 7110: generator_loss=0.648499608039856, discriminator_loss=0.7359795570373535\n",
            "step 7111: generator_loss=0.6489102244377136, discriminator_loss=0.7353012561798096\n",
            "step 7112: generator_loss=0.6431022882461548, discriminator_loss=0.7394627332687378\n",
            "step 7113: generator_loss=0.6368546485900879, discriminator_loss=0.7432025074958801\n",
            "step 7114: generator_loss=0.6483356952667236, discriminator_loss=0.7375363111495972\n",
            "step 7115: generator_loss=0.6365631818771362, discriminator_loss=0.7436140775680542\n",
            "step 7116: generator_loss=0.6532362699508667, discriminator_loss=0.7364315986633301\n",
            "step 7117: generator_loss=0.6543787717819214, discriminator_loss=0.7352859973907471\n",
            "step 7118: generator_loss=0.6497272253036499, discriminator_loss=0.7374017238616943\n",
            "step 7119: generator_loss=0.6445941925048828, discriminator_loss=0.7411181926727295\n",
            "step 7120: generator_loss=0.6509648561477661, discriminator_loss=0.7363391518592834\n",
            "step 7121: generator_loss=0.6479089260101318, discriminator_loss=0.7366515398025513\n",
            "step 7122: generator_loss=0.6469934582710266, discriminator_loss=0.7374221086502075\n",
            "step 7123: generator_loss=0.6479406356811523, discriminator_loss=0.7347908020019531\n",
            "step 7124: generator_loss=0.6512834429740906, discriminator_loss=0.7316918969154358\n",
            "step 7125: generator_loss=0.6635756492614746, discriminator_loss=0.7249252796173096\n",
            "step 7126: generator_loss=0.6783386468887329, discriminator_loss=0.7179442644119263\n",
            "step 7127: generator_loss=0.6905332803726196, discriminator_loss=0.7139946222305298\n",
            "step 7128: generator_loss=0.6905897259712219, discriminator_loss=0.7161953449249268\n",
            "step 7129: generator_loss=0.6919921040534973, discriminator_loss=0.7135505676269531\n",
            "step 7130: generator_loss=0.6996399164199829, discriminator_loss=0.708701491355896\n",
            "step 7131: generator_loss=0.6954134702682495, discriminator_loss=0.7077481150627136\n",
            "step 7132: generator_loss=0.6827051639556885, discriminator_loss=0.7112029790878296\n",
            "step 7133: generator_loss=0.6814053058624268, discriminator_loss=0.7078759670257568\n",
            "step 7134: generator_loss=0.6799615621566772, discriminator_loss=0.7049000263214111\n",
            "step 7135: generator_loss=0.6725074648857117, discriminator_loss=0.7052101492881775\n",
            "step 7136: generator_loss=0.6687175035476685, discriminator_loss=0.7041133642196655\n",
            "step 7137: generator_loss=0.669951319694519, discriminator_loss=0.6996674537658691\n",
            "step 7138: generator_loss=0.6645148992538452, discriminator_loss=0.7000559568405151\n",
            "step 7139: generator_loss=0.6674642562866211, discriminator_loss=0.6958317756652832\n",
            "step 7140: generator_loss=0.669035792350769, discriminator_loss=0.6927768588066101\n",
            "step 7141: generator_loss=0.6750712990760803, discriminator_loss=0.6885303258895874\n",
            "step 7142: generator_loss=0.683010458946228, discriminator_loss=0.684626579284668\n",
            "step 7143: generator_loss=0.6931294202804565, discriminator_loss=0.6817086338996887\n",
            "step 7144: generator_loss=0.7006632685661316, discriminator_loss=0.6822197437286377\n",
            "step 7145: generator_loss=0.7137914299964905, discriminator_loss=0.6805435419082642\n",
            "step 7146: generator_loss=0.7207699418067932, discriminator_loss=0.6799498200416565\n",
            "step 7147: generator_loss=0.724339485168457, discriminator_loss=0.6818821430206299\n",
            "step 7148: generator_loss=0.7314239144325256, discriminator_loss=0.6792284250259399\n",
            "step 7149: generator_loss=0.7321714162826538, discriminator_loss=0.67836594581604\n",
            "step 7150: generator_loss=0.7276477813720703, discriminator_loss=0.6791092753410339\n",
            "step 7151: generator_loss=0.728796660900116, discriminator_loss=0.6762539148330688\n",
            "step 7152: generator_loss=0.7233991622924805, discriminator_loss=0.6758447885513306\n",
            "step 7153: generator_loss=0.720716118812561, discriminator_loss=0.6736498475074768\n",
            "step 7154: generator_loss=0.7177455425262451, discriminator_loss=0.6720231771469116\n",
            "step 7155: generator_loss=0.7165113687515259, discriminator_loss=0.6701369881629944\n",
            "step 7156: generator_loss=0.7136551737785339, discriminator_loss=0.6696228384971619\n",
            "step 7157: generator_loss=0.7148213386535645, discriminator_loss=0.6672269105911255\n",
            "step 7158: generator_loss=0.7164766192436218, discriminator_loss=0.6659154891967773\n",
            "step 7159: generator_loss=0.7188354730606079, discriminator_loss=0.663994550704956\n",
            "step 7160: generator_loss=0.720535397529602, discriminator_loss=0.6620399951934814\n",
            "step 7161: generator_loss=0.7252702713012695, discriminator_loss=0.6600886583328247\n",
            "step 7162: generator_loss=0.7305653095245361, discriminator_loss=0.6582385301589966\n",
            "step 7163: generator_loss=0.7379113435745239, discriminator_loss=0.6559497117996216\n",
            "step 7164: generator_loss=0.740366518497467, discriminator_loss=0.655288577079773\n",
            "step 7165: generator_loss=0.7474461793899536, discriminator_loss=0.6515916585922241\n",
            "step 7166: generator_loss=0.7474524974822998, discriminator_loss=0.6511063575744629\n",
            "step 7167: generator_loss=0.7498939037322998, discriminator_loss=0.6510294675827026\n",
            "step 7168: generator_loss=0.7534003853797913, discriminator_loss=0.6483441591262817\n",
            "step 7169: generator_loss=0.748383104801178, discriminator_loss=0.6509075164794922\n",
            "step 7170: generator_loss=0.7556662559509277, discriminator_loss=0.6447200179100037\n",
            "step 7171: generator_loss=0.7481682300567627, discriminator_loss=0.6474369168281555\n",
            "step 7172: generator_loss=0.7494656443595886, discriminator_loss=0.6476296186447144\n",
            "step 7173: generator_loss=0.7481576204299927, discriminator_loss=0.6476850509643555\n",
            "step 7174: generator_loss=0.7461550831794739, discriminator_loss=0.6480228900909424\n",
            "step 7175: generator_loss=0.7580708265304565, discriminator_loss=0.642110288143158\n",
            "step 7176: generator_loss=0.7471553087234497, discriminator_loss=0.6461353898048401\n",
            "step 7177: generator_loss=0.7437302470207214, discriminator_loss=0.6457790732383728\n",
            "step 7178: generator_loss=0.736711859703064, discriminator_loss=0.6481537818908691\n",
            "step 7179: generator_loss=0.740953803062439, discriminator_loss=0.6441812515258789\n",
            "step 7180: generator_loss=0.7281887531280518, discriminator_loss=0.6494263410568237\n",
            "step 7181: generator_loss=0.7348962426185608, discriminator_loss=0.6454181671142578\n",
            "step 7182: generator_loss=0.7301336526870728, discriminator_loss=0.6473754644393921\n",
            "step 7183: generator_loss=0.7337083220481873, discriminator_loss=0.6460144519805908\n",
            "step 7184: generator_loss=0.7428493499755859, discriminator_loss=0.6433101296424866\n",
            "step 7185: generator_loss=0.7455703020095825, discriminator_loss=0.6444665193557739\n",
            "step 7186: generator_loss=0.7610145807266235, discriminator_loss=0.63775235414505\n",
            "step 7187: generator_loss=0.7532473802566528, discriminator_loss=0.6438260078430176\n",
            "step 7188: generator_loss=0.7551992535591125, discriminator_loss=0.6436446905136108\n",
            "step 7189: generator_loss=0.7589883804321289, discriminator_loss=0.6435446739196777\n",
            "step 7190: generator_loss=0.7551121711730957, discriminator_loss=0.6466227769851685\n",
            "step 7191: generator_loss=0.7578732967376709, discriminator_loss=0.6451821327209473\n",
            "step 7192: generator_loss=0.7530784010887146, discriminator_loss=0.6469192504882812\n",
            "step 7193: generator_loss=0.7492220401763916, discriminator_loss=0.6465638875961304\n",
            "step 7194: generator_loss=0.7418249249458313, discriminator_loss=0.6478480100631714\n",
            "step 7195: generator_loss=0.7346063852310181, discriminator_loss=0.6515637636184692\n",
            "step 7196: generator_loss=0.7315503358840942, discriminator_loss=0.6502799987792969\n",
            "step 7197: generator_loss=0.7364125847816467, discriminator_loss=0.6474045515060425\n",
            "step 7198: generator_loss=0.7200896739959717, discriminator_loss=0.6537747383117676\n",
            "step 7199: generator_loss=0.725574254989624, discriminator_loss=0.6513141393661499\n",
            "step 7200: generator_loss=0.7354921102523804, discriminator_loss=0.6469700336456299\n",
            "step 7201: generator_loss=0.7283899784088135, discriminator_loss=0.6503863334655762\n",
            "step 7202: generator_loss=0.732952356338501, discriminator_loss=0.6499513387680054\n",
            "step 7203: generator_loss=0.7392400503158569, discriminator_loss=0.6491810083389282\n",
            "step 7204: generator_loss=0.7388904094696045, discriminator_loss=0.6524624824523926\n",
            "step 7205: generator_loss=0.7435104250907898, discriminator_loss=0.6526665687561035\n",
            "step 7206: generator_loss=0.7494794726371765, discriminator_loss=0.6523587703704834\n",
            "step 7207: generator_loss=0.7411220073699951, discriminator_loss=0.6568436622619629\n",
            "step 7208: generator_loss=0.7424378395080566, discriminator_loss=0.6577298641204834\n",
            "step 7209: generator_loss=0.7366646528244019, discriminator_loss=0.6597676277160645\n",
            "step 7210: generator_loss=0.7302312850952148, discriminator_loss=0.6620594263076782\n",
            "step 7211: generator_loss=0.7297479510307312, discriminator_loss=0.6612726449966431\n",
            "step 7212: generator_loss=0.7171482443809509, discriminator_loss=0.664199948310852\n",
            "step 7213: generator_loss=0.7172760963439941, discriminator_loss=0.6632122993469238\n",
            "step 7214: generator_loss=0.7111811637878418, discriminator_loss=0.664229154586792\n",
            "step 7215: generator_loss=0.7100486755371094, discriminator_loss=0.6665776371955872\n",
            "step 7216: generator_loss=0.7019814252853394, discriminator_loss=0.6714472770690918\n",
            "step 7217: generator_loss=0.7068943381309509, discriminator_loss=0.6689378023147583\n",
            "step 7218: generator_loss=0.7026065587997437, discriminator_loss=0.674403727054596\n",
            "step 7219: generator_loss=0.7097450494766235, discriminator_loss=0.6737257242202759\n",
            "step 7220: generator_loss=0.7119851112365723, discriminator_loss=0.67479008436203\n",
            "step 7221: generator_loss=0.7137055397033691, discriminator_loss=0.6781061887741089\n",
            "step 7222: generator_loss=0.7109489440917969, discriminator_loss=0.6799572706222534\n",
            "step 7223: generator_loss=0.7088176012039185, discriminator_loss=0.6824035048484802\n",
            "step 7224: generator_loss=0.7091816663742065, discriminator_loss=0.6829363703727722\n",
            "step 7225: generator_loss=0.7052295207977295, discriminator_loss=0.6853122711181641\n",
            "step 7226: generator_loss=0.7111186981201172, discriminator_loss=0.6848351359367371\n",
            "step 7227: generator_loss=0.7133050560951233, discriminator_loss=0.683758020401001\n",
            "step 7228: generator_loss=0.7131458520889282, discriminator_loss=0.6840207576751709\n",
            "step 7229: generator_loss=0.7074539065361023, discriminator_loss=0.6877463459968567\n",
            "step 7230: generator_loss=0.7014360427856445, discriminator_loss=0.6908698081970215\n",
            "step 7231: generator_loss=0.7053019404411316, discriminator_loss=0.689243495464325\n",
            "step 7232: generator_loss=0.6909780502319336, discriminator_loss=0.6957249641418457\n",
            "step 7233: generator_loss=0.6915086507797241, discriminator_loss=0.6968659162521362\n",
            "step 7234: generator_loss=0.6931561231613159, discriminator_loss=0.6955186128616333\n",
            "step 7235: generator_loss=0.6785845756530762, discriminator_loss=0.7026357650756836\n",
            "step 7236: generator_loss=0.671626091003418, discriminator_loss=0.7068530321121216\n",
            "step 7237: generator_loss=0.6732890605926514, discriminator_loss=0.706562876701355\n",
            "step 7238: generator_loss=0.666597843170166, discriminator_loss=0.7117185592651367\n",
            "step 7239: generator_loss=0.6677918434143066, discriminator_loss=0.7117938995361328\n",
            "step 7240: generator_loss=0.6648334860801697, discriminator_loss=0.7145686745643616\n",
            "step 7241: generator_loss=0.6681903600692749, discriminator_loss=0.716637909412384\n",
            "step 7242: generator_loss=0.6659231185913086, discriminator_loss=0.7194541692733765\n",
            "step 7243: generator_loss=0.6631138324737549, discriminator_loss=0.7224496603012085\n",
            "step 7244: generator_loss=0.6645186543464661, discriminator_loss=0.7244049310684204\n",
            "step 7245: generator_loss=0.6609057784080505, discriminator_loss=0.7268979549407959\n",
            "step 7246: generator_loss=0.6562680006027222, discriminator_loss=0.7307752370834351\n",
            "step 7247: generator_loss=0.6605272889137268, discriminator_loss=0.7303479909896851\n",
            "step 7248: generator_loss=0.6606476306915283, discriminator_loss=0.7298608422279358\n",
            "step 7249: generator_loss=0.6518029570579529, discriminator_loss=0.7344311475753784\n",
            "step 7250: generator_loss=0.6553269624710083, discriminator_loss=0.7315455079078674\n",
            "step 7251: generator_loss=0.6524921655654907, discriminator_loss=0.7359790802001953\n",
            "step 7252: generator_loss=0.6492871046066284, discriminator_loss=0.7380249500274658\n",
            "step 7253: generator_loss=0.6516227722167969, discriminator_loss=0.7365120649337769\n",
            "step 7254: generator_loss=0.6470004320144653, discriminator_loss=0.7390713691711426\n",
            "step 7255: generator_loss=0.6446654796600342, discriminator_loss=0.7411563396453857\n",
            "step 7256: generator_loss=0.6404620409011841, discriminator_loss=0.7457432746887207\n",
            "step 7257: generator_loss=0.6500067114830017, discriminator_loss=0.7408402562141418\n",
            "step 7258: generator_loss=0.6437863707542419, discriminator_loss=0.7442446947097778\n",
            "step 7259: generator_loss=0.6385398507118225, discriminator_loss=0.7492872476577759\n",
            "step 7260: generator_loss=0.650590181350708, discriminator_loss=0.741665244102478\n",
            "step 7261: generator_loss=0.6487075090408325, discriminator_loss=0.740885317325592\n",
            "step 7262: generator_loss=0.6399715542793274, discriminator_loss=0.746645987033844\n",
            "step 7263: generator_loss=0.6429306268692017, discriminator_loss=0.7436449527740479\n",
            "step 7264: generator_loss=0.639350175857544, discriminator_loss=0.7442466616630554\n",
            "step 7265: generator_loss=0.6417591571807861, discriminator_loss=0.7406890392303467\n",
            "step 7266: generator_loss=0.6452726125717163, discriminator_loss=0.7381435632705688\n",
            "step 7267: generator_loss=0.6453902721405029, discriminator_loss=0.7351504564285278\n",
            "step 7268: generator_loss=0.6497588753700256, discriminator_loss=0.7295390963554382\n",
            "step 7269: generator_loss=0.6542009115219116, discriminator_loss=0.727056622505188\n",
            "step 7270: generator_loss=0.6598902940750122, discriminator_loss=0.7224947214126587\n",
            "step 7271: generator_loss=0.6691750288009644, discriminator_loss=0.717415452003479\n",
            "step 7272: generator_loss=0.6772403120994568, discriminator_loss=0.7155522108078003\n",
            "step 7273: generator_loss=0.6882095336914062, discriminator_loss=0.7104923129081726\n",
            "step 7274: generator_loss=0.699268102645874, discriminator_loss=0.7068891525268555\n",
            "step 7275: generator_loss=0.7102770805358887, discriminator_loss=0.703770637512207\n",
            "step 7276: generator_loss=0.7094186544418335, discriminator_loss=0.6999449729919434\n",
            "step 7277: generator_loss=0.7082180976867676, discriminator_loss=0.6976079344749451\n",
            "step 7278: generator_loss=0.7081356048583984, discriminator_loss=0.692184329032898\n",
            "step 7279: generator_loss=0.7144869565963745, discriminator_loss=0.6839582920074463\n",
            "step 7280: generator_loss=0.7014297246932983, discriminator_loss=0.6863396167755127\n",
            "step 7281: generator_loss=0.7105221152305603, discriminator_loss=0.6779643297195435\n",
            "step 7282: generator_loss=0.7144749164581299, discriminator_loss=0.672683835029602\n",
            "step 7283: generator_loss=0.7203938961029053, discriminator_loss=0.6683092713356018\n",
            "step 7284: generator_loss=0.7267218828201294, discriminator_loss=0.6654709577560425\n",
            "step 7285: generator_loss=0.7312096953392029, discriminator_loss=0.6652075052261353\n",
            "step 7286: generator_loss=0.7462639808654785, discriminator_loss=0.657698392868042\n",
            "step 7287: generator_loss=0.7360579967498779, discriminator_loss=0.6644378900527954\n",
            "step 7288: generator_loss=0.7384174466133118, discriminator_loss=0.6617557406425476\n",
            "step 7289: generator_loss=0.7467930316925049, discriminator_loss=0.6574051380157471\n",
            "step 7290: generator_loss=0.7401813864707947, discriminator_loss=0.6594325304031372\n",
            "step 7291: generator_loss=0.734127402305603, discriminator_loss=0.6612644195556641\n",
            "step 7292: generator_loss=0.7259430885314941, discriminator_loss=0.664323091506958\n",
            "step 7293: generator_loss=0.7149264812469482, discriminator_loss=0.6703624725341797\n",
            "step 7294: generator_loss=0.7166687846183777, discriminator_loss=0.6672784090042114\n",
            "step 7295: generator_loss=0.7073626518249512, discriminator_loss=0.670198380947113\n",
            "step 7296: generator_loss=0.7022302150726318, discriminator_loss=0.6730448007583618\n",
            "step 7297: generator_loss=0.6939898133277893, discriminator_loss=0.6780760288238525\n",
            "step 7298: generator_loss=0.6858376860618591, discriminator_loss=0.6819658279418945\n",
            "step 7299: generator_loss=0.6860761642456055, discriminator_loss=0.6840416789054871\n",
            "step 7300: generator_loss=0.6904458403587341, discriminator_loss=0.6853231191635132\n",
            "step 7301: generator_loss=0.6966584920883179, discriminator_loss=0.6837624311447144\n",
            "step 7302: generator_loss=0.6986148357391357, discriminator_loss=0.6838870048522949\n",
            "step 7303: generator_loss=0.696225643157959, discriminator_loss=0.6882262229919434\n",
            "step 7304: generator_loss=0.6964370608329773, discriminator_loss=0.6893781423568726\n",
            "step 7305: generator_loss=0.699382483959198, discriminator_loss=0.6930452585220337\n",
            "step 7306: generator_loss=0.7021329998970032, discriminator_loss=0.6944963932037354\n",
            "step 7307: generator_loss=0.7008522152900696, discriminator_loss=0.6966707706451416\n",
            "step 7308: generator_loss=0.6975412368774414, discriminator_loss=0.6984687447547913\n",
            "step 7309: generator_loss=0.6942755579948425, discriminator_loss=0.6997227072715759\n",
            "step 7310: generator_loss=0.689672589302063, discriminator_loss=0.7055519223213196\n",
            "step 7311: generator_loss=0.6905022859573364, discriminator_loss=0.7014864087104797\n",
            "step 7312: generator_loss=0.6807304620742798, discriminator_loss=0.7058899998664856\n",
            "step 7313: generator_loss=0.6724794507026672, discriminator_loss=0.7092322707176208\n",
            "step 7314: generator_loss=0.6734306812286377, discriminator_loss=0.7070021629333496\n",
            "step 7315: generator_loss=0.6684721112251282, discriminator_loss=0.710428774356842\n",
            "step 7316: generator_loss=0.6666902303695679, discriminator_loss=0.7111485004425049\n",
            "step 7317: generator_loss=0.6597846746444702, discriminator_loss=0.7174824476242065\n",
            "step 7318: generator_loss=0.6723843812942505, discriminator_loss=0.7097715735435486\n",
            "step 7319: generator_loss=0.6709998250007629, discriminator_loss=0.7164129018783569\n",
            "step 7320: generator_loss=0.6696521043777466, discriminator_loss=0.7164227962493896\n",
            "step 7321: generator_loss=0.6665948629379272, discriminator_loss=0.7213266491889954\n",
            "step 7322: generator_loss=0.6748141050338745, discriminator_loss=0.7197291851043701\n",
            "step 7323: generator_loss=0.6763420104980469, discriminator_loss=0.7203876376152039\n",
            "step 7324: generator_loss=0.6735217571258545, discriminator_loss=0.7213339805603027\n",
            "step 7325: generator_loss=0.6663702130317688, discriminator_loss=0.7255252599716187\n",
            "step 7326: generator_loss=0.6650581359863281, discriminator_loss=0.7259076833724976\n",
            "step 7327: generator_loss=0.6595906019210815, discriminator_loss=0.727209210395813\n",
            "step 7328: generator_loss=0.6597839593887329, discriminator_loss=0.728004515171051\n",
            "step 7329: generator_loss=0.6542425155639648, discriminator_loss=0.728333055973053\n",
            "step 7330: generator_loss=0.6533951759338379, discriminator_loss=0.7280256748199463\n",
            "step 7331: generator_loss=0.648979663848877, discriminator_loss=0.72884202003479\n",
            "step 7332: generator_loss=0.6439770460128784, discriminator_loss=0.7304960489273071\n",
            "step 7333: generator_loss=0.6489291787147522, discriminator_loss=0.7305905818939209\n",
            "step 7334: generator_loss=0.656240701675415, discriminator_loss=0.7243726253509521\n",
            "step 7335: generator_loss=0.65543532371521, discriminator_loss=0.7268545031547546\n",
            "step 7336: generator_loss=0.6674246788024902, discriminator_loss=0.7228348255157471\n",
            "step 7337: generator_loss=0.6585825681686401, discriminator_loss=0.725561261177063\n",
            "step 7338: generator_loss=0.671014666557312, discriminator_loss=0.718937873840332\n",
            "step 7339: generator_loss=0.6769351363182068, discriminator_loss=0.7142020463943481\n",
            "step 7340: generator_loss=0.6818878650665283, discriminator_loss=0.7100142240524292\n",
            "step 7341: generator_loss=0.6844013929367065, discriminator_loss=0.7051643133163452\n",
            "step 7342: generator_loss=0.6825737953186035, discriminator_loss=0.7040557265281677\n",
            "step 7343: generator_loss=0.6988191604614258, discriminator_loss=0.6960075497627258\n",
            "step 7344: generator_loss=0.7068541049957275, discriminator_loss=0.6927282214164734\n",
            "step 7345: generator_loss=0.7179380655288696, discriminator_loss=0.6865783929824829\n",
            "step 7346: generator_loss=0.7149636149406433, discriminator_loss=0.6870547533035278\n",
            "step 7347: generator_loss=0.7062671184539795, discriminator_loss=0.6877320408821106\n",
            "step 7348: generator_loss=0.703601062297821, discriminator_loss=0.6853241920471191\n",
            "step 7349: generator_loss=0.7038497924804688, discriminator_loss=0.6818138957023621\n",
            "step 7350: generator_loss=0.6990431547164917, discriminator_loss=0.6811501383781433\n",
            "step 7351: generator_loss=0.7033181190490723, discriminator_loss=0.6772418022155762\n",
            "step 7352: generator_loss=0.7040728330612183, discriminator_loss=0.6746807098388672\n",
            "step 7353: generator_loss=0.7079456448554993, discriminator_loss=0.669990062713623\n",
            "step 7354: generator_loss=0.7161778807640076, discriminator_loss=0.6675267219543457\n",
            "step 7355: generator_loss=0.7276712656021118, discriminator_loss=0.6612319946289062\n",
            "step 7356: generator_loss=0.7218317985534668, discriminator_loss=0.6637601852416992\n",
            "step 7357: generator_loss=0.7348276972770691, discriminator_loss=0.6592514514923096\n",
            "step 7358: generator_loss=0.7437024116516113, discriminator_loss=0.6557527184486389\n",
            "step 7359: generator_loss=0.7522836327552795, discriminator_loss=0.6519020795822144\n",
            "step 7360: generator_loss=0.7633528709411621, discriminator_loss=0.6452987194061279\n",
            "step 7361: generator_loss=0.766274094581604, discriminator_loss=0.6447651386260986\n",
            "step 7362: generator_loss=0.7814504504203796, discriminator_loss=0.6336916089057922\n",
            "step 7363: generator_loss=0.7861933708190918, discriminator_loss=0.6295945644378662\n",
            "step 7364: generator_loss=0.786514401435852, discriminator_loss=0.6269620060920715\n",
            "step 7365: generator_loss=0.799905002117157, discriminator_loss=0.6185502409934998\n",
            "step 7366: generator_loss=0.7898737788200378, discriminator_loss=0.6200125813484192\n",
            "step 7367: generator_loss=0.7836525440216064, discriminator_loss=0.6191007494926453\n",
            "step 7368: generator_loss=0.7817584276199341, discriminator_loss=0.6154693365097046\n",
            "step 7369: generator_loss=0.7922785878181458, discriminator_loss=0.6085845232009888\n",
            "step 7370: generator_loss=0.7812098860740662, discriminator_loss=0.6096298694610596\n",
            "step 7371: generator_loss=0.7712224721908569, discriminator_loss=0.6103730201721191\n",
            "step 7372: generator_loss=0.7795586585998535, discriminator_loss=0.6046922206878662\n",
            "step 7373: generator_loss=0.7750536203384399, discriminator_loss=0.6075534820556641\n",
            "step 7374: generator_loss=0.7680078744888306, discriminator_loss=0.6094233989715576\n",
            "step 7375: generator_loss=0.7688363790512085, discriminator_loss=0.6112086772918701\n",
            "step 7376: generator_loss=0.7761738300323486, discriminator_loss=0.6094069480895996\n",
            "step 7377: generator_loss=0.7716914415359497, discriminator_loss=0.6142980456352234\n",
            "step 7378: generator_loss=0.7617514729499817, discriminator_loss=0.6194075345993042\n",
            "step 7379: generator_loss=0.7656005024909973, discriminator_loss=0.61936354637146\n",
            "step 7380: generator_loss=0.7591162323951721, discriminator_loss=0.6247570514678955\n",
            "step 7381: generator_loss=0.7677547335624695, discriminator_loss=0.6234076023101807\n",
            "step 7382: generator_loss=0.7578944563865662, discriminator_loss=0.6298760771751404\n",
            "step 7383: generator_loss=0.7514028549194336, discriminator_loss=0.6322985291481018\n",
            "step 7384: generator_loss=0.7534763813018799, discriminator_loss=0.6323792934417725\n",
            "step 7385: generator_loss=0.7449130415916443, discriminator_loss=0.6391244530677795\n",
            "step 7386: generator_loss=0.7430894374847412, discriminator_loss=0.6377032995223999\n",
            "step 7387: generator_loss=0.7377256155014038, discriminator_loss=0.643308162689209\n",
            "step 7388: generator_loss=0.7452974319458008, discriminator_loss=0.6438779830932617\n",
            "step 7389: generator_loss=0.7411632537841797, discriminator_loss=0.6473584175109863\n",
            "step 7390: generator_loss=0.752078652381897, discriminator_loss=0.6445605158805847\n",
            "step 7391: generator_loss=0.7393298745155334, discriminator_loss=0.6547900438308716\n",
            "step 7392: generator_loss=0.7429752349853516, discriminator_loss=0.6580730080604553\n",
            "step 7393: generator_loss=0.7363321185112, discriminator_loss=0.6666682362556458\n",
            "step 7394: generator_loss=0.7376606464385986, discriminator_loss=0.6666939854621887\n",
            "step 7395: generator_loss=0.7204087376594543, discriminator_loss=0.6743054389953613\n",
            "step 7396: generator_loss=0.709717869758606, discriminator_loss=0.6817918419837952\n",
            "step 7397: generator_loss=0.6987307071685791, discriminator_loss=0.6871373653411865\n",
            "step 7398: generator_loss=0.6856003999710083, discriminator_loss=0.6936780214309692\n",
            "step 7399: generator_loss=0.6745389699935913, discriminator_loss=0.6991827487945557\n",
            "step 7400: generator_loss=0.6654403209686279, discriminator_loss=0.7032641172409058\n",
            "step 7401: generator_loss=0.6608087420463562, discriminator_loss=0.7058773636817932\n",
            "step 7402: generator_loss=0.6498863101005554, discriminator_loss=0.7160922288894653\n",
            "step 7403: generator_loss=0.6484032273292542, discriminator_loss=0.7187739014625549\n",
            "step 7404: generator_loss=0.6489729881286621, discriminator_loss=0.7233850955963135\n",
            "step 7405: generator_loss=0.651772141456604, discriminator_loss=0.7261185050010681\n",
            "step 7406: generator_loss=0.6424362659454346, discriminator_loss=0.7361227869987488\n",
            "step 7407: generator_loss=0.6464770436286926, discriminator_loss=0.7394020557403564\n",
            "step 7408: generator_loss=0.6497653722763062, discriminator_loss=0.7405686974525452\n",
            "step 7409: generator_loss=0.6399216055870056, discriminator_loss=0.7505000829696655\n",
            "step 7410: generator_loss=0.642573356628418, discriminator_loss=0.7527000904083252\n",
            "step 7411: generator_loss=0.62804114818573, discriminator_loss=0.7564263343811035\n",
            "step 7412: generator_loss=0.6279175281524658, discriminator_loss=0.7592089176177979\n",
            "step 7413: generator_loss=0.6191896200180054, discriminator_loss=0.7581559419631958\n",
            "step 7414: generator_loss=0.6197149753570557, discriminator_loss=0.7561337351799011\n",
            "step 7415: generator_loss=0.6225688457489014, discriminator_loss=0.755302906036377\n",
            "step 7416: generator_loss=0.6195778846740723, discriminator_loss=0.7542335391044617\n",
            "step 7417: generator_loss=0.6244091987609863, discriminator_loss=0.7493934631347656\n",
            "step 7418: generator_loss=0.627764880657196, discriminator_loss=0.7461292743682861\n",
            "step 7419: generator_loss=0.637656569480896, discriminator_loss=0.7395462989807129\n",
            "step 7420: generator_loss=0.6429633498191833, discriminator_loss=0.7348036766052246\n",
            "step 7421: generator_loss=0.6509870290756226, discriminator_loss=0.7315837144851685\n",
            "step 7422: generator_loss=0.6588393449783325, discriminator_loss=0.7257412075996399\n",
            "step 7423: generator_loss=0.6725540161132812, discriminator_loss=0.7194578647613525\n",
            "step 7424: generator_loss=0.6762229204177856, discriminator_loss=0.7162497043609619\n",
            "step 7425: generator_loss=0.6851152181625366, discriminator_loss=0.7134445905685425\n",
            "step 7426: generator_loss=0.6942328810691833, discriminator_loss=0.7105790376663208\n",
            "step 7427: generator_loss=0.701275646686554, discriminator_loss=0.7094432711601257\n",
            "step 7428: generator_loss=0.701980471611023, discriminator_loss=0.7082273364067078\n",
            "step 7429: generator_loss=0.7095375657081604, discriminator_loss=0.7052614688873291\n",
            "step 7430: generator_loss=0.7136470079421997, discriminator_loss=0.7005858421325684\n",
            "step 7431: generator_loss=0.7047919034957886, discriminator_loss=0.7020708918571472\n",
            "step 7432: generator_loss=0.7121421694755554, discriminator_loss=0.6982225775718689\n",
            "step 7433: generator_loss=0.7092006802558899, discriminator_loss=0.6974852085113525\n",
            "step 7434: generator_loss=0.7056313753128052, discriminator_loss=0.6972918510437012\n",
            "step 7435: generator_loss=0.7083203792572021, discriminator_loss=0.6944581270217896\n",
            "step 7436: generator_loss=0.7107175588607788, discriminator_loss=0.6918643116950989\n",
            "step 7437: generator_loss=0.7011062502861023, discriminator_loss=0.6973370909690857\n",
            "step 7438: generator_loss=0.7030766606330872, discriminator_loss=0.6952900886535645\n",
            "step 7439: generator_loss=0.7082515954971313, discriminator_loss=0.6906691193580627\n",
            "step 7440: generator_loss=0.7034530639648438, discriminator_loss=0.6902871131896973\n",
            "step 7441: generator_loss=0.7040235996246338, discriminator_loss=0.6868542432785034\n",
            "step 7442: generator_loss=0.7054182291030884, discriminator_loss=0.6826756000518799\n",
            "step 7443: generator_loss=0.6981815099716187, discriminator_loss=0.6826565265655518\n",
            "step 7444: generator_loss=0.7007998824119568, discriminator_loss=0.6802306175231934\n",
            "step 7445: generator_loss=0.6961573362350464, discriminator_loss=0.679308295249939\n",
            "step 7446: generator_loss=0.6991889476776123, discriminator_loss=0.6753765344619751\n",
            "step 7447: generator_loss=0.7072059512138367, discriminator_loss=0.6695468425750732\n",
            "step 7448: generator_loss=0.6989054679870605, discriminator_loss=0.672639787197113\n",
            "step 7449: generator_loss=0.7175506353378296, discriminator_loss=0.6624957323074341\n",
            "step 7450: generator_loss=0.7141246795654297, discriminator_loss=0.661893367767334\n",
            "step 7451: generator_loss=0.7136366367340088, discriminator_loss=0.6619051098823547\n",
            "step 7452: generator_loss=0.7301912307739258, discriminator_loss=0.6538028717041016\n",
            "step 7453: generator_loss=0.7237641215324402, discriminator_loss=0.6572959423065186\n",
            "step 7454: generator_loss=0.733582615852356, discriminator_loss=0.6555514335632324\n",
            "step 7455: generator_loss=0.7381453514099121, discriminator_loss=0.6566810607910156\n",
            "step 7456: generator_loss=0.7446202039718628, discriminator_loss=0.659225583076477\n",
            "step 7457: generator_loss=0.741209864616394, discriminator_loss=0.664863109588623\n",
            "step 7458: generator_loss=0.740416407585144, discriminator_loss=0.6699337363243103\n",
            "step 7459: generator_loss=0.7318153381347656, discriminator_loss=0.6739577054977417\n",
            "step 7460: generator_loss=0.7223018407821655, discriminator_loss=0.6820083856582642\n",
            "step 7461: generator_loss=0.7115128040313721, discriminator_loss=0.6900313496589661\n",
            "step 7462: generator_loss=0.704308271408081, discriminator_loss=0.6936452388763428\n",
            "step 7463: generator_loss=0.7104445695877075, discriminator_loss=0.6885526180267334\n",
            "step 7464: generator_loss=0.6816105246543884, discriminator_loss=0.7041165828704834\n",
            "step 7465: generator_loss=0.6878495216369629, discriminator_loss=0.7014591693878174\n",
            "step 7466: generator_loss=0.6763378381729126, discriminator_loss=0.7046072483062744\n",
            "step 7467: generator_loss=0.6638623476028442, discriminator_loss=0.711268424987793\n",
            "step 7468: generator_loss=0.6524602770805359, discriminator_loss=0.7190791964530945\n",
            "step 7469: generator_loss=0.644038200378418, discriminator_loss=0.7250199317932129\n",
            "step 7470: generator_loss=0.6415655612945557, discriminator_loss=0.7311169505119324\n",
            "step 7471: generator_loss=0.6467059850692749, discriminator_loss=0.7293998003005981\n",
            "step 7472: generator_loss=0.6270384192466736, discriminator_loss=0.7449400424957275\n",
            "step 7473: generator_loss=0.6447135210037231, discriminator_loss=0.7381914258003235\n",
            "step 7474: generator_loss=0.6460127234458923, discriminator_loss=0.7379137277603149\n",
            "step 7475: generator_loss=0.6549441814422607, discriminator_loss=0.7373086214065552\n",
            "step 7476: generator_loss=0.6408125162124634, discriminator_loss=0.7476391792297363\n",
            "step 7477: generator_loss=0.6611518859863281, discriminator_loss=0.7380388379096985\n",
            "step 7478: generator_loss=0.6630299091339111, discriminator_loss=0.735913872718811\n",
            "step 7479: generator_loss=0.6772329211235046, discriminator_loss=0.7279471158981323\n",
            "step 7480: generator_loss=0.6675131320953369, discriminator_loss=0.7343268394470215\n",
            "step 7481: generator_loss=0.6721441745758057, discriminator_loss=0.7322694063186646\n",
            "step 7482: generator_loss=0.6645913124084473, discriminator_loss=0.7336374521255493\n",
            "step 7483: generator_loss=0.6856838464736938, discriminator_loss=0.7207186818122864\n",
            "step 7484: generator_loss=0.6860635280609131, discriminator_loss=0.7206149101257324\n",
            "step 7485: generator_loss=0.6901562809944153, discriminator_loss=0.7151299715042114\n",
            "step 7486: generator_loss=0.6738581657409668, discriminator_loss=0.7203603386878967\n",
            "step 7487: generator_loss=0.6810204982757568, discriminator_loss=0.7104880213737488\n",
            "step 7488: generator_loss=0.6845718622207642, discriminator_loss=0.7037435173988342\n",
            "step 7489: generator_loss=0.6794453859329224, discriminator_loss=0.7002513408660889\n",
            "step 7490: generator_loss=0.6822073459625244, discriminator_loss=0.6927132606506348\n",
            "step 7491: generator_loss=0.6861555576324463, discriminator_loss=0.6866313219070435\n",
            "step 7492: generator_loss=0.697907567024231, discriminator_loss=0.6803435683250427\n",
            "step 7493: generator_loss=0.7007967233657837, discriminator_loss=0.6789489388465881\n",
            "step 7494: generator_loss=0.7054288387298584, discriminator_loss=0.6782003045082092\n",
            "step 7495: generator_loss=0.7007343769073486, discriminator_loss=0.6824166774749756\n",
            "step 7496: generator_loss=0.7134463787078857, discriminator_loss=0.6760097742080688\n",
            "step 7497: generator_loss=0.720207929611206, discriminator_loss=0.6741442680358887\n",
            "step 7498: generator_loss=0.7141490578651428, discriminator_loss=0.6753418445587158\n",
            "step 7499: generator_loss=0.7280157208442688, discriminator_loss=0.6673319935798645\n",
            "step 7500: generator_loss=0.7255184054374695, discriminator_loss=0.6656748056411743\n",
            "step 7501: generator_loss=0.7305726408958435, discriminator_loss=0.6625214219093323\n",
            "step 7502: generator_loss=0.7272603511810303, discriminator_loss=0.6616581678390503\n",
            "step 7503: generator_loss=0.7332031726837158, discriminator_loss=0.6556409597396851\n",
            "step 7504: generator_loss=0.7416096925735474, discriminator_loss=0.6493315100669861\n",
            "step 7505: generator_loss=0.7289276719093323, discriminator_loss=0.654137134552002\n",
            "step 7506: generator_loss=0.7387934923171997, discriminator_loss=0.6497604846954346\n",
            "step 7507: generator_loss=0.7378497123718262, discriminator_loss=0.6505522727966309\n",
            "step 7508: generator_loss=0.7512667179107666, discriminator_loss=0.6457493305206299\n",
            "step 7509: generator_loss=0.7574813365936279, discriminator_loss=0.6446915864944458\n",
            "step 7510: generator_loss=0.7496347427368164, discriminator_loss=0.6485862731933594\n",
            "step 7511: generator_loss=0.7621203660964966, discriminator_loss=0.6435519456863403\n",
            "step 7512: generator_loss=0.7542831301689148, discriminator_loss=0.6465127468109131\n",
            "step 7513: generator_loss=0.7518032789230347, discriminator_loss=0.645891547203064\n",
            "step 7514: generator_loss=0.7491167187690735, discriminator_loss=0.6458722352981567\n",
            "step 7515: generator_loss=0.7516487240791321, discriminator_loss=0.6446310877799988\n",
            "step 7516: generator_loss=0.7548249959945679, discriminator_loss=0.6423643231391907\n",
            "step 7517: generator_loss=0.7667046785354614, discriminator_loss=0.6376098394393921\n",
            "step 7518: generator_loss=0.7633787393569946, discriminator_loss=0.6410355567932129\n",
            "step 7519: generator_loss=0.7558643817901611, discriminator_loss=0.6449146270751953\n",
            "step 7520: generator_loss=0.7497395277023315, discriminator_loss=0.6485117673873901\n",
            "step 7521: generator_loss=0.738036572933197, discriminator_loss=0.6549208760261536\n",
            "step 7522: generator_loss=0.7374050617218018, discriminator_loss=0.6543788909912109\n",
            "step 7523: generator_loss=0.7379000186920166, discriminator_loss=0.6548453569412231\n",
            "step 7524: generator_loss=0.7358154058456421, discriminator_loss=0.6563155651092529\n",
            "step 7525: generator_loss=0.725351095199585, discriminator_loss=0.6608142852783203\n",
            "step 7526: generator_loss=0.7134910821914673, discriminator_loss=0.6667423248291016\n",
            "step 7527: generator_loss=0.7120269536972046, discriminator_loss=0.6681441068649292\n",
            "step 7528: generator_loss=0.7155250906944275, discriminator_loss=0.6687198877334595\n",
            "step 7529: generator_loss=0.713302493095398, discriminator_loss=0.6725262999534607\n",
            "step 7530: generator_loss=0.7099937200546265, discriminator_loss=0.6744939684867859\n",
            "step 7531: generator_loss=0.7004123330116272, discriminator_loss=0.6805405616760254\n",
            "step 7532: generator_loss=0.6938977241516113, discriminator_loss=0.6856875419616699\n",
            "step 7533: generator_loss=0.6944863796234131, discriminator_loss=0.6867090463638306\n",
            "step 7534: generator_loss=0.691612720489502, discriminator_loss=0.6895585060119629\n",
            "step 7535: generator_loss=0.6879031658172607, discriminator_loss=0.6934261322021484\n",
            "step 7536: generator_loss=0.6814579367637634, discriminator_loss=0.6981377601623535\n",
            "step 7537: generator_loss=0.6871423721313477, discriminator_loss=0.6951348781585693\n",
            "step 7538: generator_loss=0.6887643337249756, discriminator_loss=0.6961723566055298\n",
            "step 7539: generator_loss=0.6805230379104614, discriminator_loss=0.6997092962265015\n",
            "step 7540: generator_loss=0.6783950924873352, discriminator_loss=0.7017710208892822\n",
            "step 7541: generator_loss=0.6763049364089966, discriminator_loss=0.7042673826217651\n",
            "step 7542: generator_loss=0.6782149076461792, discriminator_loss=0.7024860382080078\n",
            "step 7543: generator_loss=0.6874169707298279, discriminator_loss=0.7023277878761292\n",
            "step 7544: generator_loss=0.6859480738639832, discriminator_loss=0.7007187008857727\n",
            "step 7545: generator_loss=0.6862149238586426, discriminator_loss=0.7024046182632446\n",
            "step 7546: generator_loss=0.6827313303947449, discriminator_loss=0.7038030028343201\n",
            "step 7547: generator_loss=0.6874106526374817, discriminator_loss=0.7018483877182007\n",
            "step 7548: generator_loss=0.7021003365516663, discriminator_loss=0.694054365158081\n",
            "step 7549: generator_loss=0.7046126127243042, discriminator_loss=0.6911389827728271\n",
            "step 7550: generator_loss=0.7065510749816895, discriminator_loss=0.688819169998169\n",
            "step 7551: generator_loss=0.7070602774620056, discriminator_loss=0.6854465007781982\n",
            "step 7552: generator_loss=0.702633261680603, discriminator_loss=0.6854557394981384\n",
            "step 7553: generator_loss=0.7124536037445068, discriminator_loss=0.6816011667251587\n",
            "step 7554: generator_loss=0.7100487947463989, discriminator_loss=0.681117057800293\n",
            "step 7555: generator_loss=0.7079665064811707, discriminator_loss=0.6828899383544922\n",
            "step 7556: generator_loss=0.7141393423080444, discriminator_loss=0.6811022758483887\n",
            "step 7557: generator_loss=0.7088518142700195, discriminator_loss=0.6854329109191895\n",
            "step 7558: generator_loss=0.7096279859542847, discriminator_loss=0.6872373819351196\n",
            "step 7559: generator_loss=0.716167688369751, discriminator_loss=0.6858893036842346\n",
            "step 7560: generator_loss=0.7011452317237854, discriminator_loss=0.6936543583869934\n",
            "step 7561: generator_loss=0.6983783841133118, discriminator_loss=0.6942764520645142\n",
            "step 7562: generator_loss=0.6970723271369934, discriminator_loss=0.6939926743507385\n",
            "step 7563: generator_loss=0.68950355052948, discriminator_loss=0.6981387138366699\n",
            "step 7564: generator_loss=0.6871099472045898, discriminator_loss=0.7002139091491699\n",
            "step 7565: generator_loss=0.6807287931442261, discriminator_loss=0.7014021277427673\n",
            "step 7566: generator_loss=0.6726619005203247, discriminator_loss=0.7062768936157227\n",
            "step 7567: generator_loss=0.6770835518836975, discriminator_loss=0.7036112546920776\n",
            "step 7568: generator_loss=0.6694526672363281, discriminator_loss=0.7056758999824524\n",
            "step 7569: generator_loss=0.6661067605018616, discriminator_loss=0.7080546617507935\n",
            "step 7570: generator_loss=0.6717531681060791, discriminator_loss=0.7057554125785828\n",
            "step 7571: generator_loss=0.6691924929618835, discriminator_loss=0.7086392045021057\n",
            "step 7572: generator_loss=0.6778054237365723, discriminator_loss=0.704727053642273\n",
            "step 7573: generator_loss=0.6737655401229858, discriminator_loss=0.7063995599746704\n",
            "step 7574: generator_loss=0.6852618455886841, discriminator_loss=0.6995086073875427\n",
            "step 7575: generator_loss=0.6866081953048706, discriminator_loss=0.6995341181755066\n",
            "step 7576: generator_loss=0.6844052672386169, discriminator_loss=0.7017149925231934\n",
            "step 7577: generator_loss=0.6921087503433228, discriminator_loss=0.6975902318954468\n",
            "step 7578: generator_loss=0.6919049024581909, discriminator_loss=0.6974014043807983\n",
            "step 7579: generator_loss=0.695034921169281, discriminator_loss=0.6959575414657593\n",
            "step 7580: generator_loss=0.7020601034164429, discriminator_loss=0.6918413639068604\n",
            "step 7581: generator_loss=0.6918389797210693, discriminator_loss=0.6978602409362793\n",
            "step 7582: generator_loss=0.6995043158531189, discriminator_loss=0.6937340497970581\n",
            "step 7583: generator_loss=0.6945253014564514, discriminator_loss=0.695133626461029\n",
            "step 7584: generator_loss=0.6940886974334717, discriminator_loss=0.6937233805656433\n",
            "step 7585: generator_loss=0.6900030970573425, discriminator_loss=0.694118857383728\n",
            "step 7586: generator_loss=0.6941425800323486, discriminator_loss=0.6904381513595581\n",
            "step 7587: generator_loss=0.681880533695221, discriminator_loss=0.6970341801643372\n",
            "step 7588: generator_loss=0.6804901361465454, discriminator_loss=0.7002102732658386\n",
            "step 7589: generator_loss=0.6909329295158386, discriminator_loss=0.6968874931335449\n",
            "step 7590: generator_loss=0.6812136173248291, discriminator_loss=0.7037657499313354\n",
            "step 7591: generator_loss=0.6829845905303955, discriminator_loss=0.7047415971755981\n",
            "step 7592: generator_loss=0.6805741190910339, discriminator_loss=0.7114217281341553\n",
            "step 7593: generator_loss=0.6721017360687256, discriminator_loss=0.7167392373085022\n",
            "step 7594: generator_loss=0.6934020519256592, discriminator_loss=0.7059473991394043\n",
            "step 7595: generator_loss=0.6925044655799866, discriminator_loss=0.7065590620040894\n",
            "step 7596: generator_loss=0.6757813692092896, discriminator_loss=0.714862585067749\n",
            "step 7597: generator_loss=0.6710611581802368, discriminator_loss=0.7158628702163696\n",
            "step 7598: generator_loss=0.6782845258712769, discriminator_loss=0.7123771905899048\n",
            "step 7599: generator_loss=0.6770849823951721, discriminator_loss=0.7114651799201965\n",
            "step 7600: generator_loss=0.6802868247032166, discriminator_loss=0.7079321146011353\n",
            "step 7601: generator_loss=0.6808348894119263, discriminator_loss=0.7068312764167786\n",
            "step 7602: generator_loss=0.6772698163986206, discriminator_loss=0.7089241147041321\n",
            "step 7603: generator_loss=0.6757302284240723, discriminator_loss=0.7085322141647339\n",
            "step 7604: generator_loss=0.6770825386047363, discriminator_loss=0.7066422700881958\n",
            "step 7605: generator_loss=0.6619962453842163, discriminator_loss=0.7130918502807617\n",
            "step 7606: generator_loss=0.6721195578575134, discriminator_loss=0.705197811126709\n",
            "step 7607: generator_loss=0.6612837314605713, discriminator_loss=0.7138529419898987\n",
            "step 7608: generator_loss=0.6666948795318604, discriminator_loss=0.7101070284843445\n",
            "step 7609: generator_loss=0.6781712770462036, discriminator_loss=0.7053883075714111\n",
            "step 7610: generator_loss=0.6821765899658203, discriminator_loss=0.7022159695625305\n",
            "step 7611: generator_loss=0.6879600286483765, discriminator_loss=0.7006925344467163\n",
            "step 7612: generator_loss=0.6890825033187866, discriminator_loss=0.6989829540252686\n",
            "step 7613: generator_loss=0.6921836733818054, discriminator_loss=0.6988921165466309\n",
            "step 7614: generator_loss=0.7014868259429932, discriminator_loss=0.6942116022109985\n",
            "step 7615: generator_loss=0.6913721561431885, discriminator_loss=0.6997615098953247\n",
            "step 7616: generator_loss=0.6936498880386353, discriminator_loss=0.6984806060791016\n",
            "step 7617: generator_loss=0.6980530619621277, discriminator_loss=0.6958768963813782\n",
            "step 7618: generator_loss=0.6958770751953125, discriminator_loss=0.6954501867294312\n",
            "step 7619: generator_loss=0.7020198106765747, discriminator_loss=0.6905431747436523\n",
            "step 7620: generator_loss=0.7026370763778687, discriminator_loss=0.6885413527488708\n",
            "step 7621: generator_loss=0.6910480856895447, discriminator_loss=0.6935844421386719\n",
            "step 7622: generator_loss=0.706113338470459, discriminator_loss=0.6852741837501526\n",
            "step 7623: generator_loss=0.6997898817062378, discriminator_loss=0.6867460012435913\n",
            "step 7624: generator_loss=0.7024205923080444, discriminator_loss=0.6881304979324341\n",
            "step 7625: generator_loss=0.6999302506446838, discriminator_loss=0.6890741586685181\n",
            "step 7626: generator_loss=0.700855016708374, discriminator_loss=0.6881077885627747\n",
            "step 7627: generator_loss=0.7125719785690308, discriminator_loss=0.6799299716949463\n",
            "step 7628: generator_loss=0.715571403503418, discriminator_loss=0.679127037525177\n",
            "step 7629: generator_loss=0.7092305421829224, discriminator_loss=0.6816681623458862\n",
            "step 7630: generator_loss=0.7093921899795532, discriminator_loss=0.6794212460517883\n",
            "step 7631: generator_loss=0.7109032869338989, discriminator_loss=0.6801870465278625\n",
            "step 7632: generator_loss=0.7017353177070618, discriminator_loss=0.6823961734771729\n",
            "step 7633: generator_loss=0.6968512535095215, discriminator_loss=0.6836254000663757\n",
            "step 7634: generator_loss=0.6958925127983093, discriminator_loss=0.6853944063186646\n",
            "step 7635: generator_loss=0.6963452100753784, discriminator_loss=0.6868909597396851\n",
            "step 7636: generator_loss=0.6945363283157349, discriminator_loss=0.6860406994819641\n",
            "step 7637: generator_loss=0.6931995153427124, discriminator_loss=0.688477635383606\n",
            "step 7638: generator_loss=0.6991955041885376, discriminator_loss=0.6870645880699158\n",
            "step 7639: generator_loss=0.7034242153167725, discriminator_loss=0.6870652437210083\n",
            "step 7640: generator_loss=0.6997114419937134, discriminator_loss=0.689993143081665\n",
            "step 7641: generator_loss=0.7071368098258972, discriminator_loss=0.6863775253295898\n",
            "step 7642: generator_loss=0.7059407830238342, discriminator_loss=0.686874270439148\n",
            "step 7643: generator_loss=0.7017683982849121, discriminator_loss=0.6888896226882935\n",
            "step 7644: generator_loss=0.7005925178527832, discriminator_loss=0.6894209384918213\n",
            "step 7645: generator_loss=0.7069872617721558, discriminator_loss=0.684272289276123\n",
            "step 7646: generator_loss=0.7129316926002502, discriminator_loss=0.6810060739517212\n",
            "step 7647: generator_loss=0.7165371775627136, discriminator_loss=0.6791256666183472\n",
            "step 7648: generator_loss=0.7180194854736328, discriminator_loss=0.675931453704834\n",
            "step 7649: generator_loss=0.7188012003898621, discriminator_loss=0.676476240158081\n",
            "step 7650: generator_loss=0.7313787937164307, discriminator_loss=0.6717743873596191\n",
            "step 7651: generator_loss=0.7280068397521973, discriminator_loss=0.6739853024482727\n",
            "step 7652: generator_loss=0.7274833917617798, discriminator_loss=0.6713504791259766\n",
            "step 7653: generator_loss=0.7290998101234436, discriminator_loss=0.6695111989974976\n",
            "step 7654: generator_loss=0.7352286577224731, discriminator_loss=0.6643932461738586\n",
            "step 7655: generator_loss=0.7321168184280396, discriminator_loss=0.6635446548461914\n",
            "step 7656: generator_loss=0.7309412956237793, discriminator_loss=0.6617624759674072\n",
            "step 7657: generator_loss=0.7264063954353333, discriminator_loss=0.660442054271698\n",
            "step 7658: generator_loss=0.7336341738700867, discriminator_loss=0.6559461355209351\n",
            "step 7659: generator_loss=0.725246012210846, discriminator_loss=0.6571859121322632\n",
            "step 7660: generator_loss=0.7276958227157593, discriminator_loss=0.6578248739242554\n",
            "step 7661: generator_loss=0.7242562770843506, discriminator_loss=0.6584219932556152\n",
            "step 7662: generator_loss=0.7285890579223633, discriminator_loss=0.656019389629364\n",
            "step 7663: generator_loss=0.722625732421875, discriminator_loss=0.6580837368965149\n",
            "step 7664: generator_loss=0.723602294921875, discriminator_loss=0.6584241390228271\n",
            "step 7665: generator_loss=0.7309792041778564, discriminator_loss=0.6550665497779846\n",
            "step 7666: generator_loss=0.7326535582542419, discriminator_loss=0.6573498249053955\n",
            "step 7667: generator_loss=0.7336537837982178, discriminator_loss=0.6553963422775269\n",
            "step 7668: generator_loss=0.7283064126968384, discriminator_loss=0.659247875213623\n",
            "step 7669: generator_loss=0.7229109406471252, discriminator_loss=0.6617264747619629\n",
            "step 7670: generator_loss=0.724174976348877, discriminator_loss=0.6600980162620544\n",
            "step 7671: generator_loss=0.7216764092445374, discriminator_loss=0.6629520058631897\n",
            "step 7672: generator_loss=0.7292309999465942, discriminator_loss=0.6589350700378418\n",
            "step 7673: generator_loss=0.7252856492996216, discriminator_loss=0.6638174057006836\n",
            "step 7674: generator_loss=0.7277867794036865, discriminator_loss=0.6621749997138977\n",
            "step 7675: generator_loss=0.7237558364868164, discriminator_loss=0.666890025138855\n",
            "step 7676: generator_loss=0.7157241106033325, discriminator_loss=0.672187328338623\n",
            "step 7677: generator_loss=0.712607741355896, discriminator_loss=0.6739075183868408\n",
            "step 7678: generator_loss=0.7324832081794739, discriminator_loss=0.6659107208251953\n",
            "step 7679: generator_loss=0.7213730812072754, discriminator_loss=0.6744130253791809\n",
            "step 7680: generator_loss=0.7190829515457153, discriminator_loss=0.677757978439331\n",
            "step 7681: generator_loss=0.7128043174743652, discriminator_loss=0.6817823648452759\n",
            "step 7682: generator_loss=0.7114549279212952, discriminator_loss=0.6821421980857849\n",
            "step 7683: generator_loss=0.7031662464141846, discriminator_loss=0.6867141127586365\n",
            "step 7684: generator_loss=0.6887528896331787, discriminator_loss=0.6934500932693481\n",
            "step 7685: generator_loss=0.7025051712989807, discriminator_loss=0.6858881115913391\n",
            "step 7686: generator_loss=0.6769009828567505, discriminator_loss=0.6982787847518921\n",
            "step 7687: generator_loss=0.6745097637176514, discriminator_loss=0.7006062269210815\n",
            "step 7688: generator_loss=0.6796663403511047, discriminator_loss=0.6977847218513489\n",
            "step 7689: generator_loss=0.6776914000511169, discriminator_loss=0.7007761001586914\n",
            "step 7690: generator_loss=0.6856479644775391, discriminator_loss=0.6973472833633423\n",
            "step 7691: generator_loss=0.6751508116722107, discriminator_loss=0.7023800611495972\n",
            "step 7692: generator_loss=0.6767503619194031, discriminator_loss=0.7023552656173706\n",
            "step 7693: generator_loss=0.6820389032363892, discriminator_loss=0.7024178504943848\n",
            "step 7694: generator_loss=0.6812607049942017, discriminator_loss=0.7043172121047974\n",
            "step 7695: generator_loss=0.6781980991363525, discriminator_loss=0.7083836197853088\n",
            "step 7696: generator_loss=0.6884112358093262, discriminator_loss=0.7047237157821655\n",
            "step 7697: generator_loss=0.6873630285263062, discriminator_loss=0.7069954872131348\n",
            "step 7698: generator_loss=0.6880683898925781, discriminator_loss=0.7072162628173828\n",
            "step 7699: generator_loss=0.6905474662780762, discriminator_loss=0.7091110944747925\n",
            "step 7700: generator_loss=0.6891790628433228, discriminator_loss=0.7086588144302368\n",
            "step 7701: generator_loss=0.6916104555130005, discriminator_loss=0.7073602080345154\n",
            "step 7702: generator_loss=0.6829612255096436, discriminator_loss=0.7095056772232056\n",
            "step 7703: generator_loss=0.6841832995414734, discriminator_loss=0.7067896127700806\n",
            "step 7704: generator_loss=0.6803662776947021, discriminator_loss=0.7062783241271973\n",
            "step 7705: generator_loss=0.6777494549751282, discriminator_loss=0.7048720121383667\n",
            "step 7706: generator_loss=0.6745041012763977, discriminator_loss=0.7035642862319946\n",
            "step 7707: generator_loss=0.6774438619613647, discriminator_loss=0.699251651763916\n",
            "step 7708: generator_loss=0.6867584586143494, discriminator_loss=0.6940743923187256\n",
            "step 7709: generator_loss=0.6840032339096069, discriminator_loss=0.6962853670120239\n",
            "step 7710: generator_loss=0.6900380849838257, discriminator_loss=0.694409191608429\n",
            "step 7711: generator_loss=0.6944024562835693, discriminator_loss=0.6933907270431519\n",
            "step 7712: generator_loss=0.6946092247962952, discriminator_loss=0.6928785443305969\n",
            "step 7713: generator_loss=0.7043313980102539, discriminator_loss=0.6903340816497803\n",
            "step 7714: generator_loss=0.7028226256370544, discriminator_loss=0.6917740702629089\n",
            "step 7715: generator_loss=0.7009271383285522, discriminator_loss=0.6942963600158691\n",
            "step 7716: generator_loss=0.7027548551559448, discriminator_loss=0.6946990489959717\n",
            "step 7717: generator_loss=0.6996283531188965, discriminator_loss=0.6960042119026184\n",
            "step 7718: generator_loss=0.7038406133651733, discriminator_loss=0.6935548782348633\n",
            "step 7719: generator_loss=0.6988955736160278, discriminator_loss=0.6950822472572327\n",
            "step 7720: generator_loss=0.6985464096069336, discriminator_loss=0.693706750869751\n",
            "step 7721: generator_loss=0.6987866163253784, discriminator_loss=0.6919653415679932\n",
            "step 7722: generator_loss=0.6934219598770142, discriminator_loss=0.6934711933135986\n",
            "step 7723: generator_loss=0.6868729591369629, discriminator_loss=0.6946936249732971\n",
            "step 7724: generator_loss=0.6943551301956177, discriminator_loss=0.6900689601898193\n",
            "step 7725: generator_loss=0.694054365158081, discriminator_loss=0.6881773471832275\n",
            "step 7726: generator_loss=0.697844922542572, discriminator_loss=0.6859912276268005\n",
            "step 7727: generator_loss=0.6959881782531738, discriminator_loss=0.687736988067627\n",
            "step 7728: generator_loss=0.7012125253677368, discriminator_loss=0.6846783757209778\n",
            "step 7729: generator_loss=0.7092941999435425, discriminator_loss=0.682451605796814\n",
            "step 7730: generator_loss=0.7091482877731323, discriminator_loss=0.6835780739784241\n",
            "step 7731: generator_loss=0.7093431353569031, discriminator_loss=0.6833484172821045\n",
            "step 7732: generator_loss=0.7082538604736328, discriminator_loss=0.6846674680709839\n",
            "step 7733: generator_loss=0.7047289609909058, discriminator_loss=0.6876585483551025\n",
            "step 7734: generator_loss=0.7053239941596985, discriminator_loss=0.6874679923057556\n",
            "step 7735: generator_loss=0.7033815383911133, discriminator_loss=0.6881916522979736\n",
            "step 7736: generator_loss=0.706898033618927, discriminator_loss=0.6867858171463013\n",
            "step 7737: generator_loss=0.7068738341331482, discriminator_loss=0.6848689913749695\n",
            "step 7738: generator_loss=0.6991948485374451, discriminator_loss=0.6875001192092896\n",
            "step 7739: generator_loss=0.7000970840454102, discriminator_loss=0.6863983869552612\n",
            "step 7740: generator_loss=0.690052330493927, discriminator_loss=0.6887506246566772\n",
            "step 7741: generator_loss=0.6924174427986145, discriminator_loss=0.6874005198478699\n",
            "step 7742: generator_loss=0.69449782371521, discriminator_loss=0.686754584312439\n",
            "step 7743: generator_loss=0.6971161365509033, discriminator_loss=0.684394359588623\n",
            "step 7744: generator_loss=0.6972358226776123, discriminator_loss=0.6845878958702087\n",
            "step 7745: generator_loss=0.7046245336532593, discriminator_loss=0.6787475943565369\n",
            "step 7746: generator_loss=0.7097278833389282, discriminator_loss=0.6767657399177551\n",
            "step 7747: generator_loss=0.7185575366020203, discriminator_loss=0.6751375198364258\n",
            "step 7748: generator_loss=0.7190346717834473, discriminator_loss=0.6752041578292847\n",
            "step 7749: generator_loss=0.7287598252296448, discriminator_loss=0.6723096370697021\n",
            "step 7750: generator_loss=0.7353543043136597, discriminator_loss=0.6677429676055908\n",
            "step 7751: generator_loss=0.7359029054641724, discriminator_loss=0.6662728786468506\n",
            "step 7752: generator_loss=0.7362924218177795, discriminator_loss=0.666441798210144\n",
            "step 7753: generator_loss=0.7370046377182007, discriminator_loss=0.6670986413955688\n",
            "step 7754: generator_loss=0.7350999116897583, discriminator_loss=0.6669886112213135\n",
            "step 7755: generator_loss=0.7294869422912598, discriminator_loss=0.6692452430725098\n",
            "step 7756: generator_loss=0.7248818874359131, discriminator_loss=0.6711910963058472\n",
            "step 7757: generator_loss=0.7313145995140076, discriminator_loss=0.668858528137207\n",
            "step 7758: generator_loss=0.7273834943771362, discriminator_loss=0.6692996025085449\n",
            "step 7759: generator_loss=0.71672523021698, discriminator_loss=0.6735659241676331\n",
            "step 7760: generator_loss=0.7136390209197998, discriminator_loss=0.6740094423294067\n",
            "step 7761: generator_loss=0.7083543539047241, discriminator_loss=0.6762776970863342\n",
            "step 7762: generator_loss=0.7052030563354492, discriminator_loss=0.6780390739440918\n",
            "step 7763: generator_loss=0.7011433839797974, discriminator_loss=0.680800199508667\n",
            "step 7764: generator_loss=0.6999943256378174, discriminator_loss=0.6829417943954468\n",
            "step 7765: generator_loss=0.6936522722244263, discriminator_loss=0.6869733929634094\n",
            "step 7766: generator_loss=0.6934728026390076, discriminator_loss=0.6877247095108032\n",
            "step 7767: generator_loss=0.6911708116531372, discriminator_loss=0.6902907490730286\n",
            "step 7768: generator_loss=0.689671516418457, discriminator_loss=0.692727267742157\n",
            "step 7769: generator_loss=0.6841650605201721, discriminator_loss=0.6961358189582825\n",
            "step 7770: generator_loss=0.685411810874939, discriminator_loss=0.6962988376617432\n",
            "step 7771: generator_loss=0.6840398907661438, discriminator_loss=0.6994544267654419\n",
            "step 7772: generator_loss=0.6822530627250671, discriminator_loss=0.7034838795661926\n",
            "step 7773: generator_loss=0.6824503540992737, discriminator_loss=0.7025859355926514\n",
            "step 7774: generator_loss=0.6819185614585876, discriminator_loss=0.7057077884674072\n",
            "step 7775: generator_loss=0.680854082107544, discriminator_loss=0.7066221237182617\n",
            "step 7776: generator_loss=0.6784057021141052, discriminator_loss=0.7072968482971191\n",
            "step 7777: generator_loss=0.6782850027084351, discriminator_loss=0.7081419229507446\n",
            "step 7778: generator_loss=0.6805911064147949, discriminator_loss=0.707917332649231\n",
            "step 7779: generator_loss=0.6822530031204224, discriminator_loss=0.707514762878418\n",
            "step 7780: generator_loss=0.6819965839385986, discriminator_loss=0.7096879482269287\n",
            "step 7781: generator_loss=0.6845631003379822, discriminator_loss=0.7083722352981567\n",
            "step 7782: generator_loss=0.6840640902519226, discriminator_loss=0.7106468081474304\n",
            "step 7783: generator_loss=0.6873304843902588, discriminator_loss=0.7092728614807129\n",
            "step 7784: generator_loss=0.6849881410598755, discriminator_loss=0.7104716300964355\n",
            "step 7785: generator_loss=0.6831927299499512, discriminator_loss=0.7123562097549438\n",
            "step 7786: generator_loss=0.6799479126930237, discriminator_loss=0.7143420577049255\n",
            "step 7787: generator_loss=0.6758078336715698, discriminator_loss=0.7162346243858337\n",
            "step 7788: generator_loss=0.6706969738006592, discriminator_loss=0.7184394598007202\n",
            "step 7789: generator_loss=0.6650824546813965, discriminator_loss=0.7207160592079163\n",
            "step 7790: generator_loss=0.6630548238754272, discriminator_loss=0.7211978435516357\n",
            "step 7791: generator_loss=0.6596365571022034, discriminator_loss=0.7226972579956055\n",
            "step 7792: generator_loss=0.6583348512649536, discriminator_loss=0.7244380712509155\n",
            "step 7793: generator_loss=0.658799409866333, discriminator_loss=0.7234576940536499\n",
            "step 7794: generator_loss=0.6576765179634094, discriminator_loss=0.725605309009552\n",
            "step 7795: generator_loss=0.6587284207344055, discriminator_loss=0.7250889539718628\n",
            "step 7796: generator_loss=0.6592224836349487, discriminator_loss=0.7271738052368164\n",
            "step 7797: generator_loss=0.6571605205535889, discriminator_loss=0.7306128144264221\n",
            "step 7798: generator_loss=0.6584765911102295, discriminator_loss=0.7310633659362793\n",
            "step 7799: generator_loss=0.6621972918510437, discriminator_loss=0.7286261320114136\n",
            "step 7800: generator_loss=0.6636391878128052, discriminator_loss=0.7305461168289185\n",
            "step 7801: generator_loss=0.662796676158905, discriminator_loss=0.7291642427444458\n",
            "step 7802: generator_loss=0.6661350727081299, discriminator_loss=0.7277566194534302\n",
            "step 7803: generator_loss=0.668797492980957, discriminator_loss=0.7261394262313843\n",
            "step 7804: generator_loss=0.6650209426879883, discriminator_loss=0.7267475724220276\n",
            "step 7805: generator_loss=0.667418897151947, discriminator_loss=0.725723385810852\n",
            "step 7806: generator_loss=0.6642130613327026, discriminator_loss=0.7269206047058105\n",
            "step 7807: generator_loss=0.6617766618728638, discriminator_loss=0.7266620397567749\n",
            "step 7808: generator_loss=0.6568182706832886, discriminator_loss=0.730708658695221\n",
            "step 7809: generator_loss=0.6532798409461975, discriminator_loss=0.7335790991783142\n",
            "step 7810: generator_loss=0.6513310074806213, discriminator_loss=0.7334739565849304\n",
            "step 7811: generator_loss=0.6455965638160706, discriminator_loss=0.7376601099967957\n",
            "step 7812: generator_loss=0.6456379890441895, discriminator_loss=0.7371029853820801\n",
            "step 7813: generator_loss=0.6406657695770264, discriminator_loss=0.7422097325325012\n",
            "step 7814: generator_loss=0.6394858360290527, discriminator_loss=0.7447352409362793\n",
            "step 7815: generator_loss=0.635602593421936, discriminator_loss=0.7480253577232361\n",
            "step 7816: generator_loss=0.6371036171913147, discriminator_loss=0.7478588819503784\n",
            "step 7817: generator_loss=0.634425163269043, discriminator_loss=0.750585675239563\n",
            "step 7818: generator_loss=0.6295503377914429, discriminator_loss=0.7559070587158203\n",
            "step 7819: generator_loss=0.6184861660003662, discriminator_loss=0.7621616125106812\n",
            "step 7820: generator_loss=0.6214828491210938, discriminator_loss=0.7638979554176331\n",
            "step 7821: generator_loss=0.6283046007156372, discriminator_loss=0.7612056732177734\n",
            "step 7822: generator_loss=0.615052342414856, discriminator_loss=0.7693524360656738\n",
            "step 7823: generator_loss=0.6243240237236023, discriminator_loss=0.7663281559944153\n",
            "step 7824: generator_loss=0.6259546875953674, discriminator_loss=0.7629213929176331\n",
            "step 7825: generator_loss=0.6198866367340088, discriminator_loss=0.7650744915008545\n",
            "step 7826: generator_loss=0.620454728603363, discriminator_loss=0.7652922868728638\n",
            "step 7827: generator_loss=0.6189234256744385, discriminator_loss=0.7638755440711975\n",
            "step 7828: generator_loss=0.6253006458282471, discriminator_loss=0.7577568292617798\n",
            "step 7829: generator_loss=0.6263120770454407, discriminator_loss=0.7539393901824951\n",
            "step 7830: generator_loss=0.6327323913574219, discriminator_loss=0.7442895174026489\n",
            "step 7831: generator_loss=0.6413192749023438, discriminator_loss=0.7378495931625366\n",
            "step 7832: generator_loss=0.6471059322357178, discriminator_loss=0.7298033237457275\n",
            "step 7833: generator_loss=0.653681755065918, discriminator_loss=0.7239221334457397\n",
            "step 7834: generator_loss=0.6677590608596802, discriminator_loss=0.7134774923324585\n",
            "step 7835: generator_loss=0.6829432249069214, discriminator_loss=0.7020018100738525\n",
            "step 7836: generator_loss=0.693148672580719, discriminator_loss=0.6947387456893921\n",
            "step 7837: generator_loss=0.7215310335159302, discriminator_loss=0.6796000599861145\n",
            "step 7838: generator_loss=0.7320090532302856, discriminator_loss=0.6751900911331177\n",
            "step 7839: generator_loss=0.7455527782440186, discriminator_loss=0.67083740234375\n",
            "step 7840: generator_loss=0.7582236528396606, discriminator_loss=0.667280912399292\n",
            "step 7841: generator_loss=0.7631584405899048, discriminator_loss=0.6650197505950928\n",
            "step 7842: generator_loss=0.770476222038269, discriminator_loss=0.6621043682098389\n",
            "step 7843: generator_loss=0.7631644010543823, discriminator_loss=0.662041187286377\n",
            "step 7844: generator_loss=0.7600364089012146, discriminator_loss=0.6605441570281982\n",
            "step 7845: generator_loss=0.7508006691932678, discriminator_loss=0.6591155529022217\n",
            "step 7846: generator_loss=0.7426286935806274, discriminator_loss=0.658206582069397\n",
            "step 7847: generator_loss=0.7364439964294434, discriminator_loss=0.6555176973342896\n",
            "step 7848: generator_loss=0.726752519607544, discriminator_loss=0.655090868473053\n",
            "step 7849: generator_loss=0.723699688911438, discriminator_loss=0.6542851328849792\n",
            "step 7850: generator_loss=0.7192614078521729, discriminator_loss=0.6533864140510559\n",
            "step 7851: generator_loss=0.7135882377624512, discriminator_loss=0.6554983854293823\n",
            "step 7852: generator_loss=0.7178523540496826, discriminator_loss=0.6542781591415405\n",
            "step 7853: generator_loss=0.7265251874923706, discriminator_loss=0.65128493309021\n",
            "step 7854: generator_loss=0.7326008677482605, discriminator_loss=0.6515740156173706\n",
            "step 7855: generator_loss=0.7413079738616943, discriminator_loss=0.6474634408950806\n",
            "step 7856: generator_loss=0.7449447512626648, discriminator_loss=0.6487279534339905\n",
            "step 7857: generator_loss=0.7459245324134827, discriminator_loss=0.6533290147781372\n",
            "step 7858: generator_loss=0.7540376782417297, discriminator_loss=0.6513220071792603\n",
            "step 7859: generator_loss=0.7463738918304443, discriminator_loss=0.658291220664978\n",
            "step 7860: generator_loss=0.745047390460968, discriminator_loss=0.6601254940032959\n",
            "step 7861: generator_loss=0.7464872002601624, discriminator_loss=0.6607133150100708\n",
            "step 7862: generator_loss=0.730878472328186, discriminator_loss=0.6659063100814819\n",
            "step 7863: generator_loss=0.7241631746292114, discriminator_loss=0.6689234375953674\n",
            "step 7864: generator_loss=0.7143911123275757, discriminator_loss=0.6720717549324036\n",
            "step 7865: generator_loss=0.7118973731994629, discriminator_loss=0.6718651652336121\n",
            "step 7866: generator_loss=0.701670229434967, discriminator_loss=0.6758723258972168\n",
            "step 7867: generator_loss=0.7004695534706116, discriminator_loss=0.6752874851226807\n",
            "step 7868: generator_loss=0.6909679174423218, discriminator_loss=0.6808816194534302\n",
            "step 7869: generator_loss=0.6948662996292114, discriminator_loss=0.6801266670227051\n",
            "step 7870: generator_loss=0.6967313289642334, discriminator_loss=0.6803117990493774\n",
            "step 7871: generator_loss=0.6995028853416443, discriminator_loss=0.6802301406860352\n",
            "step 7872: generator_loss=0.7017363905906677, discriminator_loss=0.6807291507720947\n",
            "step 7873: generator_loss=0.7107458114624023, discriminator_loss=0.6808211803436279\n",
            "step 7874: generator_loss=0.7029138207435608, discriminator_loss=0.6878577470779419\n",
            "step 7875: generator_loss=0.7013855576515198, discriminator_loss=0.6891403794288635\n",
            "step 7876: generator_loss=0.6941938400268555, discriminator_loss=0.6940722465515137\n",
            "step 7877: generator_loss=0.7055830955505371, discriminator_loss=0.689525306224823\n",
            "step 7878: generator_loss=0.7105872631072998, discriminator_loss=0.6880052089691162\n",
            "step 7879: generator_loss=0.7020578384399414, discriminator_loss=0.6955668926239014\n",
            "step 7880: generator_loss=0.6839306354522705, discriminator_loss=0.7057589292526245\n",
            "step 7881: generator_loss=0.7000786066055298, discriminator_loss=0.6955631971359253\n",
            "step 7882: generator_loss=0.6852474808692932, discriminator_loss=0.7071507573127747\n",
            "step 7883: generator_loss=0.685901403427124, discriminator_loss=0.7066304683685303\n",
            "step 7884: generator_loss=0.673845648765564, discriminator_loss=0.711594820022583\n",
            "step 7885: generator_loss=0.6828106641769409, discriminator_loss=0.7054175138473511\n",
            "step 7886: generator_loss=0.6734018325805664, discriminator_loss=0.707949697971344\n",
            "step 7887: generator_loss=0.6600770950317383, discriminator_loss=0.7145406603813171\n",
            "step 7888: generator_loss=0.6570208668708801, discriminator_loss=0.7136195302009583\n",
            "step 7889: generator_loss=0.673688530921936, discriminator_loss=0.7055297493934631\n",
            "step 7890: generator_loss=0.6642454862594604, discriminator_loss=0.7078477740287781\n",
            "step 7891: generator_loss=0.6553164124488831, discriminator_loss=0.7131345272064209\n",
            "step 7892: generator_loss=0.6580766439437866, discriminator_loss=0.7106504440307617\n",
            "step 7893: generator_loss=0.6665284633636475, discriminator_loss=0.7040743827819824\n",
            "step 7894: generator_loss=0.6715854406356812, discriminator_loss=0.7011069059371948\n",
            "step 7895: generator_loss=0.6815796494483948, discriminator_loss=0.6952105760574341\n",
            "step 7896: generator_loss=0.6922917366027832, discriminator_loss=0.6866757869720459\n",
            "step 7897: generator_loss=0.7018857002258301, discriminator_loss=0.6801413297653198\n",
            "step 7898: generator_loss=0.708526611328125, discriminator_loss=0.6752999424934387\n",
            "step 7899: generator_loss=0.7194691896438599, discriminator_loss=0.6704992055892944\n",
            "step 7900: generator_loss=0.7278903722763062, discriminator_loss=0.6650023460388184\n",
            "step 7901: generator_loss=0.7351276278495789, discriminator_loss=0.6617347002029419\n",
            "step 7902: generator_loss=0.7404395937919617, discriminator_loss=0.6593979001045227\n",
            "step 7903: generator_loss=0.7461563348770142, discriminator_loss=0.6561922430992126\n",
            "step 7904: generator_loss=0.7452763319015503, discriminator_loss=0.6573101878166199\n",
            "step 7905: generator_loss=0.7451798915863037, discriminator_loss=0.6573776006698608\n",
            "step 7906: generator_loss=0.7498035430908203, discriminator_loss=0.6531598567962646\n",
            "step 7907: generator_loss=0.7402272820472717, discriminator_loss=0.65784752368927\n",
            "step 7908: generator_loss=0.7361080646514893, discriminator_loss=0.6583889126777649\n",
            "step 7909: generator_loss=0.7252316474914551, discriminator_loss=0.6613162755966187\n",
            "step 7910: generator_loss=0.7234174609184265, discriminator_loss=0.6606000661849976\n",
            "step 7911: generator_loss=0.7212692499160767, discriminator_loss=0.6578994989395142\n",
            "step 7912: generator_loss=0.7061305046081543, discriminator_loss=0.6646740436553955\n",
            "step 7913: generator_loss=0.7090010643005371, discriminator_loss=0.6608422994613647\n",
            "step 7914: generator_loss=0.6975705623626709, discriminator_loss=0.6676872372627258\n",
            "step 7915: generator_loss=0.6919265985488892, discriminator_loss=0.671588122844696\n",
            "step 7916: generator_loss=0.6906431913375854, discriminator_loss=0.6725716590881348\n",
            "step 7917: generator_loss=0.6927396655082703, discriminator_loss=0.6726747155189514\n",
            "step 7918: generator_loss=0.6917638778686523, discriminator_loss=0.6748722195625305\n",
            "step 7919: generator_loss=0.7001919746398926, discriminator_loss=0.675121545791626\n",
            "step 7920: generator_loss=0.7020982503890991, discriminator_loss=0.675209641456604\n",
            "step 7921: generator_loss=0.7000279426574707, discriminator_loss=0.6793006658554077\n",
            "step 7922: generator_loss=0.7068667411804199, discriminator_loss=0.6769949197769165\n",
            "step 7923: generator_loss=0.7110228538513184, discriminator_loss=0.6762732267379761\n",
            "step 7924: generator_loss=0.7156683206558228, discriminator_loss=0.6751847863197327\n",
            "step 7925: generator_loss=0.7126754522323608, discriminator_loss=0.6786665916442871\n",
            "step 7926: generator_loss=0.719078004360199, discriminator_loss=0.6779311895370483\n",
            "step 7927: generator_loss=0.7157996296882629, discriminator_loss=0.6830319762229919\n",
            "step 7928: generator_loss=0.719397783279419, discriminator_loss=0.6842367053031921\n",
            "step 7929: generator_loss=0.7134371995925903, discriminator_loss=0.6869809031486511\n",
            "step 7930: generator_loss=0.7103308439254761, discriminator_loss=0.6896018981933594\n",
            "step 7931: generator_loss=0.7071678638458252, discriminator_loss=0.6903475522994995\n",
            "step 7932: generator_loss=0.7051251530647278, discriminator_loss=0.6901984810829163\n",
            "step 7933: generator_loss=0.7018212080001831, discriminator_loss=0.6919312477111816\n",
            "step 7934: generator_loss=0.6969386339187622, discriminator_loss=0.691502571105957\n",
            "step 7935: generator_loss=0.6961096525192261, discriminator_loss=0.6918410062789917\n",
            "step 7936: generator_loss=0.701131284236908, discriminator_loss=0.6888834834098816\n",
            "step 7937: generator_loss=0.6908378601074219, discriminator_loss=0.6921321749687195\n",
            "step 7938: generator_loss=0.69166100025177, discriminator_loss=0.689838171005249\n",
            "step 7939: generator_loss=0.6958930492401123, discriminator_loss=0.689237117767334\n",
            "step 7940: generator_loss=0.6899833679199219, discriminator_loss=0.6921911835670471\n",
            "step 7941: generator_loss=0.6885035037994385, discriminator_loss=0.6942267417907715\n",
            "step 7942: generator_loss=0.6965084075927734, discriminator_loss=0.6927337646484375\n",
            "step 7943: generator_loss=0.6990798115730286, discriminator_loss=0.6934366226196289\n",
            "step 7944: generator_loss=0.6932269334793091, discriminator_loss=0.6976044178009033\n",
            "step 7945: generator_loss=0.6946333050727844, discriminator_loss=0.6964086294174194\n",
            "step 7946: generator_loss=0.6962993741035461, discriminator_loss=0.696027934551239\n",
            "step 7947: generator_loss=0.6937446594238281, discriminator_loss=0.6977841854095459\n",
            "step 7948: generator_loss=0.6944403648376465, discriminator_loss=0.6951216459274292\n",
            "step 7949: generator_loss=0.6970083713531494, discriminator_loss=0.6918240785598755\n",
            "step 7950: generator_loss=0.7025137543678284, discriminator_loss=0.6889681816101074\n",
            "step 7951: generator_loss=0.6902827024459839, discriminator_loss=0.6934623718261719\n",
            "step 7952: generator_loss=0.6859965324401855, discriminator_loss=0.693143367767334\n",
            "step 7953: generator_loss=0.688268780708313, discriminator_loss=0.6897901296615601\n",
            "step 7954: generator_loss=0.6865012645721436, discriminator_loss=0.6871828436851501\n",
            "step 7955: generator_loss=0.6863244771957397, discriminator_loss=0.6863827705383301\n",
            "step 7956: generator_loss=0.6858159303665161, discriminator_loss=0.6852365732192993\n",
            "step 7957: generator_loss=0.6877771615982056, discriminator_loss=0.6846376657485962\n",
            "step 7958: generator_loss=0.6851210594177246, discriminator_loss=0.6871312856674194\n",
            "step 7959: generator_loss=0.6932203769683838, discriminator_loss=0.6851131319999695\n",
            "step 7960: generator_loss=0.6966269612312317, discriminator_loss=0.6835529804229736\n",
            "step 7961: generator_loss=0.6983771920204163, discriminator_loss=0.6827327609062195\n",
            "step 7962: generator_loss=0.6997363567352295, discriminator_loss=0.684413492679596\n",
            "step 7963: generator_loss=0.7025234699249268, discriminator_loss=0.6837161779403687\n",
            "step 7964: generator_loss=0.6949305534362793, discriminator_loss=0.6878739595413208\n",
            "step 7965: generator_loss=0.7023248672485352, discriminator_loss=0.6862348318099976\n",
            "step 7966: generator_loss=0.7045984268188477, discriminator_loss=0.6853541731834412\n",
            "step 7967: generator_loss=0.7087763547897339, discriminator_loss=0.684908390045166\n",
            "step 7968: generator_loss=0.7068127393722534, discriminator_loss=0.6875450611114502\n",
            "step 7969: generator_loss=0.7112623453140259, discriminator_loss=0.6822987794876099\n",
            "step 7970: generator_loss=0.7190043926239014, discriminator_loss=0.6804223656654358\n",
            "step 7971: generator_loss=0.7220245599746704, discriminator_loss=0.6778554916381836\n",
            "step 7972: generator_loss=0.7175678610801697, discriminator_loss=0.6783475875854492\n",
            "step 7973: generator_loss=0.7322426438331604, discriminator_loss=0.6723289489746094\n",
            "step 7974: generator_loss=0.7352069616317749, discriminator_loss=0.6691857576370239\n",
            "step 7975: generator_loss=0.7441489696502686, discriminator_loss=0.6658703684806824\n",
            "step 7976: generator_loss=0.7495360374450684, discriminator_loss=0.6630924940109253\n",
            "step 7977: generator_loss=0.7400563955307007, discriminator_loss=0.6668304800987244\n",
            "step 7978: generator_loss=0.7450994253158569, discriminator_loss=0.6630285978317261\n",
            "step 7979: generator_loss=0.7350388765335083, discriminator_loss=0.6654689311981201\n",
            "step 7980: generator_loss=0.7411713004112244, discriminator_loss=0.6592972278594971\n",
            "step 7981: generator_loss=0.7382817268371582, discriminator_loss=0.6591206789016724\n",
            "step 7982: generator_loss=0.7353919744491577, discriminator_loss=0.657005786895752\n",
            "step 7983: generator_loss=0.7182644605636597, discriminator_loss=0.6628640294075012\n",
            "step 7984: generator_loss=0.7248142957687378, discriminator_loss=0.6590600609779358\n",
            "step 7985: generator_loss=0.724308967590332, discriminator_loss=0.6593281030654907\n",
            "step 7986: generator_loss=0.7157194018363953, discriminator_loss=0.6633391380310059\n",
            "step 7987: generator_loss=0.7119058966636658, discriminator_loss=0.6679800152778625\n",
            "step 7988: generator_loss=0.7162437438964844, discriminator_loss=0.6671788096427917\n",
            "step 7989: generator_loss=0.7123717069625854, discriminator_loss=0.6703634262084961\n",
            "step 7990: generator_loss=0.7066446542739868, discriminator_loss=0.6765103340148926\n",
            "step 7991: generator_loss=0.7074886560440063, discriminator_loss=0.6767066121101379\n",
            "step 7992: generator_loss=0.7084758281707764, discriminator_loss=0.6782482862472534\n",
            "step 7993: generator_loss=0.6995052099227905, discriminator_loss=0.6833904981613159\n",
            "step 7994: generator_loss=0.7025459408760071, discriminator_loss=0.6845080852508545\n",
            "step 7995: generator_loss=0.6959310173988342, discriminator_loss=0.6877169609069824\n",
            "step 7996: generator_loss=0.6954344511032104, discriminator_loss=0.6884914636611938\n",
            "step 7997: generator_loss=0.7014739513397217, discriminator_loss=0.6854709982872009\n",
            "step 7998: generator_loss=0.6928845047950745, discriminator_loss=0.6904520988464355\n",
            "step 7999: generator_loss=0.6913021206855774, discriminator_loss=0.6935492753982544\n",
            "step 8000: generator_loss=0.6887507438659668, discriminator_loss=0.6935034990310669\n",
            "step 8001: generator_loss=0.6892621517181396, discriminator_loss=0.6932079195976257\n",
            "step 8002: generator_loss=0.688544750213623, discriminator_loss=0.6948015689849854\n",
            "step 8003: generator_loss=0.6861804127693176, discriminator_loss=0.6976597905158997\n",
            "step 8004: generator_loss=0.6879730224609375, discriminator_loss=0.6977598667144775\n",
            "step 8005: generator_loss=0.683603048324585, discriminator_loss=0.6999229192733765\n",
            "step 8006: generator_loss=0.6845561861991882, discriminator_loss=0.7007657289505005\n",
            "step 8007: generator_loss=0.6906003952026367, discriminator_loss=0.6971950531005859\n",
            "step 8008: generator_loss=0.691213071346283, discriminator_loss=0.6979819536209106\n",
            "step 8009: generator_loss=0.6941090226173401, discriminator_loss=0.6950393915176392\n",
            "step 8010: generator_loss=0.7003954648971558, discriminator_loss=0.6937319040298462\n",
            "step 8011: generator_loss=0.6973482370376587, discriminator_loss=0.6966269612312317\n",
            "step 8012: generator_loss=0.699305534362793, discriminator_loss=0.6952527165412903\n",
            "step 8013: generator_loss=0.6959878206253052, discriminator_loss=0.6983158588409424\n",
            "step 8014: generator_loss=0.6946900486946106, discriminator_loss=0.7004032135009766\n",
            "step 8015: generator_loss=0.6916808485984802, discriminator_loss=0.7024474143981934\n",
            "step 8016: generator_loss=0.6883372068405151, discriminator_loss=0.7045211791992188\n",
            "step 8017: generator_loss=0.6861846446990967, discriminator_loss=0.7050058245658875\n",
            "step 8018: generator_loss=0.6828689575195312, discriminator_loss=0.7062287330627441\n",
            "step 8019: generator_loss=0.6764818429946899, discriminator_loss=0.7084909677505493\n",
            "step 8020: generator_loss=0.666343092918396, discriminator_loss=0.7141150236129761\n",
            "step 8021: generator_loss=0.6628074645996094, discriminator_loss=0.713994562625885\n",
            "step 8022: generator_loss=0.6618043184280396, discriminator_loss=0.7162167429924011\n",
            "step 8023: generator_loss=0.6577677726745605, discriminator_loss=0.7173194885253906\n",
            "step 8024: generator_loss=0.6581844091415405, discriminator_loss=0.7163275480270386\n",
            "step 8025: generator_loss=0.653293251991272, discriminator_loss=0.7206254601478577\n",
            "step 8026: generator_loss=0.6606618762016296, discriminator_loss=0.715501070022583\n",
            "step 8027: generator_loss=0.6593670845031738, discriminator_loss=0.7189909219741821\n",
            "step 8028: generator_loss=0.658558189868927, discriminator_loss=0.7180402874946594\n",
            "step 8029: generator_loss=0.6607148051261902, discriminator_loss=0.7156664729118347\n",
            "step 8030: generator_loss=0.6755486726760864, discriminator_loss=0.7077771425247192\n",
            "step 8031: generator_loss=0.668372631072998, discriminator_loss=0.7122820615768433\n",
            "step 8032: generator_loss=0.6755321621894836, discriminator_loss=0.7071020007133484\n",
            "step 8033: generator_loss=0.676834225654602, discriminator_loss=0.7095723152160645\n",
            "step 8034: generator_loss=0.6732727289199829, discriminator_loss=0.7122570276260376\n",
            "step 8035: generator_loss=0.6889532804489136, discriminator_loss=0.7047251462936401\n",
            "step 8036: generator_loss=0.6864040493965149, discriminator_loss=0.7090803384780884\n",
            "step 8037: generator_loss=0.6970605254173279, discriminator_loss=0.7046577334403992\n",
            "step 8038: generator_loss=0.7030736207962036, discriminator_loss=0.7008382081985474\n",
            "step 8039: generator_loss=0.6963663101196289, discriminator_loss=0.7041147947311401\n",
            "step 8040: generator_loss=0.698860764503479, discriminator_loss=0.7007278203964233\n",
            "step 8041: generator_loss=0.6991785764694214, discriminator_loss=0.69869464635849\n",
            "step 8042: generator_loss=0.6973674893379211, discriminator_loss=0.697988748550415\n",
            "step 8043: generator_loss=0.696467399597168, discriminator_loss=0.696473240852356\n",
            "step 8044: generator_loss=0.685450553894043, discriminator_loss=0.7016400694847107\n",
            "step 8045: generator_loss=0.6905498504638672, discriminator_loss=0.7004433870315552\n",
            "step 8046: generator_loss=0.6931962966918945, discriminator_loss=0.6985210180282593\n",
            "step 8047: generator_loss=0.6840126514434814, discriminator_loss=0.7019779682159424\n",
            "step 8048: generator_loss=0.6778271198272705, discriminator_loss=0.7058388590812683\n",
            "step 8049: generator_loss=0.6820889711380005, discriminator_loss=0.7013946771621704\n",
            "step 8050: generator_loss=0.6751894950866699, discriminator_loss=0.7067225575447083\n",
            "step 8051: generator_loss=0.6756100654602051, discriminator_loss=0.7065410614013672\n",
            "step 8052: generator_loss=0.6743924617767334, discriminator_loss=0.7052689790725708\n",
            "step 8053: generator_loss=0.6739442348480225, discriminator_loss=0.7055861353874207\n",
            "step 8054: generator_loss=0.6683071255683899, discriminator_loss=0.7077776193618774\n",
            "step 8055: generator_loss=0.6754899621009827, discriminator_loss=0.7026044130325317\n",
            "step 8056: generator_loss=0.6766993999481201, discriminator_loss=0.701651394367218\n",
            "step 8057: generator_loss=0.6761032938957214, discriminator_loss=0.7024362087249756\n",
            "step 8058: generator_loss=0.684563159942627, discriminator_loss=0.6983265280723572\n",
            "step 8059: generator_loss=0.6920719742774963, discriminator_loss=0.6977196335792542\n",
            "step 8060: generator_loss=0.6935219168663025, discriminator_loss=0.6953072547912598\n",
            "step 8061: generator_loss=0.6966613531112671, discriminator_loss=0.6967593431472778\n",
            "step 8062: generator_loss=0.6979700922966003, discriminator_loss=0.6936584115028381\n",
            "step 8063: generator_loss=0.7040951251983643, discriminator_loss=0.691179633140564\n",
            "step 8064: generator_loss=0.7148387432098389, discriminator_loss=0.683603048324585\n",
            "step 8065: generator_loss=0.7164493799209595, discriminator_loss=0.6804999113082886\n",
            "step 8066: generator_loss=0.7260420918464661, discriminator_loss=0.6759682297706604\n",
            "step 8067: generator_loss=0.7265251874923706, discriminator_loss=0.6774991750717163\n",
            "step 8068: generator_loss=0.7370749115943909, discriminator_loss=0.6714291572570801\n",
            "step 8069: generator_loss=0.7428914904594421, discriminator_loss=0.6696326732635498\n",
            "step 8070: generator_loss=0.7432712316513062, discriminator_loss=0.6704771518707275\n",
            "step 8071: generator_loss=0.7456786632537842, discriminator_loss=0.6702611446380615\n",
            "step 8072: generator_loss=0.7468514442443848, discriminator_loss=0.6720163822174072\n",
            "step 8073: generator_loss=0.7436965703964233, discriminator_loss=0.6737019419670105\n",
            "step 8074: generator_loss=0.7364609241485596, discriminator_loss=0.6742618083953857\n",
            "step 8075: generator_loss=0.7299140691757202, discriminator_loss=0.6751400232315063\n",
            "step 8076: generator_loss=0.7218979597091675, discriminator_loss=0.6759271621704102\n",
            "step 8077: generator_loss=0.7093698978424072, discriminator_loss=0.6805992126464844\n",
            "step 8078: generator_loss=0.7026225924491882, discriminator_loss=0.6817417740821838\n",
            "step 8079: generator_loss=0.7007030248641968, discriminator_loss=0.6796156764030457\n",
            "step 8080: generator_loss=0.6951196193695068, discriminator_loss=0.6823229789733887\n",
            "step 8081: generator_loss=0.6896472573280334, discriminator_loss=0.6835198998451233\n",
            "step 8082: generator_loss=0.688576877117157, discriminator_loss=0.6840547919273376\n",
            "step 8083: generator_loss=0.6920905113220215, discriminator_loss=0.6830983757972717\n",
            "step 8084: generator_loss=0.7052780985832214, discriminator_loss=0.6786668300628662\n",
            "step 8085: generator_loss=0.7107658386230469, discriminator_loss=0.6795995831489563\n",
            "step 8086: generator_loss=0.7200647592544556, discriminator_loss=0.6765594482421875\n",
            "step 8087: generator_loss=0.723299503326416, discriminator_loss=0.6761167645454407\n",
            "step 8088: generator_loss=0.7317323088645935, discriminator_loss=0.6750984191894531\n",
            "step 8089: generator_loss=0.7443605065345764, discriminator_loss=0.6701207160949707\n",
            "step 8090: generator_loss=0.7519383430480957, discriminator_loss=0.6664313077926636\n",
            "step 8091: generator_loss=0.7489295601844788, discriminator_loss=0.6685242056846619\n",
            "step 8092: generator_loss=0.7480584979057312, discriminator_loss=0.6686016917228699\n",
            "step 8093: generator_loss=0.7499136924743652, discriminator_loss=0.6666741371154785\n",
            "step 8094: generator_loss=0.7430421710014343, discriminator_loss=0.6680938005447388\n",
            "step 8095: generator_loss=0.7418254017829895, discriminator_loss=0.6652930974960327\n",
            "step 8096: generator_loss=0.7304437160491943, discriminator_loss=0.6647219061851501\n",
            "step 8097: generator_loss=0.7205032706260681, discriminator_loss=0.6660904884338379\n",
            "step 8098: generator_loss=0.7188893556594849, discriminator_loss=0.6613609790802002\n",
            "step 8099: generator_loss=0.7062535285949707, discriminator_loss=0.6647813320159912\n",
            "step 8100: generator_loss=0.7017761468887329, discriminator_loss=0.6653978824615479\n",
            "step 8101: generator_loss=0.7015316486358643, discriminator_loss=0.6653662919998169\n",
            "step 8102: generator_loss=0.6985204815864563, discriminator_loss=0.6683270931243896\n",
            "step 8103: generator_loss=0.7021878361701965, discriminator_loss=0.6679797768592834\n",
            "step 8104: generator_loss=0.7011850476264954, discriminator_loss=0.6725960373878479\n",
            "step 8105: generator_loss=0.7044978737831116, discriminator_loss=0.6736187934875488\n",
            "step 8106: generator_loss=0.7078841924667358, discriminator_loss=0.6759412884712219\n",
            "step 8107: generator_loss=0.7133833765983582, discriminator_loss=0.677165687084198\n",
            "step 8108: generator_loss=0.7089468240737915, discriminator_loss=0.682570219039917\n",
            "step 8109: generator_loss=0.7088248133659363, discriminator_loss=0.6833877563476562\n",
            "step 8110: generator_loss=0.7063148021697998, discriminator_loss=0.6863416433334351\n",
            "step 8111: generator_loss=0.7050365209579468, discriminator_loss=0.6881921291351318\n",
            "step 8112: generator_loss=0.6983668208122253, discriminator_loss=0.693867027759552\n",
            "step 8113: generator_loss=0.6986483931541443, discriminator_loss=0.6929913759231567\n",
            "step 8114: generator_loss=0.6951987147331238, discriminator_loss=0.6956428289413452\n",
            "step 8115: generator_loss=0.6859839558601379, discriminator_loss=0.6992418766021729\n",
            "step 8116: generator_loss=0.6789922118186951, discriminator_loss=0.7037215232849121\n",
            "step 8117: generator_loss=0.6743406653404236, discriminator_loss=0.7080191373825073\n",
            "step 8118: generator_loss=0.6730740070343018, discriminator_loss=0.7083544731140137\n",
            "step 8119: generator_loss=0.6626279354095459, discriminator_loss=0.7132440805435181\n",
            "step 8120: generator_loss=0.6621136665344238, discriminator_loss=0.7151066064834595\n",
            "step 8121: generator_loss=0.6616701483726501, discriminator_loss=0.7159451842308044\n",
            "step 8122: generator_loss=0.6576130390167236, discriminator_loss=0.7213391661643982\n",
            "step 8123: generator_loss=0.6578302979469299, discriminator_loss=0.7228802442550659\n",
            "step 8124: generator_loss=0.6571066379547119, discriminator_loss=0.7249502539634705\n",
            "step 8125: generator_loss=0.6483997702598572, discriminator_loss=0.7314022779464722\n",
            "step 8126: generator_loss=0.6557783484458923, discriminator_loss=0.7300129532814026\n",
            "step 8127: generator_loss=0.6526598930358887, discriminator_loss=0.7338992357254028\n",
            "step 8128: generator_loss=0.6544142961502075, discriminator_loss=0.7336398959159851\n",
            "step 8129: generator_loss=0.6480798721313477, discriminator_loss=0.7394633293151855\n",
            "step 8130: generator_loss=0.6401336193084717, discriminator_loss=0.7434335350990295\n",
            "step 8131: generator_loss=0.6412261128425598, discriminator_loss=0.74578458070755\n",
            "step 8132: generator_loss=0.6434497833251953, discriminator_loss=0.7460241317749023\n",
            "step 8133: generator_loss=0.6396341323852539, discriminator_loss=0.7489355802536011\n",
            "step 8134: generator_loss=0.6358908414840698, discriminator_loss=0.7508988976478577\n",
            "step 8135: generator_loss=0.6269391179084778, discriminator_loss=0.7567776441574097\n",
            "step 8136: generator_loss=0.6338318586349487, discriminator_loss=0.7526980638504028\n",
            "step 8137: generator_loss=0.6269476413726807, discriminator_loss=0.754769504070282\n",
            "step 8138: generator_loss=0.6251817941665649, discriminator_loss=0.7555562257766724\n",
            "step 8139: generator_loss=0.6236402988433838, discriminator_loss=0.7562979459762573\n",
            "step 8140: generator_loss=0.6213298439979553, discriminator_loss=0.7572112083435059\n",
            "step 8141: generator_loss=0.6254503726959229, discriminator_loss=0.7550085783004761\n",
            "step 8142: generator_loss=0.6313895583152771, discriminator_loss=0.7521100640296936\n",
            "step 8143: generator_loss=0.633262038230896, discriminator_loss=0.7495557069778442\n",
            "step 8144: generator_loss=0.6373583078384399, discriminator_loss=0.7480431795120239\n",
            "step 8145: generator_loss=0.6498428583145142, discriminator_loss=0.7397885918617249\n",
            "step 8146: generator_loss=0.652155876159668, discriminator_loss=0.7378145456314087\n",
            "step 8147: generator_loss=0.6621811389923096, discriminator_loss=0.7304848432540894\n",
            "step 8148: generator_loss=0.667427659034729, discriminator_loss=0.7265162467956543\n",
            "step 8149: generator_loss=0.6761128902435303, discriminator_loss=0.7209628820419312\n",
            "step 8150: generator_loss=0.6818475127220154, discriminator_loss=0.7172147035598755\n",
            "step 8151: generator_loss=0.6833832263946533, discriminator_loss=0.7179979681968689\n",
            "step 8152: generator_loss=0.6821398138999939, discriminator_loss=0.7200682759284973\n",
            "step 8153: generator_loss=0.6853393912315369, discriminator_loss=0.7180873155593872\n",
            "step 8154: generator_loss=0.6835483312606812, discriminator_loss=0.7185063362121582\n",
            "step 8155: generator_loss=0.6792806386947632, discriminator_loss=0.7194923162460327\n",
            "step 8156: generator_loss=0.6774313449859619, discriminator_loss=0.7192567586898804\n",
            "step 8157: generator_loss=0.669502317905426, discriminator_loss=0.7203179597854614\n",
            "step 8158: generator_loss=0.6644318103790283, discriminator_loss=0.7214680910110474\n",
            "step 8159: generator_loss=0.6619783639907837, discriminator_loss=0.7211049795150757\n",
            "step 8160: generator_loss=0.6593369245529175, discriminator_loss=0.7207618355751038\n",
            "step 8161: generator_loss=0.6540366411209106, discriminator_loss=0.7224386930465698\n",
            "step 8162: generator_loss=0.6541107296943665, discriminator_loss=0.7213776111602783\n",
            "step 8163: generator_loss=0.6534547209739685, discriminator_loss=0.7215908765792847\n",
            "step 8164: generator_loss=0.6522960662841797, discriminator_loss=0.7230587005615234\n",
            "step 8165: generator_loss=0.6573015451431274, discriminator_loss=0.7180505394935608\n",
            "step 8166: generator_loss=0.6568111181259155, discriminator_loss=0.7199918031692505\n",
            "step 8167: generator_loss=0.6610925197601318, discriminator_loss=0.7171366214752197\n",
            "step 8168: generator_loss=0.6649565696716309, discriminator_loss=0.714937150478363\n",
            "step 8169: generator_loss=0.6716711521148682, discriminator_loss=0.7118088006973267\n",
            "step 8170: generator_loss=0.6779823303222656, discriminator_loss=0.7080749273300171\n",
            "step 8171: generator_loss=0.6827383041381836, discriminator_loss=0.7071571350097656\n",
            "step 8172: generator_loss=0.6866346597671509, discriminator_loss=0.7054543495178223\n",
            "step 8173: generator_loss=0.6921743750572205, discriminator_loss=0.7032215595245361\n",
            "step 8174: generator_loss=0.6959922313690186, discriminator_loss=0.6990249156951904\n",
            "step 8175: generator_loss=0.6988454461097717, discriminator_loss=0.6973676085472107\n",
            "step 8176: generator_loss=0.7018243074417114, discriminator_loss=0.6941617727279663\n",
            "step 8177: generator_loss=0.7024952173233032, discriminator_loss=0.6928513050079346\n",
            "step 8178: generator_loss=0.7011469006538391, discriminator_loss=0.6918762922286987\n",
            "step 8179: generator_loss=0.6990792751312256, discriminator_loss=0.6912639737129211\n",
            "step 8180: generator_loss=0.6966816186904907, discriminator_loss=0.688704788684845\n",
            "step 8181: generator_loss=0.700645923614502, discriminator_loss=0.684140682220459\n",
            "step 8182: generator_loss=0.7009745836257935, discriminator_loss=0.6795691251754761\n",
            "step 8183: generator_loss=0.7012399435043335, discriminator_loss=0.6770139932632446\n",
            "step 8184: generator_loss=0.7017140984535217, discriminator_loss=0.6732689142227173\n",
            "step 8185: generator_loss=0.704683244228363, discriminator_loss=0.6699382066726685\n",
            "step 8186: generator_loss=0.7077748775482178, discriminator_loss=0.6675645709037781\n",
            "step 8187: generator_loss=0.7112114429473877, discriminator_loss=0.6648659706115723\n",
            "step 8188: generator_loss=0.7203823924064636, discriminator_loss=0.6597788333892822\n",
            "step 8189: generator_loss=0.7230516672134399, discriminator_loss=0.6579797863960266\n",
            "step 8190: generator_loss=0.7293106317520142, discriminator_loss=0.657712459564209\n",
            "step 8191: generator_loss=0.735692024230957, discriminator_loss=0.6559610366821289\n",
            "step 8192: generator_loss=0.7406532764434814, discriminator_loss=0.6552947759628296\n",
            "step 8193: generator_loss=0.7428341507911682, discriminator_loss=0.6555564403533936\n",
            "step 8194: generator_loss=0.7446750402450562, discriminator_loss=0.6559678316116333\n",
            "step 8195: generator_loss=0.7452999949455261, discriminator_loss=0.6552959680557251\n",
            "step 8196: generator_loss=0.7508423328399658, discriminator_loss=0.651248037815094\n",
            "step 8197: generator_loss=0.7478038668632507, discriminator_loss=0.6541056036949158\n",
            "step 8198: generator_loss=0.7437565326690674, discriminator_loss=0.654870867729187\n",
            "step 8199: generator_loss=0.7374050617218018, discriminator_loss=0.6570214033126831\n",
            "step 8200: generator_loss=0.7312353253364563, discriminator_loss=0.6601518392562866\n",
            "step 8201: generator_loss=0.7320120334625244, discriminator_loss=0.6575108766555786\n",
            "step 8202: generator_loss=0.7287670373916626, discriminator_loss=0.657615602016449\n",
            "step 8203: generator_loss=0.7257603406906128, discriminator_loss=0.657556414604187\n",
            "step 8204: generator_loss=0.7206822633743286, discriminator_loss=0.6589998602867126\n",
            "step 8205: generator_loss=0.7213151454925537, discriminator_loss=0.6574704647064209\n",
            "step 8206: generator_loss=0.7155574560165405, discriminator_loss=0.6598197221755981\n",
            "step 8207: generator_loss=0.717089831829071, discriminator_loss=0.6598894596099854\n",
            "step 8208: generator_loss=0.7183146476745605, discriminator_loss=0.6592519283294678\n",
            "step 8209: generator_loss=0.7192488312721252, discriminator_loss=0.6590710282325745\n",
            "step 8210: generator_loss=0.7202075719833374, discriminator_loss=0.6616228818893433\n",
            "step 8211: generator_loss=0.7246695756912231, discriminator_loss=0.6605907082557678\n",
            "step 8212: generator_loss=0.7248188257217407, discriminator_loss=0.6614127159118652\n",
            "step 8213: generator_loss=0.7256063222885132, discriminator_loss=0.6639416217803955\n",
            "step 8214: generator_loss=0.7276333570480347, discriminator_loss=0.6635894775390625\n",
            "step 8215: generator_loss=0.72762131690979, discriminator_loss=0.6633780598640442\n",
            "step 8216: generator_loss=0.7277041673660278, discriminator_loss=0.6648944616317749\n",
            "step 8217: generator_loss=0.7226947546005249, discriminator_loss=0.6669797897338867\n",
            "step 8218: generator_loss=0.7226170301437378, discriminator_loss=0.6678411364555359\n",
            "step 8219: generator_loss=0.7215234637260437, discriminator_loss=0.6684796810150146\n",
            "step 8220: generator_loss=0.7203124761581421, discriminator_loss=0.6698459386825562\n",
            "step 8221: generator_loss=0.7172200083732605, discriminator_loss=0.6711228489875793\n",
            "step 8222: generator_loss=0.7117756605148315, discriminator_loss=0.672274112701416\n",
            "step 8223: generator_loss=0.7098244428634644, discriminator_loss=0.6729938387870789\n",
            "step 8224: generator_loss=0.7075421810150146, discriminator_loss=0.6746002435684204\n",
            "step 8225: generator_loss=0.7068118453025818, discriminator_loss=0.675835132598877\n",
            "step 8226: generator_loss=0.7057439088821411, discriminator_loss=0.6760687828063965\n",
            "step 8227: generator_loss=0.7035166621208191, discriminator_loss=0.6778043508529663\n",
            "step 8228: generator_loss=0.7033571600914001, discriminator_loss=0.6788651943206787\n",
            "step 8229: generator_loss=0.7017901539802551, discriminator_loss=0.6805769205093384\n",
            "step 8230: generator_loss=0.7017772793769836, discriminator_loss=0.681593656539917\n",
            "step 8231: generator_loss=0.7024353742599487, discriminator_loss=0.6813963651657104\n",
            "step 8232: generator_loss=0.7022539377212524, discriminator_loss=0.6814518570899963\n",
            "step 8233: generator_loss=0.701561689376831, discriminator_loss=0.6852854490280151\n",
            "step 8234: generator_loss=0.7009118795394897, discriminator_loss=0.6857202053070068\n",
            "step 8235: generator_loss=0.7017315626144409, discriminator_loss=0.6842086315155029\n",
            "step 8236: generator_loss=0.6973790526390076, discriminator_loss=0.6878395676612854\n",
            "step 8237: generator_loss=0.700992226600647, discriminator_loss=0.6879051923751831\n",
            "step 8238: generator_loss=0.6972955465316772, discriminator_loss=0.6892749667167664\n",
            "step 8239: generator_loss=0.6950883269309998, discriminator_loss=0.6903389692306519\n",
            "step 8240: generator_loss=0.6952041387557983, discriminator_loss=0.6905340552330017\n",
            "step 8241: generator_loss=0.6932024955749512, discriminator_loss=0.6917514204978943\n",
            "step 8242: generator_loss=0.69370436668396, discriminator_loss=0.6922274827957153\n",
            "step 8243: generator_loss=0.6898689866065979, discriminator_loss=0.6953067779541016\n",
            "step 8244: generator_loss=0.6887825727462769, discriminator_loss=0.6964621543884277\n",
            "step 8245: generator_loss=0.6876113414764404, discriminator_loss=0.6971662044525146\n",
            "step 8246: generator_loss=0.6861710548400879, discriminator_loss=0.6982179880142212\n",
            "step 8247: generator_loss=0.6851925849914551, discriminator_loss=0.6987435817718506\n",
            "step 8248: generator_loss=0.6838822364807129, discriminator_loss=0.7003594636917114\n",
            "step 8249: generator_loss=0.6840040683746338, discriminator_loss=0.7015825510025024\n",
            "step 8250: generator_loss=0.6826757192611694, discriminator_loss=0.7025482654571533\n",
            "step 8251: generator_loss=0.6811422109603882, discriminator_loss=0.7007428407669067\n",
            "step 8252: generator_loss=0.680957019329071, discriminator_loss=0.7031553983688354\n",
            "step 8253: generator_loss=0.6806012988090515, discriminator_loss=0.7033176422119141\n",
            "step 8254: generator_loss=0.6793975830078125, discriminator_loss=0.7053532004356384\n",
            "step 8255: generator_loss=0.6803547739982605, discriminator_loss=0.7069053053855896\n",
            "step 8256: generator_loss=0.6791127920150757, discriminator_loss=0.7071640491485596\n",
            "step 8257: generator_loss=0.6784166693687439, discriminator_loss=0.7072070837020874\n",
            "step 8258: generator_loss=0.6783400774002075, discriminator_loss=0.7092649340629578\n",
            "step 8259: generator_loss=0.6766777038574219, discriminator_loss=0.7097477316856384\n",
            "step 8260: generator_loss=0.6747053265571594, discriminator_loss=0.7118666172027588\n",
            "step 8261: generator_loss=0.6747135519981384, discriminator_loss=0.7108089327812195\n",
            "step 8262: generator_loss=0.6745133399963379, discriminator_loss=0.712949812412262\n",
            "step 8263: generator_loss=0.6738592386245728, discriminator_loss=0.7118889093399048\n",
            "step 8264: generator_loss=0.6726363897323608, discriminator_loss=0.7129672765731812\n",
            "step 8265: generator_loss=0.6696200370788574, discriminator_loss=0.7128584384918213\n",
            "step 8266: generator_loss=0.6696464419364929, discriminator_loss=0.7148176431655884\n",
            "step 8267: generator_loss=0.6696279048919678, discriminator_loss=0.7160457372665405\n",
            "step 8268: generator_loss=0.6682558655738831, discriminator_loss=0.7164252996444702\n",
            "step 8269: generator_loss=0.6668683886528015, discriminator_loss=0.718962550163269\n",
            "step 8270: generator_loss=0.6672996282577515, discriminator_loss=0.718852162361145\n",
            "step 8271: generator_loss=0.6681094169616699, discriminator_loss=0.7184457778930664\n",
            "step 8272: generator_loss=0.6660862565040588, discriminator_loss=0.7197365760803223\n",
            "step 8273: generator_loss=0.6664178967475891, discriminator_loss=0.7194406986236572\n",
            "step 8274: generator_loss=0.6681098937988281, discriminator_loss=0.7198591232299805\n",
            "step 8275: generator_loss=0.6723840236663818, discriminator_loss=0.7174043655395508\n",
            "step 8276: generator_loss=0.6733934879302979, discriminator_loss=0.7170770764350891\n",
            "step 8277: generator_loss=0.6806468963623047, discriminator_loss=0.7136120796203613\n",
            "step 8278: generator_loss=0.683112382888794, discriminator_loss=0.715320885181427\n",
            "step 8279: generator_loss=0.6845691204071045, discriminator_loss=0.7154821157455444\n",
            "step 8280: generator_loss=0.6830382347106934, discriminator_loss=0.7161657810211182\n",
            "step 8281: generator_loss=0.6843150854110718, discriminator_loss=0.7173094153404236\n",
            "step 8282: generator_loss=0.6818795204162598, discriminator_loss=0.7182217240333557\n",
            "step 8283: generator_loss=0.6776534914970398, discriminator_loss=0.7178479433059692\n",
            "step 8284: generator_loss=0.6710283756256104, discriminator_loss=0.7197330594062805\n",
            "step 8285: generator_loss=0.668238639831543, discriminator_loss=0.7176952362060547\n",
            "step 8286: generator_loss=0.6618670225143433, discriminator_loss=0.7197490930557251\n",
            "step 8287: generator_loss=0.6589049696922302, discriminator_loss=0.7205657362937927\n",
            "step 8288: generator_loss=0.6592435836791992, discriminator_loss=0.7203366756439209\n",
            "step 8289: generator_loss=0.6572448015213013, discriminator_loss=0.7206518054008484\n",
            "step 8290: generator_loss=0.6572019457817078, discriminator_loss=0.7211959958076477\n",
            "step 8291: generator_loss=0.6577743887901306, discriminator_loss=0.7199901342391968\n",
            "step 8292: generator_loss=0.6602462530136108, discriminator_loss=0.720718264579773\n",
            "step 8293: generator_loss=0.6667701601982117, discriminator_loss=0.7188624739646912\n",
            "step 8294: generator_loss=0.672491192817688, discriminator_loss=0.7169938087463379\n",
            "step 8295: generator_loss=0.6731268167495728, discriminator_loss=0.7177757024765015\n",
            "step 8296: generator_loss=0.6745837926864624, discriminator_loss=0.7187228202819824\n",
            "step 8297: generator_loss=0.6820330619812012, discriminator_loss=0.716248631477356\n",
            "step 8298: generator_loss=0.6763767600059509, discriminator_loss=0.7197234630584717\n",
            "step 8299: generator_loss=0.6753172874450684, discriminator_loss=0.7220654487609863\n",
            "step 8300: generator_loss=0.6714135408401489, discriminator_loss=0.723410427570343\n",
            "step 8301: generator_loss=0.6694022417068481, discriminator_loss=0.7248353958129883\n",
            "step 8302: generator_loss=0.6622382402420044, discriminator_loss=0.7275364398956299\n",
            "step 8303: generator_loss=0.657281756401062, discriminator_loss=0.7293794751167297\n",
            "step 8304: generator_loss=0.64808189868927, discriminator_loss=0.7329358458518982\n",
            "step 8305: generator_loss=0.6479297876358032, discriminator_loss=0.731733500957489\n",
            "step 8306: generator_loss=0.6415176391601562, discriminator_loss=0.7354629039764404\n",
            "step 8307: generator_loss=0.6506249904632568, discriminator_loss=0.7306010127067566\n",
            "step 8308: generator_loss=0.6408942341804504, discriminator_loss=0.7342590093612671\n",
            "step 8309: generator_loss=0.6534802913665771, discriminator_loss=0.7279608845710754\n",
            "step 8310: generator_loss=0.6472555994987488, discriminator_loss=0.7315351366996765\n",
            "step 8311: generator_loss=0.6493784189224243, discriminator_loss=0.7300361394882202\n",
            "step 8312: generator_loss=0.648138701915741, discriminator_loss=0.7293509840965271\n",
            "step 8313: generator_loss=0.6565645337104797, discriminator_loss=0.7254922389984131\n",
            "step 8314: generator_loss=0.660799503326416, discriminator_loss=0.7213666439056396\n",
            "step 8315: generator_loss=0.6586757898330688, discriminator_loss=0.7202556729316711\n",
            "step 8316: generator_loss=0.6778293251991272, discriminator_loss=0.7104925513267517\n",
            "step 8317: generator_loss=0.6759401559829712, discriminator_loss=0.7082580327987671\n",
            "step 8318: generator_loss=0.691234290599823, discriminator_loss=0.6967030763626099\n",
            "step 8319: generator_loss=0.6922042965888977, discriminator_loss=0.6967282891273499\n",
            "step 8320: generator_loss=0.6991839408874512, discriminator_loss=0.6894659399986267\n",
            "step 8321: generator_loss=0.704513430595398, discriminator_loss=0.6860105991363525\n",
            "step 8322: generator_loss=0.7158222198486328, discriminator_loss=0.680007815361023\n",
            "step 8323: generator_loss=0.7227208614349365, discriminator_loss=0.6738770604133606\n",
            "step 8324: generator_loss=0.7289650440216064, discriminator_loss=0.6705412864685059\n",
            "step 8325: generator_loss=0.7339094877243042, discriminator_loss=0.6645691990852356\n",
            "step 8326: generator_loss=0.7340254783630371, discriminator_loss=0.6653657555580139\n",
            "step 8327: generator_loss=0.7391179800033569, discriminator_loss=0.6608169674873352\n",
            "step 8328: generator_loss=0.7411969900131226, discriminator_loss=0.6591405868530273\n",
            "step 8329: generator_loss=0.7458069324493408, discriminator_loss=0.6560964584350586\n",
            "step 8330: generator_loss=0.7449029684066772, discriminator_loss=0.6525037884712219\n",
            "step 8331: generator_loss=0.745026707649231, discriminator_loss=0.649766206741333\n",
            "step 8332: generator_loss=0.748396098613739, discriminator_loss=0.6461769938468933\n",
            "step 8333: generator_loss=0.7475855946540833, discriminator_loss=0.6453783512115479\n",
            "step 8334: generator_loss=0.7543628215789795, discriminator_loss=0.6422579288482666\n",
            "step 8335: generator_loss=0.7571903467178345, discriminator_loss=0.6406422853469849\n",
            "step 8336: generator_loss=0.761594831943512, discriminator_loss=0.639388918876648\n",
            "step 8337: generator_loss=0.7661605477333069, discriminator_loss=0.6384242177009583\n",
            "step 8338: generator_loss=0.773688554763794, discriminator_loss=0.6359772682189941\n",
            "step 8339: generator_loss=0.777721643447876, discriminator_loss=0.6342880725860596\n",
            "step 8340: generator_loss=0.7802019119262695, discriminator_loss=0.6323081254959106\n",
            "step 8341: generator_loss=0.777180016040802, discriminator_loss=0.6312421560287476\n",
            "step 8342: generator_loss=0.7784101366996765, discriminator_loss=0.6326572895050049\n",
            "step 8343: generator_loss=0.7749677896499634, discriminator_loss=0.6327106952667236\n",
            "step 8344: generator_loss=0.7703881859779358, discriminator_loss=0.6348577737808228\n",
            "step 8345: generator_loss=0.7715107202529907, discriminator_loss=0.6343249082565308\n",
            "step 8346: generator_loss=0.7686917781829834, discriminator_loss=0.6355243921279907\n",
            "step 8347: generator_loss=0.7569085359573364, discriminator_loss=0.6385551691055298\n",
            "step 8348: generator_loss=0.7516917586326599, discriminator_loss=0.6391499042510986\n",
            "step 8349: generator_loss=0.7452075481414795, discriminator_loss=0.6412005424499512\n",
            "step 8350: generator_loss=0.7326207160949707, discriminator_loss=0.6467087268829346\n",
            "step 8351: generator_loss=0.7261255979537964, discriminator_loss=0.6482554078102112\n",
            "step 8352: generator_loss=0.7199667692184448, discriminator_loss=0.6506811380386353\n",
            "step 8353: generator_loss=0.7166903018951416, discriminator_loss=0.6536892652511597\n",
            "step 8354: generator_loss=0.7213926315307617, discriminator_loss=0.6531774401664734\n",
            "step 8355: generator_loss=0.7166619300842285, discriminator_loss=0.657709002494812\n",
            "step 8356: generator_loss=0.7195406556129456, discriminator_loss=0.6581710577011108\n",
            "step 8357: generator_loss=0.7139284610748291, discriminator_loss=0.6640430688858032\n",
            "step 8358: generator_loss=0.7203489542007446, discriminator_loss=0.6659916639328003\n",
            "step 8359: generator_loss=0.7217587232589722, discriminator_loss=0.6669408082962036\n",
            "step 8360: generator_loss=0.7270618677139282, discriminator_loss=0.6687433123588562\n",
            "step 8361: generator_loss=0.7236883044242859, discriminator_loss=0.6731047034263611\n",
            "step 8362: generator_loss=0.721771776676178, discriminator_loss=0.6775466799736023\n",
            "step 8363: generator_loss=0.7228798866271973, discriminator_loss=0.6764060258865356\n",
            "step 8364: generator_loss=0.7114948034286499, discriminator_loss=0.6821514368057251\n",
            "step 8365: generator_loss=0.697572648525238, discriminator_loss=0.6891246438026428\n",
            "step 8366: generator_loss=0.6894820332527161, discriminator_loss=0.6919784545898438\n",
            "step 8367: generator_loss=0.6871315240859985, discriminator_loss=0.691612720489502\n",
            "step 8368: generator_loss=0.6765619516372681, discriminator_loss=0.6960274577140808\n",
            "step 8369: generator_loss=0.6797508001327515, discriminator_loss=0.6948716640472412\n",
            "step 8370: generator_loss=0.6673377156257629, discriminator_loss=0.7032482624053955\n",
            "step 8371: generator_loss=0.6636044979095459, discriminator_loss=0.7065260410308838\n",
            "step 8372: generator_loss=0.6651740074157715, discriminator_loss=0.7055109739303589\n",
            "step 8373: generator_loss=0.6578642725944519, discriminator_loss=0.7121673226356506\n",
            "step 8374: generator_loss=0.656696617603302, discriminator_loss=0.7160840630531311\n",
            "step 8375: generator_loss=0.6581234931945801, discriminator_loss=0.7160276770591736\n",
            "step 8376: generator_loss=0.6631123423576355, discriminator_loss=0.7162983417510986\n",
            "step 8377: generator_loss=0.6615434885025024, discriminator_loss=0.7195720672607422\n",
            "step 8378: generator_loss=0.6602282524108887, discriminator_loss=0.7222056984901428\n",
            "step 8379: generator_loss=0.6530737280845642, discriminator_loss=0.7263667583465576\n",
            "step 8380: generator_loss=0.6585333347320557, discriminator_loss=0.7227123975753784\n",
            "step 8381: generator_loss=0.6621915698051453, discriminator_loss=0.7191570997238159\n",
            "step 8382: generator_loss=0.6535266041755676, discriminator_loss=0.7240049242973328\n",
            "step 8383: generator_loss=0.6520541906356812, discriminator_loss=0.7248948216438293\n",
            "step 8384: generator_loss=0.6645539402961731, discriminator_loss=0.7208812236785889\n",
            "step 8385: generator_loss=0.6598613262176514, discriminator_loss=0.7234753370285034\n",
            "step 8386: generator_loss=0.6625465154647827, discriminator_loss=0.725997805595398\n",
            "step 8387: generator_loss=0.670517086982727, discriminator_loss=0.7238876819610596\n",
            "step 8388: generator_loss=0.6794222593307495, discriminator_loss=0.7200480699539185\n",
            "step 8389: generator_loss=0.6652606725692749, discriminator_loss=0.7292205095291138\n",
            "step 8390: generator_loss=0.6694782972335815, discriminator_loss=0.7252574563026428\n",
            "step 8391: generator_loss=0.6611813306808472, discriminator_loss=0.7275272011756897\n",
            "step 8392: generator_loss=0.6658637523651123, discriminator_loss=0.7252007722854614\n",
            "step 8393: generator_loss=0.6600821614265442, discriminator_loss=0.7222793102264404\n",
            "step 8394: generator_loss=0.65987229347229, discriminator_loss=0.7209280729293823\n",
            "step 8395: generator_loss=0.664059042930603, discriminator_loss=0.7155637741088867\n",
            "step 8396: generator_loss=0.6776295900344849, discriminator_loss=0.7064826488494873\n",
            "step 8397: generator_loss=0.6765174865722656, discriminator_loss=0.7057822346687317\n",
            "step 8398: generator_loss=0.6933607459068298, discriminator_loss=0.6956095099449158\n",
            "step 8399: generator_loss=0.6811772584915161, discriminator_loss=0.703624963760376\n",
            "step 8400: generator_loss=0.6906832456588745, discriminator_loss=0.6983795166015625\n",
            "step 8401: generator_loss=0.7126302719116211, discriminator_loss=0.6863191723823547\n",
            "step 8402: generator_loss=0.6988414525985718, discriminator_loss=0.693579912185669\n",
            "step 8403: generator_loss=0.7179070711135864, discriminator_loss=0.6856040954589844\n",
            "step 8404: generator_loss=0.7019423246383667, discriminator_loss=0.693708598613739\n",
            "step 8405: generator_loss=0.7149676084518433, discriminator_loss=0.6867130994796753\n",
            "step 8406: generator_loss=0.7161771655082703, discriminator_loss=0.6876590251922607\n",
            "step 8407: generator_loss=0.7122207283973694, discriminator_loss=0.689651370048523\n",
            "step 8408: generator_loss=0.7070687413215637, discriminator_loss=0.690659761428833\n",
            "step 8409: generator_loss=0.7191088199615479, discriminator_loss=0.6821198463439941\n",
            "step 8410: generator_loss=0.7022966742515564, discriminator_loss=0.6893798112869263\n",
            "step 8411: generator_loss=0.7077381610870361, discriminator_loss=0.6824197173118591\n",
            "step 8412: generator_loss=0.6969561576843262, discriminator_loss=0.6859903335571289\n",
            "step 8413: generator_loss=0.6971697807312012, discriminator_loss=0.6833947896957397\n",
            "step 8414: generator_loss=0.7083975076675415, discriminator_loss=0.6808976531028748\n",
            "step 8415: generator_loss=0.7193312048912048, discriminator_loss=0.6736990213394165\n",
            "step 8416: generator_loss=0.7151249647140503, discriminator_loss=0.6768807172775269\n",
            "step 8417: generator_loss=0.7203773260116577, discriminator_loss=0.6744598150253296\n",
            "step 8418: generator_loss=0.7151663303375244, discriminator_loss=0.6768412590026855\n",
            "step 8419: generator_loss=0.7122699022293091, discriminator_loss=0.6760421991348267\n",
            "step 8420: generator_loss=0.7214065790176392, discriminator_loss=0.6717385053634644\n",
            "step 8421: generator_loss=0.7092538475990295, discriminator_loss=0.673522412776947\n",
            "step 8422: generator_loss=0.7123079299926758, discriminator_loss=0.6715883612632751\n",
            "step 8423: generator_loss=0.7127436399459839, discriminator_loss=0.6664436459541321\n",
            "step 8424: generator_loss=0.7043255567550659, discriminator_loss=0.6671155691146851\n",
            "step 8425: generator_loss=0.7066231966018677, discriminator_loss=0.662962794303894\n",
            "step 8426: generator_loss=0.7110915184020996, discriminator_loss=0.6614983677864075\n",
            "step 8427: generator_loss=0.7074356079101562, discriminator_loss=0.6659187078475952\n",
            "step 8428: generator_loss=0.7201561331748962, discriminator_loss=0.6608461141586304\n",
            "step 8429: generator_loss=0.7249799370765686, discriminator_loss=0.6605055332183838\n",
            "step 8430: generator_loss=0.7204326391220093, discriminator_loss=0.664606511592865\n",
            "step 8431: generator_loss=0.7240575551986694, discriminator_loss=0.663180410861969\n",
            "step 8432: generator_loss=0.7162545919418335, discriminator_loss=0.6682447195053101\n",
            "step 8433: generator_loss=0.7232052087783813, discriminator_loss=0.6670712232589722\n",
            "step 8434: generator_loss=0.7306176424026489, discriminator_loss=0.6643248796463013\n",
            "step 8435: generator_loss=0.7312607765197754, discriminator_loss=0.6649641990661621\n",
            "step 8436: generator_loss=0.7271735668182373, discriminator_loss=0.6675527095794678\n",
            "step 8437: generator_loss=0.7267504334449768, discriminator_loss=0.6660438776016235\n",
            "step 8438: generator_loss=0.7169090509414673, discriminator_loss=0.6689648628234863\n",
            "step 8439: generator_loss=0.7202773094177246, discriminator_loss=0.6672521233558655\n",
            "step 8440: generator_loss=0.7197363376617432, discriminator_loss=0.6663448214530945\n",
            "step 8441: generator_loss=0.7155165076255798, discriminator_loss=0.669297456741333\n",
            "step 8442: generator_loss=0.7203127145767212, discriminator_loss=0.6644246578216553\n",
            "step 8443: generator_loss=0.7220635414123535, discriminator_loss=0.6630253791809082\n",
            "step 8444: generator_loss=0.7341287136077881, discriminator_loss=0.6572659015655518\n",
            "step 8445: generator_loss=0.7359996438026428, discriminator_loss=0.6576588153839111\n",
            "step 8446: generator_loss=0.7421377897262573, discriminator_loss=0.6553261280059814\n",
            "step 8447: generator_loss=0.7484641075134277, discriminator_loss=0.657004177570343\n",
            "step 8448: generator_loss=0.7532459497451782, discriminator_loss=0.6555451154708862\n",
            "step 8449: generator_loss=0.753941535949707, discriminator_loss=0.6569769382476807\n",
            "step 8450: generator_loss=0.7536787390708923, discriminator_loss=0.6575992107391357\n",
            "step 8451: generator_loss=0.7554171681404114, discriminator_loss=0.6562861204147339\n",
            "step 8452: generator_loss=0.7534534931182861, discriminator_loss=0.6548030972480774\n",
            "step 8453: generator_loss=0.7498927116394043, discriminator_loss=0.6551759839057922\n",
            "step 8454: generator_loss=0.7418996691703796, discriminator_loss=0.6567827463150024\n",
            "step 8455: generator_loss=0.7404642105102539, discriminator_loss=0.6559218764305115\n",
            "step 8456: generator_loss=0.7321734428405762, discriminator_loss=0.6598589420318604\n",
            "step 8457: generator_loss=0.727452278137207, discriminator_loss=0.6602492332458496\n",
            "step 8458: generator_loss=0.711553692817688, discriminator_loss=0.666713297367096\n",
            "step 8459: generator_loss=0.7158254981040955, discriminator_loss=0.6631760597229004\n",
            "step 8460: generator_loss=0.7169924974441528, discriminator_loss=0.6621819734573364\n",
            "step 8461: generator_loss=0.7077723741531372, discriminator_loss=0.6677523851394653\n",
            "step 8462: generator_loss=0.705233633518219, discriminator_loss=0.668452262878418\n",
            "step 8463: generator_loss=0.7097165584564209, discriminator_loss=0.6658729314804077\n",
            "step 8464: generator_loss=0.6996745467185974, discriminator_loss=0.6708530783653259\n",
            "step 8465: generator_loss=0.7068116664886475, discriminator_loss=0.6685336828231812\n",
            "step 8466: generator_loss=0.7015585899353027, discriminator_loss=0.6735404133796692\n",
            "step 8467: generator_loss=0.7070038914680481, discriminator_loss=0.6723443269729614\n",
            "step 8468: generator_loss=0.7079625129699707, discriminator_loss=0.6739077568054199\n",
            "step 8469: generator_loss=0.7224016785621643, discriminator_loss=0.6680707931518555\n",
            "step 8470: generator_loss=0.7222826480865479, discriminator_loss=0.6718682050704956\n",
            "step 8471: generator_loss=0.7264711856842041, discriminator_loss=0.6716089248657227\n",
            "step 8472: generator_loss=0.7280595302581787, discriminator_loss=0.6743491888046265\n",
            "step 8473: generator_loss=0.7301302552223206, discriminator_loss=0.6741591691970825\n",
            "step 8474: generator_loss=0.7253589630126953, discriminator_loss=0.6787683963775635\n",
            "step 8475: generator_loss=0.7267980575561523, discriminator_loss=0.6761922836303711\n",
            "step 8476: generator_loss=0.7259683609008789, discriminator_loss=0.6780058741569519\n",
            "step 8477: generator_loss=0.716184139251709, discriminator_loss=0.6813949346542358\n",
            "step 8478: generator_loss=0.7061409950256348, discriminator_loss=0.685969889163971\n",
            "step 8479: generator_loss=0.7018182277679443, discriminator_loss=0.6869571208953857\n",
            "step 8480: generator_loss=0.6988670825958252, discriminator_loss=0.6874390840530396\n",
            "step 8481: generator_loss=0.6914367079734802, discriminator_loss=0.6901439428329468\n",
            "step 8482: generator_loss=0.6916588544845581, discriminator_loss=0.6909878849983215\n",
            "step 8483: generator_loss=0.6800685524940491, discriminator_loss=0.695094883441925\n",
            "step 8484: generator_loss=0.6850117444992065, discriminator_loss=0.6943337917327881\n",
            "step 8485: generator_loss=0.6751622557640076, discriminator_loss=0.7003578543663025\n",
            "step 8486: generator_loss=0.6785024404525757, discriminator_loss=0.6980596780776978\n",
            "step 8487: generator_loss=0.670020580291748, discriminator_loss=0.7059007287025452\n",
            "step 8488: generator_loss=0.6740925312042236, discriminator_loss=0.7039326429367065\n",
            "step 8489: generator_loss=0.6698524951934814, discriminator_loss=0.7074054479598999\n",
            "step 8490: generator_loss=0.6714408993721008, discriminator_loss=0.7093830108642578\n",
            "step 8491: generator_loss=0.6759951114654541, discriminator_loss=0.7080244421958923\n",
            "step 8492: generator_loss=0.6712641716003418, discriminator_loss=0.7120567560195923\n",
            "step 8493: generator_loss=0.6683982610702515, discriminator_loss=0.7141077518463135\n",
            "step 8494: generator_loss=0.6666536331176758, discriminator_loss=0.7158421277999878\n",
            "step 8495: generator_loss=0.6713610887527466, discriminator_loss=0.7147793769836426\n",
            "step 8496: generator_loss=0.661250114440918, discriminator_loss=0.7189740538597107\n",
            "step 8497: generator_loss=0.654008686542511, discriminator_loss=0.7219201326370239\n",
            "step 8498: generator_loss=0.6546687483787537, discriminator_loss=0.7230201959609985\n",
            "step 8499: generator_loss=0.655860960483551, discriminator_loss=0.7220666408538818\n",
            "step 8500: generator_loss=0.6567972898483276, discriminator_loss=0.7202403545379639\n",
            "step 8501: generator_loss=0.6602790951728821, discriminator_loss=0.7175179123878479\n",
            "step 8502: generator_loss=0.6580406427383423, discriminator_loss=0.7188774347305298\n",
            "step 8503: generator_loss=0.6642023324966431, discriminator_loss=0.7153239250183105\n",
            "step 8504: generator_loss=0.6689596176147461, discriminator_loss=0.7125632166862488\n",
            "step 8505: generator_loss=0.674418568611145, discriminator_loss=0.7097215056419373\n",
            "step 8506: generator_loss=0.6849064826965332, discriminator_loss=0.704340398311615\n",
            "step 8507: generator_loss=0.6931110620498657, discriminator_loss=0.7011896967887878\n",
            "step 8508: generator_loss=0.6981077790260315, discriminator_loss=0.6985405683517456\n",
            "step 8509: generator_loss=0.7122683525085449, discriminator_loss=0.6915020942687988\n",
            "step 8510: generator_loss=0.7191213369369507, discriminator_loss=0.6901301741600037\n",
            "step 8511: generator_loss=0.7232348918914795, discriminator_loss=0.6908946633338928\n",
            "step 8512: generator_loss=0.7291200757026672, discriminator_loss=0.6893026232719421\n",
            "step 8513: generator_loss=0.7218300104141235, discriminator_loss=0.6926029920578003\n",
            "step 8514: generator_loss=0.7175785303115845, discriminator_loss=0.6932101249694824\n",
            "step 8515: generator_loss=0.7145316004753113, discriminator_loss=0.6905659437179565\n",
            "step 8516: generator_loss=0.7003927826881409, discriminator_loss=0.694599449634552\n",
            "step 8517: generator_loss=0.6998547911643982, discriminator_loss=0.6933451294898987\n",
            "step 8518: generator_loss=0.6977213025093079, discriminator_loss=0.6900432705879211\n",
            "step 8519: generator_loss=0.6759750843048096, discriminator_loss=0.6996638774871826\n",
            "step 8520: generator_loss=0.6814426183700562, discriminator_loss=0.6946111917495728\n",
            "step 8521: generator_loss=0.6717297434806824, discriminator_loss=0.6987888813018799\n",
            "step 8522: generator_loss=0.6779824495315552, discriminator_loss=0.6940600872039795\n",
            "step 8523: generator_loss=0.6704099178314209, discriminator_loss=0.6981918215751648\n",
            "step 8524: generator_loss=0.6672494411468506, discriminator_loss=0.7005191445350647\n",
            "step 8525: generator_loss=0.685575544834137, discriminator_loss=0.6936209201812744\n",
            "step 8526: generator_loss=0.6794254779815674, discriminator_loss=0.6978228092193604\n",
            "step 8527: generator_loss=0.6877927780151367, discriminator_loss=0.6965029835700989\n",
            "step 8528: generator_loss=0.6965100169181824, discriminator_loss=0.6932703256607056\n",
            "step 8529: generator_loss=0.7060765027999878, discriminator_loss=0.6897825002670288\n",
            "step 8530: generator_loss=0.708509087562561, discriminator_loss=0.6918378472328186\n",
            "step 8531: generator_loss=0.7128238677978516, discriminator_loss=0.6912823915481567\n",
            "step 8532: generator_loss=0.7232259511947632, discriminator_loss=0.6868771314620972\n",
            "step 8533: generator_loss=0.7340496778488159, discriminator_loss=0.6802752614021301\n",
            "step 8534: generator_loss=0.7356497645378113, discriminator_loss=0.6768364906311035\n",
            "step 8535: generator_loss=0.7390425205230713, discriminator_loss=0.6758115291595459\n",
            "step 8536: generator_loss=0.7417949438095093, discriminator_loss=0.6709756851196289\n",
            "step 8537: generator_loss=0.7375961542129517, discriminator_loss=0.6725627183914185\n",
            "step 8538: generator_loss=0.7308202981948853, discriminator_loss=0.6745243072509766\n",
            "step 8539: generator_loss=0.7244938015937805, discriminator_loss=0.6754851937294006\n",
            "step 8540: generator_loss=0.715954601764679, discriminator_loss=0.6737295389175415\n",
            "step 8541: generator_loss=0.7088481187820435, discriminator_loss=0.6747852563858032\n",
            "step 8542: generator_loss=0.7056986093521118, discriminator_loss=0.6728872060775757\n",
            "step 8543: generator_loss=0.7018910050392151, discriminator_loss=0.6732582449913025\n",
            "step 8544: generator_loss=0.7011486887931824, discriminator_loss=0.6742826700210571\n",
            "step 8545: generator_loss=0.6948356628417969, discriminator_loss=0.6772018671035767\n",
            "step 8546: generator_loss=0.6897315382957458, discriminator_loss=0.6806875467300415\n",
            "step 8547: generator_loss=0.6969168782234192, discriminator_loss=0.6780471205711365\n",
            "step 8548: generator_loss=0.6926718950271606, discriminator_loss=0.6814286112785339\n",
            "step 8549: generator_loss=0.6961722373962402, discriminator_loss=0.6831797361373901\n",
            "step 8550: generator_loss=0.6989334225654602, discriminator_loss=0.6821067333221436\n",
            "step 8551: generator_loss=0.6975152492523193, discriminator_loss=0.685909628868103\n",
            "step 8552: generator_loss=0.7010711431503296, discriminator_loss=0.6873361468315125\n",
            "step 8553: generator_loss=0.7085585594177246, discriminator_loss=0.6865600347518921\n",
            "step 8554: generator_loss=0.6998459696769714, discriminator_loss=0.6928560137748718\n",
            "step 8555: generator_loss=0.7025848031044006, discriminator_loss=0.6932934522628784\n",
            "step 8556: generator_loss=0.6933924555778503, discriminator_loss=0.6991938948631287\n",
            "step 8557: generator_loss=0.693160891532898, discriminator_loss=0.6990762948989868\n",
            "step 8558: generator_loss=0.6851046085357666, discriminator_loss=0.7060786485671997\n",
            "step 8559: generator_loss=0.6802158951759338, discriminator_loss=0.7075672149658203\n",
            "step 8560: generator_loss=0.6875265836715698, discriminator_loss=0.7030037641525269\n",
            "step 8561: generator_loss=0.6735519170761108, discriminator_loss=0.7111266851425171\n",
            "step 8562: generator_loss=0.6774601340293884, discriminator_loss=0.7081361413002014\n",
            "step 8563: generator_loss=0.6723747253417969, discriminator_loss=0.7118885517120361\n",
            "step 8564: generator_loss=0.6699990034103394, discriminator_loss=0.7150430679321289\n",
            "step 8565: generator_loss=0.6676796674728394, discriminator_loss=0.7162421345710754\n",
            "step 8566: generator_loss=0.6732722520828247, discriminator_loss=0.7143384218215942\n",
            "step 8567: generator_loss=0.6685739755630493, discriminator_loss=0.7186782360076904\n",
            "step 8568: generator_loss=0.669626772403717, discriminator_loss=0.7179954648017883\n",
            "step 8569: generator_loss=0.66093510389328, discriminator_loss=0.722981333732605\n",
            "step 8570: generator_loss=0.663620114326477, discriminator_loss=0.7220060229301453\n",
            "step 8571: generator_loss=0.6658180952072144, discriminator_loss=0.7213720083236694\n",
            "step 8572: generator_loss=0.6637305617332458, discriminator_loss=0.723152756690979\n",
            "step 8573: generator_loss=0.6618027687072754, discriminator_loss=0.7238146066665649\n",
            "step 8574: generator_loss=0.6531009674072266, discriminator_loss=0.7268863320350647\n",
            "step 8575: generator_loss=0.6599971652030945, discriminator_loss=0.7231022119522095\n",
            "step 8576: generator_loss=0.6517319679260254, discriminator_loss=0.7257325649261475\n",
            "step 8577: generator_loss=0.6630867123603821, discriminator_loss=0.7178632020950317\n",
            "step 8578: generator_loss=0.656739354133606, discriminator_loss=0.7202920913696289\n",
            "step 8579: generator_loss=0.6704801321029663, discriminator_loss=0.7105977535247803\n",
            "step 8580: generator_loss=0.6750808358192444, discriminator_loss=0.7046165466308594\n",
            "step 8581: generator_loss=0.6891429424285889, discriminator_loss=0.6939204335212708\n",
            "step 8582: generator_loss=0.6935794353485107, discriminator_loss=0.6876688003540039\n",
            "step 8583: generator_loss=0.7083340883255005, discriminator_loss=0.6775807738304138\n",
            "step 8584: generator_loss=0.7238140106201172, discriminator_loss=0.6680349111557007\n",
            "step 8585: generator_loss=0.7271385192871094, discriminator_loss=0.6642916798591614\n",
            "step 8586: generator_loss=0.7388418912887573, discriminator_loss=0.6583890914916992\n",
            "step 8587: generator_loss=0.7475429773330688, discriminator_loss=0.6572099924087524\n",
            "step 8588: generator_loss=0.7574605345726013, discriminator_loss=0.6527066230773926\n",
            "step 8589: generator_loss=0.7607249021530151, discriminator_loss=0.6527511477470398\n",
            "step 8590: generator_loss=0.7549690008163452, discriminator_loss=0.6562942266464233\n",
            "step 8591: generator_loss=0.7599009275436401, discriminator_loss=0.6524338126182556\n",
            "step 8592: generator_loss=0.7542219161987305, discriminator_loss=0.6527634263038635\n",
            "step 8593: generator_loss=0.7482210993766785, discriminator_loss=0.6546149849891663\n",
            "step 8594: generator_loss=0.743054211139679, discriminator_loss=0.6544489860534668\n",
            "step 8595: generator_loss=0.7368376851081848, discriminator_loss=0.6553843021392822\n",
            "step 8596: generator_loss=0.7265032529830933, discriminator_loss=0.6590593457221985\n",
            "step 8597: generator_loss=0.7196958065032959, discriminator_loss=0.6585322618484497\n",
            "step 8598: generator_loss=0.7109452486038208, discriminator_loss=0.6623145937919617\n",
            "step 8599: generator_loss=0.7085786461830139, discriminator_loss=0.6616808176040649\n",
            "step 8600: generator_loss=0.7096035480499268, discriminator_loss=0.6618001461029053\n",
            "step 8601: generator_loss=0.7061768174171448, discriminator_loss=0.6618083715438843\n",
            "step 8602: generator_loss=0.7031278610229492, discriminator_loss=0.6651924848556519\n",
            "step 8603: generator_loss=0.7124232053756714, discriminator_loss=0.6616464257240295\n",
            "step 8604: generator_loss=0.7176393270492554, discriminator_loss=0.6598015427589417\n",
            "step 8605: generator_loss=0.7191199064254761, discriminator_loss=0.6612215638160706\n",
            "step 8606: generator_loss=0.7196422815322876, discriminator_loss=0.6600161194801331\n",
            "step 8607: generator_loss=0.7320997714996338, discriminator_loss=0.6557663679122925\n",
            "step 8608: generator_loss=0.7274047136306763, discriminator_loss=0.6592384576797485\n",
            "step 8609: generator_loss=0.7324696779251099, discriminator_loss=0.6565454602241516\n",
            "step 8610: generator_loss=0.7305193543434143, discriminator_loss=0.6595646142959595\n",
            "step 8611: generator_loss=0.7437706589698792, discriminator_loss=0.6527403593063354\n",
            "step 8612: generator_loss=0.7401634454727173, discriminator_loss=0.654752790927887\n",
            "step 8613: generator_loss=0.7377049922943115, discriminator_loss=0.6573860049247742\n",
            "step 8614: generator_loss=0.7373972535133362, discriminator_loss=0.6582779884338379\n",
            "step 8615: generator_loss=0.7412655353546143, discriminator_loss=0.6563296914100647\n",
            "step 8616: generator_loss=0.7409132719039917, discriminator_loss=0.6558349132537842\n",
            "step 8617: generator_loss=0.7256200313568115, discriminator_loss=0.6628726124763489\n",
            "step 8618: generator_loss=0.7221742868423462, discriminator_loss=0.6602540612220764\n",
            "step 8619: generator_loss=0.7227073907852173, discriminator_loss=0.6616702079772949\n",
            "step 8620: generator_loss=0.716656506061554, discriminator_loss=0.6630733609199524\n",
            "step 8621: generator_loss=0.7116735577583313, discriminator_loss=0.6657856702804565\n",
            "step 8622: generator_loss=0.705905556678772, discriminator_loss=0.6672834753990173\n",
            "step 8623: generator_loss=0.7048493027687073, discriminator_loss=0.6690343022346497\n",
            "step 8624: generator_loss=0.7050888538360596, discriminator_loss=0.6713249087333679\n",
            "step 8625: generator_loss=0.7030382752418518, discriminator_loss=0.673509418964386\n",
            "step 8626: generator_loss=0.712709903717041, discriminator_loss=0.6720738410949707\n",
            "step 8627: generator_loss=0.7145510911941528, discriminator_loss=0.6725325584411621\n",
            "step 8628: generator_loss=0.7131226062774658, discriminator_loss=0.675253689289093\n",
            "step 8629: generator_loss=0.7212350368499756, discriminator_loss=0.6751430034637451\n",
            "step 8630: generator_loss=0.7264921069145203, discriminator_loss=0.672104001045227\n",
            "step 8631: generator_loss=0.7224299907684326, discriminator_loss=0.6767761707305908\n",
            "step 8632: generator_loss=0.720927894115448, discriminator_loss=0.6786375641822815\n",
            "step 8633: generator_loss=0.7218198776245117, discriminator_loss=0.6790934801101685\n",
            "step 8634: generator_loss=0.7170594930648804, discriminator_loss=0.6805631518363953\n",
            "step 8635: generator_loss=0.7114711999893188, discriminator_loss=0.6824960112571716\n",
            "step 8636: generator_loss=0.7108479738235474, discriminator_loss=0.680072009563446\n",
            "step 8637: generator_loss=0.7065314650535583, discriminator_loss=0.682998776435852\n",
            "step 8638: generator_loss=0.7021046876907349, discriminator_loss=0.6840770244598389\n",
            "step 8639: generator_loss=0.6992803812026978, discriminator_loss=0.6846743822097778\n",
            "step 8640: generator_loss=0.6941959261894226, discriminator_loss=0.6871511936187744\n",
            "step 8641: generator_loss=0.6930875182151794, discriminator_loss=0.6868051886558533\n",
            "step 8642: generator_loss=0.6886471509933472, discriminator_loss=0.690125048160553\n",
            "step 8643: generator_loss=0.6912921071052551, discriminator_loss=0.6883205771446228\n",
            "step 8644: generator_loss=0.6896889209747314, discriminator_loss=0.69041907787323\n",
            "step 8645: generator_loss=0.687181830406189, discriminator_loss=0.69119793176651\n",
            "step 8646: generator_loss=0.6881376504898071, discriminator_loss=0.6914450526237488\n",
            "step 8647: generator_loss=0.6849404573440552, discriminator_loss=0.6952078342437744\n",
            "step 8648: generator_loss=0.6843268871307373, discriminator_loss=0.6980113387107849\n",
            "step 8649: generator_loss=0.6890091300010681, discriminator_loss=0.6974757313728333\n",
            "step 8650: generator_loss=0.6832047700881958, discriminator_loss=0.7029857039451599\n",
            "step 8651: generator_loss=0.685960054397583, discriminator_loss=0.7045989036560059\n",
            "step 8652: generator_loss=0.6760977506637573, discriminator_loss=0.7116808891296387\n",
            "step 8653: generator_loss=0.6797124743461609, discriminator_loss=0.7111243009567261\n",
            "step 8654: generator_loss=0.6763646602630615, discriminator_loss=0.7140020132064819\n",
            "step 8655: generator_loss=0.6700390577316284, discriminator_loss=0.7167533040046692\n",
            "step 8656: generator_loss=0.6752374768257141, discriminator_loss=0.71497642993927\n",
            "step 8657: generator_loss=0.6623620986938477, discriminator_loss=0.7242347002029419\n",
            "step 8658: generator_loss=0.6612037420272827, discriminator_loss=0.7260456085205078\n",
            "step 8659: generator_loss=0.6512643098831177, discriminator_loss=0.732459545135498\n",
            "step 8660: generator_loss=0.6544008255004883, discriminator_loss=0.7298892140388489\n",
            "step 8661: generator_loss=0.6551015377044678, discriminator_loss=0.7316514253616333\n",
            "step 8662: generator_loss=0.641610860824585, discriminator_loss=0.7400355339050293\n",
            "step 8663: generator_loss=0.6487290859222412, discriminator_loss=0.7385704517364502\n",
            "step 8664: generator_loss=0.640339195728302, discriminator_loss=0.746342658996582\n",
            "step 8665: generator_loss=0.642447292804718, discriminator_loss=0.7453382015228271\n",
            "step 8666: generator_loss=0.6404250860214233, discriminator_loss=0.747543454170227\n",
            "step 8667: generator_loss=0.6424755454063416, discriminator_loss=0.74773108959198\n",
            "step 8668: generator_loss=0.6386889219284058, discriminator_loss=0.7500112056732178\n",
            "step 8669: generator_loss=0.6366380453109741, discriminator_loss=0.7520480751991272\n",
            "step 8670: generator_loss=0.6432912349700928, discriminator_loss=0.7481322288513184\n",
            "step 8671: generator_loss=0.6375957727432251, discriminator_loss=0.7526708841323853\n",
            "step 8672: generator_loss=0.6354357004165649, discriminator_loss=0.7533606290817261\n",
            "step 8673: generator_loss=0.6427130103111267, discriminator_loss=0.749353289604187\n",
            "step 8674: generator_loss=0.6473954319953918, discriminator_loss=0.7459238171577454\n",
            "step 8675: generator_loss=0.6470561027526855, discriminator_loss=0.7451244592666626\n",
            "step 8676: generator_loss=0.6498769521713257, discriminator_loss=0.7412817478179932\n",
            "step 8677: generator_loss=0.6503967046737671, discriminator_loss=0.737210750579834\n",
            "step 8678: generator_loss=0.6560635566711426, discriminator_loss=0.7320577502250671\n",
            "step 8679: generator_loss=0.662778377532959, discriminator_loss=0.7285184860229492\n",
            "step 8680: generator_loss=0.6633225679397583, discriminator_loss=0.7259077429771423\n",
            "step 8681: generator_loss=0.6673856973648071, discriminator_loss=0.7226903438568115\n",
            "step 8682: generator_loss=0.6720547676086426, discriminator_loss=0.7201102375984192\n",
            "step 8683: generator_loss=0.6726037859916687, discriminator_loss=0.7170850038528442\n",
            "step 8684: generator_loss=0.6796075105667114, discriminator_loss=0.711647629737854\n",
            "step 8685: generator_loss=0.69377201795578, discriminator_loss=0.7009414434432983\n",
            "step 8686: generator_loss=0.6971677541732788, discriminator_loss=0.6937429904937744\n",
            "step 8687: generator_loss=0.7005196809768677, discriminator_loss=0.6890577077865601\n",
            "step 8688: generator_loss=0.7024150490760803, discriminator_loss=0.6835381388664246\n",
            "step 8689: generator_loss=0.7019474506378174, discriminator_loss=0.6800919771194458\n",
            "step 8690: generator_loss=0.7072876691818237, discriminator_loss=0.6738853454589844\n",
            "step 8691: generator_loss=0.7048675417900085, discriminator_loss=0.6748653650283813\n",
            "step 8692: generator_loss=0.7095201015472412, discriminator_loss=0.6700515747070312\n",
            "step 8693: generator_loss=0.7133610248565674, discriminator_loss=0.6663362979888916\n",
            "step 8694: generator_loss=0.7171635627746582, discriminator_loss=0.6632569432258606\n",
            "step 8695: generator_loss=0.7148357629776001, discriminator_loss=0.6659849882125854\n",
            "step 8696: generator_loss=0.714610755443573, discriminator_loss=0.6670925617218018\n",
            "step 8697: generator_loss=0.720641016960144, discriminator_loss=0.6656734347343445\n",
            "step 8698: generator_loss=0.7229698896408081, discriminator_loss=0.6651163697242737\n",
            "step 8699: generator_loss=0.7163984775543213, discriminator_loss=0.6703976988792419\n",
            "step 8700: generator_loss=0.7273975610733032, discriminator_loss=0.6661791205406189\n",
            "step 8701: generator_loss=0.7202557325363159, discriminator_loss=0.6715709567070007\n",
            "step 8702: generator_loss=0.7206542491912842, discriminator_loss=0.6702207326889038\n",
            "step 8703: generator_loss=0.7193376421928406, discriminator_loss=0.6733246445655823\n",
            "step 8704: generator_loss=0.7083702683448792, discriminator_loss=0.6795527935028076\n",
            "step 8705: generator_loss=0.7130537033081055, discriminator_loss=0.6792138814926147\n",
            "step 8706: generator_loss=0.7022413015365601, discriminator_loss=0.6858519315719604\n",
            "step 8707: generator_loss=0.7015050053596497, discriminator_loss=0.6866613030433655\n",
            "step 8708: generator_loss=0.6989625692367554, discriminator_loss=0.6880998015403748\n",
            "step 8709: generator_loss=0.6961769461631775, discriminator_loss=0.6902157068252563\n",
            "step 8710: generator_loss=0.6910464763641357, discriminator_loss=0.6917944550514221\n",
            "step 8711: generator_loss=0.680580735206604, discriminator_loss=0.697630763053894\n",
            "step 8712: generator_loss=0.6886682510375977, discriminator_loss=0.6942181587219238\n",
            "step 8713: generator_loss=0.6852595806121826, discriminator_loss=0.6963016986846924\n",
            "step 8714: generator_loss=0.6725379228591919, discriminator_loss=0.7059714794158936\n",
            "step 8715: generator_loss=0.6869802474975586, discriminator_loss=0.6987059116363525\n",
            "step 8716: generator_loss=0.691092312335968, discriminator_loss=0.6982481479644775\n",
            "step 8717: generator_loss=0.6792652010917664, discriminator_loss=0.7031115293502808\n",
            "step 8718: generator_loss=0.6954774260520935, discriminator_loss=0.6952604055404663\n",
            "step 8719: generator_loss=0.683353066444397, discriminator_loss=0.7016544342041016\n",
            "step 8720: generator_loss=0.6776018142700195, discriminator_loss=0.7052557468414307\n",
            "step 8721: generator_loss=0.6932829022407532, discriminator_loss=0.697089433670044\n",
            "step 8722: generator_loss=0.6981496214866638, discriminator_loss=0.6959480047225952\n",
            "step 8723: generator_loss=0.7078282833099365, discriminator_loss=0.6930288076400757\n",
            "step 8724: generator_loss=0.6975458860397339, discriminator_loss=0.6991506814956665\n",
            "step 8725: generator_loss=0.6942572593688965, discriminator_loss=0.7036021947860718\n",
            "step 8726: generator_loss=0.6830790638923645, discriminator_loss=0.7112650871276855\n",
            "step 8727: generator_loss=0.6899014711380005, discriminator_loss=0.7089203000068665\n",
            "step 8728: generator_loss=0.6869268417358398, discriminator_loss=0.710797905921936\n",
            "step 8729: generator_loss=0.6959335803985596, discriminator_loss=0.7056412696838379\n",
            "step 8730: generator_loss=0.6953282952308655, discriminator_loss=0.7051107883453369\n",
            "step 8731: generator_loss=0.6821898221969604, discriminator_loss=0.7108534574508667\n",
            "step 8732: generator_loss=0.6858199834823608, discriminator_loss=0.7074493169784546\n",
            "step 8733: generator_loss=0.6916693449020386, discriminator_loss=0.7019039392471313\n",
            "step 8734: generator_loss=0.6904489398002625, discriminator_loss=0.6999471187591553\n",
            "step 8735: generator_loss=0.6762667894363403, discriminator_loss=0.707329511642456\n",
            "step 8736: generator_loss=0.6821065545082092, discriminator_loss=0.7028670310974121\n",
            "step 8737: generator_loss=0.6841774582862854, discriminator_loss=0.7011466026306152\n",
            "step 8738: generator_loss=0.6788891553878784, discriminator_loss=0.7033651471138\n",
            "step 8739: generator_loss=0.67730712890625, discriminator_loss=0.7032007575035095\n",
            "step 8740: generator_loss=0.6803117990493774, discriminator_loss=0.7023956179618835\n",
            "step 8741: generator_loss=0.6902258396148682, discriminator_loss=0.6977822184562683\n",
            "step 8742: generator_loss=0.691433846950531, discriminator_loss=0.6976454257965088\n",
            "step 8743: generator_loss=0.693394660949707, discriminator_loss=0.6977654695510864\n",
            "step 8744: generator_loss=0.6985586881637573, discriminator_loss=0.6962282657623291\n",
            "step 8745: generator_loss=0.6925294399261475, discriminator_loss=0.6968920230865479\n",
            "step 8746: generator_loss=0.7053964138031006, discriminator_loss=0.6913770437240601\n",
            "step 8747: generator_loss=0.709978461265564, discriminator_loss=0.6892409324645996\n",
            "step 8748: generator_loss=0.7097384929656982, discriminator_loss=0.6888365745544434\n",
            "step 8749: generator_loss=0.7117860317230225, discriminator_loss=0.6862956285476685\n",
            "step 8750: generator_loss=0.708794355392456, discriminator_loss=0.6870846748352051\n",
            "step 8751: generator_loss=0.7152944803237915, discriminator_loss=0.6837987899780273\n",
            "step 8752: generator_loss=0.7228643298149109, discriminator_loss=0.6785977482795715\n",
            "step 8753: generator_loss=0.7175725698471069, discriminator_loss=0.6781665086746216\n",
            "step 8754: generator_loss=0.7190122604370117, discriminator_loss=0.6752597093582153\n",
            "step 8755: generator_loss=0.7211690545082092, discriminator_loss=0.671249508857727\n",
            "step 8756: generator_loss=0.7173998355865479, discriminator_loss=0.6705085039138794\n",
            "step 8757: generator_loss=0.712327778339386, discriminator_loss=0.671583354473114\n",
            "step 8758: generator_loss=0.7272799015045166, discriminator_loss=0.6617012023925781\n",
            "step 8759: generator_loss=0.7231270670890808, discriminator_loss=0.6609265208244324\n",
            "step 8760: generator_loss=0.7405251264572144, discriminator_loss=0.6536775827407837\n",
            "step 8761: generator_loss=0.7374844551086426, discriminator_loss=0.6540695428848267\n",
            "step 8762: generator_loss=0.7387221455574036, discriminator_loss=0.6528366804122925\n",
            "step 8763: generator_loss=0.7535672187805176, discriminator_loss=0.6494467258453369\n",
            "step 8764: generator_loss=0.7449725866317749, discriminator_loss=0.6542775630950928\n",
            "step 8765: generator_loss=0.7574281692504883, discriminator_loss=0.6494460105895996\n",
            "step 8766: generator_loss=0.7536962032318115, discriminator_loss=0.6518551111221313\n",
            "step 8767: generator_loss=0.7548742890357971, discriminator_loss=0.6500232815742493\n",
            "step 8768: generator_loss=0.7646999359130859, discriminator_loss=0.6429570913314819\n",
            "step 8769: generator_loss=0.74207603931427, discriminator_loss=0.6510660648345947\n",
            "step 8770: generator_loss=0.7330042719841003, discriminator_loss=0.65196692943573\n",
            "step 8771: generator_loss=0.7356853485107422, discriminator_loss=0.6488995552062988\n",
            "step 8772: generator_loss=0.7321352362632751, discriminator_loss=0.6484676599502563\n",
            "step 8773: generator_loss=0.7291585206985474, discriminator_loss=0.6492459177970886\n",
            "step 8774: generator_loss=0.7278767824172974, discriminator_loss=0.6497085094451904\n",
            "step 8775: generator_loss=0.7305760383605957, discriminator_loss=0.6481651067733765\n",
            "step 8776: generator_loss=0.7310459613800049, discriminator_loss=0.6502593755722046\n",
            "step 8777: generator_loss=0.7383465766906738, discriminator_loss=0.6494544744491577\n",
            "step 8778: generator_loss=0.7451733350753784, discriminator_loss=0.648029088973999\n",
            "step 8779: generator_loss=0.7434310913085938, discriminator_loss=0.6500248908996582\n",
            "step 8780: generator_loss=0.7504377365112305, discriminator_loss=0.6493105888366699\n",
            "step 8781: generator_loss=0.7472879886627197, discriminator_loss=0.6500226259231567\n",
            "step 8782: generator_loss=0.7645678520202637, discriminator_loss=0.6429336071014404\n",
            "step 8783: generator_loss=0.7462470531463623, discriminator_loss=0.6526409387588501\n",
            "step 8784: generator_loss=0.7417528629302979, discriminator_loss=0.6551836729049683\n",
            "step 8785: generator_loss=0.7526190280914307, discriminator_loss=0.6486423015594482\n",
            "step 8786: generator_loss=0.7406843900680542, discriminator_loss=0.6528821587562561\n",
            "step 8787: generator_loss=0.7438898682594299, discriminator_loss=0.6508135199546814\n",
            "step 8788: generator_loss=0.7382744550704956, discriminator_loss=0.6532222032546997\n",
            "step 8789: generator_loss=0.7365626692771912, discriminator_loss=0.6544152498245239\n",
            "step 8790: generator_loss=0.7500643730163574, discriminator_loss=0.6480504870414734\n",
            "step 8791: generator_loss=0.7181713581085205, discriminator_loss=0.6606490015983582\n",
            "step 8792: generator_loss=0.7223135828971863, discriminator_loss=0.6586390137672424\n",
            "step 8793: generator_loss=0.7127847075462341, discriminator_loss=0.66339111328125\n",
            "step 8794: generator_loss=0.7178035974502563, discriminator_loss=0.6594941020011902\n",
            "step 8795: generator_loss=0.7178574800491333, discriminator_loss=0.6628944873809814\n",
            "step 8796: generator_loss=0.7219356298446655, discriminator_loss=0.6621124744415283\n",
            "step 8797: generator_loss=0.7245426774024963, discriminator_loss=0.6610503792762756\n",
            "step 8798: generator_loss=0.7167655825614929, discriminator_loss=0.6660486459732056\n",
            "step 8799: generator_loss=0.7257186770439148, discriminator_loss=0.6605803966522217\n",
            "step 8800: generator_loss=0.7289841175079346, discriminator_loss=0.6619822978973389\n",
            "step 8801: generator_loss=0.7284305691719055, discriminator_loss=0.663794994354248\n",
            "step 8802: generator_loss=0.7214382886886597, discriminator_loss=0.6696378588676453\n",
            "step 8803: generator_loss=0.726784348487854, discriminator_loss=0.6688555479049683\n",
            "step 8804: generator_loss=0.7220191359519958, discriminator_loss=0.6707521677017212\n",
            "step 8805: generator_loss=0.7123807668685913, discriminator_loss=0.6756476163864136\n",
            "step 8806: generator_loss=0.713666558265686, discriminator_loss=0.6761338114738464\n",
            "step 8807: generator_loss=0.7166324257850647, discriminator_loss=0.6766976118087769\n",
            "step 8808: generator_loss=0.7163474559783936, discriminator_loss=0.6779264211654663\n",
            "step 8809: generator_loss=0.7065050601959229, discriminator_loss=0.679962158203125\n",
            "step 8810: generator_loss=0.713109016418457, discriminator_loss=0.6795444488525391\n",
            "step 8811: generator_loss=0.7054920196533203, discriminator_loss=0.6816760301589966\n",
            "step 8812: generator_loss=0.7080353498458862, discriminator_loss=0.6820526123046875\n",
            "step 8813: generator_loss=0.7087159156799316, discriminator_loss=0.6813424825668335\n",
            "step 8814: generator_loss=0.7098027467727661, discriminator_loss=0.6826575994491577\n",
            "step 8815: generator_loss=0.7040261626243591, discriminator_loss=0.6850272417068481\n",
            "step 8816: generator_loss=0.7001635432243347, discriminator_loss=0.6869966387748718\n",
            "step 8817: generator_loss=0.6999827027320862, discriminator_loss=0.6895716190338135\n",
            "step 8818: generator_loss=0.6999719142913818, discriminator_loss=0.6896244883537292\n",
            "step 8819: generator_loss=0.6976199150085449, discriminator_loss=0.6923606395721436\n",
            "step 8820: generator_loss=0.7044752240180969, discriminator_loss=0.6882989406585693\n",
            "step 8821: generator_loss=0.7008768320083618, discriminator_loss=0.6911035776138306\n",
            "step 8822: generator_loss=0.7012076377868652, discriminator_loss=0.691161036491394\n",
            "step 8823: generator_loss=0.6978292465209961, discriminator_loss=0.6946579217910767\n",
            "step 8824: generator_loss=0.699847936630249, discriminator_loss=0.6949159502983093\n",
            "step 8825: generator_loss=0.6944292187690735, discriminator_loss=0.6984516382217407\n",
            "step 8826: generator_loss=0.6869317889213562, discriminator_loss=0.7021020650863647\n",
            "step 8827: generator_loss=0.6861987113952637, discriminator_loss=0.7023637294769287\n",
            "step 8828: generator_loss=0.6824753284454346, discriminator_loss=0.7056983709335327\n",
            "step 8829: generator_loss=0.6788409352302551, discriminator_loss=0.7068660259246826\n",
            "step 8830: generator_loss=0.6740456819534302, discriminator_loss=0.7092658281326294\n",
            "step 8831: generator_loss=0.670316755771637, discriminator_loss=0.7109314203262329\n",
            "step 8832: generator_loss=0.6671755313873291, discriminator_loss=0.7151229977607727\n",
            "step 8833: generator_loss=0.6653130650520325, discriminator_loss=0.7161073684692383\n",
            "step 8834: generator_loss=0.6653388738632202, discriminator_loss=0.7177392244338989\n",
            "step 8835: generator_loss=0.6656110286712646, discriminator_loss=0.7172915935516357\n",
            "step 8836: generator_loss=0.6641223430633545, discriminator_loss=0.718039870262146\n",
            "step 8837: generator_loss=0.6638703346252441, discriminator_loss=0.7206103801727295\n",
            "step 8838: generator_loss=0.6657973527908325, discriminator_loss=0.7213016152381897\n",
            "step 8839: generator_loss=0.6647218465805054, discriminator_loss=0.7244927883148193\n",
            "step 8840: generator_loss=0.6623871326446533, discriminator_loss=0.7262359261512756\n",
            "step 8841: generator_loss=0.660214900970459, discriminator_loss=0.7289016246795654\n",
            "step 8842: generator_loss=0.6571505069732666, discriminator_loss=0.7312772274017334\n",
            "step 8843: generator_loss=0.6581118106842041, discriminator_loss=0.7313922643661499\n",
            "step 8844: generator_loss=0.6545031070709229, discriminator_loss=0.7340610027313232\n",
            "step 8845: generator_loss=0.6474438905715942, discriminator_loss=0.7385431528091431\n",
            "step 8846: generator_loss=0.6420051455497742, discriminator_loss=0.7409330606460571\n",
            "step 8847: generator_loss=0.6429237127304077, discriminator_loss=0.7405599355697632\n",
            "step 8848: generator_loss=0.6362411975860596, discriminator_loss=0.7463623881340027\n",
            "step 8849: generator_loss=0.6368228197097778, discriminator_loss=0.7454695701599121\n",
            "step 8850: generator_loss=0.635530948638916, discriminator_loss=0.7455622553825378\n",
            "step 8851: generator_loss=0.6306340098381042, discriminator_loss=0.7482919096946716\n",
            "step 8852: generator_loss=0.6394170522689819, discriminator_loss=0.7442339062690735\n",
            "step 8853: generator_loss=0.6367577314376831, discriminator_loss=0.7475184202194214\n",
            "step 8854: generator_loss=0.6399134397506714, discriminator_loss=0.7471150159835815\n",
            "step 8855: generator_loss=0.6440672874450684, discriminator_loss=0.7476001977920532\n",
            "step 8856: generator_loss=0.6477067470550537, discriminator_loss=0.7441713809967041\n",
            "step 8857: generator_loss=0.6441164612770081, discriminator_loss=0.7481417655944824\n",
            "step 8858: generator_loss=0.6625504493713379, discriminator_loss=0.7370381355285645\n",
            "step 8859: generator_loss=0.6408036947250366, discriminator_loss=0.752301037311554\n",
            "step 8860: generator_loss=0.6570180654525757, discriminator_loss=0.7455441951751709\n",
            "step 8861: generator_loss=0.66310715675354, discriminator_loss=0.7406065464019775\n",
            "step 8862: generator_loss=0.6544292569160461, discriminator_loss=0.7464514970779419\n",
            "step 8863: generator_loss=0.6446211338043213, discriminator_loss=0.750321626663208\n",
            "step 8864: generator_loss=0.6341153383255005, discriminator_loss=0.7523791193962097\n",
            "step 8865: generator_loss=0.6272869110107422, discriminator_loss=0.7529464960098267\n",
            "step 8866: generator_loss=0.613788366317749, discriminator_loss=0.7558737993240356\n",
            "step 8867: generator_loss=0.6156210899353027, discriminator_loss=0.7531373500823975\n",
            "step 8868: generator_loss=0.6097900867462158, discriminator_loss=0.7525383234024048\n",
            "step 8869: generator_loss=0.6138345003128052, discriminator_loss=0.7493792772293091\n",
            "step 8870: generator_loss=0.6152653694152832, discriminator_loss=0.7479740381240845\n",
            "step 8871: generator_loss=0.6186890602111816, discriminator_loss=0.7470380067825317\n",
            "step 8872: generator_loss=0.635135293006897, discriminator_loss=0.7390859127044678\n",
            "step 8873: generator_loss=0.6478612422943115, discriminator_loss=0.7331982254981995\n",
            "step 8874: generator_loss=0.6599874496459961, discriminator_loss=0.7285497188568115\n",
            "step 8875: generator_loss=0.6762641072273254, discriminator_loss=0.7235573530197144\n",
            "step 8876: generator_loss=0.6824450492858887, discriminator_loss=0.7224560976028442\n",
            "step 8877: generator_loss=0.6911590695381165, discriminator_loss=0.719097912311554\n",
            "step 8878: generator_loss=0.7055636644363403, discriminator_loss=0.7107182145118713\n",
            "step 8879: generator_loss=0.7079799175262451, discriminator_loss=0.7097561955451965\n",
            "step 8880: generator_loss=0.7182023525238037, discriminator_loss=0.7041964530944824\n",
            "step 8881: generator_loss=0.7226061820983887, discriminator_loss=0.7022427916526794\n",
            "step 8882: generator_loss=0.724453866481781, discriminator_loss=0.7001199126243591\n",
            "step 8883: generator_loss=0.7240952849388123, discriminator_loss=0.6989675760269165\n",
            "step 8884: generator_loss=0.725884199142456, discriminator_loss=0.6944549679756165\n",
            "step 8885: generator_loss=0.7210140824317932, discriminator_loss=0.693222165107727\n",
            "step 8886: generator_loss=0.7126889228820801, discriminator_loss=0.689980685710907\n",
            "step 8887: generator_loss=0.7116166949272156, discriminator_loss=0.6863776445388794\n",
            "step 8888: generator_loss=0.705289363861084, discriminator_loss=0.6843304634094238\n",
            "step 8889: generator_loss=0.7036364674568176, discriminator_loss=0.6806708574295044\n",
            "step 8890: generator_loss=0.7058408260345459, discriminator_loss=0.677174985408783\n",
            "step 8891: generator_loss=0.7064566612243652, discriminator_loss=0.6742103099822998\n",
            "step 8892: generator_loss=0.6963763236999512, discriminator_loss=0.6797643899917603\n",
            "step 8893: generator_loss=0.700689435005188, discriminator_loss=0.6771273612976074\n",
            "step 8894: generator_loss=0.7021745443344116, discriminator_loss=0.6765079498291016\n",
            "step 8895: generator_loss=0.7028887271881104, discriminator_loss=0.6750860810279846\n",
            "step 8896: generator_loss=0.7063138484954834, discriminator_loss=0.6743572354316711\n",
            "step 8897: generator_loss=0.7104354500770569, discriminator_loss=0.6719363331794739\n",
            "step 8898: generator_loss=0.7134358882904053, discriminator_loss=0.6707218885421753\n",
            "step 8899: generator_loss=0.7191474437713623, discriminator_loss=0.6678057312965393\n",
            "step 8900: generator_loss=0.717778742313385, discriminator_loss=0.6688117980957031\n",
            "step 8901: generator_loss=0.7250299453735352, discriminator_loss=0.6655715107917786\n",
            "step 8902: generator_loss=0.731732964515686, discriminator_loss=0.6639983654022217\n",
            "step 8903: generator_loss=0.7339332103729248, discriminator_loss=0.6628620624542236\n",
            "step 8904: generator_loss=0.7344708442687988, discriminator_loss=0.6627273559570312\n",
            "step 8905: generator_loss=0.7403354644775391, discriminator_loss=0.6599371433258057\n",
            "step 8906: generator_loss=0.7360285520553589, discriminator_loss=0.6629867553710938\n",
            "step 8907: generator_loss=0.737686812877655, discriminator_loss=0.6631817817687988\n",
            "step 8908: generator_loss=0.7398542165756226, discriminator_loss=0.6613737344741821\n",
            "step 8909: generator_loss=0.7337571978569031, discriminator_loss=0.6653268337249756\n",
            "step 8910: generator_loss=0.7303880453109741, discriminator_loss=0.6658977270126343\n",
            "step 8911: generator_loss=0.7337391972541809, discriminator_loss=0.6627532243728638\n",
            "step 8912: generator_loss=0.7243670225143433, discriminator_loss=0.666913628578186\n",
            "step 8913: generator_loss=0.7152365446090698, discriminator_loss=0.6690578460693359\n",
            "step 8914: generator_loss=0.713303804397583, discriminator_loss=0.6675122380256653\n",
            "step 8915: generator_loss=0.7201477289199829, discriminator_loss=0.6644534468650818\n",
            "step 8916: generator_loss=0.7185819149017334, discriminator_loss=0.6633549332618713\n",
            "step 8917: generator_loss=0.7225991487503052, discriminator_loss=0.6609991788864136\n",
            "step 8918: generator_loss=0.7220909595489502, discriminator_loss=0.6621088981628418\n",
            "step 8919: generator_loss=0.7278931736946106, discriminator_loss=0.6604683995246887\n",
            "step 8920: generator_loss=0.7332026958465576, discriminator_loss=0.6600993871688843\n",
            "step 8921: generator_loss=0.7318814992904663, discriminator_loss=0.6628050804138184\n",
            "step 8922: generator_loss=0.732781708240509, discriminator_loss=0.6659845113754272\n",
            "step 8923: generator_loss=0.7366834878921509, discriminator_loss=0.6651439666748047\n",
            "step 8924: generator_loss=0.7269246578216553, discriminator_loss=0.6701167821884155\n",
            "step 8925: generator_loss=0.7273479104042053, discriminator_loss=0.6706206798553467\n",
            "step 8926: generator_loss=0.7222620248794556, discriminator_loss=0.6742404699325562\n",
            "step 8927: generator_loss=0.714918851852417, discriminator_loss=0.6764627695083618\n",
            "step 8928: generator_loss=0.7074584364891052, discriminator_loss=0.6788535118103027\n",
            "step 8929: generator_loss=0.6946002244949341, discriminator_loss=0.6845595836639404\n",
            "step 8930: generator_loss=0.6868406534194946, discriminator_loss=0.6882280707359314\n",
            "step 8931: generator_loss=0.6842533349990845, discriminator_loss=0.6899306178092957\n",
            "step 8932: generator_loss=0.6754439473152161, discriminator_loss=0.6940339207649231\n",
            "step 8933: generator_loss=0.6684720516204834, discriminator_loss=0.6983745098114014\n",
            "step 8934: generator_loss=0.6752225160598755, discriminator_loss=0.6967332363128662\n",
            "step 8935: generator_loss=0.66946941614151, discriminator_loss=0.6985290050506592\n",
            "step 8936: generator_loss=0.6689189672470093, discriminator_loss=0.703848659992218\n",
            "step 8937: generator_loss=0.6713681221008301, discriminator_loss=0.7060480117797852\n",
            "step 8938: generator_loss=0.6742080450057983, discriminator_loss=0.7072967290878296\n",
            "step 8939: generator_loss=0.6737524271011353, discriminator_loss=0.7110990881919861\n",
            "step 8940: generator_loss=0.6808474063873291, discriminator_loss=0.709843635559082\n",
            "step 8941: generator_loss=0.6821671724319458, discriminator_loss=0.7093081474304199\n",
            "step 8942: generator_loss=0.6736553311347961, discriminator_loss=0.7134878635406494\n",
            "step 8943: generator_loss=0.6841578483581543, discriminator_loss=0.7074978351593018\n",
            "step 8944: generator_loss=0.6784590482711792, discriminator_loss=0.7087041139602661\n",
            "step 8945: generator_loss=0.6842718720436096, discriminator_loss=0.7047833204269409\n",
            "step 8946: generator_loss=0.6838209629058838, discriminator_loss=0.7035454511642456\n",
            "step 8947: generator_loss=0.6934208869934082, discriminator_loss=0.6951912641525269\n",
            "step 8948: generator_loss=0.6912315487861633, discriminator_loss=0.6948280334472656\n",
            "step 8949: generator_loss=0.6884410381317139, discriminator_loss=0.6961115002632141\n",
            "step 8950: generator_loss=0.6851425170898438, discriminator_loss=0.6961009502410889\n",
            "step 8951: generator_loss=0.6807440519332886, discriminator_loss=0.6978187561035156\n",
            "step 8952: generator_loss=0.6902524828910828, discriminator_loss=0.6914399862289429\n",
            "step 8953: generator_loss=0.6849390864372253, discriminator_loss=0.694111168384552\n",
            "step 8954: generator_loss=0.6892579793930054, discriminator_loss=0.6902755498886108\n",
            "step 8955: generator_loss=0.6952074766159058, discriminator_loss=0.6864715814590454\n",
            "step 8956: generator_loss=0.7056419849395752, discriminator_loss=0.6801673173904419\n",
            "step 8957: generator_loss=0.6983097195625305, discriminator_loss=0.6823022365570068\n",
            "step 8958: generator_loss=0.7006610035896301, discriminator_loss=0.6794929504394531\n",
            "step 8959: generator_loss=0.7127126455307007, discriminator_loss=0.6746011972427368\n",
            "step 8960: generator_loss=0.704048752784729, discriminator_loss=0.6797329187393188\n",
            "step 8961: generator_loss=0.7274290919303894, discriminator_loss=0.6704061031341553\n",
            "step 8962: generator_loss=0.7351086735725403, discriminator_loss=0.6687549352645874\n",
            "step 8963: generator_loss=0.7398515939712524, discriminator_loss=0.6673341989517212\n",
            "step 8964: generator_loss=0.7471140623092651, discriminator_loss=0.6639806628227234\n",
            "step 8965: generator_loss=0.7322112321853638, discriminator_loss=0.6703824996948242\n",
            "step 8966: generator_loss=0.7346069812774658, discriminator_loss=0.6692801713943481\n",
            "step 8967: generator_loss=0.7430509924888611, discriminator_loss=0.6644697189331055\n",
            "step 8968: generator_loss=0.7432937622070312, discriminator_loss=0.6635205149650574\n",
            "step 8969: generator_loss=0.7479885816574097, discriminator_loss=0.6617035269737244\n",
            "step 8970: generator_loss=0.7439484596252441, discriminator_loss=0.6660915613174438\n",
            "step 8971: generator_loss=0.7485209107398987, discriminator_loss=0.6656874418258667\n",
            "step 8972: generator_loss=0.7490684986114502, discriminator_loss=0.6645197868347168\n",
            "step 8973: generator_loss=0.7361235022544861, discriminator_loss=0.6682630777359009\n",
            "step 8974: generator_loss=0.7261999249458313, discriminator_loss=0.6714029312133789\n",
            "step 8975: generator_loss=0.7143891453742981, discriminator_loss=0.6751335859298706\n",
            "step 8976: generator_loss=0.7203165292739868, discriminator_loss=0.6711627244949341\n",
            "step 8977: generator_loss=0.6978548169136047, discriminator_loss=0.6797952651977539\n",
            "step 8978: generator_loss=0.702973484992981, discriminator_loss=0.6765642166137695\n",
            "step 8979: generator_loss=0.6941887140274048, discriminator_loss=0.6827772259712219\n",
            "step 8980: generator_loss=0.6866455674171448, discriminator_loss=0.6862996220588684\n",
            "step 8981: generator_loss=0.6915335655212402, discriminator_loss=0.6870853304862976\n",
            "step 8982: generator_loss=0.6954628229141235, discriminator_loss=0.6868106126785278\n",
            "step 8983: generator_loss=0.6896228194236755, discriminator_loss=0.692419171333313\n",
            "step 8984: generator_loss=0.7039552927017212, discriminator_loss=0.6837827563285828\n",
            "step 8985: generator_loss=0.6935724020004272, discriminator_loss=0.6899623870849609\n",
            "step 8986: generator_loss=0.687691330909729, discriminator_loss=0.6947267055511475\n",
            "step 8987: generator_loss=0.6941550970077515, discriminator_loss=0.6918174028396606\n",
            "step 8988: generator_loss=0.6979600787162781, discriminator_loss=0.6924002170562744\n",
            "step 8989: generator_loss=0.693242073059082, discriminator_loss=0.6946687698364258\n",
            "step 8990: generator_loss=0.6830697059631348, discriminator_loss=0.7013312578201294\n",
            "step 8991: generator_loss=0.6936454176902771, discriminator_loss=0.6952179074287415\n",
            "step 8992: generator_loss=0.686333179473877, discriminator_loss=0.6987273097038269\n",
            "step 8993: generator_loss=0.6875619292259216, discriminator_loss=0.6985555291175842\n",
            "step 8994: generator_loss=0.6860525608062744, discriminator_loss=0.6993160247802734\n",
            "step 8995: generator_loss=0.681584358215332, discriminator_loss=0.6994604468345642\n",
            "step 8996: generator_loss=0.6824175715446472, discriminator_loss=0.7014667987823486\n",
            "step 8997: generator_loss=0.6758440732955933, discriminator_loss=0.7034674286842346\n",
            "step 8998: generator_loss=0.677294909954071, discriminator_loss=0.7012726068496704\n",
            "step 8999: generator_loss=0.6740266680717468, discriminator_loss=0.7008837461471558\n",
            "step 9000: generator_loss=0.6756051778793335, discriminator_loss=0.7003002166748047\n",
            "step 9001: generator_loss=0.6748442649841309, discriminator_loss=0.6976548433303833\n",
            "step 9002: generator_loss=0.6803680658340454, discriminator_loss=0.6931340098381042\n",
            "step 9003: generator_loss=0.6793721914291382, discriminator_loss=0.6932413578033447\n",
            "step 9004: generator_loss=0.6827559471130371, discriminator_loss=0.6929739713668823\n",
            "step 9005: generator_loss=0.6839627027511597, discriminator_loss=0.6942753791809082\n",
            "step 9006: generator_loss=0.6884304881095886, discriminator_loss=0.69416743516922\n",
            "step 9007: generator_loss=0.6884105205535889, discriminator_loss=0.6956992149353027\n",
            "step 9008: generator_loss=0.6937665343284607, discriminator_loss=0.6958523392677307\n",
            "step 9009: generator_loss=0.6932990550994873, discriminator_loss=0.6961512565612793\n",
            "step 9010: generator_loss=0.697534441947937, discriminator_loss=0.6945469379425049\n",
            "step 9011: generator_loss=0.6946573853492737, discriminator_loss=0.697209358215332\n",
            "step 9012: generator_loss=0.6927561163902283, discriminator_loss=0.6970186829566956\n",
            "step 9013: generator_loss=0.6902388334274292, discriminator_loss=0.6976687908172607\n",
            "step 9014: generator_loss=0.6890044808387756, discriminator_loss=0.6963741779327393\n",
            "step 9015: generator_loss=0.6892805099487305, discriminator_loss=0.6949000358581543\n",
            "step 9016: generator_loss=0.6868896484375, discriminator_loss=0.6950324773788452\n",
            "step 9017: generator_loss=0.6895229816436768, discriminator_loss=0.6940587162971497\n",
            "step 9018: generator_loss=0.68869948387146, discriminator_loss=0.692567765712738\n",
            "step 9019: generator_loss=0.6885885000228882, discriminator_loss=0.6944027543067932\n",
            "step 9020: generator_loss=0.6834357976913452, discriminator_loss=0.6974947452545166\n",
            "step 9021: generator_loss=0.6883862614631653, discriminator_loss=0.6967334747314453\n",
            "step 9022: generator_loss=0.6866499185562134, discriminator_loss=0.6969718933105469\n",
            "step 9023: generator_loss=0.6844563484191895, discriminator_loss=0.7004667520523071\n",
            "step 9024: generator_loss=0.6848505735397339, discriminator_loss=0.7014291882514954\n",
            "step 9025: generator_loss=0.6864677667617798, discriminator_loss=0.7017550468444824\n",
            "step 9026: generator_loss=0.6841158270835876, discriminator_loss=0.7050141096115112\n",
            "step 9027: generator_loss=0.6804162263870239, discriminator_loss=0.7077802419662476\n",
            "step 9028: generator_loss=0.6877057552337646, discriminator_loss=0.7057855725288391\n",
            "step 9029: generator_loss=0.6884105205535889, discriminator_loss=0.7056055068969727\n",
            "step 9030: generator_loss=0.6863956451416016, discriminator_loss=0.7066015005111694\n",
            "step 9031: generator_loss=0.6870712041854858, discriminator_loss=0.7076411247253418\n",
            "step 9032: generator_loss=0.6826211214065552, discriminator_loss=0.7102936506271362\n",
            "step 9033: generator_loss=0.6756162643432617, discriminator_loss=0.714676022529602\n",
            "step 9034: generator_loss=0.6751842498779297, discriminator_loss=0.7140681743621826\n",
            "step 9035: generator_loss=0.6779059171676636, discriminator_loss=0.7120254635810852\n",
            "step 9036: generator_loss=0.668680727481842, discriminator_loss=0.716621994972229\n",
            "step 9037: generator_loss=0.6749236583709717, discriminator_loss=0.7129799127578735\n",
            "step 9038: generator_loss=0.6754014492034912, discriminator_loss=0.7123055458068848\n",
            "step 9039: generator_loss=0.675402045249939, discriminator_loss=0.7115907073020935\n",
            "step 9040: generator_loss=0.6637461185455322, discriminator_loss=0.7200251817703247\n",
            "step 9041: generator_loss=0.6722259521484375, discriminator_loss=0.716188371181488\n",
            "step 9042: generator_loss=0.6827844977378845, discriminator_loss=0.7100762724876404\n",
            "step 9043: generator_loss=0.6767117977142334, discriminator_loss=0.7155138254165649\n",
            "step 9044: generator_loss=0.6775493621826172, discriminator_loss=0.7135911583900452\n",
            "step 9045: generator_loss=0.6790587306022644, discriminator_loss=0.710379421710968\n",
            "step 9046: generator_loss=0.6700937747955322, discriminator_loss=0.712599515914917\n",
            "step 9047: generator_loss=0.6725770235061646, discriminator_loss=0.7095417976379395\n",
            "step 9048: generator_loss=0.6752172708511353, discriminator_loss=0.7057765126228333\n",
            "step 9049: generator_loss=0.6629494428634644, discriminator_loss=0.7118583917617798\n",
            "step 9050: generator_loss=0.6842051148414612, discriminator_loss=0.6993945240974426\n",
            "step 9051: generator_loss=0.6681894063949585, discriminator_loss=0.7067501544952393\n",
            "step 9052: generator_loss=0.6683987379074097, discriminator_loss=0.7056154012680054\n",
            "step 9053: generator_loss=0.6796600818634033, discriminator_loss=0.7021901607513428\n",
            "step 9054: generator_loss=0.6815403699874878, discriminator_loss=0.7018532752990723\n",
            "step 9055: generator_loss=0.6857843399047852, discriminator_loss=0.7011198997497559\n",
            "step 9056: generator_loss=0.6788135766983032, discriminator_loss=0.7057744264602661\n",
            "step 9057: generator_loss=0.6930029392242432, discriminator_loss=0.6997101306915283\n",
            "step 9058: generator_loss=0.6936479806900024, discriminator_loss=0.6977493762969971\n",
            "step 9059: generator_loss=0.696620523929596, discriminator_loss=0.6954213380813599\n",
            "step 9060: generator_loss=0.6953023076057434, discriminator_loss=0.6945359706878662\n",
            "step 9061: generator_loss=0.6933163404464722, discriminator_loss=0.6939095258712769\n",
            "step 9062: generator_loss=0.7041345238685608, discriminator_loss=0.6856549382209778\n",
            "step 9063: generator_loss=0.6951925754547119, discriminator_loss=0.688386857509613\n",
            "step 9064: generator_loss=0.7046253085136414, discriminator_loss=0.680867075920105\n",
            "step 9065: generator_loss=0.7096310257911682, discriminator_loss=0.6786552667617798\n",
            "step 9066: generator_loss=0.709587574005127, discriminator_loss=0.6784937381744385\n",
            "step 9067: generator_loss=0.7152604460716248, discriminator_loss=0.6756399273872375\n",
            "step 9068: generator_loss=0.7237122058868408, discriminator_loss=0.67063307762146\n",
            "step 9069: generator_loss=0.7339186072349548, discriminator_loss=0.6645203828811646\n",
            "step 9070: generator_loss=0.7441053986549377, discriminator_loss=0.6605592966079712\n",
            "step 9071: generator_loss=0.735299825668335, discriminator_loss=0.6631754636764526\n",
            "step 9072: generator_loss=0.7483507990837097, discriminator_loss=0.6583147644996643\n",
            "step 9073: generator_loss=0.7542959451675415, discriminator_loss=0.6557860374450684\n",
            "step 9074: generator_loss=0.7545120716094971, discriminator_loss=0.6560912132263184\n",
            "step 9075: generator_loss=0.7477092146873474, discriminator_loss=0.6583111882209778\n",
            "step 9076: generator_loss=0.7517434358596802, discriminator_loss=0.6566872596740723\n",
            "step 9077: generator_loss=0.7348446846008301, discriminator_loss=0.6625001430511475\n",
            "step 9078: generator_loss=0.7405456304550171, discriminator_loss=0.6578704714775085\n",
            "step 9079: generator_loss=0.7285571098327637, discriminator_loss=0.660637378692627\n",
            "step 9080: generator_loss=0.745058000087738, discriminator_loss=0.6526297330856323\n",
            "step 9081: generator_loss=0.7414546608924866, discriminator_loss=0.6508448719978333\n",
            "step 9082: generator_loss=0.7254775762557983, discriminator_loss=0.6582229137420654\n",
            "step 9083: generator_loss=0.7407898902893066, discriminator_loss=0.651385486125946\n",
            "step 9084: generator_loss=0.7388451099395752, discriminator_loss=0.6531417369842529\n",
            "step 9085: generator_loss=0.7328554391860962, discriminator_loss=0.6555669903755188\n",
            "step 9086: generator_loss=0.7386676073074341, discriminator_loss=0.6534973978996277\n",
            "step 9087: generator_loss=0.7336878776550293, discriminator_loss=0.6561284065246582\n",
            "step 9088: generator_loss=0.7430741786956787, discriminator_loss=0.6537958979606628\n",
            "step 9089: generator_loss=0.7553860545158386, discriminator_loss=0.6491951942443848\n",
            "step 9090: generator_loss=0.7439279556274414, discriminator_loss=0.6544961929321289\n",
            "step 9091: generator_loss=0.7474995255470276, discriminator_loss=0.6549155712127686\n",
            "step 9092: generator_loss=0.7363987565040588, discriminator_loss=0.6593979597091675\n",
            "step 9093: generator_loss=0.7456252574920654, discriminator_loss=0.6565892696380615\n",
            "step 9094: generator_loss=0.7389311194419861, discriminator_loss=0.6593978404998779\n",
            "step 9095: generator_loss=0.737655520439148, discriminator_loss=0.6612071990966797\n",
            "step 9096: generator_loss=0.7288756370544434, discriminator_loss=0.6647633910179138\n",
            "step 9097: generator_loss=0.7211605310440063, discriminator_loss=0.6687362194061279\n",
            "step 9098: generator_loss=0.7098619937896729, discriminator_loss=0.6743167638778687\n",
            "step 9099: generator_loss=0.7105498313903809, discriminator_loss=0.6747596859931946\n",
            "step 9100: generator_loss=0.6968424320220947, discriminator_loss=0.6806955933570862\n",
            "step 9101: generator_loss=0.6947599649429321, discriminator_loss=0.6839326620101929\n",
            "step 9102: generator_loss=0.6955663561820984, discriminator_loss=0.6829683184623718\n",
            "step 9103: generator_loss=0.6909689903259277, discriminator_loss=0.6863652467727661\n",
            "step 9104: generator_loss=0.6882928609848022, discriminator_loss=0.68971848487854\n",
            "step 9105: generator_loss=0.6831283569335938, discriminator_loss=0.6943540573120117\n",
            "step 9106: generator_loss=0.6884006261825562, discriminator_loss=0.6931266784667969\n",
            "step 9107: generator_loss=0.6862099170684814, discriminator_loss=0.6970646381378174\n",
            "step 9108: generator_loss=0.685920000076294, discriminator_loss=0.6991122961044312\n",
            "step 9109: generator_loss=0.6873950958251953, discriminator_loss=0.7012553811073303\n",
            "step 9110: generator_loss=0.686730146408081, discriminator_loss=0.7015732526779175\n",
            "step 9111: generator_loss=0.6951390504837036, discriminator_loss=0.7001689076423645\n",
            "step 9112: generator_loss=0.6892753839492798, discriminator_loss=0.7031759023666382\n",
            "step 9113: generator_loss=0.6935549974441528, discriminator_loss=0.7025788426399231\n",
            "step 9114: generator_loss=0.6969920992851257, discriminator_loss=0.7007400989532471\n",
            "step 9115: generator_loss=0.6990960836410522, discriminator_loss=0.7010185122489929\n",
            "step 9116: generator_loss=0.6917794942855835, discriminator_loss=0.7067741751670837\n",
            "step 9117: generator_loss=0.6902692317962646, discriminator_loss=0.7067908048629761\n",
            "step 9118: generator_loss=0.6865039467811584, discriminator_loss=0.7086851596832275\n",
            "step 9119: generator_loss=0.6801680326461792, discriminator_loss=0.7123394012451172\n",
            "step 9120: generator_loss=0.6724144220352173, discriminator_loss=0.7123073935508728\n",
            "step 9121: generator_loss=0.6703763008117676, discriminator_loss=0.7139012217521667\n",
            "step 9122: generator_loss=0.6675654649734497, discriminator_loss=0.7156990766525269\n",
            "step 9123: generator_loss=0.6607140302658081, discriminator_loss=0.7170959711074829\n",
            "step 9124: generator_loss=0.6611713171005249, discriminator_loss=0.7173565626144409\n",
            "step 9125: generator_loss=0.6607006788253784, discriminator_loss=0.7151423096656799\n",
            "step 9126: generator_loss=0.6627824902534485, discriminator_loss=0.7152476906776428\n",
            "step 9127: generator_loss=0.6651637554168701, discriminator_loss=0.7153239846229553\n",
            "step 9128: generator_loss=0.6706774234771729, discriminator_loss=0.7129840850830078\n",
            "step 9129: generator_loss=0.6735053062438965, discriminator_loss=0.712665319442749\n",
            "step 9130: generator_loss=0.6783488988876343, discriminator_loss=0.7127828598022461\n",
            "step 9131: generator_loss=0.6811686754226685, discriminator_loss=0.7136116623878479\n",
            "step 9132: generator_loss=0.6823406219482422, discriminator_loss=0.7175803184509277\n",
            "step 9133: generator_loss=0.6815645694732666, discriminator_loss=0.7181318998336792\n",
            "step 9134: generator_loss=0.6805371642112732, discriminator_loss=0.7183898091316223\n",
            "step 9135: generator_loss=0.6779236793518066, discriminator_loss=0.7204697728157043\n",
            "step 9136: generator_loss=0.6742197275161743, discriminator_loss=0.7197949886322021\n",
            "step 9137: generator_loss=0.6689443588256836, discriminator_loss=0.7227429747581482\n",
            "step 9138: generator_loss=0.6637299060821533, discriminator_loss=0.724082350730896\n",
            "step 9139: generator_loss=0.6588861346244812, discriminator_loss=0.7264842391014099\n",
            "step 9140: generator_loss=0.6553366780281067, discriminator_loss=0.7258477807044983\n",
            "step 9141: generator_loss=0.6525838375091553, discriminator_loss=0.7269502878189087\n",
            "step 9142: generator_loss=0.650162935256958, discriminator_loss=0.7289531230926514\n",
            "step 9143: generator_loss=0.6496596336364746, discriminator_loss=0.73027503490448\n",
            "step 9144: generator_loss=0.648418664932251, discriminator_loss=0.7325178384780884\n",
            "step 9145: generator_loss=0.6480503082275391, discriminator_loss=0.7340298295021057\n",
            "step 9146: generator_loss=0.6489582657814026, discriminator_loss=0.7353222370147705\n",
            "step 9147: generator_loss=0.6498374938964844, discriminator_loss=0.7349786758422852\n",
            "step 9148: generator_loss=0.6511911749839783, discriminator_loss=0.735824704170227\n",
            "step 9149: generator_loss=0.653209388256073, discriminator_loss=0.7360818386077881\n",
            "step 9150: generator_loss=0.6526975631713867, discriminator_loss=0.7387778759002686\n",
            "step 9151: generator_loss=0.6546211242675781, discriminator_loss=0.7378627061843872\n",
            "step 9152: generator_loss=0.6545339822769165, discriminator_loss=0.7397817373275757\n",
            "step 9153: generator_loss=0.655892014503479, discriminator_loss=0.7371957302093506\n",
            "step 9154: generator_loss=0.6571927070617676, discriminator_loss=0.7364174723625183\n",
            "step 9155: generator_loss=0.6614896655082703, discriminator_loss=0.7341397404670715\n",
            "step 9156: generator_loss=0.663062572479248, discriminator_loss=0.7335408926010132\n",
            "step 9157: generator_loss=0.667418360710144, discriminator_loss=0.7291964292526245\n",
            "step 9158: generator_loss=0.669217050075531, discriminator_loss=0.7278491258621216\n",
            "step 9159: generator_loss=0.6731832027435303, discriminator_loss=0.7246009111404419\n",
            "step 9160: generator_loss=0.6760213375091553, discriminator_loss=0.7212300300598145\n",
            "step 9161: generator_loss=0.68077552318573, discriminator_loss=0.7189065217971802\n",
            "step 9162: generator_loss=0.6838585734367371, discriminator_loss=0.7151225209236145\n",
            "step 9163: generator_loss=0.6816586852073669, discriminator_loss=0.7170027494430542\n",
            "step 9164: generator_loss=0.682658314704895, discriminator_loss=0.7162917256355286\n",
            "step 9165: generator_loss=0.6798498630523682, discriminator_loss=0.7174233198165894\n",
            "step 9166: generator_loss=0.6776988506317139, discriminator_loss=0.7186830043792725\n",
            "step 9167: generator_loss=0.67404705286026, discriminator_loss=0.7182005047798157\n",
            "step 9168: generator_loss=0.670840859413147, discriminator_loss=0.7197527885437012\n",
            "step 9169: generator_loss=0.6689354181289673, discriminator_loss=0.7198591828346252\n",
            "step 9170: generator_loss=0.6650650501251221, discriminator_loss=0.7200149297714233\n",
            "step 9171: generator_loss=0.6627302169799805, discriminator_loss=0.7210873365402222\n",
            "step 9172: generator_loss=0.659756600856781, discriminator_loss=0.7227059602737427\n",
            "step 9173: generator_loss=0.6586767435073853, discriminator_loss=0.7225013375282288\n",
            "step 9174: generator_loss=0.6588256359100342, discriminator_loss=0.7237931489944458\n",
            "step 9175: generator_loss=0.6578211784362793, discriminator_loss=0.7223398089408875\n",
            "step 9176: generator_loss=0.6587077975273132, discriminator_loss=0.7232391834259033\n",
            "step 9177: generator_loss=0.6592470407485962, discriminator_loss=0.7247630953788757\n",
            "step 9178: generator_loss=0.6586076617240906, discriminator_loss=0.7256338596343994\n",
            "step 9179: generator_loss=0.6607142090797424, discriminator_loss=0.7250214219093323\n",
            "step 9180: generator_loss=0.6599850058555603, discriminator_loss=0.7269657850265503\n",
            "step 9181: generator_loss=0.6580692529678345, discriminator_loss=0.7292377948760986\n",
            "step 9182: generator_loss=0.660699725151062, discriminator_loss=0.7278094291687012\n",
            "step 9183: generator_loss=0.6579419374465942, discriminator_loss=0.7294148206710815\n",
            "step 9184: generator_loss=0.656785249710083, discriminator_loss=0.7308576107025146\n",
            "step 9185: generator_loss=0.6565533876419067, discriminator_loss=0.7318843603134155\n",
            "step 9186: generator_loss=0.6542955040931702, discriminator_loss=0.7312997579574585\n",
            "step 9187: generator_loss=0.6509206295013428, discriminator_loss=0.7334482073783875\n",
            "step 9188: generator_loss=0.6512531042098999, discriminator_loss=0.7322529554367065\n",
            "step 9189: generator_loss=0.6455825567245483, discriminator_loss=0.7342732548713684\n",
            "step 9190: generator_loss=0.6462576389312744, discriminator_loss=0.7334344387054443\n",
            "step 9191: generator_loss=0.6477397680282593, discriminator_loss=0.7323268055915833\n",
            "step 9192: generator_loss=0.6445923447608948, discriminator_loss=0.7327850461006165\n",
            "step 9193: generator_loss=0.650258481502533, discriminator_loss=0.7280527353286743\n",
            "step 9194: generator_loss=0.6482073068618774, discriminator_loss=0.7275835871696472\n",
            "step 9195: generator_loss=0.6546977758407593, discriminator_loss=0.7222360372543335\n",
            "step 9196: generator_loss=0.658450722694397, discriminator_loss=0.7192407846450806\n",
            "step 9197: generator_loss=0.662439227104187, discriminator_loss=0.7162365913391113\n",
            "step 9198: generator_loss=0.6684361696243286, discriminator_loss=0.7097040414810181\n",
            "step 9199: generator_loss=0.6792646646499634, discriminator_loss=0.7034693956375122\n",
            "step 9200: generator_loss=0.6822683811187744, discriminator_loss=0.7037066221237183\n",
            "step 9201: generator_loss=0.6872981786727905, discriminator_loss=0.70424485206604\n",
            "step 9202: generator_loss=0.6997095346450806, discriminator_loss=0.6996607780456543\n",
            "step 9203: generator_loss=0.7029279470443726, discriminator_loss=0.7003922462463379\n",
            "step 9204: generator_loss=0.7032716870307922, discriminator_loss=0.7017856240272522\n",
            "step 9205: generator_loss=0.699281632900238, discriminator_loss=0.7049850225448608\n",
            "step 9206: generator_loss=0.7066682577133179, discriminator_loss=0.7006443738937378\n",
            "step 9207: generator_loss=0.7019948959350586, discriminator_loss=0.7027914524078369\n",
            "step 9208: generator_loss=0.688558042049408, discriminator_loss=0.7093996405601501\n",
            "step 9209: generator_loss=0.6839032173156738, discriminator_loss=0.7095403671264648\n",
            "step 9210: generator_loss=0.6808527708053589, discriminator_loss=0.7085803151130676\n",
            "step 9211: generator_loss=0.6794764399528503, discriminator_loss=0.7075813412666321\n",
            "step 9212: generator_loss=0.6713560223579407, discriminator_loss=0.7100056409835815\n",
            "step 9213: generator_loss=0.6710323095321655, discriminator_loss=0.7075002193450928\n",
            "step 9214: generator_loss=0.664454460144043, discriminator_loss=0.7102081775665283\n",
            "step 9215: generator_loss=0.6631160378456116, discriminator_loss=0.7100476026535034\n",
            "step 9216: generator_loss=0.6678886413574219, discriminator_loss=0.7063458561897278\n",
            "step 9217: generator_loss=0.6588599681854248, discriminator_loss=0.7121632099151611\n",
            "step 9218: generator_loss=0.6575052738189697, discriminator_loss=0.7133569717407227\n",
            "step 9219: generator_loss=0.6708427667617798, discriminator_loss=0.7089482545852661\n",
            "step 9220: generator_loss=0.672247052192688, discriminator_loss=0.709630012512207\n",
            "step 9221: generator_loss=0.6711038947105408, discriminator_loss=0.7126044631004333\n",
            "step 9222: generator_loss=0.6812412738800049, discriminator_loss=0.7110990285873413\n",
            "step 9223: generator_loss=0.6933207511901855, discriminator_loss=0.7062312364578247\n",
            "step 9224: generator_loss=0.6895001530647278, discriminator_loss=0.7093088626861572\n",
            "step 9225: generator_loss=0.6867870092391968, discriminator_loss=0.7133100032806396\n",
            "step 9226: generator_loss=0.6955987215042114, discriminator_loss=0.7089824080467224\n",
            "step 9227: generator_loss=0.6834702491760254, discriminator_loss=0.7138200402259827\n",
            "step 9228: generator_loss=0.6833584308624268, discriminator_loss=0.7107237577438354\n",
            "step 9229: generator_loss=0.6837174296379089, discriminator_loss=0.7097708582878113\n",
            "step 9230: generator_loss=0.6806768178939819, discriminator_loss=0.7070562839508057\n",
            "step 9231: generator_loss=0.6716018915176392, discriminator_loss=0.7091197371482849\n",
            "step 9232: generator_loss=0.6705383062362671, discriminator_loss=0.7054835557937622\n",
            "step 9233: generator_loss=0.6577200293540955, discriminator_loss=0.7096883058547974\n",
            "step 9234: generator_loss=0.6582170724868774, discriminator_loss=0.7085835933685303\n",
            "step 9235: generator_loss=0.6684371829032898, discriminator_loss=0.7008587121963501\n",
            "step 9236: generator_loss=0.6698248386383057, discriminator_loss=0.7007873058319092\n",
            "step 9237: generator_loss=0.6694797277450562, discriminator_loss=0.7000365257263184\n",
            "step 9238: generator_loss=0.6748256683349609, discriminator_loss=0.6976708173751831\n",
            "step 9239: generator_loss=0.6785515546798706, discriminator_loss=0.6989208459854126\n",
            "step 9240: generator_loss=0.6895655989646912, discriminator_loss=0.6944430470466614\n",
            "step 9241: generator_loss=0.6966501474380493, discriminator_loss=0.691702663898468\n",
            "step 9242: generator_loss=0.7008454203605652, discriminator_loss=0.6906009912490845\n",
            "step 9243: generator_loss=0.7059841156005859, discriminator_loss=0.688490629196167\n",
            "step 9244: generator_loss=0.7143582105636597, discriminator_loss=0.6858945488929749\n",
            "step 9245: generator_loss=0.7201893329620361, discriminator_loss=0.6862705945968628\n",
            "step 9246: generator_loss=0.7258321046829224, discriminator_loss=0.6812493801116943\n",
            "step 9247: generator_loss=0.7317556738853455, discriminator_loss=0.6792412996292114\n",
            "step 9248: generator_loss=0.733144998550415, discriminator_loss=0.67585688829422\n",
            "step 9249: generator_loss=0.7287420034408569, discriminator_loss=0.6742630004882812\n",
            "step 9250: generator_loss=0.7283692359924316, discriminator_loss=0.6694799661636353\n",
            "step 9251: generator_loss=0.7307320833206177, discriminator_loss=0.6644431352615356\n",
            "step 9252: generator_loss=0.7294408082962036, discriminator_loss=0.661043107509613\n",
            "step 9253: generator_loss=0.7311915159225464, discriminator_loss=0.6581501364707947\n",
            "step 9254: generator_loss=0.737350583076477, discriminator_loss=0.6523919105529785\n",
            "step 9255: generator_loss=0.7405058145523071, discriminator_loss=0.6509791612625122\n",
            "step 9256: generator_loss=0.7424539923667908, discriminator_loss=0.6503033638000488\n",
            "step 9257: generator_loss=0.7430890202522278, discriminator_loss=0.6515516638755798\n",
            "step 9258: generator_loss=0.7460030913352966, discriminator_loss=0.6525657176971436\n",
            "step 9259: generator_loss=0.7439783215522766, discriminator_loss=0.6550885438919067\n",
            "step 9260: generator_loss=0.7430025339126587, discriminator_loss=0.6557562351226807\n",
            "step 9261: generator_loss=0.7377336025238037, discriminator_loss=0.6582068204879761\n",
            "step 9262: generator_loss=0.7349861860275269, discriminator_loss=0.6595491170883179\n",
            "step 9263: generator_loss=0.732458233833313, discriminator_loss=0.6590527296066284\n",
            "step 9264: generator_loss=0.7201147079467773, discriminator_loss=0.6641402244567871\n",
            "step 9265: generator_loss=0.7219247221946716, discriminator_loss=0.6620481610298157\n",
            "step 9266: generator_loss=0.7098373174667358, discriminator_loss=0.6668269038200378\n",
            "step 9267: generator_loss=0.7092772126197815, discriminator_loss=0.6672167778015137\n",
            "step 9268: generator_loss=0.7143154144287109, discriminator_loss=0.664080023765564\n",
            "step 9269: generator_loss=0.709665060043335, discriminator_loss=0.6658852696418762\n",
            "step 9270: generator_loss=0.7120203971862793, discriminator_loss=0.6664488315582275\n",
            "step 9271: generator_loss=0.7130268812179565, discriminator_loss=0.6653121709823608\n",
            "step 9272: generator_loss=0.7063776254653931, discriminator_loss=0.6688390970230103\n",
            "step 9273: generator_loss=0.7086406946182251, discriminator_loss=0.669664740562439\n",
            "step 9274: generator_loss=0.7169599533081055, discriminator_loss=0.6682188510894775\n",
            "step 9275: generator_loss=0.7206194996833801, discriminator_loss=0.6684519052505493\n",
            "step 9276: generator_loss=0.7210308313369751, discriminator_loss=0.6714315414428711\n",
            "step 9277: generator_loss=0.724764347076416, discriminator_loss=0.6699529886245728\n",
            "step 9278: generator_loss=0.725568413734436, discriminator_loss=0.6691887378692627\n",
            "step 9279: generator_loss=0.7244464159011841, discriminator_loss=0.6707979440689087\n",
            "step 9280: generator_loss=0.725617527961731, discriminator_loss=0.6731051206588745\n",
            "step 9281: generator_loss=0.7222801446914673, discriminator_loss=0.6757996678352356\n",
            "step 9282: generator_loss=0.7146352529525757, discriminator_loss=0.678156852722168\n",
            "step 9283: generator_loss=0.7113984227180481, discriminator_loss=0.6796557307243347\n",
            "step 9284: generator_loss=0.708277702331543, discriminator_loss=0.6796747446060181\n",
            "step 9285: generator_loss=0.6905308961868286, discriminator_loss=0.6883562803268433\n",
            "step 9286: generator_loss=0.7013816833496094, discriminator_loss=0.6809583306312561\n",
            "step 9287: generator_loss=0.6956720352172852, discriminator_loss=0.6858162879943848\n",
            "step 9288: generator_loss=0.6855604648590088, discriminator_loss=0.6906155347824097\n",
            "step 9289: generator_loss=0.6819199323654175, discriminator_loss=0.6912170648574829\n",
            "step 9290: generator_loss=0.6802978515625, discriminator_loss=0.6932002902030945\n",
            "step 9291: generator_loss=0.687220573425293, discriminator_loss=0.6921616196632385\n",
            "step 9292: generator_loss=0.6850975751876831, discriminator_loss=0.6933422088623047\n",
            "step 9293: generator_loss=0.6868808269500732, discriminator_loss=0.6935064792633057\n",
            "step 9294: generator_loss=0.6884142160415649, discriminator_loss=0.696044921875\n",
            "step 9295: generator_loss=0.6910369396209717, discriminator_loss=0.6950031518936157\n",
            "step 9296: generator_loss=0.690536618232727, discriminator_loss=0.6969594955444336\n",
            "step 9297: generator_loss=0.6932633519172668, discriminator_loss=0.6976697444915771\n",
            "step 9298: generator_loss=0.6899517178535461, discriminator_loss=0.7008758783340454\n",
            "step 9299: generator_loss=0.6984467506408691, discriminator_loss=0.6977691650390625\n",
            "step 9300: generator_loss=0.7012492418289185, discriminator_loss=0.6958239674568176\n",
            "step 9301: generator_loss=0.7074999809265137, discriminator_loss=0.6935222148895264\n",
            "step 9302: generator_loss=0.708528459072113, discriminator_loss=0.6919005513191223\n",
            "step 9303: generator_loss=0.7041868567466736, discriminator_loss=0.6924983263015747\n",
            "step 9304: generator_loss=0.708473265171051, discriminator_loss=0.6894515752792358\n",
            "step 9305: generator_loss=0.7033336162567139, discriminator_loss=0.6905039548873901\n",
            "step 9306: generator_loss=0.7076638340950012, discriminator_loss=0.6864280700683594\n",
            "step 9307: generator_loss=0.7114821672439575, discriminator_loss=0.6813750267028809\n",
            "step 9308: generator_loss=0.7075866460800171, discriminator_loss=0.6824385523796082\n",
            "step 9309: generator_loss=0.7107217311859131, discriminator_loss=0.6805374622344971\n",
            "step 9310: generator_loss=0.7088381052017212, discriminator_loss=0.6817852258682251\n",
            "step 9311: generator_loss=0.7082986831665039, discriminator_loss=0.6817550659179688\n",
            "step 9312: generator_loss=0.711650550365448, discriminator_loss=0.6813497543334961\n",
            "step 9313: generator_loss=0.7133710384368896, discriminator_loss=0.681195855140686\n",
            "step 9314: generator_loss=0.7087063789367676, discriminator_loss=0.6829346418380737\n",
            "step 9315: generator_loss=0.7038419246673584, discriminator_loss=0.686065673828125\n",
            "step 9316: generator_loss=0.7105308771133423, discriminator_loss=0.6834545731544495\n",
            "step 9317: generator_loss=0.7090898752212524, discriminator_loss=0.6851539611816406\n",
            "step 9318: generator_loss=0.7021563053131104, discriminator_loss=0.6863510608673096\n",
            "step 9319: generator_loss=0.6978223919868469, discriminator_loss=0.688847541809082\n",
            "step 9320: generator_loss=0.7027500867843628, discriminator_loss=0.6846886873245239\n",
            "step 9321: generator_loss=0.6938784122467041, discriminator_loss=0.686565101146698\n",
            "step 9322: generator_loss=0.6992232799530029, discriminator_loss=0.6826951503753662\n",
            "step 9323: generator_loss=0.6934152841567993, discriminator_loss=0.6844403743743896\n",
            "step 9324: generator_loss=0.6919225454330444, discriminator_loss=0.6840968132019043\n",
            "step 9325: generator_loss=0.6865432262420654, discriminator_loss=0.6864198446273804\n",
            "step 9326: generator_loss=0.6932584047317505, discriminator_loss=0.6834162473678589\n",
            "step 9327: generator_loss=0.696958065032959, discriminator_loss=0.6815623044967651\n",
            "step 9328: generator_loss=0.7042620182037354, discriminator_loss=0.679284930229187\n",
            "step 9329: generator_loss=0.7037898302078247, discriminator_loss=0.6817137002944946\n",
            "step 9330: generator_loss=0.7057578563690186, discriminator_loss=0.6825214624404907\n",
            "step 9331: generator_loss=0.7128816843032837, discriminator_loss=0.6779116988182068\n",
            "step 9332: generator_loss=0.7124946117401123, discriminator_loss=0.6815723776817322\n",
            "step 9333: generator_loss=0.7160823345184326, discriminator_loss=0.6799835562705994\n",
            "step 9334: generator_loss=0.715438723564148, discriminator_loss=0.6809815168380737\n",
            "step 9335: generator_loss=0.7125300765037537, discriminator_loss=0.6822187304496765\n",
            "step 9336: generator_loss=0.7087512016296387, discriminator_loss=0.6822847127914429\n",
            "step 9337: generator_loss=0.7124500274658203, discriminator_loss=0.6781728267669678\n",
            "step 9338: generator_loss=0.7103108763694763, discriminator_loss=0.6779694557189941\n",
            "step 9339: generator_loss=0.7085190415382385, discriminator_loss=0.6757147312164307\n",
            "step 9340: generator_loss=0.7082341909408569, discriminator_loss=0.6739484071731567\n",
            "step 9341: generator_loss=0.7088114023208618, discriminator_loss=0.671404242515564\n",
            "step 9342: generator_loss=0.7175077199935913, discriminator_loss=0.6654784679412842\n",
            "step 9343: generator_loss=0.722553551197052, discriminator_loss=0.6636358499526978\n",
            "step 9344: generator_loss=0.7263060808181763, discriminator_loss=0.661913275718689\n",
            "step 9345: generator_loss=0.7302145957946777, discriminator_loss=0.6609551906585693\n",
            "step 9346: generator_loss=0.7308907508850098, discriminator_loss=0.6610274314880371\n",
            "step 9347: generator_loss=0.7393285036087036, discriminator_loss=0.6580934524536133\n",
            "step 9348: generator_loss=0.738449215888977, discriminator_loss=0.6577810049057007\n",
            "step 9349: generator_loss=0.7387174367904663, discriminator_loss=0.6588139533996582\n",
            "step 9350: generator_loss=0.739307165145874, discriminator_loss=0.6599588394165039\n",
            "step 9351: generator_loss=0.7393344640731812, discriminator_loss=0.6601189374923706\n",
            "step 9352: generator_loss=0.7401003837585449, discriminator_loss=0.6601710915565491\n",
            "step 9353: generator_loss=0.7439664602279663, discriminator_loss=0.6579047441482544\n",
            "step 9354: generator_loss=0.737765908241272, discriminator_loss=0.6595126986503601\n",
            "step 9355: generator_loss=0.739311933517456, discriminator_loss=0.6605428457260132\n",
            "step 9356: generator_loss=0.7306376695632935, discriminator_loss=0.6642981767654419\n",
            "step 9357: generator_loss=0.7227576971054077, discriminator_loss=0.6685214042663574\n",
            "step 9358: generator_loss=0.7279919385910034, discriminator_loss=0.6669440269470215\n",
            "step 9359: generator_loss=0.7198459506034851, discriminator_loss=0.6730571985244751\n",
            "step 9360: generator_loss=0.7234928011894226, discriminator_loss=0.6717443466186523\n",
            "step 9361: generator_loss=0.7140511274337769, discriminator_loss=0.6786025166511536\n",
            "step 9362: generator_loss=0.7179833054542542, discriminator_loss=0.6771205067634583\n",
            "step 9363: generator_loss=0.702593982219696, discriminator_loss=0.6842219829559326\n",
            "step 9364: generator_loss=0.7028368711471558, discriminator_loss=0.6856032609939575\n",
            "step 9365: generator_loss=0.6983057856559753, discriminator_loss=0.6883682012557983\n",
            "step 9366: generator_loss=0.6921589970588684, discriminator_loss=0.6932621598243713\n",
            "step 9367: generator_loss=0.6948739886283875, discriminator_loss=0.6892452239990234\n",
            "step 9368: generator_loss=0.6838337182998657, discriminator_loss=0.6966698169708252\n",
            "step 9369: generator_loss=0.6822497844696045, discriminator_loss=0.6982072591781616\n",
            "step 9370: generator_loss=0.6881961822509766, discriminator_loss=0.6970508098602295\n",
            "step 9371: generator_loss=0.6872206926345825, discriminator_loss=0.6985844373703003\n",
            "step 9372: generator_loss=0.6845918893814087, discriminator_loss=0.7012969255447388\n",
            "step 9373: generator_loss=0.6809413433074951, discriminator_loss=0.7061270475387573\n",
            "step 9374: generator_loss=0.6811238527297974, discriminator_loss=0.7053378224372864\n",
            "step 9375: generator_loss=0.6816452741622925, discriminator_loss=0.705909013748169\n",
            "step 9376: generator_loss=0.6705408096313477, discriminator_loss=0.7127931118011475\n",
            "step 9377: generator_loss=0.6798044443130493, discriminator_loss=0.7073585987091064\n",
            "step 9378: generator_loss=0.6625733375549316, discriminator_loss=0.717418909072876\n",
            "step 9379: generator_loss=0.6619820594787598, discriminator_loss=0.717722475528717\n",
            "step 9380: generator_loss=0.6635487079620361, discriminator_loss=0.7166571617126465\n",
            "step 9381: generator_loss=0.6633020639419556, discriminator_loss=0.717089831829071\n",
            "step 9382: generator_loss=0.6673575639724731, discriminator_loss=0.7125455737113953\n",
            "step 9383: generator_loss=0.6525068283081055, discriminator_loss=0.719715416431427\n",
            "step 9384: generator_loss=0.6622911095619202, discriminator_loss=0.7122952938079834\n",
            "step 9385: generator_loss=0.6647055149078369, discriminator_loss=0.7102474570274353\n",
            "step 9386: generator_loss=0.6691253185272217, discriminator_loss=0.708881676197052\n",
            "step 9387: generator_loss=0.67038494348526, discriminator_loss=0.7087829113006592\n",
            "step 9388: generator_loss=0.6732165813446045, discriminator_loss=0.7087671160697937\n",
            "step 9389: generator_loss=0.6698022484779358, discriminator_loss=0.7106441259384155\n",
            "step 9390: generator_loss=0.6791835427284241, discriminator_loss=0.7078198194503784\n",
            "step 9391: generator_loss=0.6836085319519043, discriminator_loss=0.705096960067749\n",
            "step 9392: generator_loss=0.6816962957382202, discriminator_loss=0.7075146436691284\n",
            "step 9393: generator_loss=0.6855193376541138, discriminator_loss=0.7034807205200195\n",
            "step 9394: generator_loss=0.6908189058303833, discriminator_loss=0.7008339166641235\n",
            "step 9395: generator_loss=0.6946330070495605, discriminator_loss=0.6988308429718018\n",
            "step 9396: generator_loss=0.6984253525733948, discriminator_loss=0.6964803338050842\n",
            "step 9397: generator_loss=0.6981813907623291, discriminator_loss=0.6940321326255798\n",
            "step 9398: generator_loss=0.7010034322738647, discriminator_loss=0.6908897757530212\n",
            "step 9399: generator_loss=0.7087811231613159, discriminator_loss=0.6845557689666748\n",
            "step 9400: generator_loss=0.7062126398086548, discriminator_loss=0.683286726474762\n",
            "step 9401: generator_loss=0.7166099548339844, discriminator_loss=0.6775917410850525\n",
            "step 9402: generator_loss=0.7181074619293213, discriminator_loss=0.6749305725097656\n",
            "step 9403: generator_loss=0.7266350984573364, discriminator_loss=0.670438289642334\n",
            "step 9404: generator_loss=0.7313974499702454, discriminator_loss=0.6659328937530518\n",
            "step 9405: generator_loss=0.7323511242866516, discriminator_loss=0.6641499996185303\n",
            "step 9406: generator_loss=0.7354652881622314, discriminator_loss=0.6619447469711304\n",
            "step 9407: generator_loss=0.7442234754562378, discriminator_loss=0.6557300090789795\n",
            "step 9408: generator_loss=0.7442607879638672, discriminator_loss=0.6567462682723999\n",
            "step 9409: generator_loss=0.7476538419723511, discriminator_loss=0.6566377878189087\n",
            "step 9410: generator_loss=0.7425627708435059, discriminator_loss=0.6615158319473267\n",
            "step 9411: generator_loss=0.7483088374137878, discriminator_loss=0.6592837572097778\n",
            "step 9412: generator_loss=0.7413339614868164, discriminator_loss=0.6609296798706055\n",
            "step 9413: generator_loss=0.7386831045150757, discriminator_loss=0.6626008749008179\n",
            "step 9414: generator_loss=0.7254953384399414, discriminator_loss=0.6669765114784241\n",
            "step 9415: generator_loss=0.723209023475647, discriminator_loss=0.6677001714706421\n",
            "step 9416: generator_loss=0.7178792357444763, discriminator_loss=0.6675286889076233\n",
            "step 9417: generator_loss=0.7087763547897339, discriminator_loss=0.6713674068450928\n",
            "step 9418: generator_loss=0.7039815187454224, discriminator_loss=0.6726781129837036\n",
            "step 9419: generator_loss=0.7027052044868469, discriminator_loss=0.6731728315353394\n",
            "step 9420: generator_loss=0.6986914277076721, discriminator_loss=0.674323558807373\n",
            "step 9421: generator_loss=0.6945149898529053, discriminator_loss=0.6761795282363892\n",
            "step 9422: generator_loss=0.6948482990264893, discriminator_loss=0.6748949289321899\n",
            "step 9423: generator_loss=0.6937272548675537, discriminator_loss=0.6782302856445312\n",
            "step 9424: generator_loss=0.6951348781585693, discriminator_loss=0.6768057942390442\n",
            "step 9425: generator_loss=0.6955686807632446, discriminator_loss=0.6786150932312012\n",
            "step 9426: generator_loss=0.6993664503097534, discriminator_loss=0.6782999634742737\n",
            "step 9427: generator_loss=0.7017626166343689, discriminator_loss=0.6789869070053101\n",
            "step 9428: generator_loss=0.7041282653808594, discriminator_loss=0.6814340353012085\n",
            "step 9429: generator_loss=0.7083138227462769, discriminator_loss=0.6815518140792847\n",
            "step 9430: generator_loss=0.7100175023078918, discriminator_loss=0.6825199723243713\n",
            "step 9431: generator_loss=0.710720956325531, discriminator_loss=0.6833009123802185\n",
            "step 9432: generator_loss=0.7104138135910034, discriminator_loss=0.6854449510574341\n",
            "step 9433: generator_loss=0.7124340534210205, discriminator_loss=0.685824990272522\n",
            "step 9434: generator_loss=0.7082739472389221, discriminator_loss=0.6887969970703125\n",
            "step 9435: generator_loss=0.7055824995040894, discriminator_loss=0.6900765895843506\n",
            "step 9436: generator_loss=0.700242817401886, discriminator_loss=0.6899489164352417\n",
            "step 9437: generator_loss=0.6979519128799438, discriminator_loss=0.6917097568511963\n",
            "step 9438: generator_loss=0.6933658719062805, discriminator_loss=0.6939524412155151\n",
            "step 9439: generator_loss=0.6892654895782471, discriminator_loss=0.693679690361023\n",
            "step 9440: generator_loss=0.6833274364471436, discriminator_loss=0.6967213749885559\n",
            "step 9441: generator_loss=0.6838891506195068, discriminator_loss=0.6944124698638916\n",
            "step 9442: generator_loss=0.6794472336769104, discriminator_loss=0.6961469650268555\n",
            "step 9443: generator_loss=0.6787221431732178, discriminator_loss=0.6964871883392334\n",
            "step 9444: generator_loss=0.6799136400222778, discriminator_loss=0.6954078674316406\n",
            "step 9445: generator_loss=0.6805269718170166, discriminator_loss=0.6961474418640137\n",
            "step 9446: generator_loss=0.68314129114151, discriminator_loss=0.6971970796585083\n",
            "step 9447: generator_loss=0.6866775751113892, discriminator_loss=0.6957512497901917\n",
            "step 9448: generator_loss=0.6862354278564453, discriminator_loss=0.698646068572998\n",
            "step 9449: generator_loss=0.68809974193573, discriminator_loss=0.6974539756774902\n",
            "step 9450: generator_loss=0.6881745457649231, discriminator_loss=0.7000487446784973\n",
            "step 9451: generator_loss=0.6899375915527344, discriminator_loss=0.700302243232727\n",
            "step 9452: generator_loss=0.6907676458358765, discriminator_loss=0.700752854347229\n",
            "step 9453: generator_loss=0.6900729537010193, discriminator_loss=0.7019544243812561\n",
            "step 9454: generator_loss=0.6881370544433594, discriminator_loss=0.702997088432312\n",
            "step 9455: generator_loss=0.6910369396209717, discriminator_loss=0.7000012993812561\n",
            "step 9456: generator_loss=0.6880812644958496, discriminator_loss=0.6999530792236328\n",
            "step 9457: generator_loss=0.6858090162277222, discriminator_loss=0.7004334926605225\n",
            "step 9458: generator_loss=0.6858186721801758, discriminator_loss=0.6996446847915649\n",
            "step 9459: generator_loss=0.6884952187538147, discriminator_loss=0.6995847225189209\n",
            "step 9460: generator_loss=0.6874345541000366, discriminator_loss=0.6995453238487244\n",
            "step 9461: generator_loss=0.6898441314697266, discriminator_loss=0.6971014142036438\n",
            "step 9462: generator_loss=0.6895577907562256, discriminator_loss=0.6997159123420715\n",
            "step 9463: generator_loss=0.6931838989257812, discriminator_loss=0.6978713274002075\n",
            "step 9464: generator_loss=0.6948387622833252, discriminator_loss=0.6959500312805176\n",
            "step 9465: generator_loss=0.699783980846405, discriminator_loss=0.6955757141113281\n",
            "step 9466: generator_loss=0.6995479464530945, discriminator_loss=0.6956907510757446\n",
            "step 9467: generator_loss=0.701464056968689, discriminator_loss=0.6951799392700195\n",
            "step 9468: generator_loss=0.6990897059440613, discriminator_loss=0.6954033374786377\n",
            "step 9469: generator_loss=0.6994168758392334, discriminator_loss=0.694775402545929\n",
            "step 9470: generator_loss=0.7008439898490906, discriminator_loss=0.6937568783760071\n",
            "step 9471: generator_loss=0.6966232061386108, discriminator_loss=0.6947471499443054\n",
            "step 9472: generator_loss=0.6938321590423584, discriminator_loss=0.695763111114502\n",
            "step 9473: generator_loss=0.6938424706459045, discriminator_loss=0.6946525573730469\n",
            "step 9474: generator_loss=0.6895079612731934, discriminator_loss=0.6964803338050842\n",
            "step 9475: generator_loss=0.6885911822319031, discriminator_loss=0.6958437561988831\n",
            "step 9476: generator_loss=0.6828547716140747, discriminator_loss=0.6980953216552734\n",
            "step 9477: generator_loss=0.682341456413269, discriminator_loss=0.6989253759384155\n",
            "step 9478: generator_loss=0.6802090406417847, discriminator_loss=0.6987302899360657\n",
            "step 9479: generator_loss=0.6802181005477905, discriminator_loss=0.6991739273071289\n",
            "step 9480: generator_loss=0.6786260604858398, discriminator_loss=0.7008477449417114\n",
            "step 9481: generator_loss=0.6786723136901855, discriminator_loss=0.7020810842514038\n",
            "step 9482: generator_loss=0.680841326713562, discriminator_loss=0.7017948627471924\n",
            "step 9483: generator_loss=0.6795941591262817, discriminator_loss=0.7025846242904663\n",
            "step 9484: generator_loss=0.682202935218811, discriminator_loss=0.7029191255569458\n",
            "step 9485: generator_loss=0.6805981993675232, discriminator_loss=0.704332172870636\n",
            "step 9486: generator_loss=0.6818451881408691, discriminator_loss=0.7029762268066406\n",
            "step 9487: generator_loss=0.6815725564956665, discriminator_loss=0.7042992115020752\n",
            "step 9488: generator_loss=0.6831845045089722, discriminator_loss=0.7036668062210083\n",
            "step 9489: generator_loss=0.6804379820823669, discriminator_loss=0.7052013278007507\n",
            "step 9490: generator_loss=0.6804229021072388, discriminator_loss=0.7072599530220032\n",
            "step 9491: generator_loss=0.6826516389846802, discriminator_loss=0.7067450284957886\n",
            "step 9492: generator_loss=0.6827763319015503, discriminator_loss=0.7056987285614014\n",
            "step 9493: generator_loss=0.6805211901664734, discriminator_loss=0.7061604261398315\n",
            "step 9494: generator_loss=0.6769623756408691, discriminator_loss=0.7083953619003296\n",
            "step 9495: generator_loss=0.6761786937713623, discriminator_loss=0.7073354721069336\n",
            "step 9496: generator_loss=0.6754012703895569, discriminator_loss=0.7066344618797302\n",
            "step 9497: generator_loss=0.6729779839515686, discriminator_loss=0.7085254192352295\n",
            "step 9498: generator_loss=0.6736709475517273, discriminator_loss=0.7079132795333862\n",
            "step 9499: generator_loss=0.6728360652923584, discriminator_loss=0.7061161994934082\n",
            "step 9500: generator_loss=0.6770971417427063, discriminator_loss=0.7034700512886047\n",
            "step 9501: generator_loss=0.6769773960113525, discriminator_loss=0.7019401788711548\n",
            "step 9502: generator_loss=0.678501546382904, discriminator_loss=0.69953453540802\n",
            "step 9503: generator_loss=0.6785809993743896, discriminator_loss=0.6998230218887329\n",
            "step 9504: generator_loss=0.6828475594520569, discriminator_loss=0.697593629360199\n",
            "step 9505: generator_loss=0.6842488646507263, discriminator_loss=0.6972520351409912\n",
            "step 9506: generator_loss=0.6887847781181335, discriminator_loss=0.6955045461654663\n",
            "step 9507: generator_loss=0.6905142068862915, discriminator_loss=0.6976310014724731\n",
            "step 9508: generator_loss=0.6945395469665527, discriminator_loss=0.696938693523407\n",
            "step 9509: generator_loss=0.693705677986145, discriminator_loss=0.6974945664405823\n",
            "step 9510: generator_loss=0.6948060989379883, discriminator_loss=0.6973212957382202\n",
            "step 9511: generator_loss=0.6932975053787231, discriminator_loss=0.6987475156784058\n",
            "step 9512: generator_loss=0.6919903755187988, discriminator_loss=0.6990982294082642\n",
            "step 9513: generator_loss=0.6898206472396851, discriminator_loss=0.6995854377746582\n",
            "step 9514: generator_loss=0.6895790100097656, discriminator_loss=0.6984323859214783\n",
            "step 9515: generator_loss=0.6878373622894287, discriminator_loss=0.6995538473129272\n",
            "step 9516: generator_loss=0.6843295097351074, discriminator_loss=0.7003438472747803\n",
            "step 9517: generator_loss=0.6834343075752258, discriminator_loss=0.6998600959777832\n",
            "step 9518: generator_loss=0.6843398809432983, discriminator_loss=0.6995335817337036\n",
            "step 9519: generator_loss=0.6830288171768188, discriminator_loss=0.6992253661155701\n",
            "step 9520: generator_loss=0.6846821308135986, discriminator_loss=0.698789119720459\n",
            "step 9521: generator_loss=0.6878235340118408, discriminator_loss=0.6959238052368164\n",
            "step 9522: generator_loss=0.6897269487380981, discriminator_loss=0.695790708065033\n",
            "step 9523: generator_loss=0.6928830146789551, discriminator_loss=0.6937986612319946\n",
            "step 9524: generator_loss=0.6944135427474976, discriminator_loss=0.6947989463806152\n",
            "step 9525: generator_loss=0.6998002529144287, discriminator_loss=0.6930025219917297\n",
            "step 9526: generator_loss=0.7038185596466064, discriminator_loss=0.6918298006057739\n",
            "step 9527: generator_loss=0.7029691338539124, discriminator_loss=0.6924831867218018\n",
            "step 9528: generator_loss=0.7049421072006226, discriminator_loss=0.6920956373214722\n",
            "step 9529: generator_loss=0.7045581936836243, discriminator_loss=0.6909840106964111\n",
            "step 9530: generator_loss=0.703225314617157, discriminator_loss=0.6912270784378052\n",
            "step 9531: generator_loss=0.7006146907806396, discriminator_loss=0.69272780418396\n",
            "step 9532: generator_loss=0.6956239938735962, discriminator_loss=0.6930456161499023\n",
            "step 9533: generator_loss=0.6946344971656799, discriminator_loss=0.6937904357910156\n",
            "step 9534: generator_loss=0.6901195049285889, discriminator_loss=0.6934118866920471\n",
            "step 9535: generator_loss=0.6883095502853394, discriminator_loss=0.6933191418647766\n",
            "step 9536: generator_loss=0.6849271059036255, discriminator_loss=0.6932704448699951\n",
            "step 9537: generator_loss=0.6849842667579651, discriminator_loss=0.6929678916931152\n",
            "step 9538: generator_loss=0.6868210434913635, discriminator_loss=0.6914012432098389\n",
            "step 9539: generator_loss=0.6851781606674194, discriminator_loss=0.6918454170227051\n",
            "step 9540: generator_loss=0.689721941947937, discriminator_loss=0.6884956955909729\n",
            "step 9541: generator_loss=0.6924197673797607, discriminator_loss=0.6864842176437378\n",
            "step 9542: generator_loss=0.6993682384490967, discriminator_loss=0.681950569152832\n",
            "step 9543: generator_loss=0.7062909007072449, discriminator_loss=0.6802342534065247\n",
            "step 9544: generator_loss=0.7113844156265259, discriminator_loss=0.6779110431671143\n",
            "step 9545: generator_loss=0.7182989120483398, discriminator_loss=0.6753852367401123\n",
            "step 9546: generator_loss=0.7241072654724121, discriminator_loss=0.6730770468711853\n",
            "step 9547: generator_loss=0.729332447052002, discriminator_loss=0.6712164282798767\n",
            "step 9548: generator_loss=0.7291690111160278, discriminator_loss=0.6735779047012329\n",
            "step 9549: generator_loss=0.7292648553848267, discriminator_loss=0.6736358404159546\n",
            "step 9550: generator_loss=0.7257530689239502, discriminator_loss=0.6756854057312012\n",
            "step 9551: generator_loss=0.7246216535568237, discriminator_loss=0.6755839586257935\n",
            "step 9552: generator_loss=0.7177178263664246, discriminator_loss=0.6762030720710754\n",
            "step 9553: generator_loss=0.7133111953735352, discriminator_loss=0.6766676306724548\n",
            "step 9554: generator_loss=0.7072667479515076, discriminator_loss=0.6792834997177124\n",
            "step 9555: generator_loss=0.7016009092330933, discriminator_loss=0.6789466142654419\n",
            "step 9556: generator_loss=0.697970449924469, discriminator_loss=0.6797055006027222\n",
            "step 9557: generator_loss=0.6965101361274719, discriminator_loss=0.6791355609893799\n",
            "step 9558: generator_loss=0.6913313269615173, discriminator_loss=0.6818650960922241\n",
            "step 9559: generator_loss=0.6909123659133911, discriminator_loss=0.6831661462783813\n",
            "step 9560: generator_loss=0.6902472972869873, discriminator_loss=0.6845071315765381\n",
            "step 9561: generator_loss=0.6926401853561401, discriminator_loss=0.6837630867958069\n",
            "step 9562: generator_loss=0.6943943500518799, discriminator_loss=0.6841272711753845\n",
            "step 9563: generator_loss=0.6963117122650146, discriminator_loss=0.6855908632278442\n",
            "step 9564: generator_loss=0.6971765160560608, discriminator_loss=0.6869919300079346\n",
            "step 9565: generator_loss=0.7005139589309692, discriminator_loss=0.6873477697372437\n",
            "step 9566: generator_loss=0.7032120823860168, discriminator_loss=0.6873401403427124\n",
            "step 9567: generator_loss=0.701015293598175, discriminator_loss=0.6893328428268433\n",
            "step 9568: generator_loss=0.7029596567153931, discriminator_loss=0.688764750957489\n",
            "step 9569: generator_loss=0.7003971338272095, discriminator_loss=0.6899147629737854\n",
            "step 9570: generator_loss=0.6994867324829102, discriminator_loss=0.689067006111145\n",
            "step 9571: generator_loss=0.6969099640846252, discriminator_loss=0.6907269358634949\n",
            "step 9572: generator_loss=0.6945330500602722, discriminator_loss=0.6919240355491638\n",
            "step 9573: generator_loss=0.6910770535469055, discriminator_loss=0.6938815116882324\n",
            "step 9574: generator_loss=0.6917057633399963, discriminator_loss=0.6922747492790222\n",
            "step 9575: generator_loss=0.6881722211837769, discriminator_loss=0.694129228591919\n",
            "step 9576: generator_loss=0.6847914457321167, discriminator_loss=0.6956160664558411\n",
            "step 9577: generator_loss=0.6833166480064392, discriminator_loss=0.696923017501831\n",
            "step 9578: generator_loss=0.6827576756477356, discriminator_loss=0.6979914307594299\n",
            "step 9579: generator_loss=0.6792871952056885, discriminator_loss=0.7013534307479858\n",
            "step 9580: generator_loss=0.6816479563713074, discriminator_loss=0.7007247805595398\n",
            "step 9581: generator_loss=0.6811887621879578, discriminator_loss=0.7038642168045044\n",
            "step 9582: generator_loss=0.6780137419700623, discriminator_loss=0.7059301137924194\n",
            "step 9583: generator_loss=0.6764081716537476, discriminator_loss=0.7073014378547668\n",
            "step 9584: generator_loss=0.6759056448936462, discriminator_loss=0.708593487739563\n",
            "step 9585: generator_loss=0.6741918325424194, discriminator_loss=0.7115846276283264\n",
            "step 9586: generator_loss=0.6718641519546509, discriminator_loss=0.7131676077842712\n",
            "step 9587: generator_loss=0.6715364456176758, discriminator_loss=0.7144190073013306\n",
            "step 9588: generator_loss=0.6649923324584961, discriminator_loss=0.7196273803710938\n",
            "step 9589: generator_loss=0.6584920287132263, discriminator_loss=0.7229542136192322\n",
            "step 9590: generator_loss=0.6679441928863525, discriminator_loss=0.7183196544647217\n",
            "step 9591: generator_loss=0.6635958552360535, discriminator_loss=0.7216408848762512\n",
            "step 9592: generator_loss=0.6604712009429932, discriminator_loss=0.7244088649749756\n",
            "step 9593: generator_loss=0.6464719176292419, discriminator_loss=0.7341137528419495\n",
            "step 9594: generator_loss=0.6425763964653015, discriminator_loss=0.7365488409996033\n",
            "step 9595: generator_loss=0.6609025001525879, discriminator_loss=0.7267912030220032\n",
            "step 9596: generator_loss=0.64170241355896, discriminator_loss=0.7402783036231995\n",
            "step 9597: generator_loss=0.6301679015159607, discriminator_loss=0.7477655410766602\n",
            "step 9598: generator_loss=0.6468531489372253, discriminator_loss=0.7394702434539795\n",
            "step 9599: generator_loss=0.6471812725067139, discriminator_loss=0.7414594888687134\n",
            "step 9600: generator_loss=0.644930362701416, discriminator_loss=0.7428861856460571\n",
            "step 9601: generator_loss=0.6470547318458557, discriminator_loss=0.741187334060669\n",
            "step 9602: generator_loss=0.6384885311126709, discriminator_loss=0.7463310956954956\n",
            "step 9603: generator_loss=0.6414502859115601, discriminator_loss=0.7443511486053467\n",
            "step 9604: generator_loss=0.6381848454475403, discriminator_loss=0.7470753192901611\n",
            "step 9605: generator_loss=0.649836540222168, discriminator_loss=0.737992525100708\n",
            "step 9606: generator_loss=0.64546138048172, discriminator_loss=0.7402024865150452\n",
            "step 9607: generator_loss=0.6432292461395264, discriminator_loss=0.7416185140609741\n",
            "step 9608: generator_loss=0.6354122757911682, discriminator_loss=0.7455306053161621\n",
            "step 9609: generator_loss=0.6411952972412109, discriminator_loss=0.7434293031692505\n",
            "step 9610: generator_loss=0.6426844596862793, discriminator_loss=0.7426118850708008\n",
            "step 9611: generator_loss=0.6347625255584717, discriminator_loss=0.7478393316268921\n",
            "step 9612: generator_loss=0.6465498208999634, discriminator_loss=0.7424010038375854\n",
            "step 9613: generator_loss=0.6531780362129211, discriminator_loss=0.7375936508178711\n",
            "step 9614: generator_loss=0.645375669002533, discriminator_loss=0.7425100803375244\n",
            "step 9615: generator_loss=0.6557403206825256, discriminator_loss=0.7383164167404175\n",
            "step 9616: generator_loss=0.6599310636520386, discriminator_loss=0.7336534261703491\n",
            "step 9617: generator_loss=0.6549847722053528, discriminator_loss=0.7364182472229004\n",
            "step 9618: generator_loss=0.6629816293716431, discriminator_loss=0.7301286458969116\n",
            "step 9619: generator_loss=0.6778087615966797, discriminator_loss=0.7179845571517944\n",
            "step 9620: generator_loss=0.6729713678359985, discriminator_loss=0.7180204391479492\n",
            "step 9621: generator_loss=0.6922754049301147, discriminator_loss=0.705026388168335\n",
            "step 9622: generator_loss=0.6949804425239563, discriminator_loss=0.7003477811813354\n",
            "step 9623: generator_loss=0.7077195644378662, discriminator_loss=0.6930661201477051\n",
            "step 9624: generator_loss=0.7053619027137756, discriminator_loss=0.6948842406272888\n",
            "step 9625: generator_loss=0.7192366123199463, discriminator_loss=0.6893289089202881\n",
            "step 9626: generator_loss=0.7103385925292969, discriminator_loss=0.69279944896698\n",
            "step 9627: generator_loss=0.7047044038772583, discriminator_loss=0.6944857835769653\n",
            "step 9628: generator_loss=0.7094356417655945, discriminator_loss=0.6887041330337524\n",
            "step 9629: generator_loss=0.7096716165542603, discriminator_loss=0.6840201616287231\n",
            "step 9630: generator_loss=0.701714038848877, discriminator_loss=0.6857540607452393\n",
            "step 9631: generator_loss=0.7008458375930786, discriminator_loss=0.6840084791183472\n",
            "step 9632: generator_loss=0.695939838886261, discriminator_loss=0.6825063824653625\n",
            "step 9633: generator_loss=0.6939971446990967, discriminator_loss=0.6803393363952637\n",
            "step 9634: generator_loss=0.6923969984054565, discriminator_loss=0.6803287863731384\n",
            "step 9635: generator_loss=0.6965451240539551, discriminator_loss=0.6777728796005249\n",
            "step 9636: generator_loss=0.6937869787216187, discriminator_loss=0.6788812875747681\n",
            "step 9637: generator_loss=0.7001549005508423, discriminator_loss=0.676650881767273\n",
            "step 9638: generator_loss=0.6983282566070557, discriminator_loss=0.6778676509857178\n",
            "step 9639: generator_loss=0.7010382413864136, discriminator_loss=0.677160382270813\n",
            "step 9640: generator_loss=0.7049634456634521, discriminator_loss=0.677437424659729\n",
            "step 9641: generator_loss=0.7150148153305054, discriminator_loss=0.6736022233963013\n",
            "step 9642: generator_loss=0.7170424461364746, discriminator_loss=0.6758594512939453\n",
            "step 9643: generator_loss=0.7300838828086853, discriminator_loss=0.6717991232872009\n",
            "step 9644: generator_loss=0.7355326414108276, discriminator_loss=0.6720468997955322\n",
            "step 9645: generator_loss=0.7394323348999023, discriminator_loss=0.6725064516067505\n",
            "step 9646: generator_loss=0.7415398359298706, discriminator_loss=0.6730000972747803\n",
            "step 9647: generator_loss=0.7399654984474182, discriminator_loss=0.6751160025596619\n",
            "step 9648: generator_loss=0.7389755249023438, discriminator_loss=0.6735249757766724\n",
            "step 9649: generator_loss=0.7370794415473938, discriminator_loss=0.6729137897491455\n",
            "step 9650: generator_loss=0.7305989861488342, discriminator_loss=0.674251914024353\n",
            "step 9651: generator_loss=0.724065899848938, discriminator_loss=0.6738367676734924\n",
            "step 9652: generator_loss=0.7190647721290588, discriminator_loss=0.6734930276870728\n",
            "step 9653: generator_loss=0.7144131660461426, discriminator_loss=0.6716240644454956\n",
            "step 9654: generator_loss=0.7087517380714417, discriminator_loss=0.6724591255187988\n",
            "step 9655: generator_loss=0.7065560221672058, discriminator_loss=0.6717449426651001\n",
            "step 9656: generator_loss=0.7076255083084106, discriminator_loss=0.6713802218437195\n",
            "step 9657: generator_loss=0.7090326547622681, discriminator_loss=0.6710586547851562\n",
            "step 9658: generator_loss=0.7094605565071106, discriminator_loss=0.6715302467346191\n",
            "step 9659: generator_loss=0.7083889842033386, discriminator_loss=0.672194242477417\n",
            "step 9660: generator_loss=0.7104425430297852, discriminator_loss=0.6721180081367493\n",
            "step 9661: generator_loss=0.7112226486206055, discriminator_loss=0.6720314025878906\n",
            "step 9662: generator_loss=0.7156485915184021, discriminator_loss=0.6704837679862976\n",
            "step 9663: generator_loss=0.7204412817955017, discriminator_loss=0.6689748764038086\n",
            "step 9664: generator_loss=0.7221812605857849, discriminator_loss=0.6705496311187744\n",
            "step 9665: generator_loss=0.7264366149902344, discriminator_loss=0.6700265407562256\n",
            "step 9666: generator_loss=0.7268896102905273, discriminator_loss=0.668878436088562\n",
            "step 9667: generator_loss=0.7239217758178711, discriminator_loss=0.6702732443809509\n",
            "step 9668: generator_loss=0.7362746000289917, discriminator_loss=0.6662258505821228\n",
            "step 9669: generator_loss=0.7407164573669434, discriminator_loss=0.664491593837738\n",
            "step 9670: generator_loss=0.7307757139205933, discriminator_loss=0.6670022010803223\n",
            "step 9671: generator_loss=0.740191638469696, discriminator_loss=0.6626385450363159\n",
            "step 9672: generator_loss=0.7295197248458862, discriminator_loss=0.6671265363693237\n",
            "step 9673: generator_loss=0.7329327464103699, discriminator_loss=0.665505051612854\n",
            "step 9674: generator_loss=0.7297181487083435, discriminator_loss=0.6650589108467102\n",
            "step 9675: generator_loss=0.7366991639137268, discriminator_loss=0.6601912975311279\n",
            "step 9676: generator_loss=0.7372347712516785, discriminator_loss=0.660057783126831\n",
            "step 9677: generator_loss=0.7339459657669067, discriminator_loss=0.6609358787536621\n",
            "step 9678: generator_loss=0.732883870601654, discriminator_loss=0.6610890626907349\n",
            "step 9679: generator_loss=0.7372919321060181, discriminator_loss=0.6579595804214478\n",
            "step 9680: generator_loss=0.7311469912528992, discriminator_loss=0.6623847484588623\n",
            "step 9681: generator_loss=0.7296596765518188, discriminator_loss=0.6630445718765259\n",
            "step 9682: generator_loss=0.7274639010429382, discriminator_loss=0.6644187569618225\n",
            "step 9683: generator_loss=0.7258042097091675, discriminator_loss=0.6651562452316284\n",
            "step 9684: generator_loss=0.7239066958427429, discriminator_loss=0.6647747755050659\n",
            "step 9685: generator_loss=0.7208863496780396, discriminator_loss=0.6656534671783447\n",
            "step 9686: generator_loss=0.7168219685554504, discriminator_loss=0.6672606468200684\n",
            "step 9687: generator_loss=0.722158670425415, discriminator_loss=0.6652718782424927\n",
            "step 9688: generator_loss=0.7151021957397461, discriminator_loss=0.6672521233558655\n",
            "step 9689: generator_loss=0.712826132774353, discriminator_loss=0.6678297519683838\n",
            "step 9690: generator_loss=0.7118141055107117, discriminator_loss=0.668248176574707\n",
            "step 9691: generator_loss=0.7228145003318787, discriminator_loss=0.6645970940589905\n",
            "step 9692: generator_loss=0.719987154006958, discriminator_loss=0.6652811765670776\n",
            "step 9693: generator_loss=0.7141618728637695, discriminator_loss=0.6691202521324158\n",
            "step 9694: generator_loss=0.7173218727111816, discriminator_loss=0.6685424447059631\n",
            "step 9695: generator_loss=0.7090345621109009, discriminator_loss=0.6746532917022705\n",
            "step 9696: generator_loss=0.7163115739822388, discriminator_loss=0.673075795173645\n",
            "step 9697: generator_loss=0.7133725881576538, discriminator_loss=0.6751118898391724\n",
            "step 9698: generator_loss=0.724195659160614, discriminator_loss=0.6711679697036743\n",
            "step 9699: generator_loss=0.7094482779502869, discriminator_loss=0.678950309753418\n",
            "step 9700: generator_loss=0.7073215246200562, discriminator_loss=0.6796557903289795\n",
            "step 9701: generator_loss=0.702770471572876, discriminator_loss=0.6800891757011414\n",
            "step 9702: generator_loss=0.6938988566398621, discriminator_loss=0.6846287250518799\n",
            "step 9703: generator_loss=0.697297990322113, discriminator_loss=0.6814967393875122\n",
            "step 9704: generator_loss=0.690570056438446, discriminator_loss=0.6859861612319946\n",
            "step 9705: generator_loss=0.691713809967041, discriminator_loss=0.6864665150642395\n",
            "step 9706: generator_loss=0.6901328563690186, discriminator_loss=0.6864889860153198\n",
            "step 9707: generator_loss=0.6802886128425598, discriminator_loss=0.6919023990631104\n",
            "step 9708: generator_loss=0.6937469244003296, discriminator_loss=0.6861948370933533\n",
            "step 9709: generator_loss=0.6920558214187622, discriminator_loss=0.6880773305892944\n",
            "step 9710: generator_loss=0.6936309933662415, discriminator_loss=0.6902468800544739\n",
            "step 9711: generator_loss=0.6800873279571533, discriminator_loss=0.6983009576797485\n",
            "step 9712: generator_loss=0.6943203210830688, discriminator_loss=0.6920568346977234\n",
            "step 9713: generator_loss=0.6954162120819092, discriminator_loss=0.6941992044448853\n",
            "step 9714: generator_loss=0.6938794255256653, discriminator_loss=0.6959247589111328\n",
            "step 9715: generator_loss=0.6933228969573975, discriminator_loss=0.6984795331954956\n",
            "step 9716: generator_loss=0.6898391246795654, discriminator_loss=0.6996634602546692\n",
            "step 9717: generator_loss=0.6861438751220703, discriminator_loss=0.700911283493042\n",
            "step 9718: generator_loss=0.68980473279953, discriminator_loss=0.6994588971138\n",
            "step 9719: generator_loss=0.6967344284057617, discriminator_loss=0.6958156228065491\n",
            "step 9720: generator_loss=0.6934637427330017, discriminator_loss=0.6975600123405457\n",
            "step 9721: generator_loss=0.6995126605033875, discriminator_loss=0.6923627257347107\n",
            "step 9722: generator_loss=0.6920711994171143, discriminator_loss=0.6957232356071472\n",
            "step 9723: generator_loss=0.6935913562774658, discriminator_loss=0.6926703453063965\n",
            "step 9724: generator_loss=0.6978064775466919, discriminator_loss=0.6913295984268188\n",
            "step 9725: generator_loss=0.6969463229179382, discriminator_loss=0.6911677122116089\n",
            "step 9726: generator_loss=0.7008435726165771, discriminator_loss=0.6862112879753113\n",
            "step 9727: generator_loss=0.7043027877807617, discriminator_loss=0.6829957962036133\n",
            "step 9728: generator_loss=0.7068438529968262, discriminator_loss=0.6810123920440674\n",
            "step 9729: generator_loss=0.7052329778671265, discriminator_loss=0.6798508167266846\n",
            "step 9730: generator_loss=0.7037793397903442, discriminator_loss=0.6786171197891235\n",
            "step 9731: generator_loss=0.714674711227417, discriminator_loss=0.6724449396133423\n",
            "step 9732: generator_loss=0.7146095037460327, discriminator_loss=0.6723295450210571\n",
            "step 9733: generator_loss=0.7192594408988953, discriminator_loss=0.6703702807426453\n",
            "step 9734: generator_loss=0.7243163585662842, discriminator_loss=0.6667026281356812\n",
            "step 9735: generator_loss=0.73012375831604, discriminator_loss=0.6642186641693115\n",
            "step 9736: generator_loss=0.7299685478210449, discriminator_loss=0.6660541296005249\n",
            "step 9737: generator_loss=0.7388752698898315, discriminator_loss=0.6606266498565674\n",
            "step 9738: generator_loss=0.7338911294937134, discriminator_loss=0.6650517582893372\n",
            "step 9739: generator_loss=0.7303203344345093, discriminator_loss=0.6658973693847656\n",
            "step 9740: generator_loss=0.7251012325286865, discriminator_loss=0.6648810505867004\n",
            "step 9741: generator_loss=0.7222752571105957, discriminator_loss=0.6651371717453003\n",
            "step 9742: generator_loss=0.7129721641540527, discriminator_loss=0.6671621799468994\n",
            "step 9743: generator_loss=0.7063593864440918, discriminator_loss=0.6663196682929993\n",
            "step 9744: generator_loss=0.7020101547241211, discriminator_loss=0.6643913984298706\n",
            "step 9745: generator_loss=0.6950610280036926, discriminator_loss=0.6660897135734558\n",
            "step 9746: generator_loss=0.6910889744758606, discriminator_loss=0.6660537123680115\n",
            "step 9747: generator_loss=0.6883926391601562, discriminator_loss=0.666527509689331\n",
            "step 9748: generator_loss=0.6920708417892456, discriminator_loss=0.6656355857849121\n",
            "step 9749: generator_loss=0.6949375867843628, discriminator_loss=0.6680068969726562\n",
            "step 9750: generator_loss=0.7046530842781067, discriminator_loss=0.6661490201950073\n",
            "step 9751: generator_loss=0.7076325416564941, discriminator_loss=0.6686652898788452\n",
            "step 9752: generator_loss=0.7172367572784424, discriminator_loss=0.669685959815979\n",
            "step 9753: generator_loss=0.7229886054992676, discriminator_loss=0.6725940108299255\n",
            "step 9754: generator_loss=0.7255008220672607, discriminator_loss=0.674069881439209\n",
            "step 9755: generator_loss=0.726257860660553, discriminator_loss=0.676161527633667\n",
            "step 9756: generator_loss=0.7218215465545654, discriminator_loss=0.6786402463912964\n",
            "step 9757: generator_loss=0.7220275402069092, discriminator_loss=0.6782472133636475\n",
            "step 9758: generator_loss=0.7101845741271973, discriminator_loss=0.6811513900756836\n",
            "step 9759: generator_loss=0.7102212309837341, discriminator_loss=0.6783058643341064\n",
            "step 9760: generator_loss=0.688325047492981, discriminator_loss=0.6875321865081787\n",
            "step 9761: generator_loss=0.6874281764030457, discriminator_loss=0.6856318712234497\n",
            "step 9762: generator_loss=0.682374119758606, discriminator_loss=0.6876786351203918\n",
            "step 9763: generator_loss=0.6827282905578613, discriminator_loss=0.6879762411117554\n",
            "step 9764: generator_loss=0.6817189455032349, discriminator_loss=0.6859432458877563\n",
            "step 9765: generator_loss=0.6769545078277588, discriminator_loss=0.6927211284637451\n",
            "step 9766: generator_loss=0.6730868816375732, discriminator_loss=0.6942577362060547\n",
            "step 9767: generator_loss=0.6821367144584656, discriminator_loss=0.6921729445457458\n",
            "step 9768: generator_loss=0.689825713634491, discriminator_loss=0.6918842196464539\n",
            "step 9769: generator_loss=0.6897720098495483, discriminator_loss=0.6948180794715881\n",
            "step 9770: generator_loss=0.6852637529373169, discriminator_loss=0.7007123231887817\n",
            "step 9771: generator_loss=0.6940352916717529, discriminator_loss=0.6979403495788574\n",
            "step 9772: generator_loss=0.6837857961654663, discriminator_loss=0.7044907212257385\n",
            "step 9773: generator_loss=0.6936829090118408, discriminator_loss=0.6999839544296265\n",
            "step 9774: generator_loss=0.6840978860855103, discriminator_loss=0.7068435549736023\n",
            "step 9775: generator_loss=0.6888548135757446, discriminator_loss=0.7037138342857361\n",
            "step 9776: generator_loss=0.6984663605690002, discriminator_loss=0.6998147964477539\n",
            "step 9777: generator_loss=0.6872589588165283, discriminator_loss=0.7029305696487427\n",
            "step 9778: generator_loss=0.6745902299880981, discriminator_loss=0.7108170390129089\n",
            "step 9779: generator_loss=0.6756483912467957, discriminator_loss=0.7104790806770325\n",
            "step 9780: generator_loss=0.6821197271347046, discriminator_loss=0.704980194568634\n",
            "step 9781: generator_loss=0.6730194091796875, discriminator_loss=0.7082227468490601\n",
            "step 9782: generator_loss=0.656747043132782, discriminator_loss=0.7157334089279175\n",
            "step 9783: generator_loss=0.659520149230957, discriminator_loss=0.7136812210083008\n",
            "step 9784: generator_loss=0.6621605157852173, discriminator_loss=0.7129151821136475\n",
            "step 9785: generator_loss=0.6577445268630981, discriminator_loss=0.714586615562439\n",
            "step 9786: generator_loss=0.6646724939346313, discriminator_loss=0.7107885479927063\n",
            "step 9787: generator_loss=0.6639894843101501, discriminator_loss=0.7141210436820984\n",
            "step 9788: generator_loss=0.6743340492248535, discriminator_loss=0.7097673416137695\n",
            "step 9789: generator_loss=0.6818212270736694, discriminator_loss=0.707963764667511\n",
            "step 9790: generator_loss=0.6799135208129883, discriminator_loss=0.7093990445137024\n",
            "step 9791: generator_loss=0.6846307516098022, discriminator_loss=0.7081893682479858\n",
            "step 9792: generator_loss=0.6883530616760254, discriminator_loss=0.7071568369865417\n",
            "step 9793: generator_loss=0.6836423873901367, discriminator_loss=0.7079290151596069\n",
            "step 9794: generator_loss=0.6849174499511719, discriminator_loss=0.7066745162010193\n",
            "step 9795: generator_loss=0.6867185831069946, discriminator_loss=0.7026584148406982\n",
            "step 9796: generator_loss=0.6828570365905762, discriminator_loss=0.7020770907402039\n",
            "step 9797: generator_loss=0.6868528723716736, discriminator_loss=0.6964596509933472\n",
            "step 9798: generator_loss=0.6872087717056274, discriminator_loss=0.6911863088607788\n",
            "step 9799: generator_loss=0.6948118805885315, discriminator_loss=0.6877164840698242\n",
            "step 9800: generator_loss=0.7001876831054688, discriminator_loss=0.6847171187400818\n",
            "step 9801: generator_loss=0.7022339105606079, discriminator_loss=0.6836649179458618\n",
            "step 9802: generator_loss=0.7125186920166016, discriminator_loss=0.6829922199249268\n",
            "step 9803: generator_loss=0.7182422876358032, discriminator_loss=0.6806221008300781\n",
            "step 9804: generator_loss=0.7240138053894043, discriminator_loss=0.680060625076294\n",
            "step 9805: generator_loss=0.7237357497215271, discriminator_loss=0.6826255917549133\n",
            "step 9806: generator_loss=0.7286933064460754, discriminator_loss=0.6800008416175842\n",
            "step 9807: generator_loss=0.7280135154724121, discriminator_loss=0.6791435480117798\n",
            "step 9808: generator_loss=0.7255211472511292, discriminator_loss=0.6802752017974854\n",
            "step 9809: generator_loss=0.7278871536254883, discriminator_loss=0.6784325838088989\n",
            "step 9810: generator_loss=0.7254818677902222, discriminator_loss=0.678084135055542\n",
            "step 9811: generator_loss=0.7187793254852295, discriminator_loss=0.6803317070007324\n",
            "step 9812: generator_loss=0.7153187990188599, discriminator_loss=0.6813551187515259\n",
            "step 9813: generator_loss=0.7080457210540771, discriminator_loss=0.6829571723937988\n",
            "step 9814: generator_loss=0.7060788869857788, discriminator_loss=0.6824094653129578\n",
            "step 9815: generator_loss=0.7010266780853271, discriminator_loss=0.6823699474334717\n",
            "step 9816: generator_loss=0.698426365852356, discriminator_loss=0.684141993522644\n",
            "step 9817: generator_loss=0.6944531798362732, discriminator_loss=0.6858954429626465\n",
            "step 9818: generator_loss=0.6952448487281799, discriminator_loss=0.6851454973220825\n",
            "step 9819: generator_loss=0.6962324976921082, discriminator_loss=0.684999942779541\n",
            "step 9820: generator_loss=0.6936119794845581, discriminator_loss=0.6878434419631958\n",
            "step 9821: generator_loss=0.7020334005355835, discriminator_loss=0.6847654581069946\n",
            "step 9822: generator_loss=0.6981478333473206, discriminator_loss=0.687929630279541\n",
            "step 9823: generator_loss=0.7001957297325134, discriminator_loss=0.6888889074325562\n",
            "step 9824: generator_loss=0.6991408467292786, discriminator_loss=0.689422607421875\n",
            "step 9825: generator_loss=0.7023888230323792, discriminator_loss=0.688930094242096\n",
            "step 9826: generator_loss=0.6971117258071899, discriminator_loss=0.6916040182113647\n",
            "step 9827: generator_loss=0.698951780796051, discriminator_loss=0.6905142068862915\n",
            "step 9828: generator_loss=0.6991622447967529, discriminator_loss=0.6917599439620972\n",
            "step 9829: generator_loss=0.6991756558418274, discriminator_loss=0.6902300119400024\n",
            "step 9830: generator_loss=0.6961352229118347, discriminator_loss=0.6930925846099854\n",
            "step 9831: generator_loss=0.6950769424438477, discriminator_loss=0.6921007633209229\n",
            "step 9832: generator_loss=0.6947633028030396, discriminator_loss=0.6923545598983765\n",
            "step 9833: generator_loss=0.6915875673294067, discriminator_loss=0.694043755531311\n",
            "step 9834: generator_loss=0.6900377869606018, discriminator_loss=0.6946108341217041\n",
            "step 9835: generator_loss=0.6886022686958313, discriminator_loss=0.6963490843772888\n",
            "step 9836: generator_loss=0.6889985799789429, discriminator_loss=0.6967567205429077\n",
            "step 9837: generator_loss=0.6844618320465088, discriminator_loss=0.6995056867599487\n",
            "step 9838: generator_loss=0.6863725185394287, discriminator_loss=0.6980319023132324\n",
            "step 9839: generator_loss=0.686037003993988, discriminator_loss=0.698356032371521\n",
            "step 9840: generator_loss=0.6813899874687195, discriminator_loss=0.7014068365097046\n",
            "step 9841: generator_loss=0.6848735809326172, discriminator_loss=0.698319673538208\n",
            "step 9842: generator_loss=0.6823415160179138, discriminator_loss=0.7002642154693604\n",
            "step 9843: generator_loss=0.6818013191223145, discriminator_loss=0.7007045745849609\n",
            "step 9844: generator_loss=0.6819629669189453, discriminator_loss=0.7010225057601929\n",
            "step 9845: generator_loss=0.685667872428894, discriminator_loss=0.6997336149215698\n",
            "step 9846: generator_loss=0.6842643022537231, discriminator_loss=0.7016432881355286\n",
            "step 9847: generator_loss=0.685319185256958, discriminator_loss=0.7028870582580566\n",
            "step 9848: generator_loss=0.6883304119110107, discriminator_loss=0.7015680074691772\n",
            "step 9849: generator_loss=0.6869078874588013, discriminator_loss=0.7027201652526855\n",
            "step 9850: generator_loss=0.6851797103881836, discriminator_loss=0.7039228677749634\n",
            "step 9851: generator_loss=0.6862088441848755, discriminator_loss=0.7025312781333923\n",
            "step 9852: generator_loss=0.684148371219635, discriminator_loss=0.7041338682174683\n",
            "step 9853: generator_loss=0.6862877607345581, discriminator_loss=0.7029082775115967\n",
            "step 9854: generator_loss=0.6866791248321533, discriminator_loss=0.7015612125396729\n",
            "step 9855: generator_loss=0.6830705404281616, discriminator_loss=0.7034350633621216\n",
            "step 9856: generator_loss=0.6855970621109009, discriminator_loss=0.7025455832481384\n",
            "step 9857: generator_loss=0.6838407516479492, discriminator_loss=0.7018313407897949\n",
            "step 9858: generator_loss=0.6847997307777405, discriminator_loss=0.7009292244911194\n",
            "step 9859: generator_loss=0.6858171224594116, discriminator_loss=0.7021759748458862\n",
            "step 9860: generator_loss=0.6812245845794678, discriminator_loss=0.7042231559753418\n",
            "step 9861: generator_loss=0.6838839054107666, discriminator_loss=0.7025044560432434\n",
            "step 9862: generator_loss=0.6804299354553223, discriminator_loss=0.7033582329750061\n",
            "step 9863: generator_loss=0.681115984916687, discriminator_loss=0.7030391693115234\n",
            "step 9864: generator_loss=0.6795167326927185, discriminator_loss=0.7026688456535339\n",
            "step 9865: generator_loss=0.6802666783332825, discriminator_loss=0.702191948890686\n",
            "step 9866: generator_loss=0.6782317161560059, discriminator_loss=0.7031440734863281\n",
            "step 9867: generator_loss=0.6787464618682861, discriminator_loss=0.7016608715057373\n",
            "step 9868: generator_loss=0.6762888431549072, discriminator_loss=0.7031371593475342\n",
            "step 9869: generator_loss=0.6763430237770081, discriminator_loss=0.7027978897094727\n",
            "step 9870: generator_loss=0.6776806712150574, discriminator_loss=0.7009778618812561\n",
            "step 9871: generator_loss=0.6751226186752319, discriminator_loss=0.7008417844772339\n",
            "step 9872: generator_loss=0.680026113986969, discriminator_loss=0.6988131403923035\n",
            "step 9873: generator_loss=0.6790355443954468, discriminator_loss=0.7003317475318909\n",
            "step 9874: generator_loss=0.6833701729774475, discriminator_loss=0.6977034211158752\n",
            "step 9875: generator_loss=0.6861090064048767, discriminator_loss=0.6977681517601013\n",
            "step 9876: generator_loss=0.6852929592132568, discriminator_loss=0.6991606950759888\n",
            "step 9877: generator_loss=0.6906818151473999, discriminator_loss=0.6980074048042297\n",
            "step 9878: generator_loss=0.6899998188018799, discriminator_loss=0.6997132301330566\n",
            "step 9879: generator_loss=0.6891756653785706, discriminator_loss=0.7005953788757324\n",
            "step 9880: generator_loss=0.6901311278343201, discriminator_loss=0.7009400129318237\n",
            "step 9881: generator_loss=0.6909534931182861, discriminator_loss=0.7001842260360718\n",
            "step 9882: generator_loss=0.6924705505371094, discriminator_loss=0.6986576914787292\n",
            "step 9883: generator_loss=0.6896090507507324, discriminator_loss=0.6998913288116455\n",
            "step 9884: generator_loss=0.6924170255661011, discriminator_loss=0.6991374492645264\n",
            "step 9885: generator_loss=0.6944479942321777, discriminator_loss=0.6978155374526978\n",
            "step 9886: generator_loss=0.6945778131484985, discriminator_loss=0.6975029110908508\n",
            "step 9887: generator_loss=0.6897667646408081, discriminator_loss=0.6996500492095947\n",
            "step 9888: generator_loss=0.6895426511764526, discriminator_loss=0.7005980014801025\n",
            "step 9889: generator_loss=0.6874653100967407, discriminator_loss=0.7009297609329224\n",
            "step 9890: generator_loss=0.6853814721107483, discriminator_loss=0.7018258571624756\n",
            "step 9891: generator_loss=0.6843327283859253, discriminator_loss=0.7020677328109741\n",
            "step 9892: generator_loss=0.6817452907562256, discriminator_loss=0.7032380104064941\n",
            "step 9893: generator_loss=0.6798185110092163, discriminator_loss=0.7032658457756042\n",
            "step 9894: generator_loss=0.678963303565979, discriminator_loss=0.7029519081115723\n",
            "step 9895: generator_loss=0.6804825663566589, discriminator_loss=0.7019839882850647\n",
            "step 9896: generator_loss=0.6830497980117798, discriminator_loss=0.7011696696281433\n",
            "step 9897: generator_loss=0.6846615076065063, discriminator_loss=0.7015529870986938\n",
            "step 9898: generator_loss=0.6882249712944031, discriminator_loss=0.7017785906791687\n",
            "step 9899: generator_loss=0.6909695267677307, discriminator_loss=0.7017490267753601\n",
            "step 9900: generator_loss=0.6909803152084351, discriminator_loss=0.7014168500900269\n",
            "step 9901: generator_loss=0.6901639699935913, discriminator_loss=0.7024009227752686\n",
            "step 9902: generator_loss=0.6897419691085815, discriminator_loss=0.7024681568145752\n",
            "step 9903: generator_loss=0.6897217035293579, discriminator_loss=0.7020964026451111\n",
            "step 9904: generator_loss=0.6886154413223267, discriminator_loss=0.7011061310768127\n",
            "step 9905: generator_loss=0.6849087476730347, discriminator_loss=0.7017859816551208\n",
            "step 9906: generator_loss=0.683666467666626, discriminator_loss=0.7006058692932129\n",
            "step 9907: generator_loss=0.6809061169624329, discriminator_loss=0.700346827507019\n",
            "step 9908: generator_loss=0.6791175007820129, discriminator_loss=0.7007718086242676\n",
            "step 9909: generator_loss=0.6799479722976685, discriminator_loss=0.6999110579490662\n",
            "step 9910: generator_loss=0.6800320148468018, discriminator_loss=0.7007445096969604\n",
            "step 9911: generator_loss=0.6773999333381653, discriminator_loss=0.7035666704177856\n",
            "step 9912: generator_loss=0.6779645681381226, discriminator_loss=0.702232837677002\n",
            "step 9913: generator_loss=0.6761962175369263, discriminator_loss=0.7043144106864929\n",
            "step 9914: generator_loss=0.6795383095741272, discriminator_loss=0.703737735748291\n",
            "step 9915: generator_loss=0.6746720671653748, discriminator_loss=0.7074503898620605\n",
            "step 9916: generator_loss=0.6806350946426392, discriminator_loss=0.7052366137504578\n",
            "step 9917: generator_loss=0.6805189847946167, discriminator_loss=0.7058027386665344\n",
            "step 9918: generator_loss=0.6770830154418945, discriminator_loss=0.7085961699485779\n",
            "step 9919: generator_loss=0.6793606281280518, discriminator_loss=0.7091561555862427\n",
            "step 9920: generator_loss=0.6819102168083191, discriminator_loss=0.707796037197113\n",
            "step 9921: generator_loss=0.6804832220077515, discriminator_loss=0.7093839049339294\n",
            "step 9922: generator_loss=0.6808579564094543, discriminator_loss=0.7097731232643127\n",
            "step 9923: generator_loss=0.681873083114624, discriminator_loss=0.7091081738471985\n",
            "step 9924: generator_loss=0.6790087223052979, discriminator_loss=0.7115736603736877\n",
            "step 9925: generator_loss=0.6745613813400269, discriminator_loss=0.7138038873672485\n",
            "step 9926: generator_loss=0.6764379739761353, discriminator_loss=0.7131668329238892\n",
            "step 9927: generator_loss=0.6739861965179443, discriminator_loss=0.7135410308837891\n",
            "step 9928: generator_loss=0.6701856255531311, discriminator_loss=0.7147741317749023\n",
            "step 9929: generator_loss=0.6688541173934937, discriminator_loss=0.7141687273979187\n",
            "step 9930: generator_loss=0.6663072109222412, discriminator_loss=0.7147202491760254\n",
            "step 9931: generator_loss=0.6661312580108643, discriminator_loss=0.7147933840751648\n",
            "step 9932: generator_loss=0.6642923355102539, discriminator_loss=0.715050220489502\n",
            "step 9933: generator_loss=0.6632987856864929, discriminator_loss=0.7140882015228271\n",
            "step 9934: generator_loss=0.6650516986846924, discriminator_loss=0.7140093445777893\n",
            "step 9935: generator_loss=0.666284441947937, discriminator_loss=0.713207483291626\n",
            "step 9936: generator_loss=0.6658722162246704, discriminator_loss=0.7131186723709106\n",
            "step 9937: generator_loss=0.66922926902771, discriminator_loss=0.7112491130828857\n",
            "step 9938: generator_loss=0.673376739025116, discriminator_loss=0.7099200487136841\n",
            "step 9939: generator_loss=0.6736418008804321, discriminator_loss=0.7110340595245361\n",
            "step 9940: generator_loss=0.6759870052337646, discriminator_loss=0.7117061614990234\n",
            "step 9941: generator_loss=0.6741602420806885, discriminator_loss=0.7135070562362671\n",
            "step 9942: generator_loss=0.6748666763305664, discriminator_loss=0.7130328416824341\n",
            "step 9943: generator_loss=0.6810999512672424, discriminator_loss=0.7099682688713074\n",
            "step 9944: generator_loss=0.6824555397033691, discriminator_loss=0.7090436220169067\n",
            "step 9945: generator_loss=0.6830886602401733, discriminator_loss=0.707811176776886\n",
            "step 9946: generator_loss=0.6890946626663208, discriminator_loss=0.704764187335968\n",
            "step 9947: generator_loss=0.6883958578109741, discriminator_loss=0.7043706178665161\n",
            "step 9948: generator_loss=0.6888178586959839, discriminator_loss=0.7035913467407227\n",
            "step 9949: generator_loss=0.6945593357086182, discriminator_loss=0.6997984647750854\n",
            "step 9950: generator_loss=0.6876450777053833, discriminator_loss=0.7017382383346558\n",
            "step 9951: generator_loss=0.6909340620040894, discriminator_loss=0.6977988481521606\n",
            "step 9952: generator_loss=0.6909700632095337, discriminator_loss=0.6958929300308228\n",
            "step 9953: generator_loss=0.6925415992736816, discriminator_loss=0.6917174458503723\n",
            "step 9954: generator_loss=0.6938685178756714, discriminator_loss=0.69077467918396\n",
            "step 9955: generator_loss=0.6936554908752441, discriminator_loss=0.689175009727478\n",
            "step 9956: generator_loss=0.6958962678909302, discriminator_loss=0.6875211000442505\n",
            "step 9957: generator_loss=0.7006239891052246, discriminator_loss=0.6862465143203735\n",
            "step 9958: generator_loss=0.7029942274093628, discriminator_loss=0.6840829849243164\n",
            "step 9959: generator_loss=0.7101724147796631, discriminator_loss=0.6813583374023438\n",
            "step 9960: generator_loss=0.7071909308433533, discriminator_loss=0.682458758354187\n",
            "step 9961: generator_loss=0.707833468914032, discriminator_loss=0.6818358898162842\n",
            "step 9962: generator_loss=0.714199423789978, discriminator_loss=0.6777856945991516\n",
            "step 9963: generator_loss=0.7158886790275574, discriminator_loss=0.6753599047660828\n",
            "step 9964: generator_loss=0.7157516479492188, discriminator_loss=0.6747332811355591\n",
            "step 9965: generator_loss=0.7243727445602417, discriminator_loss=0.669268786907196\n",
            "step 9966: generator_loss=0.7247387766838074, discriminator_loss=0.6692804098129272\n",
            "step 9967: generator_loss=0.7292860746383667, discriminator_loss=0.6684324741363525\n",
            "step 9968: generator_loss=0.7349872589111328, discriminator_loss=0.665616512298584\n",
            "step 9969: generator_loss=0.7358366250991821, discriminator_loss=0.6670308709144592\n",
            "step 9970: generator_loss=0.7350996732711792, discriminator_loss=0.6678712368011475\n",
            "step 9971: generator_loss=0.7326318025588989, discriminator_loss=0.6698556542396545\n",
            "step 9972: generator_loss=0.7298547029495239, discriminator_loss=0.6698739528656006\n",
            "step 9973: generator_loss=0.7250854969024658, discriminator_loss=0.6721916198730469\n",
            "step 9974: generator_loss=0.7151069045066833, discriminator_loss=0.6730560660362244\n",
            "step 9975: generator_loss=0.7134362459182739, discriminator_loss=0.672250509262085\n",
            "step 9976: generator_loss=0.7032963633537292, discriminator_loss=0.6757429242134094\n",
            "step 9977: generator_loss=0.7029556035995483, discriminator_loss=0.6746265888214111\n",
            "step 9978: generator_loss=0.6969628930091858, discriminator_loss=0.676361620426178\n",
            "step 9979: generator_loss=0.6913064122200012, discriminator_loss=0.6786139011383057\n",
            "step 9980: generator_loss=0.6916261911392212, discriminator_loss=0.6781233549118042\n",
            "step 9981: generator_loss=0.6907577514648438, discriminator_loss=0.6789526343345642\n",
            "step 9982: generator_loss=0.6907734870910645, discriminator_loss=0.6804295182228088\n",
            "step 9983: generator_loss=0.6905393600463867, discriminator_loss=0.6811385154724121\n",
            "step 9984: generator_loss=0.6879158020019531, discriminator_loss=0.6833029985427856\n",
            "step 9985: generator_loss=0.6903992891311646, discriminator_loss=0.6813941597938538\n",
            "step 9986: generator_loss=0.6948943734169006, discriminator_loss=0.6820055246353149\n",
            "step 9987: generator_loss=0.6980539560317993, discriminator_loss=0.6814107298851013\n",
            "step 9988: generator_loss=0.708574652671814, discriminator_loss=0.6777053475379944\n",
            "step 9989: generator_loss=0.7082827091217041, discriminator_loss=0.6801155805587769\n",
            "step 9990: generator_loss=0.7078037261962891, discriminator_loss=0.6840394735336304\n",
            "step 9991: generator_loss=0.7168165445327759, discriminator_loss=0.6810992956161499\n",
            "step 9992: generator_loss=0.7215341925621033, discriminator_loss=0.6813181042671204\n",
            "step 9993: generator_loss=0.7182599902153015, discriminator_loss=0.6836682558059692\n",
            "step 9994: generator_loss=0.7162209153175354, discriminator_loss=0.6849941611289978\n",
            "step 9995: generator_loss=0.7201572060585022, discriminator_loss=0.6847010254859924\n",
            "step 9996: generator_loss=0.7126278877258301, discriminator_loss=0.6879245042800903\n",
            "step 9997: generator_loss=0.7074430584907532, discriminator_loss=0.6907626390457153\n",
            "step 9998: generator_loss=0.7009940147399902, discriminator_loss=0.6919484734535217\n",
            "step 9999: generator_loss=0.6947609186172485, discriminator_loss=0.6940872669219971\n",
            "step 10000: generator_loss=0.6940827369689941, discriminator_loss=0.6936600208282471\n"
          ]
        }
      ],
      "source": [
        "# Continue training from the most recent checkpoint\n",
        "# Assumes only that `train_data` has already been defined.\n",
        "\n",
        "# --- Recreate GAN / optimizer hyperparameters to match original run ---\n",
        "learning_rate = 1e-3\n",
        "batch_size = 128\n",
        "latent_dim = 64\n",
        "loss_type = \"nonsaturating\"\n",
        "steps_per_save = 250\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "# --- Resume from last checkpoint and continue training ---\n",
        "extra_steps = 5_000  # number of additional training steps\n",
        "resume_seed = 1  # RNG seed for resumed training\n",
        "key = jax.random.key(resume_seed)\n",
        "\n",
        "experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "steps = sorted(ckpt_manager.all_steps())\n",
        "if not steps:\n",
        "    raise ValueError(f\"No checkpoints found for experiment_name={experiment_name!r}.\")\n",
        "\n",
        "last_step = steps[-1]\n",
        "print(f\"Resuming training from global step {last_step} (experiment_name={experiment_name!r})\")\n",
        "\n",
        "# Template states for restoring checkpoints\n",
        "gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "    optimizer=optimizer,\n",
        "    key=jax.random.key(0),\n",
        "    latent_dim=latent_dim,\n",
        ")\n",
        "restored = ckpt_manager.restore(\n",
        "    last_step,\n",
        "    args=ocp.args.StandardRestore(\n",
        "        item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "    ),\n",
        ")\n",
        "\n",
        "generator_training_state = restored[\"generator\"]\n",
        "discriminator_training_state = restored[\"discriminator\"]\n",
        "\n",
        "# Run additional training and keep saving checkpoints with global step indices\n",
        "generator_training_state, discriminator_training_state, key = run_training_gan(\n",
        "    train_data=train_data,\n",
        "    n_steps=extra_steps,\n",
        "    generator_training_state=generator_training_state,\n",
        "    discriminator_training_state=discriminator_training_state,\n",
        "    key=key,\n",
        "    steps_per_save=steps_per_save,\n",
        "    checkpoint_manager=ckpt_manager,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        "    start_step=last_step,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1bd0cbe4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot real vs generated histogram after extended training (up to 10,000 steps)\n",
        "\n",
        "plot_real_vs_generated_histogram_from_last_checkpoint(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        "    y_max=0.02,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3bab6254",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot GAN training dynamics using all available checkpoints\n",
        "# (original run + extended training)\n",
        "\n",
        "plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
