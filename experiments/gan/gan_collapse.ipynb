{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax import vmap\n",
        "\n",
        "import numpy as np\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.collections import LineCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2741d19",
      "metadata": {},
      "source": [
        "### Mixture of Gaussians\n",
        "\n",
        "First we're going to generate a mixture of two Gaussians that the generator is supposed to match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ccb3e26d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pmf_gaussian_single(\n",
        "    mu: float,\n",
        "    sigma: float,\n",
        "    n_pixels: int = 784,\n",
        "):\n",
        "    \"\"\"\n",
        "    Discrete Gaussian over 784 bins\n",
        "    Uses probability densities at bin centres rather than \n",
        "    integrated probability masses over the bin intervals\n",
        "    \"\"\"\n",
        "    pixel_midpoints = jnp.arange(n_pixels) + 0.5\n",
        "    pixel_frequencies = jnp.exp(-0.5 * ((pixel_midpoints - mu) / sigma) ** 2)\n",
        "    pixel_probabilities = pixel_frequencies / pixel_frequencies.sum()\n",
        "    return pixel_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1e74b31",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pmf_gaussian_mixed(\n",
        "    mu1: float,\n",
        "    sigma1: float,\n",
        "    mu2: float,\n",
        "    sigma2: float,\n",
        "    alpha: float,\n",
        "    n_pixels: int = 784,\n",
        "):\n",
        "    \"\"\"pmf for α*N(mean1, std1) + (1-α)*N(mean2, std2) on discrete bins.\"\"\"\n",
        "    p1 = pmf_gaussian_single(mu1, sigma1, n_pixels)\n",
        "    p2 = pmf_gaussian_single(mu2, sigma2, n_pixels)\n",
        "    probs = alpha * p1 + (1.0 - alpha) * p2\n",
        "    return probs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "064ed82b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def r_gaussian_mixed(\n",
        "    key,\n",
        "    mu1: float,\n",
        "    sigma1: float,\n",
        "    mu2: float,\n",
        "    sigma2: float,\n",
        "    alpha: float,\n",
        "    n_pixels: int = 784,\n",
        "    n_samples: int = 100,\n",
        "    n_training_samples: int = 10,\n",
        "):\n",
        "    \"\"\"Draw histograms from the 2-Gaussian mixture.\n",
        "\n",
        "    Returns an array of shape (n_training_samples, n_pixels), where each row\n",
        "    is a histogram (normalized to sum to 1) from `n_samples` draws.\n",
        "    \"\"\"\n",
        "    probs = pmf_gaussian_mixed(mu1, sigma1, mu2, sigma2, alpha, n_pixels)\n",
        "    probs = jnp.broadcast_to(probs, (n_training_samples, n_pixels))\n",
        "    counts = jr.multinomial(key, n=n_samples, p=probs)\n",
        "    return counts / n_samples\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "15a25657",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss_single_label(logits, label):\n",
        "    targets = jnp.full_like(logits, label)\n",
        "    return optax.sigmoid_binary_cross_entropy(logits, targets).mean()\n",
        "\n",
        "def generator_loss_nonsaturating(logits_real_given_fake):\n",
        "    \"\"\"Objective: maximise p(predicted real | fake)\"\"\"\n",
        "    return + cross_entropy_loss_single_label(logits=logits_real_given_fake, label=1)\n",
        "\n",
        "def generator_loss_saturating(logits_real_given_fake):\n",
        "    \"\"\"Objective: minimise p(predicted fake | fake)\"\"\"\n",
        "    return - cross_entropy_loss_single_label(logits=logits_real_given_fake, label=0)\n",
        "\n",
        "def discriminator_loss(logits_real_given_real, logits_real_given_fake):\n",
        "    loss_given_real = cross_entropy_loss_single_label(logits=logits_real_given_real, label=1)\n",
        "    loss_given_fake = cross_entropy_loss_single_label(logits=logits_real_given_fake, label=0)\n",
        "    return (loss_given_real + loss_given_fake) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b22555e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_generator_loss(generator_params, discriminator_params, generator_apply_fn, discriminator_apply_fn, z_vector, loss_type=\"nonsaturating\"):\n",
        "    fake_images = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    logits_real_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    if loss_type == \"nonsaturating\":\n",
        "        return generator_loss_nonsaturating(logits_real_given_fake)\n",
        "    elif loss_type == \"saturating\":\n",
        "        return generator_loss_saturating(logits_real_given_fake)\n",
        "    else:\n",
        "        raise ValueError(f\"incorrect loss type specified: {loss_type}\")\n",
        "\n",
        "\n",
        "def calculate_discriminator_loss(discriminator_params, generator_params, generator_apply_fn, discriminator_apply_fn, z_vector, real_images):\n",
        "    fake_images = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    logits_real_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    logits_real_given_real = discriminator_apply_fn({\"params\": discriminator_params}, real_images)\n",
        "    return discriminator_loss(logits_real_given_real, logits_real_given_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "efbab3ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_generator_step_nonsaturating(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        \"nonsaturating\",\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_generator_step_saturating(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        \"saturating\",\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_discriminator_step(generator, discriminator, z_vector, real_images):\n",
        "    grads_by_params_fn = jax.grad(calculate_discriminator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        discriminator.params,\n",
        "        generator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        real_images,\n",
        "    )\n",
        "    return discriminator.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a156d045",
      "metadata": {},
      "outputs": [],
      "source": [
        "def subsample_images_for_batch(key, images_full_sample, batch_size):\n",
        "    image_ids = jax.random.randint(key, (batch_size,), 0, images_full_sample.shape[0])\n",
        "    return images_full_sample[image_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e79102c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training_gan(\n",
        "    train_data,\n",
        "    n_steps,\n",
        "    generator_training_state,\n",
        "    discriminator_training_state,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    checkpoint_manager,\n",
        "    batch_size: int,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    start_step: int = 0,\n",
        "):\n",
        "    \"\"\"Train a GAN using random mini-batches of real images.\n",
        "\n",
        "    Shapes:\n",
        "      - train_data[\"image\"]: (N, n_pixels)\n",
        "      - real_images_batch:   (batch_size, n_pixels)\n",
        "      - z_vectors:           (batch_size, latent_dim)\n",
        "\n",
        "    Args:\n",
        "      start_step: starting global step index (for resuming from checkpoints).\n",
        "    \"\"\"\n",
        "    real_images = train_data\n",
        "\n",
        "    if loss_type == \"nonsaturating\":\n",
        "        take_generator_step = take_generator_step_nonsaturating\n",
        "    elif loss_type == \"saturating\":\n",
        "        take_generator_step = take_generator_step_saturating\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
        "\n",
        "    for step in range(1, n_steps + 1):\n",
        "        key, key_z_generation, key_real_subsample = jax.random.split(key, 3)\n",
        "\n",
        "        # Random mini-batch of real images\n",
        "        real_images_batch = subsample_images_for_batch(\n",
        "            key_real_subsample,\n",
        "            real_images,\n",
        "            batch_size,\n",
        "        )\n",
        "\n",
        "        # Latent vectors for generator\n",
        "        z_vectors = jax.random.normal(\n",
        "            key_z_generation,\n",
        "            (batch_size, latent_dim),\n",
        "        )\n",
        "\n",
        "        # Update discriminator then generator\n",
        "        discriminator_training_state = take_discriminator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "        generator_training_state = take_generator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "        )\n",
        "\n",
        "        # Monitor losses\n",
        "        generator_loss_value = calculate_generator_loss(\n",
        "            generator_training_state.params,\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            loss_type=loss_type,\n",
        "        )\n",
        "        discriminator_loss_value = calculate_discriminator_loss(\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "\n",
        "        global_step = start_step + step\n",
        "        print(\n",
        "            f\"step {global_step}: generator_loss={generator_loss_value}, \"\n",
        "            f\"discriminator_loss={discriminator_loss_value}\"\n",
        "        )\n",
        "\n",
        "        if step == 1 or global_step % steps_per_save == 0:\n",
        "            checkpoint_manager.save(\n",
        "                global_step,\n",
        "                args=ocp.args.StandardSave(\n",
        "                    {\n",
        "                        \"generator\": generator_training_state,\n",
        "                        \"discriminator\": discriminator_training_state,\n",
        "                    }\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_batch(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4b0c85d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_gan_training(optimizer, key, latent_dim):\n",
        "    N_PIXELS = 784\n",
        "    N_HIDDEN_LAYER = 128\n",
        "    N_BINARY_CATEGORIES = 1\n",
        "\n",
        "    # Generator maps from latent space (latent_dim) to image space (N_PIXELS)\n",
        "    layer_sizes_generator = [latent_dim, N_HIDDEN_LAYER, N_PIXELS]\n",
        "    layer_sizes_discriminator = [N_PIXELS, N_HIDDEN_LAYER, N_BINARY_CATEGORIES]\n",
        "\n",
        "    g_key, d_key = jax.random.split(key)\n",
        "    generator_training_state = create_training_state(layer_sizes_generator, optimizer, g_key)\n",
        "    discriminator_training_state = create_training_state(layer_sizes_discriminator, optimizer, d_key)\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6007df0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(optimizer):\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"gan_{opt_name}\"\n",
        "\n",
        "\n",
        "def make_constraints_experiment_name(optimizer, loss_type: str) -> str:\n",
        "    \"\"\"Experiment name for this notebook's mixture-of-Gaussians GAN runs.\"\"\"\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"gan_constraints_{opt_name}_{loss_type}\"\n",
        "\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"gan\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "42aaa206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_gan(\n",
        "    train_data, \n",
        "    optimizer, \n",
        "    n_steps=10**3, \n",
        "    steps_per_save=100, \n",
        "    key=jax.random.key(0),\n",
        "    batch_size: int = 128,\n",
        "    latent_dim: int = 64,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    ):\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=key,\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = run_training_gan(\n",
        "        train_data=train_data,\n",
        "        n_steps=n_steps,\n",
        "        generator_training_state=generator_training_state,\n",
        "        discriminator_training_state=discriminator_training_state,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        batch_size=batch_size,\n",
        "        latent_dim=latent_dim,\n",
        "        loss_type=loss_type,\n",
        "    )\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**4) \n",
        "# test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d1ad17b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.key(0)\n",
        "n_training_samples = 10_000\n",
        "n_pixels = 784\n",
        "n_samples = 10_000\n",
        "\n",
        "mu1 = 200\n",
        "sigma1 = 50\n",
        "mu2 = 350\n",
        "sigma2 = 50 \n",
        "alpha = 0.5\n",
        "train_data = r_gaussian_mixed(key, mu1, sigma1, mu2, sigma2, alpha, n_pixels, n_samples, n_training_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "aa6a1022",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: generator_loss=0.2309345155954361, discriminator_loss=1.3712272644042969\n",
            "step 2: generator_loss=0.16097933053970337, discriminator_loss=1.5459084510803223\n",
            "step 3: generator_loss=0.13246533274650574, discriminator_loss=1.6685304641723633\n",
            "step 4: generator_loss=0.12622371315956116, discriminator_loss=1.7081263065338135\n",
            "step 5: generator_loss=0.08316975831985474, discriminator_loss=1.870084285736084\n",
            "step 6: generator_loss=0.06533394753932953, discriminator_loss=2.092430591583252\n",
            "step 7: generator_loss=0.04134879261255264, discriminator_loss=2.2522802352905273\n",
            "step 8: generator_loss=0.03975551575422287, discriminator_loss=2.286003351211548\n",
            "step 9: generator_loss=0.055900685489177704, discriminator_loss=2.1255462169647217\n",
            "step 10: generator_loss=0.05546342581510544, discriminator_loss=2.076913356781006\n",
            "step 11: generator_loss=0.05941787362098694, discriminator_loss=2.036372661590576\n",
            "step 12: generator_loss=0.07214837521314621, discriminator_loss=1.9659740924835205\n",
            "step 13: generator_loss=0.0718962550163269, discriminator_loss=1.9601988792419434\n",
            "step 14: generator_loss=0.10487005114555359, discriminator_loss=1.780239462852478\n",
            "step 15: generator_loss=0.1012478917837143, discriminator_loss=1.8027660846710205\n",
            "step 16: generator_loss=0.11640757322311401, discriminator_loss=1.7067281007766724\n",
            "step 17: generator_loss=0.14146466553211212, discriminator_loss=1.6095446348190308\n",
            "step 18: generator_loss=0.1445244550704956, discriminator_loss=1.6315562725067139\n",
            "step 19: generator_loss=0.15461723506450653, discriminator_loss=1.5626533031463623\n",
            "step 20: generator_loss=0.19202393293380737, discriminator_loss=1.4888732433319092\n",
            "step 21: generator_loss=0.20649844408035278, discriminator_loss=1.498916745185852\n",
            "step 22: generator_loss=0.20760703086853027, discriminator_loss=1.4543678760528564\n",
            "step 23: generator_loss=0.20590102672576904, discriminator_loss=1.5055451393127441\n",
            "step 24: generator_loss=0.2041027843952179, discriminator_loss=1.4682477712631226\n",
            "step 25: generator_loss=0.20505645871162415, discriminator_loss=1.4823952913284302\n",
            "step 26: generator_loss=0.23659509420394897, discriminator_loss=1.4568475484848022\n",
            "step 27: generator_loss=0.20285436511039734, discriminator_loss=1.588182806968689\n",
            "step 28: generator_loss=0.18462564051151276, discriminator_loss=1.504342794418335\n",
            "step 29: generator_loss=0.17792169749736786, discriminator_loss=1.5211422443389893\n",
            "step 30: generator_loss=0.17945867776870728, discriminator_loss=1.5757484436035156\n",
            "step 31: generator_loss=0.18257278203964233, discriminator_loss=1.5328209400177002\n",
            "step 32: generator_loss=0.2319028675556183, discriminator_loss=1.5049711465835571\n",
            "step 33: generator_loss=0.1884908378124237, discriminator_loss=1.5531892776489258\n",
            "step 34: generator_loss=0.19135794043540955, discriminator_loss=1.5605461597442627\n",
            "step 35: generator_loss=0.22711387276649475, discriminator_loss=1.4813120365142822\n",
            "step 36: generator_loss=0.22655421495437622, discriminator_loss=1.4727766513824463\n",
            "step 37: generator_loss=0.20983010530471802, discriminator_loss=1.5387036800384521\n",
            "step 38: generator_loss=0.26225829124450684, discriminator_loss=1.3938679695129395\n",
            "step 39: generator_loss=0.3036637604236603, discriminator_loss=1.3004571199417114\n",
            "step 40: generator_loss=0.3052382171154022, discriminator_loss=1.2615675926208496\n",
            "step 41: generator_loss=0.3226510286331177, discriminator_loss=1.2381149530410767\n",
            "step 42: generator_loss=0.3471081852912903, discriminator_loss=1.1860231161117554\n",
            "step 43: generator_loss=0.47851240634918213, discriminator_loss=1.0512070655822754\n",
            "step 44: generator_loss=0.5173665881156921, discriminator_loss=0.996528685092926\n",
            "step 45: generator_loss=0.6205812692642212, discriminator_loss=0.9150745868682861\n",
            "step 46: generator_loss=0.6743077039718628, discriminator_loss=0.8483102917671204\n",
            "step 47: generator_loss=0.831407904624939, discriminator_loss=0.7991119623184204\n",
            "step 48: generator_loss=0.9367043972015381, discriminator_loss=0.7105979919433594\n",
            "step 49: generator_loss=1.253965139389038, discriminator_loss=0.6229598522186279\n",
            "step 50: generator_loss=1.219306230545044, discriminator_loss=0.604932427406311\n",
            "step 51: generator_loss=1.424879789352417, discriminator_loss=0.5592082738876343\n",
            "step 52: generator_loss=1.6218769550323486, discriminator_loss=0.5382763147354126\n",
            "step 53: generator_loss=1.847104787826538, discriminator_loss=0.48466023802757263\n",
            "step 54: generator_loss=1.9055399894714355, discriminator_loss=0.4800579249858856\n",
            "step 55: generator_loss=2.042538642883301, discriminator_loss=0.4545765519142151\n",
            "step 56: generator_loss=2.243995189666748, discriminator_loss=0.4419064521789551\n",
            "step 57: generator_loss=2.2338905334472656, discriminator_loss=0.43462419509887695\n",
            "step 58: generator_loss=2.414841651916504, discriminator_loss=0.4249802827835083\n",
            "step 59: generator_loss=2.3610329627990723, discriminator_loss=0.42452186346054077\n",
            "step 60: generator_loss=2.4648280143737793, discriminator_loss=0.4127829372882843\n",
            "step 61: generator_loss=2.736334800720215, discriminator_loss=0.39595726132392883\n",
            "step 62: generator_loss=2.690748453140259, discriminator_loss=0.3950918912887573\n",
            "step 63: generator_loss=2.6682844161987305, discriminator_loss=0.4004805088043213\n",
            "step 64: generator_loss=2.8059284687042236, discriminator_loss=0.3856084942817688\n",
            "step 65: generator_loss=2.827486753463745, discriminator_loss=0.3871583938598633\n",
            "step 66: generator_loss=2.9834160804748535, discriminator_loss=0.37441977858543396\n",
            "step 67: generator_loss=2.94832706451416, discriminator_loss=0.3799426853656769\n",
            "step 68: generator_loss=3.029232978820801, discriminator_loss=0.3680075705051422\n",
            "step 69: generator_loss=3.034280300140381, discriminator_loss=0.37287992238998413\n",
            "step 70: generator_loss=3.0041470527648926, discriminator_loss=0.37414199113845825\n",
            "step 71: generator_loss=3.053638458251953, discriminator_loss=0.36796700954437256\n",
            "step 72: generator_loss=3.2249507904052734, discriminator_loss=0.35747379064559937\n",
            "step 73: generator_loss=3.1144237518310547, discriminator_loss=0.3549031615257263\n",
            "step 74: generator_loss=3.1058177947998047, discriminator_loss=0.35400837659835815\n",
            "step 75: generator_loss=3.230134963989258, discriminator_loss=0.350929856300354\n",
            "step 76: generator_loss=3.283878803253174, discriminator_loss=0.34442058205604553\n",
            "step 77: generator_loss=3.3657314777374268, discriminator_loss=0.3479776978492737\n",
            "step 78: generator_loss=3.330843687057495, discriminator_loss=0.3369726836681366\n",
            "step 79: generator_loss=3.4181301593780518, discriminator_loss=0.33351317048072815\n",
            "step 80: generator_loss=3.783625602722168, discriminator_loss=0.32835081219673157\n",
            "step 81: generator_loss=3.603428840637207, discriminator_loss=0.33249008655548096\n",
            "step 82: generator_loss=3.5318660736083984, discriminator_loss=0.3256220519542694\n",
            "step 83: generator_loss=3.7132577896118164, discriminator_loss=0.3258460462093353\n",
            "step 84: generator_loss=3.728868007659912, discriminator_loss=0.3191879987716675\n",
            "step 85: generator_loss=3.9535202980041504, discriminator_loss=0.32117849588394165\n",
            "step 86: generator_loss=3.7168657779693604, discriminator_loss=0.3180757761001587\n",
            "step 87: generator_loss=3.615114212036133, discriminator_loss=0.3186236023902893\n",
            "step 88: generator_loss=3.585284948348999, discriminator_loss=0.3144657015800476\n",
            "step 89: generator_loss=3.721792221069336, discriminator_loss=0.3143143355846405\n",
            "step 90: generator_loss=3.581195592880249, discriminator_loss=0.31907379627227783\n",
            "step 91: generator_loss=3.681837558746338, discriminator_loss=0.31182897090911865\n",
            "step 92: generator_loss=3.6909775733947754, discriminator_loss=0.31616902351379395\n",
            "step 93: generator_loss=3.869673252105713, discriminator_loss=0.3065882921218872\n",
            "step 94: generator_loss=3.9923954010009766, discriminator_loss=0.3079145848751068\n",
            "step 95: generator_loss=4.354138374328613, discriminator_loss=0.2958716154098511\n",
            "step 96: generator_loss=4.236023902893066, discriminator_loss=0.29167935252189636\n",
            "step 97: generator_loss=4.189749717712402, discriminator_loss=0.2928421199321747\n",
            "step 98: generator_loss=4.035361289978027, discriminator_loss=0.2959485650062561\n",
            "step 99: generator_loss=4.174637794494629, discriminator_loss=0.2902759313583374\n",
            "step 100: generator_loss=4.140946388244629, discriminator_loss=0.288642942905426\n",
            "step 101: generator_loss=4.293050765991211, discriminator_loss=0.2825643718242645\n",
            "step 102: generator_loss=4.055825233459473, discriminator_loss=0.28568175435066223\n",
            "step 103: generator_loss=4.2893548011779785, discriminator_loss=0.28163278102874756\n",
            "step 104: generator_loss=4.3509321212768555, discriminator_loss=0.2748258113861084\n",
            "step 105: generator_loss=4.586503028869629, discriminator_loss=0.27307960391044617\n",
            "step 106: generator_loss=4.73262357711792, discriminator_loss=0.27069708704948425\n",
            "step 107: generator_loss=4.82620096206665, discriminator_loss=0.26885923743247986\n",
            "step 108: generator_loss=4.748188495635986, discriminator_loss=0.2665117383003235\n",
            "step 109: generator_loss=4.875361442565918, discriminator_loss=0.2638959288597107\n",
            "step 110: generator_loss=4.615473747253418, discriminator_loss=0.26698970794677734\n",
            "step 111: generator_loss=4.734767436981201, discriminator_loss=0.2619308531284332\n",
            "step 112: generator_loss=4.4539031982421875, discriminator_loss=0.26135602593421936\n",
            "step 113: generator_loss=4.731020450592041, discriminator_loss=0.2600915729999542\n",
            "step 114: generator_loss=4.946103096008301, discriminator_loss=0.2569701373577118\n",
            "step 115: generator_loss=4.68813943862915, discriminator_loss=0.25696104764938354\n",
            "step 116: generator_loss=4.739267349243164, discriminator_loss=0.25499168038368225\n",
            "step 117: generator_loss=4.68137264251709, discriminator_loss=0.2544232904911041\n",
            "step 118: generator_loss=4.724268913269043, discriminator_loss=0.2542295455932617\n",
            "step 119: generator_loss=4.400676727294922, discriminator_loss=0.2523733973503113\n",
            "step 120: generator_loss=4.600804328918457, discriminator_loss=0.24960169196128845\n",
            "step 121: generator_loss=4.79948091506958, discriminator_loss=0.2472764253616333\n",
            "step 122: generator_loss=4.918092727661133, discriminator_loss=0.24533052742481232\n",
            "step 123: generator_loss=4.678013801574707, discriminator_loss=0.24610576033592224\n",
            "step 124: generator_loss=5.152431488037109, discriminator_loss=0.2422507256269455\n",
            "step 125: generator_loss=4.787982940673828, discriminator_loss=0.24357068538665771\n",
            "step 126: generator_loss=5.031857013702393, discriminator_loss=0.240473672747612\n",
            "step 127: generator_loss=4.976137161254883, discriminator_loss=0.239409938454628\n",
            "step 128: generator_loss=5.185166358947754, discriminator_loss=0.236810564994812\n",
            "step 129: generator_loss=4.907234191894531, discriminator_loss=0.23680081963539124\n",
            "step 130: generator_loss=4.82036828994751, discriminator_loss=0.23722398281097412\n",
            "step 131: generator_loss=5.008814811706543, discriminator_loss=0.23482535779476166\n",
            "step 132: generator_loss=4.8883867263793945, discriminator_loss=0.23519207537174225\n",
            "step 133: generator_loss=4.810650825500488, discriminator_loss=0.2346799224615097\n",
            "step 134: generator_loss=4.935277938842773, discriminator_loss=0.23133261501789093\n",
            "step 135: generator_loss=5.064827919006348, discriminator_loss=0.22954759001731873\n",
            "step 136: generator_loss=5.049178123474121, discriminator_loss=0.22838138043880463\n",
            "step 137: generator_loss=5.271172046661377, discriminator_loss=0.22537463903427124\n",
            "step 138: generator_loss=5.390637397766113, discriminator_loss=0.22412048280239105\n",
            "step 139: generator_loss=5.244537353515625, discriminator_loss=0.22394439578056335\n",
            "step 140: generator_loss=5.256522178649902, discriminator_loss=0.22191111743450165\n",
            "step 141: generator_loss=4.7129011154174805, discriminator_loss=0.22534236311912537\n",
            "step 142: generator_loss=4.963324546813965, discriminator_loss=0.22162483632564545\n",
            "step 143: generator_loss=4.919997692108154, discriminator_loss=0.22130829095840454\n",
            "step 144: generator_loss=4.892350196838379, discriminator_loss=0.2212280035018921\n",
            "step 145: generator_loss=4.763195037841797, discriminator_loss=0.2196495234966278\n",
            "step 146: generator_loss=4.795588493347168, discriminator_loss=0.21827752888202667\n",
            "step 147: generator_loss=4.942580699920654, discriminator_loss=0.2164343148469925\n",
            "step 148: generator_loss=4.873615741729736, discriminator_loss=0.21547363698482513\n",
            "step 149: generator_loss=4.799458980560303, discriminator_loss=0.21469171345233917\n",
            "step 150: generator_loss=4.782420635223389, discriminator_loss=0.21467263996601105\n",
            "step 151: generator_loss=4.968807220458984, discriminator_loss=0.2127259373664856\n",
            "step 152: generator_loss=4.919550895690918, discriminator_loss=0.21169401705265045\n",
            "step 153: generator_loss=5.111968517303467, discriminator_loss=0.210125133395195\n",
            "step 154: generator_loss=5.018946647644043, discriminator_loss=0.2087279111146927\n",
            "step 155: generator_loss=5.26398229598999, discriminator_loss=0.20595844089984894\n",
            "step 156: generator_loss=5.2621965408325195, discriminator_loss=0.2052268385887146\n",
            "step 157: generator_loss=5.0743255615234375, discriminator_loss=0.20525439083576202\n",
            "step 158: generator_loss=4.950392723083496, discriminator_loss=0.20475506782531738\n",
            "step 159: generator_loss=4.76242733001709, discriminator_loss=0.20539388060569763\n",
            "step 160: generator_loss=5.154055118560791, discriminator_loss=0.20175090432167053\n",
            "step 161: generator_loss=5.153646945953369, discriminator_loss=0.20085206627845764\n",
            "step 162: generator_loss=4.8072733879089355, discriminator_loss=0.20171068608760834\n",
            "step 163: generator_loss=4.946316242218018, discriminator_loss=0.20029424130916595\n",
            "step 164: generator_loss=4.828713417053223, discriminator_loss=0.19986800849437714\n",
            "step 165: generator_loss=4.936391830444336, discriminator_loss=0.19761468470096588\n",
            "step 166: generator_loss=5.128504753112793, discriminator_loss=0.19617803394794464\n",
            "step 167: generator_loss=4.8154706954956055, discriminator_loss=0.1965457797050476\n",
            "step 168: generator_loss=5.058721542358398, discriminator_loss=0.19408132135868073\n",
            "step 169: generator_loss=5.035681247711182, discriminator_loss=0.19269360601902008\n",
            "step 170: generator_loss=4.853116989135742, discriminator_loss=0.19323429465293884\n",
            "step 171: generator_loss=5.00878381729126, discriminator_loss=0.19127075374126434\n",
            "step 172: generator_loss=4.956234931945801, discriminator_loss=0.19094307720661163\n",
            "step 173: generator_loss=5.161986351013184, discriminator_loss=0.18859204649925232\n",
            "step 174: generator_loss=4.985730171203613, discriminator_loss=0.18809472024440765\n",
            "step 175: generator_loss=5.0897932052612305, discriminator_loss=0.18732070922851562\n",
            "step 176: generator_loss=5.206299304962158, discriminator_loss=0.1848607063293457\n",
            "step 177: generator_loss=5.027519702911377, discriminator_loss=0.18532903492450714\n",
            "step 178: generator_loss=4.874886989593506, discriminator_loss=0.18397384881973267\n",
            "step 179: generator_loss=4.838913440704346, discriminator_loss=0.18329215049743652\n",
            "step 180: generator_loss=4.725101470947266, discriminator_loss=0.18297941982746124\n",
            "step 181: generator_loss=4.627065181732178, discriminator_loss=0.18360763788223267\n",
            "step 182: generator_loss=4.998238563537598, discriminator_loss=0.18014511466026306\n",
            "step 183: generator_loss=4.8184967041015625, discriminator_loss=0.1791149526834488\n",
            "step 184: generator_loss=4.627399444580078, discriminator_loss=0.18041904270648956\n",
            "step 185: generator_loss=4.623793601989746, discriminator_loss=0.17936979234218597\n",
            "step 186: generator_loss=4.549319267272949, discriminator_loss=0.178457111120224\n",
            "step 187: generator_loss=4.716729164123535, discriminator_loss=0.17629265785217285\n",
            "step 188: generator_loss=4.709902286529541, discriminator_loss=0.17557917535305023\n",
            "step 189: generator_loss=4.631369590759277, discriminator_loss=0.17497341334819794\n",
            "step 190: generator_loss=4.537951946258545, discriminator_loss=0.17460514605045319\n",
            "step 191: generator_loss=4.537755012512207, discriminator_loss=0.17393559217453003\n",
            "step 192: generator_loss=4.479595184326172, discriminator_loss=0.17292018234729767\n",
            "step 193: generator_loss=4.585555076599121, discriminator_loss=0.1718868613243103\n",
            "step 194: generator_loss=4.526317596435547, discriminator_loss=0.1705804467201233\n",
            "step 195: generator_loss=4.554306983947754, discriminator_loss=0.16970491409301758\n",
            "step 196: generator_loss=4.412557601928711, discriminator_loss=0.1701286882162094\n",
            "step 197: generator_loss=4.455318450927734, discriminator_loss=0.16896866261959076\n",
            "step 198: generator_loss=4.4083051681518555, discriminator_loss=0.16828420758247375\n",
            "step 199: generator_loss=4.406913757324219, discriminator_loss=0.16769839823246002\n",
            "step 200: generator_loss=4.1278533935546875, discriminator_loss=0.16931219398975372\n",
            "step 201: generator_loss=4.382284164428711, discriminator_loss=0.16720730066299438\n",
            "step 202: generator_loss=4.458574295043945, discriminator_loss=0.16541306674480438\n",
            "step 203: generator_loss=4.4007978439331055, discriminator_loss=0.16375210881233215\n",
            "step 204: generator_loss=4.337236404418945, discriminator_loss=0.1662680208683014\n",
            "step 205: generator_loss=4.311260223388672, discriminator_loss=0.16401566565036774\n",
            "step 206: generator_loss=4.2022600173950195, discriminator_loss=0.16367074847221375\n",
            "step 207: generator_loss=4.38187313079834, discriminator_loss=0.16229330003261566\n",
            "step 208: generator_loss=4.120431423187256, discriminator_loss=0.162568137049675\n",
            "step 209: generator_loss=4.169382095336914, discriminator_loss=0.1617221236228943\n",
            "step 210: generator_loss=4.168129920959473, discriminator_loss=0.16115297377109528\n",
            "step 211: generator_loss=4.175187587738037, discriminator_loss=0.16108505427837372\n",
            "step 212: generator_loss=4.0633344650268555, discriminator_loss=0.16187222301959991\n",
            "step 213: generator_loss=4.0662994384765625, discriminator_loss=0.16006389260292053\n",
            "step 214: generator_loss=4.2066450119018555, discriminator_loss=0.15801593661308289\n",
            "step 215: generator_loss=4.0543107986450195, discriminator_loss=0.15916410088539124\n",
            "step 216: generator_loss=3.944624900817871, discriminator_loss=0.1610347032546997\n",
            "step 217: generator_loss=4.0083699226379395, discriminator_loss=0.15883192420005798\n",
            "step 218: generator_loss=4.201277256011963, discriminator_loss=0.15494515001773834\n",
            "step 219: generator_loss=3.983837366104126, discriminator_loss=0.15774226188659668\n",
            "step 220: generator_loss=4.251527786254883, discriminator_loss=0.1531568318605423\n",
            "step 221: generator_loss=4.15568208694458, discriminator_loss=0.15209834277629852\n",
            "step 222: generator_loss=3.916250705718994, discriminator_loss=0.155140221118927\n",
            "step 223: generator_loss=4.054537773132324, discriminator_loss=0.15268826484680176\n",
            "step 224: generator_loss=4.103403091430664, discriminator_loss=0.1517537385225296\n",
            "step 225: generator_loss=3.975773572921753, discriminator_loss=0.1530878096818924\n",
            "step 226: generator_loss=3.8732833862304688, discriminator_loss=0.15305478870868683\n",
            "step 227: generator_loss=3.967444896697998, discriminator_loss=0.15136122703552246\n",
            "step 228: generator_loss=3.922041177749634, discriminator_loss=0.1509176343679428\n",
            "step 229: generator_loss=4.087681293487549, discriminator_loss=0.14786793291568756\n",
            "step 230: generator_loss=3.8494458198547363, discriminator_loss=0.1518639177083969\n",
            "step 231: generator_loss=3.8443236351013184, discriminator_loss=0.1496724635362625\n",
            "step 232: generator_loss=3.9278175830841064, discriminator_loss=0.14553852379322052\n",
            "step 233: generator_loss=3.889721155166626, discriminator_loss=0.14715805649757385\n",
            "step 234: generator_loss=3.764054775238037, discriminator_loss=0.14766594767570496\n",
            "step 235: generator_loss=3.4950668811798096, discriminator_loss=0.15191656351089478\n",
            "step 236: generator_loss=3.682450771331787, discriminator_loss=0.14614351093769073\n",
            "step 237: generator_loss=3.614236354827881, discriminator_loss=0.14762310683727264\n",
            "step 238: generator_loss=3.5963680744171143, discriminator_loss=0.14597739279270172\n",
            "step 239: generator_loss=3.7481117248535156, discriminator_loss=0.1453273445367813\n",
            "step 240: generator_loss=3.6546409130096436, discriminator_loss=0.14558862149715424\n",
            "step 241: generator_loss=3.6733169555664062, discriminator_loss=0.14219897985458374\n",
            "step 242: generator_loss=3.557206392288208, discriminator_loss=0.14401854574680328\n",
            "step 243: generator_loss=3.5904018878936768, discriminator_loss=0.1428721845149994\n",
            "step 244: generator_loss=3.5640316009521484, discriminator_loss=0.14293748140335083\n",
            "step 245: generator_loss=3.5790772438049316, discriminator_loss=0.14080990850925446\n",
            "step 246: generator_loss=3.608313798904419, discriminator_loss=0.13967591524124146\n",
            "step 247: generator_loss=3.6055908203125, discriminator_loss=0.14000503718852997\n",
            "step 248: generator_loss=3.4804604053497314, discriminator_loss=0.14259299635887146\n",
            "step 249: generator_loss=3.630049705505371, discriminator_loss=0.13660836219787598\n",
            "step 250: generator_loss=3.598987579345703, discriminator_loss=0.137538343667984\n",
            "step 251: generator_loss=3.390719175338745, discriminator_loss=0.13851499557495117\n",
            "step 252: generator_loss=3.5230414867401123, discriminator_loss=0.13523074984550476\n",
            "step 253: generator_loss=3.3178656101226807, discriminator_loss=0.13794419169425964\n",
            "step 254: generator_loss=3.418257713317871, discriminator_loss=0.13530248403549194\n",
            "step 255: generator_loss=3.2488484382629395, discriminator_loss=0.13935908675193787\n",
            "step 256: generator_loss=3.2848520278930664, discriminator_loss=0.13731105625629425\n",
            "step 257: generator_loss=3.263176679611206, discriminator_loss=0.1383012980222702\n",
            "step 258: generator_loss=3.3034262657165527, discriminator_loss=0.134160578250885\n",
            "step 259: generator_loss=3.2294609546661377, discriminator_loss=0.13398867845535278\n",
            "step 260: generator_loss=3.2314529418945312, discriminator_loss=0.13340523838996887\n",
            "step 261: generator_loss=3.128230094909668, discriminator_loss=0.1358126401901245\n",
            "step 262: generator_loss=3.1850666999816895, discriminator_loss=0.13237911462783813\n",
            "step 263: generator_loss=3.0350685119628906, discriminator_loss=0.1352936178445816\n",
            "step 264: generator_loss=3.218752861022949, discriminator_loss=0.1324269026517868\n",
            "step 265: generator_loss=3.0693860054016113, discriminator_loss=0.13369616866111755\n",
            "step 266: generator_loss=3.1157619953155518, discriminator_loss=0.13181471824645996\n",
            "step 267: generator_loss=3.1083436012268066, discriminator_loss=0.13096588850021362\n",
            "step 268: generator_loss=3.0850753784179688, discriminator_loss=0.13098397850990295\n",
            "step 269: generator_loss=3.0197596549987793, discriminator_loss=0.13113652169704437\n",
            "step 270: generator_loss=3.0727498531341553, discriminator_loss=0.130662739276886\n",
            "step 271: generator_loss=2.917477607727051, discriminator_loss=0.1333209127187729\n",
            "step 272: generator_loss=2.9507033824920654, discriminator_loss=0.13178950548171997\n",
            "step 273: generator_loss=2.9265432357788086, discriminator_loss=0.13171574473381042\n",
            "step 274: generator_loss=2.914213180541992, discriminator_loss=0.1306230127811432\n",
            "step 275: generator_loss=2.906101703643799, discriminator_loss=0.13031618297100067\n",
            "step 276: generator_loss=2.9575724601745605, discriminator_loss=0.1284390389919281\n",
            "step 277: generator_loss=2.88653302192688, discriminator_loss=0.1292733997106552\n",
            "step 278: generator_loss=2.9772889614105225, discriminator_loss=0.12668214738368988\n",
            "step 279: generator_loss=2.8349595069885254, discriminator_loss=0.12913675606250763\n",
            "step 280: generator_loss=2.8105483055114746, discriminator_loss=0.1292022466659546\n",
            "step 281: generator_loss=2.871229887008667, discriminator_loss=0.12719129025936127\n",
            "step 282: generator_loss=2.8596203327178955, discriminator_loss=0.12613803148269653\n",
            "step 283: generator_loss=2.7843739986419678, discriminator_loss=0.12810790538787842\n",
            "step 284: generator_loss=2.7093820571899414, discriminator_loss=0.12939125299453735\n",
            "step 285: generator_loss=2.811694860458374, discriminator_loss=0.12561282515525818\n",
            "step 286: generator_loss=2.748424530029297, discriminator_loss=0.12709254026412964\n",
            "step 287: generator_loss=2.769198417663574, discriminator_loss=0.12621253728866577\n",
            "step 288: generator_loss=2.7074265480041504, discriminator_loss=0.1277191936969757\n",
            "step 289: generator_loss=2.633354663848877, discriminator_loss=0.12953004240989685\n",
            "step 290: generator_loss=2.739527702331543, discriminator_loss=0.125334233045578\n",
            "step 291: generator_loss=2.7323222160339355, discriminator_loss=0.1243172362446785\n",
            "step 292: generator_loss=2.6503512859344482, discriminator_loss=0.1270131915807724\n",
            "step 293: generator_loss=2.665073871612549, discriminator_loss=0.12585557997226715\n",
            "step 294: generator_loss=2.6737663745880127, discriminator_loss=0.12496403604745865\n",
            "step 295: generator_loss=2.7016239166259766, discriminator_loss=0.12298041582107544\n",
            "step 296: generator_loss=2.6046252250671387, discriminator_loss=0.12617439031600952\n",
            "step 297: generator_loss=2.6263556480407715, discriminator_loss=0.12528181076049805\n",
            "step 298: generator_loss=2.6283984184265137, discriminator_loss=0.12478525191545486\n",
            "step 299: generator_loss=2.59608793258667, discriminator_loss=0.12522348761558533\n",
            "step 300: generator_loss=2.63552188873291, discriminator_loss=0.1235535517334938\n",
            "step 301: generator_loss=2.6084656715393066, discriminator_loss=0.12378951162099838\n",
            "step 302: generator_loss=2.583252429962158, discriminator_loss=0.12533539533615112\n",
            "step 303: generator_loss=2.582735061645508, discriminator_loss=0.12459778040647507\n",
            "step 304: generator_loss=2.5995917320251465, discriminator_loss=0.1236569806933403\n",
            "step 305: generator_loss=2.589378595352173, discriminator_loss=0.12415020912885666\n",
            "step 306: generator_loss=2.6581203937530518, discriminator_loss=0.11941257119178772\n",
            "step 307: generator_loss=2.600593090057373, discriminator_loss=0.12229249626398087\n",
            "step 308: generator_loss=2.5590672492980957, discriminator_loss=0.12355750054121017\n",
            "step 309: generator_loss=2.5272092819213867, discriminator_loss=0.12555231153964996\n",
            "step 310: generator_loss=2.5142009258270264, discriminator_loss=0.1256483644247055\n",
            "step 311: generator_loss=2.4954166412353516, discriminator_loss=0.12670794129371643\n",
            "step 312: generator_loss=2.489625930786133, discriminator_loss=0.12615284323692322\n",
            "step 313: generator_loss=2.504727840423584, discriminator_loss=0.125315859913826\n",
            "step 314: generator_loss=2.4880900382995605, discriminator_loss=0.12552325427532196\n",
            "step 315: generator_loss=2.505924940109253, discriminator_loss=0.12539492547512054\n",
            "step 316: generator_loss=2.5223894119262695, discriminator_loss=0.12375257909297943\n",
            "step 317: generator_loss=2.540912628173828, discriminator_loss=0.12138655036687851\n",
            "step 318: generator_loss=2.4661736488342285, discriminator_loss=0.12609705328941345\n",
            "step 319: generator_loss=2.475396156311035, discriminator_loss=0.12549424171447754\n",
            "step 320: generator_loss=2.486880302429199, discriminator_loss=0.12521837651729584\n",
            "step 321: generator_loss=2.4609904289245605, discriminator_loss=0.12713924050331116\n",
            "step 322: generator_loss=2.397306203842163, discriminator_loss=0.13049748539924622\n",
            "step 323: generator_loss=2.493546962738037, discriminator_loss=0.12301459163427353\n",
            "step 324: generator_loss=2.437602996826172, discriminator_loss=0.12693655490875244\n",
            "step 325: generator_loss=2.3412704467773438, discriminator_loss=0.13056424260139465\n",
            "step 326: generator_loss=2.4598047733306885, discriminator_loss=0.12324153631925583\n",
            "step 327: generator_loss=2.3878865242004395, discriminator_loss=0.1282319724559784\n",
            "step 328: generator_loss=2.361931562423706, discriminator_loss=0.1300644874572754\n",
            "step 329: generator_loss=2.4465198516845703, discriminator_loss=0.12354579567909241\n",
            "step 330: generator_loss=2.3898911476135254, discriminator_loss=0.12467361986637115\n",
            "step 331: generator_loss=2.374885082244873, discriminator_loss=0.12821772694587708\n",
            "step 332: generator_loss=2.331714630126953, discriminator_loss=0.12757442891597748\n",
            "step 333: generator_loss=2.3636796474456787, discriminator_loss=0.12498761713504791\n",
            "step 334: generator_loss=2.4074435234069824, discriminator_loss=0.12229377031326294\n",
            "step 335: generator_loss=2.3447299003601074, discriminator_loss=0.12527881562709808\n",
            "step 336: generator_loss=2.3242924213409424, discriminator_loss=0.12746185064315796\n",
            "step 337: generator_loss=2.4046812057495117, discriminator_loss=0.12095747888088226\n",
            "step 338: generator_loss=2.3627090454101562, discriminator_loss=0.12288856506347656\n",
            "step 339: generator_loss=2.396409511566162, discriminator_loss=0.12058904767036438\n",
            "step 340: generator_loss=2.353203773498535, discriminator_loss=0.12262129038572311\n",
            "step 341: generator_loss=2.363995313644409, discriminator_loss=0.1229763850569725\n",
            "step 342: generator_loss=2.3354885578155518, discriminator_loss=0.12278449535369873\n",
            "step 343: generator_loss=2.342416286468506, discriminator_loss=0.12177621573209763\n",
            "step 344: generator_loss=2.3145947456359863, discriminator_loss=0.12382803112268448\n",
            "step 345: generator_loss=2.233649730682373, discriminator_loss=0.1278533637523651\n",
            "step 346: generator_loss=2.301687479019165, discriminator_loss=0.12355963885784149\n",
            "step 347: generator_loss=2.319349765777588, discriminator_loss=0.12182340025901794\n",
            "step 348: generator_loss=2.267000675201416, discriminator_loss=0.12653088569641113\n",
            "step 349: generator_loss=2.354301929473877, discriminator_loss=0.11800194531679153\n",
            "step 350: generator_loss=2.293727159500122, discriminator_loss=0.12416495382785797\n",
            "step 351: generator_loss=2.260892391204834, discriminator_loss=0.12572172284126282\n",
            "step 352: generator_loss=2.2641568183898926, discriminator_loss=0.12479296326637268\n",
            "step 353: generator_loss=2.316502809524536, discriminator_loss=0.12058477103710175\n",
            "step 354: generator_loss=2.2595138549804688, discriminator_loss=0.12306791543960571\n",
            "step 355: generator_loss=2.2840399742126465, discriminator_loss=0.12196581065654755\n",
            "step 356: generator_loss=2.2542808055877686, discriminator_loss=0.12355200946331024\n",
            "step 357: generator_loss=2.288058280944824, discriminator_loss=0.12087452411651611\n",
            "step 358: generator_loss=2.2478318214416504, discriminator_loss=0.12308535724878311\n",
            "step 359: generator_loss=2.290773868560791, discriminator_loss=0.11897195875644684\n",
            "step 360: generator_loss=2.256174087524414, discriminator_loss=0.1210494339466095\n",
            "step 361: generator_loss=2.2091970443725586, discriminator_loss=0.12399625033140182\n",
            "step 362: generator_loss=2.172985076904297, discriminator_loss=0.12587898969650269\n",
            "step 363: generator_loss=2.259519100189209, discriminator_loss=0.11849108338356018\n",
            "step 364: generator_loss=2.2196435928344727, discriminator_loss=0.1209779679775238\n",
            "step 365: generator_loss=2.2231056690216064, discriminator_loss=0.12058188021183014\n",
            "step 366: generator_loss=2.1756560802459717, discriminator_loss=0.12312132865190506\n",
            "step 367: generator_loss=2.2061610221862793, discriminator_loss=0.1201704740524292\n",
            "step 368: generator_loss=2.1292495727539062, discriminator_loss=0.12554317712783813\n",
            "step 369: generator_loss=2.1055335998535156, discriminator_loss=0.12679390609264374\n",
            "step 370: generator_loss=2.1285862922668457, discriminator_loss=0.12435297667980194\n",
            "step 371: generator_loss=2.146453857421875, discriminator_loss=0.12258529663085938\n",
            "step 372: generator_loss=2.106204032897949, discriminator_loss=0.1253838986158371\n",
            "step 373: generator_loss=2.079224109649658, discriminator_loss=0.12683117389678955\n",
            "step 374: generator_loss=2.034646511077881, discriminator_loss=0.1289764940738678\n",
            "step 375: generator_loss=2.0669026374816895, discriminator_loss=0.12634092569351196\n",
            "step 376: generator_loss=2.0559823513031006, discriminator_loss=0.12665337324142456\n",
            "step 377: generator_loss=2.0407748222351074, discriminator_loss=0.1272200047969818\n",
            "step 378: generator_loss=2.0161266326904297, discriminator_loss=0.129146009683609\n",
            "step 379: generator_loss=2.0070767402648926, discriminator_loss=0.1291297972202301\n",
            "step 380: generator_loss=2.001115322113037, discriminator_loss=0.12980249524116516\n",
            "step 381: generator_loss=2.010578155517578, discriminator_loss=0.12872862815856934\n",
            "step 382: generator_loss=1.9741239547729492, discriminator_loss=0.13126035034656525\n",
            "step 383: generator_loss=1.99534273147583, discriminator_loss=0.12945261597633362\n",
            "step 384: generator_loss=1.9915709495544434, discriminator_loss=0.12954393029212952\n",
            "step 385: generator_loss=1.977455973625183, discriminator_loss=0.1305966079235077\n",
            "step 386: generator_loss=1.9166924953460693, discriminator_loss=0.13531461358070374\n",
            "step 387: generator_loss=1.9187363386154175, discriminator_loss=0.13586601614952087\n",
            "step 388: generator_loss=1.9683032035827637, discriminator_loss=0.13225069642066956\n",
            "step 389: generator_loss=1.9032292366027832, discriminator_loss=0.13708345592021942\n",
            "step 390: generator_loss=1.9325220584869385, discriminator_loss=0.1357513964176178\n",
            "step 391: generator_loss=1.9087319374084473, discriminator_loss=0.13684076070785522\n",
            "step 392: generator_loss=1.9013454914093018, discriminator_loss=0.14162075519561768\n",
            "step 393: generator_loss=1.8435676097869873, discriminator_loss=0.14576244354248047\n",
            "step 394: generator_loss=1.8551928997039795, discriminator_loss=0.14511053264141083\n",
            "step 395: generator_loss=1.8599238395690918, discriminator_loss=0.14395928382873535\n",
            "step 396: generator_loss=1.848850131034851, discriminator_loss=0.15112075209617615\n",
            "step 397: generator_loss=1.8468879461288452, discriminator_loss=0.1506934016942978\n",
            "step 398: generator_loss=1.927785038948059, discriminator_loss=0.14168089628219604\n",
            "step 399: generator_loss=1.7724794149398804, discriminator_loss=0.15885627269744873\n",
            "step 400: generator_loss=1.9261234998703003, discriminator_loss=0.14124375581741333\n",
            "step 401: generator_loss=1.9517626762390137, discriminator_loss=0.14392998814582825\n",
            "step 402: generator_loss=1.913423776626587, discriminator_loss=0.14492353796958923\n",
            "step 403: generator_loss=1.8726003170013428, discriminator_loss=0.15300318598747253\n",
            "step 404: generator_loss=1.8559751510620117, discriminator_loss=0.1546896994113922\n",
            "step 405: generator_loss=1.961279034614563, discriminator_loss=0.1461842805147171\n",
            "step 406: generator_loss=2.028210401535034, discriminator_loss=0.1422111690044403\n",
            "step 407: generator_loss=1.9381343126296997, discriminator_loss=0.1465064287185669\n",
            "step 408: generator_loss=1.9399197101593018, discriminator_loss=0.1496589183807373\n",
            "step 409: generator_loss=2.001866579055786, discriminator_loss=0.14214172959327698\n",
            "step 410: generator_loss=1.9459989070892334, discriminator_loss=0.14851973950862885\n",
            "step 411: generator_loss=1.9904630184173584, discriminator_loss=0.1430528312921524\n",
            "step 412: generator_loss=2.0087857246398926, discriminator_loss=0.13856878876686096\n",
            "step 413: generator_loss=2.1370372772216797, discriminator_loss=0.12910649180412292\n",
            "step 414: generator_loss=2.0806057453155518, discriminator_loss=0.13257184624671936\n",
            "step 415: generator_loss=2.112328052520752, discriminator_loss=0.13116788864135742\n",
            "step 416: generator_loss=2.084505558013916, discriminator_loss=0.1301271915435791\n",
            "step 417: generator_loss=2.191347599029541, discriminator_loss=0.12012971937656403\n",
            "step 418: generator_loss=2.113034725189209, discriminator_loss=0.12900292873382568\n",
            "step 419: generator_loss=2.1158924102783203, discriminator_loss=0.12576812505722046\n",
            "step 420: generator_loss=2.1874735355377197, discriminator_loss=0.12081678956747055\n",
            "step 421: generator_loss=2.173048973083496, discriminator_loss=0.12038886547088623\n",
            "step 422: generator_loss=2.1526174545288086, discriminator_loss=0.1226358711719513\n",
            "step 423: generator_loss=2.2489047050476074, discriminator_loss=0.11423808336257935\n",
            "step 424: generator_loss=2.1750543117523193, discriminator_loss=0.12194390594959259\n",
            "step 425: generator_loss=2.104954957962036, discriminator_loss=0.1223636120557785\n",
            "step 426: generator_loss=2.142263412475586, discriminator_loss=0.12135349959135056\n",
            "step 427: generator_loss=2.098771572113037, discriminator_loss=0.12265181541442871\n",
            "step 428: generator_loss=2.1246089935302734, discriminator_loss=0.12066366523504257\n",
            "step 429: generator_loss=2.1946253776550293, discriminator_loss=0.11908785998821259\n",
            "step 430: generator_loss=2.1545634269714355, discriminator_loss=0.12020359933376312\n",
            "step 431: generator_loss=2.1048779487609863, discriminator_loss=0.12380039691925049\n",
            "step 432: generator_loss=2.1292145252227783, discriminator_loss=0.12180408835411072\n",
            "step 433: generator_loss=1.9836924076080322, discriminator_loss=0.12830151617527008\n",
            "step 434: generator_loss=2.015106201171875, discriminator_loss=0.12811723351478577\n",
            "step 435: generator_loss=2.0152692794799805, discriminator_loss=0.12803992629051208\n",
            "step 436: generator_loss=2.0702552795410156, discriminator_loss=0.12517784535884857\n",
            "step 437: generator_loss=2.0203514099121094, discriminator_loss=0.12775838375091553\n",
            "step 438: generator_loss=1.9850423336029053, discriminator_loss=0.13130684196949005\n",
            "step 439: generator_loss=1.971222162246704, discriminator_loss=0.13194109499454498\n",
            "step 440: generator_loss=2.0209226608276367, discriminator_loss=0.12884141504764557\n",
            "step 441: generator_loss=1.9742662906646729, discriminator_loss=0.12832669913768768\n",
            "step 442: generator_loss=1.803017020225525, discriminator_loss=0.14341579377651215\n",
            "step 443: generator_loss=1.8704835176467896, discriminator_loss=0.1377382129430771\n",
            "step 444: generator_loss=1.7924096584320068, discriminator_loss=0.14442360401153564\n",
            "step 445: generator_loss=1.80918550491333, discriminator_loss=0.14135625958442688\n",
            "step 446: generator_loss=1.731179118156433, discriminator_loss=0.1495494544506073\n",
            "step 447: generator_loss=1.845862627029419, discriminator_loss=0.13987284898757935\n",
            "step 448: generator_loss=1.8630146980285645, discriminator_loss=0.13602617383003235\n",
            "step 449: generator_loss=1.8091686964035034, discriminator_loss=0.14286872744560242\n",
            "step 450: generator_loss=1.8056902885437012, discriminator_loss=0.14150701463222504\n",
            "step 451: generator_loss=1.7669048309326172, discriminator_loss=0.14530807733535767\n",
            "step 452: generator_loss=1.7412474155426025, discriminator_loss=0.14745116233825684\n",
            "step 453: generator_loss=1.686995506286621, discriminator_loss=0.15372590720653534\n",
            "step 454: generator_loss=1.6792232990264893, discriminator_loss=0.15518522262573242\n",
            "step 455: generator_loss=1.6793208122253418, discriminator_loss=0.15336298942565918\n",
            "step 456: generator_loss=1.655683994293213, discriminator_loss=0.1572127342224121\n",
            "step 457: generator_loss=1.6780198812484741, discriminator_loss=0.1544049233198166\n",
            "step 458: generator_loss=1.6259715557098389, discriminator_loss=0.16061295568943024\n",
            "step 459: generator_loss=1.589457631111145, discriminator_loss=0.16444645822048187\n",
            "step 460: generator_loss=1.5594176054000854, discriminator_loss=0.16867050528526306\n",
            "step 461: generator_loss=1.5594240427017212, discriminator_loss=0.1687052845954895\n",
            "step 462: generator_loss=1.5228486061096191, discriminator_loss=0.17358101904392242\n",
            "step 463: generator_loss=1.4981350898742676, discriminator_loss=0.17751353979110718\n",
            "step 464: generator_loss=1.540653944015503, discriminator_loss=0.1726292371749878\n",
            "step 465: generator_loss=1.5057194232940674, discriminator_loss=0.17811763286590576\n",
            "step 466: generator_loss=1.4929680824279785, discriminator_loss=0.17907479405403137\n",
            "step 467: generator_loss=1.474006175994873, discriminator_loss=0.18287788331508636\n",
            "step 468: generator_loss=1.4043793678283691, discriminator_loss=0.19441553950309753\n",
            "step 469: generator_loss=1.4243332147598267, discriminator_loss=0.19183726608753204\n",
            "step 470: generator_loss=1.4283742904663086, discriminator_loss=0.19390356540679932\n",
            "step 471: generator_loss=1.42642343044281, discriminator_loss=0.19634412229061127\n",
            "step 472: generator_loss=1.377361536026001, discriminator_loss=0.20304660499095917\n",
            "step 473: generator_loss=1.32159423828125, discriminator_loss=0.2146657556295395\n",
            "step 474: generator_loss=1.2730016708374023, discriminator_loss=0.22665347158908844\n",
            "step 475: generator_loss=1.2534531354904175, discriminator_loss=0.23449698090553284\n",
            "step 476: generator_loss=1.181480050086975, discriminator_loss=0.24880364537239075\n",
            "step 477: generator_loss=1.1678298711776733, discriminator_loss=0.2585843503475189\n",
            "step 478: generator_loss=1.102134108543396, discriminator_loss=0.27636006474494934\n",
            "step 479: generator_loss=0.9794782400131226, discriminator_loss=0.3228311240673065\n",
            "step 480: generator_loss=0.9680156707763672, discriminator_loss=0.32214367389678955\n",
            "step 481: generator_loss=0.8754009008407593, discriminator_loss=0.3699507415294647\n",
            "step 482: generator_loss=0.757529616355896, discriminator_loss=0.42263171076774597\n",
            "step 483: generator_loss=0.6984531283378601, discriminator_loss=0.45077794790267944\n",
            "step 484: generator_loss=0.595032811164856, discriminator_loss=0.5227760076522827\n",
            "step 485: generator_loss=0.525945782661438, discriminator_loss=0.5841646790504456\n",
            "step 486: generator_loss=0.49213486909866333, discriminator_loss=0.6151954531669617\n",
            "step 487: generator_loss=0.4051577150821686, discriminator_loss=0.7091968059539795\n",
            "step 488: generator_loss=0.4155459403991699, discriminator_loss=0.711823046207428\n",
            "step 489: generator_loss=0.3880477547645569, discriminator_loss=0.7309683561325073\n",
            "step 490: generator_loss=0.3730129897594452, discriminator_loss=0.7745649218559265\n",
            "step 491: generator_loss=0.3787892758846283, discriminator_loss=0.780057966709137\n",
            "step 492: generator_loss=0.39510029554367065, discriminator_loss=0.7536184787750244\n",
            "step 493: generator_loss=0.4023730158805847, discriminator_loss=0.7435898780822754\n",
            "step 494: generator_loss=0.4557596445083618, discriminator_loss=0.7152833938598633\n",
            "step 495: generator_loss=0.5201162695884705, discriminator_loss=0.669002115726471\n",
            "step 496: generator_loss=0.5126742124557495, discriminator_loss=0.6611548662185669\n",
            "step 497: generator_loss=0.5826293230056763, discriminator_loss=0.6204389333724976\n",
            "step 498: generator_loss=0.7229427099227905, discriminator_loss=0.5374075770378113\n",
            "step 499: generator_loss=0.7462050914764404, discriminator_loss=0.536557137966156\n",
            "step 500: generator_loss=0.7579736709594727, discriminator_loss=0.5367876887321472\n",
            "step 501: generator_loss=0.8442507386207581, discriminator_loss=0.4872460961341858\n",
            "step 502: generator_loss=0.8398370146751404, discriminator_loss=0.48755502700805664\n",
            "step 503: generator_loss=0.903383731842041, discriminator_loss=0.4741421937942505\n",
            "step 504: generator_loss=0.9349696636199951, discriminator_loss=0.47209322452545166\n",
            "step 505: generator_loss=0.9045015573501587, discriminator_loss=0.49329033493995667\n",
            "step 506: generator_loss=0.9676848649978638, discriminator_loss=0.48218053579330444\n",
            "step 507: generator_loss=0.9778931736946106, discriminator_loss=0.4802054464817047\n",
            "step 508: generator_loss=1.0411957502365112, discriminator_loss=0.4729885160923004\n",
            "step 509: generator_loss=1.0095700025558472, discriminator_loss=0.5035266876220703\n",
            "step 510: generator_loss=1.0097951889038086, discriminator_loss=0.5090118050575256\n",
            "step 511: generator_loss=1.021494746208191, discriminator_loss=0.5188225507736206\n",
            "step 512: generator_loss=1.0353024005889893, discriminator_loss=0.5192960500717163\n",
            "step 513: generator_loss=1.0474250316619873, discriminator_loss=0.5280522704124451\n",
            "step 514: generator_loss=1.064105749130249, discriminator_loss=0.5340546369552612\n",
            "step 515: generator_loss=1.0558526515960693, discriminator_loss=0.5441590547561646\n",
            "step 516: generator_loss=1.0598315000534058, discriminator_loss=0.5697239637374878\n",
            "step 517: generator_loss=1.0163209438323975, discriminator_loss=0.5776245594024658\n",
            "step 518: generator_loss=0.9533852338790894, discriminator_loss=0.6240137815475464\n",
            "step 519: generator_loss=0.9994926452636719, discriminator_loss=0.6584752798080444\n",
            "step 520: generator_loss=1.058143138885498, discriminator_loss=0.6271584033966064\n",
            "step 521: generator_loss=1.079556941986084, discriminator_loss=0.652480959892273\n",
            "step 522: generator_loss=1.0494390726089478, discriminator_loss=0.7144023180007935\n",
            "step 523: generator_loss=1.0005991458892822, discriminator_loss=0.8004248142242432\n",
            "step 524: generator_loss=1.0206620693206787, discriminator_loss=0.7888047695159912\n",
            "step 525: generator_loss=0.9723718762397766, discriminator_loss=0.8400859832763672\n",
            "step 526: generator_loss=1.0946834087371826, discriminator_loss=0.9829302430152893\n",
            "step 527: generator_loss=1.128830909729004, discriminator_loss=0.971711277961731\n",
            "step 528: generator_loss=1.1164400577545166, discriminator_loss=0.9758785367012024\n",
            "step 529: generator_loss=1.1698663234710693, discriminator_loss=1.0835635662078857\n",
            "step 530: generator_loss=1.335498571395874, discriminator_loss=1.1235803365707397\n",
            "step 531: generator_loss=1.4138318300247192, discriminator_loss=1.1040000915527344\n",
            "step 532: generator_loss=1.5722031593322754, discriminator_loss=1.115050196647644\n",
            "step 533: generator_loss=1.5086220502853394, discriminator_loss=1.1757216453552246\n",
            "step 534: generator_loss=1.792896032333374, discriminator_loss=1.0414717197418213\n",
            "step 535: generator_loss=1.3721511363983154, discriminator_loss=1.1698719263076782\n",
            "step 536: generator_loss=1.7639983892440796, discriminator_loss=1.0752723217010498\n",
            "step 537: generator_loss=1.8891987800598145, discriminator_loss=0.9680914282798767\n",
            "step 538: generator_loss=2.2417354583740234, discriminator_loss=0.9640920758247375\n",
            "step 539: generator_loss=2.2137842178344727, discriminator_loss=0.8328683376312256\n",
            "step 540: generator_loss=2.466623067855835, discriminator_loss=0.8509862422943115\n",
            "step 541: generator_loss=2.1426327228546143, discriminator_loss=0.8431989550590515\n",
            "step 542: generator_loss=2.472795248031616, discriminator_loss=0.8101154565811157\n",
            "step 543: generator_loss=2.092620849609375, discriminator_loss=0.8714753985404968\n",
            "step 544: generator_loss=2.82658052444458, discriminator_loss=0.6492196321487427\n",
            "step 545: generator_loss=2.5575318336486816, discriminator_loss=0.7144635319709778\n",
            "step 546: generator_loss=2.3981637954711914, discriminator_loss=0.7074335813522339\n",
            "step 547: generator_loss=2.6490161418914795, discriminator_loss=0.6306902766227722\n",
            "step 548: generator_loss=2.738253116607666, discriminator_loss=0.5562058687210083\n",
            "step 549: generator_loss=2.651111125946045, discriminator_loss=0.559755802154541\n",
            "step 550: generator_loss=3.0888404846191406, discriminator_loss=0.4302715063095093\n",
            "step 551: generator_loss=2.863145351409912, discriminator_loss=0.42893898487091064\n",
            "step 552: generator_loss=2.9278793334960938, discriminator_loss=0.3911612629890442\n",
            "step 553: generator_loss=3.0281152725219727, discriminator_loss=0.3860907554626465\n",
            "step 554: generator_loss=2.8618521690368652, discriminator_loss=0.3843025863170624\n",
            "step 555: generator_loss=3.2425639629364014, discriminator_loss=0.3339688181877136\n",
            "step 556: generator_loss=2.959620475769043, discriminator_loss=0.3474990129470825\n",
            "step 557: generator_loss=2.9680442810058594, discriminator_loss=0.343660444021225\n",
            "step 558: generator_loss=3.229257106781006, discriminator_loss=0.32975178956985474\n",
            "step 559: generator_loss=3.1662211418151855, discriminator_loss=0.32521864771842957\n",
            "step 560: generator_loss=3.0240769386291504, discriminator_loss=0.3343244194984436\n",
            "step 561: generator_loss=2.8449277877807617, discriminator_loss=0.3636060357093811\n",
            "step 562: generator_loss=2.840488910675049, discriminator_loss=0.3687300682067871\n",
            "step 563: generator_loss=3.200463056564331, discriminator_loss=0.32923534512519836\n",
            "step 564: generator_loss=2.8311383724212646, discriminator_loss=0.380809485912323\n",
            "step 565: generator_loss=2.983926773071289, discriminator_loss=0.3757334351539612\n",
            "step 566: generator_loss=2.8261539936065674, discriminator_loss=0.42560526728630066\n",
            "step 567: generator_loss=2.825108289718628, discriminator_loss=0.42178285121917725\n",
            "step 568: generator_loss=2.638108253479004, discriminator_loss=0.45881155133247375\n",
            "step 569: generator_loss=3.418339967727661, discriminator_loss=0.36439216136932373\n",
            "step 570: generator_loss=3.040079355239868, discriminator_loss=0.4547196626663208\n",
            "step 571: generator_loss=3.249546766281128, discriminator_loss=0.3455325961112976\n",
            "step 572: generator_loss=3.003796100616455, discriminator_loss=0.42118018865585327\n",
            "step 573: generator_loss=3.647143840789795, discriminator_loss=0.41529929637908936\n",
            "step 574: generator_loss=3.837285041809082, discriminator_loss=0.401467502117157\n",
            "step 575: generator_loss=4.025603771209717, discriminator_loss=0.3665871024131775\n",
            "step 576: generator_loss=3.5496420860290527, discriminator_loss=0.41946175694465637\n",
            "step 577: generator_loss=4.252781867980957, discriminator_loss=0.39243292808532715\n",
            "step 578: generator_loss=4.120530605316162, discriminator_loss=0.33916208148002625\n",
            "step 579: generator_loss=4.362554550170898, discriminator_loss=0.36260420083999634\n",
            "step 580: generator_loss=4.244803428649902, discriminator_loss=0.29549524188041687\n",
            "step 581: generator_loss=4.2852864265441895, discriminator_loss=0.3313145339488983\n",
            "step 582: generator_loss=4.025786399841309, discriminator_loss=0.2859605550765991\n",
            "step 583: generator_loss=4.277022361755371, discriminator_loss=0.3053773045539856\n",
            "step 584: generator_loss=4.2308244705200195, discriminator_loss=0.27370694279670715\n",
            "step 585: generator_loss=4.686106204986572, discriminator_loss=0.2372734248638153\n",
            "step 586: generator_loss=4.533347129821777, discriminator_loss=0.23216208815574646\n",
            "step 587: generator_loss=4.660206317901611, discriminator_loss=0.22896936535835266\n",
            "step 588: generator_loss=4.292135715484619, discriminator_loss=0.2287675440311432\n",
            "step 589: generator_loss=4.572879791259766, discriminator_loss=0.21763549745082855\n",
            "step 590: generator_loss=4.147250175476074, discriminator_loss=0.2217869758605957\n",
            "step 591: generator_loss=3.952439546585083, discriminator_loss=0.2232811450958252\n",
            "step 592: generator_loss=4.114667892456055, discriminator_loss=0.2153111696243286\n",
            "step 593: generator_loss=3.8187389373779297, discriminator_loss=0.22057278454303741\n",
            "step 594: generator_loss=3.6404449939727783, discriminator_loss=0.21931913495063782\n",
            "step 595: generator_loss=3.229645252227783, discriminator_loss=0.22811061143875122\n",
            "step 596: generator_loss=3.31314754486084, discriminator_loss=0.23344913125038147\n",
            "step 597: generator_loss=2.974766254425049, discriminator_loss=0.2424805760383606\n",
            "step 598: generator_loss=2.8489129543304443, discriminator_loss=0.24983292818069458\n",
            "step 599: generator_loss=2.66120982170105, discriminator_loss=0.25078481435775757\n",
            "step 600: generator_loss=2.5626330375671387, discriminator_loss=0.26127445697784424\n",
            "step 601: generator_loss=2.656649351119995, discriminator_loss=0.24733147025108337\n",
            "step 602: generator_loss=2.686176061630249, discriminator_loss=0.2503833770751953\n",
            "step 603: generator_loss=2.6924962997436523, discriminator_loss=0.2823715806007385\n",
            "step 604: generator_loss=2.9988691806793213, discriminator_loss=0.23950865864753723\n",
            "step 605: generator_loss=3.0586588382720947, discriminator_loss=0.24807843565940857\n",
            "step 606: generator_loss=3.40022349357605, discriminator_loss=0.2186288833618164\n",
            "step 607: generator_loss=3.405318260192871, discriminator_loss=0.2211829125881195\n",
            "step 608: generator_loss=3.5935425758361816, discriminator_loss=0.21463534235954285\n",
            "step 609: generator_loss=3.9766712188720703, discriminator_loss=0.21411864459514618\n",
            "step 610: generator_loss=4.051143169403076, discriminator_loss=0.20838923752307892\n",
            "step 611: generator_loss=3.8982200622558594, discriminator_loss=0.2134712040424347\n",
            "step 612: generator_loss=4.196732044219971, discriminator_loss=0.2087080478668213\n",
            "step 613: generator_loss=4.079802989959717, discriminator_loss=0.21597132086753845\n",
            "step 614: generator_loss=4.143760681152344, discriminator_loss=0.20598891377449036\n",
            "step 615: generator_loss=4.226899147033691, discriminator_loss=0.20857937633991241\n",
            "step 616: generator_loss=4.077727317810059, discriminator_loss=0.2337135374546051\n",
            "step 617: generator_loss=4.33261251449585, discriminator_loss=0.21393626928329468\n",
            "step 618: generator_loss=4.306483268737793, discriminator_loss=0.2193848341703415\n",
            "step 619: generator_loss=4.143098831176758, discriminator_loss=0.2238515317440033\n",
            "step 620: generator_loss=4.503665924072266, discriminator_loss=0.21500079333782196\n",
            "step 621: generator_loss=4.254133224487305, discriminator_loss=0.2158781886100769\n",
            "step 622: generator_loss=4.348444938659668, discriminator_loss=0.23080211877822876\n",
            "step 623: generator_loss=4.418240547180176, discriminator_loss=0.21021032333374023\n",
            "step 624: generator_loss=4.366626739501953, discriminator_loss=0.20158299803733826\n",
            "step 625: generator_loss=4.339786052703857, discriminator_loss=0.22068116068840027\n",
            "step 626: generator_loss=4.340339660644531, discriminator_loss=0.20411604642868042\n",
            "step 627: generator_loss=4.05738639831543, discriminator_loss=0.21698957681655884\n",
            "step 628: generator_loss=4.1622395515441895, discriminator_loss=0.20416705310344696\n",
            "step 629: generator_loss=4.058492660522461, discriminator_loss=0.21347102522850037\n",
            "step 630: generator_loss=4.298579216003418, discriminator_loss=0.1969471275806427\n",
            "step 631: generator_loss=3.9371581077575684, discriminator_loss=0.21062932908535004\n",
            "step 632: generator_loss=4.078248500823975, discriminator_loss=0.20631331205368042\n",
            "step 633: generator_loss=4.067564964294434, discriminator_loss=0.2046341598033905\n",
            "step 634: generator_loss=4.0994110107421875, discriminator_loss=0.19365838170051575\n",
            "step 635: generator_loss=3.99704647064209, discriminator_loss=0.19608812034130096\n",
            "step 636: generator_loss=4.264162540435791, discriminator_loss=0.18873991072177887\n",
            "step 637: generator_loss=3.956234931945801, discriminator_loss=0.19329184293746948\n",
            "step 638: generator_loss=4.121382713317871, discriminator_loss=0.19018903374671936\n",
            "step 639: generator_loss=3.729515552520752, discriminator_loss=0.19358880817890167\n",
            "step 640: generator_loss=3.91141939163208, discriminator_loss=0.18735069036483765\n",
            "step 641: generator_loss=3.6099114418029785, discriminator_loss=0.19754339754581451\n",
            "step 642: generator_loss=3.6698834896087646, discriminator_loss=0.19597341120243073\n",
            "step 643: generator_loss=3.641010046005249, discriminator_loss=0.1936003565788269\n",
            "step 644: generator_loss=3.7080559730529785, discriminator_loss=0.195283904671669\n",
            "step 645: generator_loss=3.2907700538635254, discriminator_loss=0.20510414242744446\n",
            "step 646: generator_loss=3.2864840030670166, discriminator_loss=0.20092201232910156\n",
            "step 647: generator_loss=3.441532850265503, discriminator_loss=0.20449447631835938\n",
            "step 648: generator_loss=3.3533596992492676, discriminator_loss=0.20521830022335052\n",
            "step 649: generator_loss=3.0943472385406494, discriminator_loss=0.20813891291618347\n",
            "step 650: generator_loss=2.978030204772949, discriminator_loss=0.21583372354507446\n",
            "step 651: generator_loss=3.038086414337158, discriminator_loss=0.21380388736724854\n",
            "step 652: generator_loss=3.203456401824951, discriminator_loss=0.2072247713804245\n",
            "step 653: generator_loss=3.13613224029541, discriminator_loss=0.20566493272781372\n",
            "step 654: generator_loss=3.137472629547119, discriminator_loss=0.21227994561195374\n",
            "step 655: generator_loss=3.0906898975372314, discriminator_loss=0.21390505135059357\n",
            "step 656: generator_loss=3.010101795196533, discriminator_loss=0.21673698723316193\n",
            "step 657: generator_loss=3.166071891784668, discriminator_loss=0.21332629024982452\n",
            "step 658: generator_loss=3.126408576965332, discriminator_loss=0.21622757613658905\n",
            "step 659: generator_loss=2.980762481689453, discriminator_loss=0.21598008275032043\n",
            "step 660: generator_loss=2.954707384109497, discriminator_loss=0.2241845577955246\n",
            "step 661: generator_loss=3.1489791870117188, discriminator_loss=0.21392375230789185\n",
            "step 662: generator_loss=3.0847017765045166, discriminator_loss=0.2144082635641098\n",
            "step 663: generator_loss=3.0047638416290283, discriminator_loss=0.21918630599975586\n",
            "step 664: generator_loss=3.0809617042541504, discriminator_loss=0.2166650891304016\n",
            "step 665: generator_loss=3.261953353881836, discriminator_loss=0.2078111618757248\n",
            "step 666: generator_loss=3.3384110927581787, discriminator_loss=0.20297476649284363\n",
            "step 667: generator_loss=3.288797378540039, discriminator_loss=0.2047332227230072\n",
            "step 668: generator_loss=3.5228841304779053, discriminator_loss=0.19654548168182373\n",
            "step 669: generator_loss=3.474632501602173, discriminator_loss=0.20169727504253387\n",
            "step 670: generator_loss=3.4402916431427, discriminator_loss=0.2000698745250702\n",
            "step 671: generator_loss=3.379288911819458, discriminator_loss=0.19908952713012695\n",
            "step 672: generator_loss=3.190373182296753, discriminator_loss=0.20872461795806885\n",
            "step 673: generator_loss=3.125101089477539, discriminator_loss=0.2063349485397339\n",
            "step 674: generator_loss=3.198495388031006, discriminator_loss=0.21334640681743622\n",
            "step 675: generator_loss=3.151370048522949, discriminator_loss=0.2060883641242981\n",
            "step 676: generator_loss=3.2263314723968506, discriminator_loss=0.207602858543396\n",
            "step 677: generator_loss=2.930366039276123, discriminator_loss=0.21095135807991028\n",
            "step 678: generator_loss=3.010756492614746, discriminator_loss=0.2117883712053299\n",
            "step 679: generator_loss=2.7930827140808105, discriminator_loss=0.21836254000663757\n",
            "step 680: generator_loss=2.8310227394104004, discriminator_loss=0.219595804810524\n",
            "step 681: generator_loss=3.086658239364624, discriminator_loss=0.20368757843971252\n",
            "step 682: generator_loss=3.022325277328491, discriminator_loss=0.21193179488182068\n",
            "step 683: generator_loss=2.9967212677001953, discriminator_loss=0.21107009053230286\n",
            "step 684: generator_loss=2.7847063541412354, discriminator_loss=0.215773344039917\n",
            "step 685: generator_loss=3.019021511077881, discriminator_loss=0.20619504153728485\n",
            "step 686: generator_loss=2.742452621459961, discriminator_loss=0.213483065366745\n",
            "step 687: generator_loss=3.0622615814208984, discriminator_loss=0.20431137084960938\n",
            "step 688: generator_loss=2.936955451965332, discriminator_loss=0.21500010788440704\n",
            "step 689: generator_loss=3.113403558731079, discriminator_loss=0.20311316847801208\n",
            "step 690: generator_loss=2.7061424255371094, discriminator_loss=0.2134772390127182\n",
            "step 691: generator_loss=2.877912998199463, discriminator_loss=0.2049148976802826\n",
            "step 692: generator_loss=2.86125111579895, discriminator_loss=0.2082349956035614\n",
            "step 693: generator_loss=2.782822608947754, discriminator_loss=0.21211223304271698\n",
            "step 694: generator_loss=2.9926986694335938, discriminator_loss=0.20494350790977478\n",
            "step 695: generator_loss=2.730229616165161, discriminator_loss=0.21028770506381989\n",
            "step 696: generator_loss=2.7391247749328613, discriminator_loss=0.21553927659988403\n",
            "step 697: generator_loss=2.747847557067871, discriminator_loss=0.21037523448467255\n",
            "step 698: generator_loss=2.568039894104004, discriminator_loss=0.2205706536769867\n",
            "step 699: generator_loss=2.71895170211792, discriminator_loss=0.20942828059196472\n",
            "step 700: generator_loss=2.8770852088928223, discriminator_loss=0.21101021766662598\n",
            "step 701: generator_loss=2.654078483581543, discriminator_loss=0.2107410430908203\n",
            "step 702: generator_loss=2.6153464317321777, discriminator_loss=0.2115793377161026\n",
            "step 703: generator_loss=2.4267401695251465, discriminator_loss=0.22314152121543884\n",
            "step 704: generator_loss=2.5896902084350586, discriminator_loss=0.21922028064727783\n",
            "step 705: generator_loss=2.6289172172546387, discriminator_loss=0.2182680070400238\n",
            "step 706: generator_loss=2.6579501628875732, discriminator_loss=0.21581174433231354\n",
            "step 707: generator_loss=2.443027973175049, discriminator_loss=0.22171244025230408\n",
            "step 708: generator_loss=2.7580971717834473, discriminator_loss=0.21497264504432678\n",
            "step 709: generator_loss=2.422771692276001, discriminator_loss=0.22572457790374756\n",
            "step 710: generator_loss=2.410590410232544, discriminator_loss=0.22550523281097412\n",
            "step 711: generator_loss=2.4269442558288574, discriminator_loss=0.22314439713954926\n",
            "step 712: generator_loss=2.3249351978302, discriminator_loss=0.22780176997184753\n",
            "step 713: generator_loss=2.4558756351470947, discriminator_loss=0.22055894136428833\n",
            "step 714: generator_loss=2.175288200378418, discriminator_loss=0.23393650352954865\n",
            "step 715: generator_loss=2.0587658882141113, discriminator_loss=0.24397405982017517\n",
            "step 716: generator_loss=2.305046558380127, discriminator_loss=0.2290615737438202\n",
            "step 717: generator_loss=2.1831135749816895, discriminator_loss=0.23999494314193726\n",
            "step 718: generator_loss=2.227126121520996, discriminator_loss=0.22889845073223114\n",
            "step 719: generator_loss=2.234928846359253, discriminator_loss=0.23192214965820312\n",
            "step 720: generator_loss=2.226621150970459, discriminator_loss=0.24036520719528198\n",
            "step 721: generator_loss=2.173449993133545, discriminator_loss=0.2413502186536789\n",
            "step 722: generator_loss=2.130673885345459, discriminator_loss=0.23556242883205414\n",
            "step 723: generator_loss=2.184607982635498, discriminator_loss=0.235681414604187\n",
            "step 724: generator_loss=2.1346988677978516, discriminator_loss=0.23517395555973053\n",
            "step 725: generator_loss=1.9890419244766235, discriminator_loss=0.2421320229768753\n",
            "step 726: generator_loss=2.137329578399658, discriminator_loss=0.23182126879692078\n",
            "step 727: generator_loss=2.183037281036377, discriminator_loss=0.23068484663963318\n",
            "step 728: generator_loss=1.9992256164550781, discriminator_loss=0.23813121020793915\n",
            "step 729: generator_loss=2.051135301589966, discriminator_loss=0.2346210479736328\n",
            "step 730: generator_loss=2.124912977218628, discriminator_loss=0.23165886104106903\n",
            "step 731: generator_loss=2.171330690383911, discriminator_loss=0.23097294569015503\n",
            "step 732: generator_loss=2.0007236003875732, discriminator_loss=0.23809918761253357\n",
            "step 733: generator_loss=2.063112258911133, discriminator_loss=0.23254276812076569\n",
            "step 734: generator_loss=2.1634130477905273, discriminator_loss=0.22146382927894592\n",
            "step 735: generator_loss=2.1528778076171875, discriminator_loss=0.22463712096214294\n",
            "step 736: generator_loss=2.0289430618286133, discriminator_loss=0.22909463942050934\n",
            "step 737: generator_loss=2.148010492324829, discriminator_loss=0.22301708161830902\n",
            "step 738: generator_loss=2.126695394515991, discriminator_loss=0.22597458958625793\n",
            "step 739: generator_loss=2.1044812202453613, discriminator_loss=0.2227027714252472\n",
            "step 740: generator_loss=2.1651835441589355, discriminator_loss=0.2142421007156372\n",
            "step 741: generator_loss=2.1561079025268555, discriminator_loss=0.21911504864692688\n",
            "step 742: generator_loss=2.103997230529785, discriminator_loss=0.21793708205223083\n",
            "step 743: generator_loss=2.1455302238464355, discriminator_loss=0.220912903547287\n",
            "step 744: generator_loss=2.0328011512756348, discriminator_loss=0.22097176313400269\n",
            "step 745: generator_loss=2.1459155082702637, discriminator_loss=0.21495196223258972\n",
            "step 746: generator_loss=2.151724338531494, discriminator_loss=0.2164076566696167\n",
            "step 747: generator_loss=2.1892125606536865, discriminator_loss=0.21364443004131317\n",
            "step 748: generator_loss=2.2601070404052734, discriminator_loss=0.21016378700733185\n",
            "step 749: generator_loss=2.245723247528076, discriminator_loss=0.20858551561832428\n",
            "step 750: generator_loss=2.284400463104248, discriminator_loss=0.20550407469272614\n",
            "step 751: generator_loss=2.210782527923584, discriminator_loss=0.21090994775295258\n",
            "step 752: generator_loss=2.221848249435425, discriminator_loss=0.20815710723400116\n",
            "step 753: generator_loss=2.186554431915283, discriminator_loss=0.2140469253063202\n",
            "step 754: generator_loss=2.2368335723876953, discriminator_loss=0.2134876251220703\n",
            "step 755: generator_loss=2.1238675117492676, discriminator_loss=0.21368494629859924\n",
            "step 756: generator_loss=2.281309127807617, discriminator_loss=0.20740634202957153\n",
            "step 757: generator_loss=2.2494096755981445, discriminator_loss=0.20852670073509216\n",
            "step 758: generator_loss=2.204354763031006, discriminator_loss=0.20967495441436768\n",
            "step 759: generator_loss=2.2580273151397705, discriminator_loss=0.20568208396434784\n",
            "step 760: generator_loss=2.1863269805908203, discriminator_loss=0.21194060146808624\n",
            "step 761: generator_loss=2.1375656127929688, discriminator_loss=0.21322354674339294\n",
            "step 762: generator_loss=2.12754487991333, discriminator_loss=0.21441352367401123\n",
            "step 763: generator_loss=2.052656412124634, discriminator_loss=0.22030071914196014\n",
            "step 764: generator_loss=2.1726441383361816, discriminator_loss=0.2111506164073944\n",
            "step 765: generator_loss=2.1058549880981445, discriminator_loss=0.2173154652118683\n",
            "step 766: generator_loss=2.082240104675293, discriminator_loss=0.21634170413017273\n",
            "step 767: generator_loss=2.0297346115112305, discriminator_loss=0.22037957608699799\n",
            "step 768: generator_loss=2.1434593200683594, discriminator_loss=0.21431496739387512\n",
            "step 769: generator_loss=1.993124008178711, discriminator_loss=0.22113721072673798\n",
            "step 770: generator_loss=2.0845160484313965, discriminator_loss=0.21462780237197876\n",
            "step 771: generator_loss=2.095264434814453, discriminator_loss=0.21542534232139587\n",
            "step 772: generator_loss=1.9957797527313232, discriminator_loss=0.21987278759479523\n",
            "step 773: generator_loss=2.0346028804779053, discriminator_loss=0.21712681651115417\n",
            "step 774: generator_loss=1.9317013025283813, discriminator_loss=0.2234622836112976\n",
            "step 775: generator_loss=1.8868664503097534, discriminator_loss=0.22973045706748962\n",
            "step 776: generator_loss=1.9602181911468506, discriminator_loss=0.22186757624149323\n",
            "step 777: generator_loss=1.9238556623458862, discriminator_loss=0.22616183757781982\n",
            "step 778: generator_loss=1.8816741704940796, discriminator_loss=0.22779759764671326\n",
            "step 779: generator_loss=1.878872036933899, discriminator_loss=0.22758224606513977\n",
            "step 780: generator_loss=1.9048582315444946, discriminator_loss=0.22407682240009308\n",
            "step 781: generator_loss=1.844921350479126, discriminator_loss=0.22988355159759521\n",
            "step 782: generator_loss=1.8138015270233154, discriminator_loss=0.23209768533706665\n",
            "step 783: generator_loss=1.7965338230133057, discriminator_loss=0.23566894233226776\n",
            "step 784: generator_loss=1.7874298095703125, discriminator_loss=0.23356646299362183\n",
            "step 785: generator_loss=1.6605000495910645, discriminator_loss=0.24372811615467072\n",
            "step 786: generator_loss=1.7357176542282104, discriminator_loss=0.24004890024662018\n",
            "step 787: generator_loss=1.666359782218933, discriminator_loss=0.2446713149547577\n",
            "step 788: generator_loss=1.7100989818572998, discriminator_loss=0.24138358235359192\n",
            "step 789: generator_loss=1.5972564220428467, discriminator_loss=0.2518198490142822\n",
            "step 790: generator_loss=1.5570158958435059, discriminator_loss=0.2560793459415436\n",
            "step 791: generator_loss=1.5644629001617432, discriminator_loss=0.2549896240234375\n",
            "step 792: generator_loss=1.5342910289764404, discriminator_loss=0.26008546352386475\n",
            "step 793: generator_loss=1.578718900680542, discriminator_loss=0.25414085388183594\n",
            "step 794: generator_loss=1.5218706130981445, discriminator_loss=0.25964224338531494\n",
            "step 795: generator_loss=1.4635436534881592, discriminator_loss=0.26793205738067627\n",
            "step 796: generator_loss=1.5236399173736572, discriminator_loss=0.2631731331348419\n",
            "step 797: generator_loss=1.4402196407318115, discriminator_loss=0.2731507420539856\n",
            "step 798: generator_loss=1.455567479133606, discriminator_loss=0.27297329902648926\n",
            "step 799: generator_loss=1.403741717338562, discriminator_loss=0.2815203368663788\n",
            "step 800: generator_loss=1.4358702898025513, discriminator_loss=0.2741471529006958\n",
            "step 801: generator_loss=1.4148938655853271, discriminator_loss=0.279845267534256\n",
            "step 802: generator_loss=1.380034327507019, discriminator_loss=0.28608477115631104\n",
            "step 803: generator_loss=1.3988714218139648, discriminator_loss=0.28464022278785706\n",
            "step 804: generator_loss=1.3632447719573975, discriminator_loss=0.29279229044914246\n",
            "step 805: generator_loss=1.4381930828094482, discriminator_loss=0.27981168031692505\n",
            "step 806: generator_loss=1.4646415710449219, discriminator_loss=0.27760863304138184\n",
            "step 807: generator_loss=1.4488168954849243, discriminator_loss=0.27705883979797363\n",
            "step 808: generator_loss=1.4984383583068848, discriminator_loss=0.27071303129196167\n",
            "step 809: generator_loss=1.5044045448303223, discriminator_loss=0.2702326774597168\n",
            "step 810: generator_loss=1.448390245437622, discriminator_loss=0.27392080426216125\n",
            "step 811: generator_loss=1.4561704397201538, discriminator_loss=0.2746734023094177\n",
            "step 812: generator_loss=1.4763164520263672, discriminator_loss=0.2702041268348694\n",
            "step 813: generator_loss=1.4945467710494995, discriminator_loss=0.2672542333602905\n",
            "step 814: generator_loss=1.5051875114440918, discriminator_loss=0.2635575234889984\n",
            "step 815: generator_loss=1.5332624912261963, discriminator_loss=0.26144495606422424\n",
            "step 816: generator_loss=1.6106986999511719, discriminator_loss=0.2498120367527008\n",
            "step 817: generator_loss=1.5048946142196655, discriminator_loss=0.26080042123794556\n",
            "step 818: generator_loss=1.5038206577301025, discriminator_loss=0.26129233837127686\n",
            "step 819: generator_loss=1.5246050357818604, discriminator_loss=0.25692838430404663\n",
            "step 820: generator_loss=1.5296721458435059, discriminator_loss=0.2543489933013916\n",
            "step 821: generator_loss=1.523239254951477, discriminator_loss=0.2563326358795166\n",
            "step 822: generator_loss=1.5563790798187256, discriminator_loss=0.2519902288913727\n",
            "step 823: generator_loss=1.568428874015808, discriminator_loss=0.24785059690475464\n",
            "step 824: generator_loss=1.5304043292999268, discriminator_loss=0.25347697734832764\n",
            "step 825: generator_loss=1.5512722730636597, discriminator_loss=0.2508423626422882\n",
            "step 826: generator_loss=1.5160043239593506, discriminator_loss=0.2549252510070801\n",
            "step 827: generator_loss=1.5177154541015625, discriminator_loss=0.25415265560150146\n",
            "step 828: generator_loss=1.539587378501892, discriminator_loss=0.25199317932128906\n",
            "step 829: generator_loss=1.5360181331634521, discriminator_loss=0.25094953179359436\n",
            "step 830: generator_loss=1.4755233526229858, discriminator_loss=0.26140642166137695\n",
            "step 831: generator_loss=1.5052971839904785, discriminator_loss=0.2572641372680664\n",
            "step 832: generator_loss=1.540848731994629, discriminator_loss=0.25368833541870117\n",
            "step 833: generator_loss=1.477982997894287, discriminator_loss=0.2607879340648651\n",
            "step 834: generator_loss=1.4487046003341675, discriminator_loss=0.2666960656642914\n",
            "step 835: generator_loss=1.5046813488006592, discriminator_loss=0.2575277090072632\n",
            "step 836: generator_loss=1.4634840488433838, discriminator_loss=0.2646171748638153\n",
            "step 837: generator_loss=1.4142587184906006, discriminator_loss=0.2735908627510071\n",
            "step 838: generator_loss=1.4223647117614746, discriminator_loss=0.2744362950325012\n",
            "step 839: generator_loss=1.434064269065857, discriminator_loss=0.2703542411327362\n",
            "step 840: generator_loss=1.3335317373275757, discriminator_loss=0.2911226451396942\n",
            "step 841: generator_loss=1.3415979146957397, discriminator_loss=0.29223182797431946\n",
            "step 842: generator_loss=1.3716247081756592, discriminator_loss=0.2886573076248169\n",
            "step 843: generator_loss=1.3463712930679321, discriminator_loss=0.2925032675266266\n",
            "step 844: generator_loss=1.3243427276611328, discriminator_loss=0.2977818250656128\n",
            "step 845: generator_loss=1.2839553356170654, discriminator_loss=0.30760934948921204\n",
            "step 846: generator_loss=1.3186427354812622, discriminator_loss=0.3018312156200409\n",
            "step 847: generator_loss=1.2823171615600586, discriminator_loss=0.3111477494239807\n",
            "step 848: generator_loss=1.252976894378662, discriminator_loss=0.3216016888618469\n",
            "step 849: generator_loss=1.261580467224121, discriminator_loss=0.3232266306877136\n",
            "step 850: generator_loss=1.2828209400177002, discriminator_loss=0.31145620346069336\n",
            "step 851: generator_loss=1.2491605281829834, discriminator_loss=0.3276423215866089\n",
            "step 852: generator_loss=1.2032641172409058, discriminator_loss=0.3391302824020386\n",
            "step 853: generator_loss=1.167079210281372, discriminator_loss=0.34886717796325684\n",
            "step 854: generator_loss=1.2318115234375, discriminator_loss=0.3342394530773163\n",
            "step 855: generator_loss=1.1272441148757935, discriminator_loss=0.36167043447494507\n",
            "step 856: generator_loss=1.1482758522033691, discriminator_loss=0.3557532727718353\n",
            "step 857: generator_loss=1.169411301612854, discriminator_loss=0.34526219964027405\n",
            "step 858: generator_loss=1.1885590553283691, discriminator_loss=0.3390475809574127\n",
            "step 859: generator_loss=1.2176629304885864, discriminator_loss=0.33419740200042725\n",
            "step 860: generator_loss=1.2037070989608765, discriminator_loss=0.32801553606987\n",
            "step 861: generator_loss=1.1917040348052979, discriminator_loss=0.32991042733192444\n",
            "step 862: generator_loss=1.2373650074005127, discriminator_loss=0.3180239796638489\n",
            "step 863: generator_loss=1.2476484775543213, discriminator_loss=0.3143657147884369\n",
            "step 864: generator_loss=1.181706190109253, discriminator_loss=0.3286592960357666\n",
            "step 865: generator_loss=1.200960636138916, discriminator_loss=0.32222604751586914\n",
            "step 866: generator_loss=1.2092344760894775, discriminator_loss=0.31954264640808105\n",
            "step 867: generator_loss=1.2293813228607178, discriminator_loss=0.31513524055480957\n",
            "step 868: generator_loss=1.2303192615509033, discriminator_loss=0.31508028507232666\n",
            "step 869: generator_loss=1.2210874557495117, discriminator_loss=0.31738096475601196\n",
            "step 870: generator_loss=1.1974338293075562, discriminator_loss=0.32094770669937134\n",
            "step 871: generator_loss=1.1817294359207153, discriminator_loss=0.32497888803482056\n",
            "step 872: generator_loss=1.1730687618255615, discriminator_loss=0.3283360004425049\n",
            "step 873: generator_loss=1.1731281280517578, discriminator_loss=0.32777225971221924\n",
            "step 874: generator_loss=1.205107569694519, discriminator_loss=0.3228132426738739\n",
            "step 875: generator_loss=1.2031925916671753, discriminator_loss=0.3237830698490143\n",
            "step 876: generator_loss=1.1947389841079712, discriminator_loss=0.32599538564682007\n",
            "step 877: generator_loss=1.1907036304473877, discriminator_loss=0.3285377025604248\n",
            "step 878: generator_loss=1.1942569017410278, discriminator_loss=0.3272021412849426\n",
            "step 879: generator_loss=1.1605629920959473, discriminator_loss=0.33705228567123413\n",
            "step 880: generator_loss=1.1821672916412354, discriminator_loss=0.3330979645252228\n",
            "step 881: generator_loss=1.2208445072174072, discriminator_loss=0.3300936222076416\n",
            "step 882: generator_loss=1.2197892665863037, discriminator_loss=0.32965323328971863\n",
            "step 883: generator_loss=1.1851060390472412, discriminator_loss=0.33730441331863403\n",
            "step 884: generator_loss=1.1852275133132935, discriminator_loss=0.3415127694606781\n",
            "step 885: generator_loss=1.125225305557251, discriminator_loss=0.35276007652282715\n",
            "step 886: generator_loss=1.1535650491714478, discriminator_loss=0.3471956253051758\n",
            "step 887: generator_loss=1.1249834299087524, discriminator_loss=0.3581068515777588\n",
            "step 888: generator_loss=1.1366618871688843, discriminator_loss=0.3577253818511963\n",
            "step 889: generator_loss=1.1277494430541992, discriminator_loss=0.3605859875679016\n",
            "step 890: generator_loss=1.1077593564987183, discriminator_loss=0.3639791011810303\n",
            "step 891: generator_loss=1.0742998123168945, discriminator_loss=0.37233826518058777\n",
            "step 892: generator_loss=1.047250747680664, discriminator_loss=0.3792824149131775\n",
            "step 893: generator_loss=1.0461540222167969, discriminator_loss=0.38335680961608887\n",
            "step 894: generator_loss=1.0858713388442993, discriminator_loss=0.37556928396224976\n",
            "step 895: generator_loss=1.0213292837142944, discriminator_loss=0.38910114765167236\n",
            "step 896: generator_loss=0.9863640069961548, discriminator_loss=0.4016650319099426\n",
            "step 897: generator_loss=1.0236246585845947, discriminator_loss=0.394727885723114\n",
            "step 898: generator_loss=0.9795964956283569, discriminator_loss=0.4071059823036194\n",
            "step 899: generator_loss=0.9895980954170227, discriminator_loss=0.40553706884384155\n",
            "step 900: generator_loss=0.9994994401931763, discriminator_loss=0.40306609869003296\n",
            "step 901: generator_loss=0.9832737445831299, discriminator_loss=0.40728551149368286\n",
            "step 902: generator_loss=0.9633536338806152, discriminator_loss=0.4142979681491852\n",
            "step 903: generator_loss=0.9801579713821411, discriminator_loss=0.41081562638282776\n",
            "step 904: generator_loss=0.9545871615409851, discriminator_loss=0.41828736662864685\n",
            "step 905: generator_loss=1.0026602745056152, discriminator_loss=0.4070069193840027\n",
            "step 906: generator_loss=1.0072883367538452, discriminator_loss=0.40606409311294556\n",
            "step 907: generator_loss=1.0038872957229614, discriminator_loss=0.4071798026561737\n",
            "step 908: generator_loss=1.0083732604980469, discriminator_loss=0.4068217873573303\n",
            "step 909: generator_loss=1.0493381023406982, discriminator_loss=0.40009239315986633\n",
            "step 910: generator_loss=1.030083417892456, discriminator_loss=0.4050443172454834\n",
            "step 911: generator_loss=1.0403783321380615, discriminator_loss=0.4021250605583191\n",
            "step 912: generator_loss=1.0426450967788696, discriminator_loss=0.4020776152610779\n",
            "step 913: generator_loss=1.0793873071670532, discriminator_loss=0.3928180932998657\n",
            "step 914: generator_loss=1.074576735496521, discriminator_loss=0.39414507150650024\n",
            "step 915: generator_loss=1.0427699089050293, discriminator_loss=0.4038618206977844\n",
            "step 916: generator_loss=1.0401663780212402, discriminator_loss=0.4018571376800537\n",
            "step 917: generator_loss=1.0640195608139038, discriminator_loss=0.3988072872161865\n",
            "step 918: generator_loss=1.0426771640777588, discriminator_loss=0.4031020402908325\n",
            "step 919: generator_loss=1.0452806949615479, discriminator_loss=0.4020349979400635\n",
            "step 920: generator_loss=1.0593762397766113, discriminator_loss=0.4002020061016083\n",
            "step 921: generator_loss=1.043513298034668, discriminator_loss=0.40517473220825195\n",
            "step 922: generator_loss=1.0446805953979492, discriminator_loss=0.404293954372406\n",
            "step 923: generator_loss=1.049736738204956, discriminator_loss=0.4036339819431305\n",
            "step 924: generator_loss=1.0524311065673828, discriminator_loss=0.4038679599761963\n",
            "step 925: generator_loss=1.0459160804748535, discriminator_loss=0.4056832194328308\n",
            "step 926: generator_loss=1.0613422393798828, discriminator_loss=0.40250587463378906\n",
            "step 927: generator_loss=1.0636911392211914, discriminator_loss=0.40280529856681824\n",
            "step 928: generator_loss=1.0687273740768433, discriminator_loss=0.4021463096141815\n",
            "step 929: generator_loss=1.0770158767700195, discriminator_loss=0.400921493768692\n",
            "step 930: generator_loss=1.0878043174743652, discriminator_loss=0.3989217281341553\n",
            "step 931: generator_loss=1.0857253074645996, discriminator_loss=0.39985358715057373\n",
            "step 932: generator_loss=1.0885205268859863, discriminator_loss=0.39917895197868347\n",
            "step 933: generator_loss=1.0949262380599976, discriminator_loss=0.39803749322891235\n",
            "step 934: generator_loss=1.0887093544006348, discriminator_loss=0.3995136320590973\n",
            "step 935: generator_loss=1.0821945667266846, discriminator_loss=0.40094348788261414\n",
            "step 936: generator_loss=1.0888919830322266, discriminator_loss=0.3991096019744873\n",
            "step 937: generator_loss=1.0770853757858276, discriminator_loss=0.4017723798751831\n",
            "step 938: generator_loss=1.0806827545166016, discriminator_loss=0.40059927105903625\n",
            "step 939: generator_loss=1.0704677104949951, discriminator_loss=0.40278172492980957\n",
            "step 940: generator_loss=1.0582144260406494, discriminator_loss=0.4054044485092163\n",
            "step 941: generator_loss=1.0533322095870972, discriminator_loss=0.406648725271225\n",
            "step 942: generator_loss=1.0380288362503052, discriminator_loss=0.4099939167499542\n",
            "step 943: generator_loss=1.0274674892425537, discriminator_loss=0.4124213457107544\n",
            "step 944: generator_loss=1.012322187423706, discriminator_loss=0.41646432876586914\n",
            "step 945: generator_loss=1.0017294883728027, discriminator_loss=0.41906216740608215\n",
            "step 946: generator_loss=0.9815913438796997, discriminator_loss=0.42468172311782837\n",
            "step 947: generator_loss=0.9642808437347412, discriminator_loss=0.42965611815452576\n",
            "step 948: generator_loss=0.9559122323989868, discriminator_loss=0.43194469809532166\n",
            "step 949: generator_loss=0.9391791820526123, discriminator_loss=0.43747037649154663\n",
            "step 950: generator_loss=0.9256173968315125, discriminator_loss=0.4417409598827362\n",
            "step 951: generator_loss=0.9072067737579346, discriminator_loss=0.44801127910614014\n",
            "step 952: generator_loss=0.883866548538208, discriminator_loss=0.4564323127269745\n",
            "step 953: generator_loss=0.8709121346473694, discriminator_loss=0.4603176712989807\n",
            "step 954: generator_loss=0.8485339879989624, discriminator_loss=0.46902382373809814\n",
            "step 955: generator_loss=0.8362715840339661, discriminator_loss=0.4738343358039856\n",
            "step 956: generator_loss=0.8235114812850952, discriminator_loss=0.4788937270641327\n",
            "step 957: generator_loss=0.8170481324195862, discriminator_loss=0.4813189208507538\n",
            "step 958: generator_loss=0.8118396997451782, discriminator_loss=0.48395127058029175\n",
            "step 959: generator_loss=0.8083863854408264, discriminator_loss=0.48552221059799194\n",
            "step 960: generator_loss=0.8046185374259949, discriminator_loss=0.4868669807910919\n",
            "step 961: generator_loss=0.8304023742675781, discriminator_loss=0.4770617187023163\n",
            "step 962: generator_loss=0.8523539900779724, discriminator_loss=0.46799468994140625\n",
            "step 963: generator_loss=0.853350043296814, discriminator_loss=0.46788692474365234\n",
            "step 964: generator_loss=0.8813966512680054, discriminator_loss=0.4579600989818573\n",
            "step 965: generator_loss=0.9028090238571167, discriminator_loss=0.45022645592689514\n",
            "step 966: generator_loss=0.9404129385948181, discriminator_loss=0.43836510181427\n",
            "step 967: generator_loss=0.9627445340156555, discriminator_loss=0.43109190464019775\n",
            "step 968: generator_loss=1.0025405883789062, discriminator_loss=0.41982313990592957\n",
            "step 969: generator_loss=1.0250928401947021, discriminator_loss=0.41277021169662476\n",
            "step 970: generator_loss=1.048851490020752, discriminator_loss=0.40533366799354553\n",
            "step 971: generator_loss=1.0795929431915283, discriminator_loss=0.396767258644104\n",
            "step 972: generator_loss=1.1048868894577026, discriminator_loss=0.3901766240596771\n",
            "step 973: generator_loss=1.1353694200515747, discriminator_loss=0.3820115923881531\n",
            "step 974: generator_loss=1.1625635623931885, discriminator_loss=0.37577372789382935\n",
            "step 975: generator_loss=1.2168413400650024, discriminator_loss=0.36363616585731506\n",
            "step 976: generator_loss=1.2072526216506958, discriminator_loss=0.36434924602508545\n",
            "step 977: generator_loss=1.2336395978927612, discriminator_loss=0.35813015699386597\n",
            "step 978: generator_loss=1.2230851650238037, discriminator_loss=0.3593887686729431\n",
            "step 979: generator_loss=1.2474777698516846, discriminator_loss=0.35366976261138916\n",
            "step 980: generator_loss=1.2473949193954468, discriminator_loss=0.3532616198062897\n",
            "step 981: generator_loss=1.2534191608428955, discriminator_loss=0.35100027918815613\n",
            "step 982: generator_loss=1.2129652500152588, discriminator_loss=0.3591774106025696\n",
            "step 983: generator_loss=1.1957862377166748, discriminator_loss=0.36274391412734985\n",
            "step 984: generator_loss=1.225249171257019, discriminator_loss=0.3552301526069641\n",
            "step 985: generator_loss=1.2125412225723267, discriminator_loss=0.3576017916202545\n",
            "step 986: generator_loss=1.17206609249115, discriminator_loss=0.36597341299057007\n",
            "step 987: generator_loss=1.1462969779968262, discriminator_loss=0.3712872564792633\n",
            "step 988: generator_loss=1.1235072612762451, discriminator_loss=0.3773500919342041\n",
            "step 989: generator_loss=1.0908278226852417, discriminator_loss=0.3849503993988037\n",
            "step 990: generator_loss=1.0840356349945068, discriminator_loss=0.38566824793815613\n",
            "step 991: generator_loss=1.0670548677444458, discriminator_loss=0.39033347368240356\n",
            "step 992: generator_loss=1.0401479005813599, discriminator_loss=0.3972828686237335\n",
            "step 993: generator_loss=1.0222692489624023, discriminator_loss=0.40185433626174927\n",
            "step 994: generator_loss=1.0100524425506592, discriminator_loss=0.4056498110294342\n",
            "step 995: generator_loss=0.9692007303237915, discriminator_loss=0.4175143241882324\n",
            "step 996: generator_loss=0.9646981954574585, discriminator_loss=0.4190356135368347\n",
            "step 997: generator_loss=0.9571056962013245, discriminator_loss=0.42201897501945496\n",
            "step 998: generator_loss=0.9624248147010803, discriminator_loss=0.42124712467193604\n",
            "step 999: generator_loss=0.9256969690322876, discriminator_loss=0.4328104555606842\n",
            "step 1000: generator_loss=0.9300146698951721, discriminator_loss=0.4323093891143799\n",
            "step 1001: generator_loss=0.9263883233070374, discriminator_loss=0.43336498737335205\n",
            "step 1002: generator_loss=0.9225311875343323, discriminator_loss=0.4350985288619995\n",
            "step 1003: generator_loss=0.912655770778656, discriminator_loss=0.438720166683197\n",
            "step 1004: generator_loss=0.9016159176826477, discriminator_loss=0.44392192363739014\n",
            "step 1005: generator_loss=0.9139628410339355, discriminator_loss=0.44028180837631226\n",
            "step 1006: generator_loss=0.8938436508178711, discriminator_loss=0.4474394917488098\n",
            "step 1007: generator_loss=0.8888875842094421, discriminator_loss=0.4490559697151184\n",
            "step 1008: generator_loss=0.910831868648529, discriminator_loss=0.44244569540023804\n",
            "step 1009: generator_loss=0.9090325832366943, discriminator_loss=0.44170570373535156\n",
            "step 1010: generator_loss=0.9142001867294312, discriminator_loss=0.4412921667098999\n",
            "step 1011: generator_loss=0.8825310468673706, discriminator_loss=0.45358455181121826\n",
            "step 1012: generator_loss=0.9115235209465027, discriminator_loss=0.4425097703933716\n",
            "step 1013: generator_loss=0.9267037510871887, discriminator_loss=0.4378819465637207\n",
            "step 1014: generator_loss=0.943792462348938, discriminator_loss=0.43282902240753174\n",
            "step 1015: generator_loss=0.9602547883987427, discriminator_loss=0.42833200097084045\n",
            "step 1016: generator_loss=0.9580855369567871, discriminator_loss=0.4291120171546936\n",
            "step 1017: generator_loss=0.9747332334518433, discriminator_loss=0.4245840311050415\n",
            "step 1018: generator_loss=1.0104572772979736, discriminator_loss=0.41354501247406006\n",
            "step 1019: generator_loss=1.0234695672988892, discriminator_loss=0.41092225909233093\n",
            "step 1020: generator_loss=1.0476740598678589, discriminator_loss=0.40324535965919495\n",
            "step 1021: generator_loss=1.0225348472595215, discriminator_loss=0.4128020405769348\n",
            "step 1022: generator_loss=1.054145336151123, discriminator_loss=0.4046406149864197\n",
            "step 1023: generator_loss=1.100858211517334, discriminator_loss=0.3905576467514038\n",
            "step 1024: generator_loss=1.1132893562316895, discriminator_loss=0.3894720673561096\n",
            "step 1025: generator_loss=1.1439414024353027, discriminator_loss=0.38094180822372437\n",
            "step 1026: generator_loss=1.1495921611785889, discriminator_loss=0.3787803053855896\n",
            "step 1027: generator_loss=1.1707401275634766, discriminator_loss=0.3735576272010803\n",
            "step 1028: generator_loss=1.2140448093414307, discriminator_loss=0.3644448518753052\n",
            "step 1029: generator_loss=1.2205228805541992, discriminator_loss=0.3635205030441284\n",
            "step 1030: generator_loss=1.2373716831207275, discriminator_loss=0.3588496148586273\n",
            "step 1031: generator_loss=1.2525551319122314, discriminator_loss=0.35546088218688965\n",
            "step 1032: generator_loss=1.2836120128631592, discriminator_loss=0.34904584288597107\n",
            "step 1033: generator_loss=1.2975454330444336, discriminator_loss=0.3455345332622528\n",
            "step 1034: generator_loss=1.3226521015167236, discriminator_loss=0.34069928526878357\n",
            "step 1035: generator_loss=1.3363986015319824, discriminator_loss=0.33818912506103516\n",
            "step 1036: generator_loss=1.3374565839767456, discriminator_loss=0.3371855616569519\n",
            "step 1037: generator_loss=1.355513095855713, discriminator_loss=0.3343183398246765\n",
            "step 1038: generator_loss=1.3494975566864014, discriminator_loss=0.33442431688308716\n",
            "step 1039: generator_loss=1.3402835130691528, discriminator_loss=0.33623790740966797\n",
            "step 1040: generator_loss=1.3403081893920898, discriminator_loss=0.335281640291214\n",
            "step 1041: generator_loss=1.3452683687210083, discriminator_loss=0.3334832191467285\n",
            "step 1042: generator_loss=1.3293207883834839, discriminator_loss=0.3356242775917053\n",
            "step 1043: generator_loss=1.3291410207748413, discriminator_loss=0.33510124683380127\n",
            "step 1044: generator_loss=1.3259971141815186, discriminator_loss=0.3352808356285095\n",
            "step 1045: generator_loss=1.3022410869598389, discriminator_loss=0.3388604521751404\n",
            "step 1046: generator_loss=1.3111987113952637, discriminator_loss=0.33654409646987915\n",
            "step 1047: generator_loss=1.2927696704864502, discriminator_loss=0.3384707272052765\n",
            "step 1048: generator_loss=1.2618958950042725, discriminator_loss=0.3440651297569275\n",
            "step 1049: generator_loss=1.2762126922607422, discriminator_loss=0.34023401141166687\n",
            "step 1050: generator_loss=1.2662034034729004, discriminator_loss=0.3414531946182251\n",
            "step 1051: generator_loss=1.2436716556549072, discriminator_loss=0.34540531039237976\n",
            "step 1052: generator_loss=1.2331745624542236, discriminator_loss=0.34680142998695374\n",
            "step 1053: generator_loss=1.2072184085845947, discriminator_loss=0.35182178020477295\n",
            "step 1054: generator_loss=1.190955400466919, discriminator_loss=0.35536208748817444\n",
            "step 1055: generator_loss=1.170435905456543, discriminator_loss=0.35881125926971436\n",
            "step 1056: generator_loss=1.1465294361114502, discriminator_loss=0.3641819655895233\n",
            "step 1057: generator_loss=1.1470448970794678, discriminator_loss=0.36384880542755127\n",
            "step 1058: generator_loss=1.1320444345474243, discriminator_loss=0.3670153021812439\n",
            "step 1059: generator_loss=1.1296911239624023, discriminator_loss=0.36772090196609497\n",
            "step 1060: generator_loss=1.1210031509399414, discriminator_loss=0.36985641717910767\n",
            "step 1061: generator_loss=1.107344150543213, discriminator_loss=0.37278613448143005\n",
            "step 1062: generator_loss=1.1069856882095337, discriminator_loss=0.374400794506073\n",
            "step 1063: generator_loss=1.1034399271011353, discriminator_loss=0.3752827048301697\n",
            "step 1064: generator_loss=1.0855894088745117, discriminator_loss=0.3803755044937134\n",
            "step 1065: generator_loss=1.0670149326324463, discriminator_loss=0.38506513833999634\n",
            "step 1066: generator_loss=1.0667800903320312, discriminator_loss=0.3858603239059448\n",
            "step 1067: generator_loss=1.0505694150924683, discriminator_loss=0.39045965671539307\n",
            "step 1068: generator_loss=1.0562634468078613, discriminator_loss=0.3899831175804138\n",
            "step 1069: generator_loss=1.023817777633667, discriminator_loss=0.3991393744945526\n",
            "step 1070: generator_loss=1.0271390676498413, discriminator_loss=0.398967444896698\n",
            "step 1071: generator_loss=1.0155510902404785, discriminator_loss=0.4025366008281708\n",
            "step 1072: generator_loss=0.9980701208114624, discriminator_loss=0.40907177329063416\n",
            "step 1073: generator_loss=1.0049346685409546, discriminator_loss=0.40785229206085205\n",
            "step 1074: generator_loss=0.9965256452560425, discriminator_loss=0.41162529587745667\n",
            "step 1075: generator_loss=0.9837080240249634, discriminator_loss=0.41660773754119873\n",
            "step 1076: generator_loss=0.9848686456680298, discriminator_loss=0.416314959526062\n",
            "step 1077: generator_loss=0.9766273498535156, discriminator_loss=0.41993892192840576\n",
            "step 1078: generator_loss=0.9719820022583008, discriminator_loss=0.42148739099502563\n",
            "step 1079: generator_loss=0.9876598119735718, discriminator_loss=0.4173082113265991\n",
            "step 1080: generator_loss=0.9984365105628967, discriminator_loss=0.415080189704895\n",
            "step 1081: generator_loss=1.0303950309753418, discriminator_loss=0.40731868147850037\n",
            "step 1082: generator_loss=1.041668176651001, discriminator_loss=0.4032374620437622\n",
            "step 1083: generator_loss=1.0872678756713867, discriminator_loss=0.3918203115463257\n",
            "step 1084: generator_loss=1.095391035079956, discriminator_loss=0.38957488536834717\n",
            "step 1085: generator_loss=1.1216397285461426, discriminator_loss=0.3833841383457184\n",
            "step 1086: generator_loss=1.1505827903747559, discriminator_loss=0.37672558426856995\n",
            "step 1087: generator_loss=1.1862099170684814, discriminator_loss=0.369118869304657\n",
            "step 1088: generator_loss=1.221964716911316, discriminator_loss=0.3603843152523041\n",
            "step 1089: generator_loss=1.2223362922668457, discriminator_loss=0.35986408591270447\n",
            "step 1090: generator_loss=1.233290672302246, discriminator_loss=0.3565079867839813\n",
            "step 1091: generator_loss=1.2636357545852661, discriminator_loss=0.3505266010761261\n",
            "step 1092: generator_loss=1.2763842344284058, discriminator_loss=0.34729254245758057\n",
            "step 1093: generator_loss=1.294715166091919, discriminator_loss=0.34331950545310974\n",
            "step 1094: generator_loss=1.308597207069397, discriminator_loss=0.3402314782142639\n",
            "step 1095: generator_loss=1.3205451965332031, discriminator_loss=0.33691614866256714\n",
            "step 1096: generator_loss=1.3228075504302979, discriminator_loss=0.33545219898223877\n",
            "step 1097: generator_loss=1.3470475673675537, discriminator_loss=0.33047276735305786\n",
            "step 1098: generator_loss=1.3594392538070679, discriminator_loss=0.32733577489852905\n",
            "step 1099: generator_loss=1.3803000450134277, discriminator_loss=0.32291316986083984\n",
            "step 1100: generator_loss=1.3935699462890625, discriminator_loss=0.31984397768974304\n",
            "step 1101: generator_loss=1.3944063186645508, discriminator_loss=0.31911981105804443\n",
            "step 1102: generator_loss=1.4142065048217773, discriminator_loss=0.3153111934661865\n",
            "step 1103: generator_loss=1.411180019378662, discriminator_loss=0.3151220977306366\n",
            "step 1104: generator_loss=1.4047489166259766, discriminator_loss=0.31544047594070435\n",
            "step 1105: generator_loss=1.394228219985962, discriminator_loss=0.3162539601325989\n",
            "step 1106: generator_loss=1.383072853088379, discriminator_loss=0.31745755672454834\n",
            "step 1107: generator_loss=1.3588279485702515, discriminator_loss=0.32076287269592285\n",
            "step 1108: generator_loss=1.3449205160140991, discriminator_loss=0.3226425051689148\n",
            "step 1109: generator_loss=1.320949673652649, discriminator_loss=0.32623291015625\n",
            "step 1110: generator_loss=1.2911514043807983, discriminator_loss=0.3310835063457489\n",
            "step 1111: generator_loss=1.2727845907211304, discriminator_loss=0.3339528441429138\n",
            "step 1112: generator_loss=1.2501871585845947, discriminator_loss=0.33787524700164795\n",
            "step 1113: generator_loss=1.2331304550170898, discriminator_loss=0.3407123386859894\n",
            "step 1114: generator_loss=1.2129826545715332, discriminator_loss=0.34444713592529297\n",
            "step 1115: generator_loss=1.2000231742858887, discriminator_loss=0.34677547216415405\n",
            "step 1116: generator_loss=1.1859055757522583, discriminator_loss=0.34953826665878296\n",
            "step 1117: generator_loss=1.172980785369873, discriminator_loss=0.35218483209609985\n",
            "step 1118: generator_loss=1.1642793416976929, discriminator_loss=0.35390686988830566\n",
            "step 1119: generator_loss=1.1572003364562988, discriminator_loss=0.3554781377315521\n",
            "step 1120: generator_loss=1.1535840034484863, discriminator_loss=0.35633236169815063\n",
            "step 1121: generator_loss=1.1421277523040771, discriminator_loss=0.3589283227920532\n",
            "step 1122: generator_loss=1.1353881359100342, discriminator_loss=0.3605579137802124\n",
            "step 1123: generator_loss=1.129195213317871, discriminator_loss=0.36196357011795044\n",
            "step 1124: generator_loss=1.1226917505264282, discriminator_loss=0.3635793924331665\n",
            "step 1125: generator_loss=1.1130789518356323, discriminator_loss=0.36593565344810486\n",
            "step 1126: generator_loss=1.1130549907684326, discriminator_loss=0.3661111295223236\n",
            "step 1127: generator_loss=1.107993721961975, discriminator_loss=0.3675611913204193\n",
            "step 1128: generator_loss=1.1083645820617676, discriminator_loss=0.3676787316799164\n",
            "step 1129: generator_loss=1.1138830184936523, discriminator_loss=0.36650675535202026\n",
            "step 1130: generator_loss=1.1189517974853516, discriminator_loss=0.3656211495399475\n",
            "step 1131: generator_loss=1.1275382041931152, discriminator_loss=0.3636348247528076\n",
            "step 1132: generator_loss=1.1336498260498047, discriminator_loss=0.3624480664730072\n",
            "step 1133: generator_loss=1.1418931484222412, discriminator_loss=0.36079728603363037\n",
            "step 1134: generator_loss=1.1524477005004883, discriminator_loss=0.35863053798675537\n",
            "step 1135: generator_loss=1.152831792831421, discriminator_loss=0.3587130308151245\n",
            "step 1136: generator_loss=1.1536811590194702, discriminator_loss=0.358654260635376\n",
            "step 1137: generator_loss=1.1435552835464478, discriminator_loss=0.36104094982147217\n",
            "step 1138: generator_loss=1.1522743701934814, discriminator_loss=0.35930734872817993\n",
            "step 1139: generator_loss=1.149845838546753, discriminator_loss=0.3599553108215332\n",
            "step 1140: generator_loss=1.158665657043457, discriminator_loss=0.3581051230430603\n",
            "step 1141: generator_loss=1.1560614109039307, discriminator_loss=0.3587470054626465\n",
            "step 1142: generator_loss=1.156448245048523, discriminator_loss=0.3586702346801758\n",
            "step 1143: generator_loss=1.1582872867584229, discriminator_loss=0.3583943545818329\n",
            "step 1144: generator_loss=1.1592867374420166, discriminator_loss=0.3583272695541382\n",
            "step 1145: generator_loss=1.160933017730713, discriminator_loss=0.3582803010940552\n",
            "step 1146: generator_loss=1.1678860187530518, discriminator_loss=0.35698139667510986\n",
            "step 1147: generator_loss=1.1744977235794067, discriminator_loss=0.35584187507629395\n",
            "step 1148: generator_loss=1.1823211908340454, discriminator_loss=0.3544395864009857\n",
            "step 1149: generator_loss=1.1938793659210205, discriminator_loss=0.3522324562072754\n",
            "step 1150: generator_loss=1.2028565406799316, discriminator_loss=0.35065650939941406\n",
            "step 1151: generator_loss=1.21430504322052, discriminator_loss=0.34885483980178833\n",
            "step 1152: generator_loss=1.2223865985870361, discriminator_loss=0.3477576673030853\n",
            "step 1153: generator_loss=1.2287944555282593, discriminator_loss=0.347065269947052\n",
            "step 1154: generator_loss=1.2347291707992554, discriminator_loss=0.3464468717575073\n",
            "step 1155: generator_loss=1.2340939044952393, discriminator_loss=0.3472982943058014\n",
            "step 1156: generator_loss=1.2371366024017334, discriminator_loss=0.34738489985466003\n",
            "step 1157: generator_loss=1.234205961227417, discriminator_loss=0.34845662117004395\n",
            "step 1158: generator_loss=1.2331795692443848, discriminator_loss=0.349173903465271\n",
            "step 1159: generator_loss=1.2313945293426514, discriminator_loss=0.35003745555877686\n",
            "step 1160: generator_loss=1.2416801452636719, discriminator_loss=0.3482540249824524\n",
            "step 1161: generator_loss=1.2560333013534546, discriminator_loss=0.3455839157104492\n",
            "step 1162: generator_loss=1.275685429573059, discriminator_loss=0.34190833568573\n",
            "step 1163: generator_loss=1.2982701063156128, discriminator_loss=0.3378129005432129\n",
            "step 1164: generator_loss=1.3143662214279175, discriminator_loss=0.334977924823761\n",
            "step 1165: generator_loss=1.323306679725647, discriminator_loss=0.33318695425987244\n",
            "step 1166: generator_loss=1.3289180994033813, discriminator_loss=0.33216744661331177\n",
            "step 1167: generator_loss=1.3311316967010498, discriminator_loss=0.33154287934303284\n",
            "step 1168: generator_loss=1.3253767490386963, discriminator_loss=0.33209317922592163\n",
            "step 1169: generator_loss=1.315155029296875, discriminator_loss=0.3336523175239563\n",
            "step 1170: generator_loss=1.304753303527832, discriminator_loss=0.33516696095466614\n",
            "step 1171: generator_loss=1.2890948057174683, discriminator_loss=0.3377912938594818\n",
            "step 1172: generator_loss=1.27789306640625, discriminator_loss=0.33961498737335205\n",
            "step 1173: generator_loss=1.266363263130188, discriminator_loss=0.3415653109550476\n",
            "step 1174: generator_loss=1.2556126117706299, discriminator_loss=0.3433815836906433\n",
            "step 1175: generator_loss=1.240041971206665, discriminator_loss=0.3461959958076477\n",
            "step 1176: generator_loss=1.2355051040649414, discriminator_loss=0.34696611762046814\n",
            "step 1177: generator_loss=1.2297395467758179, discriminator_loss=0.34794050455093384\n",
            "step 1178: generator_loss=1.2195240259170532, discriminator_loss=0.3499460220336914\n",
            "step 1179: generator_loss=1.2064064741134644, discriminator_loss=0.352481871843338\n",
            "step 1180: generator_loss=1.1974456310272217, discriminator_loss=0.3543376922607422\n",
            "step 1181: generator_loss=1.1828618049621582, discriminator_loss=0.3574354648590088\n",
            "step 1182: generator_loss=1.173827886581421, discriminator_loss=0.3594150245189667\n",
            "step 1183: generator_loss=1.159926414489746, discriminator_loss=0.36253979802131653\n",
            "step 1184: generator_loss=1.1449609994888306, discriminator_loss=0.3659208416938782\n",
            "step 1185: generator_loss=1.133424997329712, discriminator_loss=0.36868003010749817\n",
            "step 1186: generator_loss=1.1204137802124023, discriminator_loss=0.37195897102355957\n",
            "step 1187: generator_loss=1.106374740600586, discriminator_loss=0.3757144808769226\n",
            "step 1188: generator_loss=1.098013162612915, discriminator_loss=0.3781788945198059\n",
            "step 1189: generator_loss=1.0911766290664673, discriminator_loss=0.38044363260269165\n",
            "step 1190: generator_loss=1.0795042514801025, discriminator_loss=0.3839668333530426\n",
            "step 1191: generator_loss=1.0750010013580322, discriminator_loss=0.38570553064346313\n",
            "step 1192: generator_loss=1.0731918811798096, discriminator_loss=0.38673514127731323\n",
            "step 1193: generator_loss=1.0685627460479736, discriminator_loss=0.3885561227798462\n",
            "step 1194: generator_loss=1.0636935234069824, discriminator_loss=0.3903515040874481\n",
            "step 1195: generator_loss=1.0623823404312134, discriminator_loss=0.39120614528656006\n",
            "step 1196: generator_loss=1.0617774724960327, discriminator_loss=0.3918178379535675\n",
            "step 1197: generator_loss=1.069476842880249, discriminator_loss=0.39024877548217773\n",
            "step 1198: generator_loss=1.0775761604309082, discriminator_loss=0.38850873708724976\n",
            "step 1199: generator_loss=1.0923199653625488, discriminator_loss=0.38511034846305847\n",
            "step 1200: generator_loss=1.1105717420578003, discriminator_loss=0.3810758590698242\n",
            "step 1201: generator_loss=1.1267313957214355, discriminator_loss=0.3774316906929016\n",
            "step 1202: generator_loss=1.1470187902450562, discriminator_loss=0.372999906539917\n",
            "step 1203: generator_loss=1.165355920791626, discriminator_loss=0.36889714002609253\n",
            "step 1204: generator_loss=1.1779539585113525, discriminator_loss=0.3663981556892395\n",
            "step 1205: generator_loss=1.1930580139160156, discriminator_loss=0.363291472196579\n",
            "step 1206: generator_loss=1.201905608177185, discriminator_loss=0.36166253685951233\n",
            "step 1207: generator_loss=1.2184514999389648, discriminator_loss=0.3582972288131714\n",
            "step 1208: generator_loss=1.2347491979599, discriminator_loss=0.35509389638900757\n",
            "step 1209: generator_loss=1.2454205751419067, discriminator_loss=0.3529115319252014\n",
            "step 1210: generator_loss=1.254730463027954, discriminator_loss=0.3509901165962219\n",
            "step 1211: generator_loss=1.2632066011428833, discriminator_loss=0.3493950664997101\n",
            "step 1212: generator_loss=1.2710952758789062, discriminator_loss=0.34792688488960266\n",
            "step 1213: generator_loss=1.2778992652893066, discriminator_loss=0.3467862606048584\n",
            "step 1214: generator_loss=1.285900354385376, discriminator_loss=0.34546971321105957\n",
            "step 1215: generator_loss=1.2893158197402954, discriminator_loss=0.3450719714164734\n",
            "step 1216: generator_loss=1.2941960096359253, discriminator_loss=0.3441814184188843\n",
            "step 1217: generator_loss=1.2976038455963135, discriminator_loss=0.3434864580631256\n",
            "step 1218: generator_loss=1.2984826564788818, discriminator_loss=0.34327438473701477\n",
            "step 1219: generator_loss=1.3024871349334717, discriminator_loss=0.3421754539012909\n",
            "step 1220: generator_loss=1.302823781967163, discriminator_loss=0.3417162299156189\n",
            "step 1221: generator_loss=1.3000624179840088, discriminator_loss=0.3418627083301544\n",
            "step 1222: generator_loss=1.2974958419799805, discriminator_loss=0.3416997790336609\n",
            "step 1223: generator_loss=1.2944257259368896, discriminator_loss=0.341726690530777\n",
            "step 1224: generator_loss=1.288381814956665, discriminator_loss=0.34220170974731445\n",
            "step 1225: generator_loss=1.281576156616211, discriminator_loss=0.34280622005462646\n",
            "step 1226: generator_loss=1.2778205871582031, discriminator_loss=0.3428516089916229\n",
            "step 1227: generator_loss=1.2697863578796387, discriminator_loss=0.3438093662261963\n",
            "step 1228: generator_loss=1.276982307434082, discriminator_loss=0.34163570404052734\n",
            "step 1229: generator_loss=1.2706921100616455, discriminator_loss=0.34210139513015747\n",
            "step 1230: generator_loss=1.2698209285736084, discriminator_loss=0.34152328968048096\n",
            "step 1231: generator_loss=1.2667087316513062, discriminator_loss=0.341439813375473\n",
            "step 1232: generator_loss=1.2602697610855103, discriminator_loss=0.3421155512332916\n",
            "step 1233: generator_loss=1.253007173538208, discriminator_loss=0.34293970465660095\n",
            "step 1234: generator_loss=1.241891860961914, discriminator_loss=0.3444417417049408\n",
            "step 1235: generator_loss=1.2384531497955322, discriminator_loss=0.344588965177536\n",
            "step 1236: generator_loss=1.2313525676727295, discriminator_loss=0.34567344188690186\n",
            "step 1237: generator_loss=1.2286702394485474, discriminator_loss=0.34606361389160156\n",
            "step 1238: generator_loss=1.2260239124298096, discriminator_loss=0.3464793562889099\n",
            "step 1239: generator_loss=1.2227058410644531, discriminator_loss=0.34709468483924866\n",
            "step 1240: generator_loss=1.2159548997879028, discriminator_loss=0.34840691089630127\n",
            "step 1241: generator_loss=1.2108738422393799, discriminator_loss=0.34943848848342896\n",
            "step 1242: generator_loss=1.1987330913543701, discriminator_loss=0.35194510221481323\n",
            "step 1243: generator_loss=1.1861436367034912, discriminator_loss=0.35449671745300293\n",
            "step 1244: generator_loss=1.178515076637268, discriminator_loss=0.35623711347579956\n",
            "step 1245: generator_loss=1.1645982265472412, discriminator_loss=0.35964512825012207\n",
            "step 1246: generator_loss=1.1496870517730713, discriminator_loss=0.36353379487991333\n",
            "step 1247: generator_loss=1.1300170421600342, discriminator_loss=0.36870086193084717\n",
            "step 1248: generator_loss=1.1101734638214111, discriminator_loss=0.37384018301963806\n",
            "step 1249: generator_loss=1.0929090976715088, discriminator_loss=0.37869974970817566\n",
            "step 1250: generator_loss=1.0736287832260132, discriminator_loss=0.38428956270217896\n",
            "step 1251: generator_loss=1.056151032447815, discriminator_loss=0.38961470127105713\n",
            "step 1252: generator_loss=1.0429506301879883, discriminator_loss=0.39398813247680664\n",
            "step 1253: generator_loss=1.0361979007720947, discriminator_loss=0.396767795085907\n",
            "step 1254: generator_loss=1.0358853340148926, discriminator_loss=0.3977205753326416\n",
            "step 1255: generator_loss=1.039963722229004, discriminator_loss=0.3975064754486084\n",
            "step 1256: generator_loss=1.0497443675994873, discriminator_loss=0.39564377069473267\n",
            "step 1257: generator_loss=1.0617315769195557, discriminator_loss=0.3932085633277893\n",
            "step 1258: generator_loss=1.0801347494125366, discriminator_loss=0.38914498686790466\n",
            "step 1259: generator_loss=1.1035834550857544, discriminator_loss=0.3836006224155426\n",
            "step 1260: generator_loss=1.130752444267273, discriminator_loss=0.377554327249527\n",
            "step 1261: generator_loss=1.15377676486969, discriminator_loss=0.3726075291633606\n",
            "step 1262: generator_loss=1.17197847366333, discriminator_loss=0.3687262535095215\n",
            "step 1263: generator_loss=1.1842694282531738, discriminator_loss=0.3660178780555725\n",
            "step 1264: generator_loss=1.1921021938323975, discriminator_loss=0.3643433749675751\n",
            "step 1265: generator_loss=1.1879475116729736, discriminator_loss=0.36551088094711304\n",
            "step 1266: generator_loss=1.196342945098877, discriminator_loss=0.36377599835395813\n",
            "step 1267: generator_loss=1.198921799659729, discriminator_loss=0.36367279291152954\n",
            "step 1268: generator_loss=1.2053189277648926, discriminator_loss=0.3623724579811096\n",
            "step 1269: generator_loss=1.2066210508346558, discriminator_loss=0.36224520206451416\n",
            "step 1270: generator_loss=1.202216625213623, discriminator_loss=0.36344730854034424\n",
            "step 1271: generator_loss=1.2134766578674316, discriminator_loss=0.36127012968063354\n",
            "step 1272: generator_loss=1.2316138744354248, discriminator_loss=0.35777270793914795\n",
            "step 1273: generator_loss=1.2287756204605103, discriminator_loss=0.3582225441932678\n",
            "step 1274: generator_loss=1.2372651100158691, discriminator_loss=0.3563491702079773\n",
            "step 1275: generator_loss=1.243578314781189, discriminator_loss=0.3545566201210022\n",
            "step 1276: generator_loss=1.2386517524719238, discriminator_loss=0.3550415635108948\n",
            "step 1277: generator_loss=1.228227138519287, discriminator_loss=0.35664841532707214\n",
            "step 1278: generator_loss=1.224637508392334, discriminator_loss=0.35652992129325867\n",
            "step 1279: generator_loss=1.2238407135009766, discriminator_loss=0.3559330701828003\n",
            "step 1280: generator_loss=1.2271994352340698, discriminator_loss=0.3546065390110016\n",
            "step 1281: generator_loss=1.2356975078582764, discriminator_loss=0.3520345687866211\n",
            "step 1282: generator_loss=1.255577802658081, discriminator_loss=0.34739693999290466\n",
            "step 1283: generator_loss=1.2825641632080078, discriminator_loss=0.34171533584594727\n",
            "step 1284: generator_loss=1.3116967678070068, discriminator_loss=0.33579033613204956\n",
            "step 1285: generator_loss=1.340677261352539, discriminator_loss=0.3300808370113373\n",
            "step 1286: generator_loss=1.3663220405578613, discriminator_loss=0.32530975341796875\n",
            "step 1287: generator_loss=1.3897147178649902, discriminator_loss=0.32099831104278564\n",
            "step 1288: generator_loss=1.4146922826766968, discriminator_loss=0.31650909781455994\n",
            "step 1289: generator_loss=1.4399774074554443, discriminator_loss=0.3119649589061737\n",
            "step 1290: generator_loss=1.4781156778335571, discriminator_loss=0.30554091930389404\n",
            "step 1291: generator_loss=1.5089008808135986, discriminator_loss=0.30021700263023376\n",
            "step 1292: generator_loss=1.535044550895691, discriminator_loss=0.2957109212875366\n",
            "step 1293: generator_loss=1.5600334405899048, discriminator_loss=0.291294127702713\n",
            "step 1294: generator_loss=1.5800862312316895, discriminator_loss=0.2877185046672821\n",
            "step 1295: generator_loss=1.5942785739898682, discriminator_loss=0.28490808606147766\n",
            "step 1296: generator_loss=1.6040188074111938, discriminator_loss=0.28279921412467957\n",
            "step 1297: generator_loss=1.614172101020813, discriminator_loss=0.2805410623550415\n",
            "step 1298: generator_loss=1.6201095581054688, discriminator_loss=0.2787228524684906\n",
            "step 1299: generator_loss=1.619644045829773, discriminator_loss=0.27775174379348755\n",
            "step 1300: generator_loss=1.612642526626587, discriminator_loss=0.2776031196117401\n",
            "step 1301: generator_loss=1.6087524890899658, discriminator_loss=0.27727749943733215\n",
            "step 1302: generator_loss=1.6013226509094238, discriminator_loss=0.27741217613220215\n",
            "step 1303: generator_loss=1.5914959907531738, discriminator_loss=0.2778646945953369\n",
            "step 1304: generator_loss=1.5822654962539673, discriminator_loss=0.278285950422287\n",
            "step 1305: generator_loss=1.5659549236297607, discriminator_loss=0.27948975563049316\n",
            "step 1306: generator_loss=1.5537374019622803, discriminator_loss=0.2802673578262329\n",
            "step 1307: generator_loss=1.5348869562149048, discriminator_loss=0.2819599509239197\n",
            "step 1308: generator_loss=1.5136117935180664, discriminator_loss=0.283986896276474\n",
            "step 1309: generator_loss=1.4868333339691162, discriminator_loss=0.2870410680770874\n",
            "step 1310: generator_loss=1.4609096050262451, discriminator_loss=0.2901744544506073\n",
            "step 1311: generator_loss=1.436112403869629, discriminator_loss=0.29352372884750366\n",
            "step 1312: generator_loss=1.4049899578094482, discriminator_loss=0.29790636897087097\n",
            "step 1313: generator_loss=1.3782392740249634, discriminator_loss=0.3018779158592224\n",
            "step 1314: generator_loss=1.3572824001312256, discriminator_loss=0.3050476312637329\n",
            "step 1315: generator_loss=1.3390710353851318, discriminator_loss=0.3079821467399597\n",
            "step 1316: generator_loss=1.3251807689666748, discriminator_loss=0.3102763295173645\n",
            "step 1317: generator_loss=1.3112294673919678, discriminator_loss=0.31274378299713135\n",
            "step 1318: generator_loss=1.2979391813278198, discriminator_loss=0.31508368253707886\n",
            "step 1319: generator_loss=1.283872365951538, discriminator_loss=0.3178950548171997\n",
            "step 1320: generator_loss=1.2731393575668335, discriminator_loss=0.3201361298561096\n",
            "step 1321: generator_loss=1.2655988931655884, discriminator_loss=0.3219088315963745\n",
            "step 1322: generator_loss=1.2565038204193115, discriminator_loss=0.324066162109375\n",
            "step 1323: generator_loss=1.2530772686004639, discriminator_loss=0.32515186071395874\n",
            "step 1324: generator_loss=1.2491308450698853, discriminator_loss=0.32616040110588074\n",
            "step 1325: generator_loss=1.2434784173965454, discriminator_loss=0.32764726877212524\n",
            "step 1326: generator_loss=1.2394421100616455, discriminator_loss=0.3287278413772583\n",
            "step 1327: generator_loss=1.2340338230133057, discriminator_loss=0.3301575481891632\n",
            "step 1328: generator_loss=1.2269054651260376, discriminator_loss=0.33193838596343994\n",
            "step 1329: generator_loss=1.2253143787384033, discriminator_loss=0.3326539993286133\n",
            "step 1330: generator_loss=1.2268946170806885, discriminator_loss=0.33273348212242126\n",
            "step 1331: generator_loss=1.2269058227539062, discriminator_loss=0.3332476317882538\n",
            "step 1332: generator_loss=1.2308069467544556, discriminator_loss=0.332890123128891\n",
            "step 1333: generator_loss=1.2375935316085815, discriminator_loss=0.33196306228637695\n",
            "step 1334: generator_loss=1.2473692893981934, discriminator_loss=0.3304798901081085\n",
            "step 1335: generator_loss=1.261335849761963, discriminator_loss=0.3280155658721924\n",
            "step 1336: generator_loss=1.2774293422698975, discriminator_loss=0.3254200220108032\n",
            "step 1337: generator_loss=1.2893965244293213, discriminator_loss=0.3236984610557556\n",
            "step 1338: generator_loss=1.3024100065231323, discriminator_loss=0.32172539830207825\n",
            "step 1339: generator_loss=1.3176662921905518, discriminator_loss=0.3193988502025604\n",
            "step 1340: generator_loss=1.3315531015396118, discriminator_loss=0.3173374831676483\n",
            "step 1341: generator_loss=1.347280740737915, discriminator_loss=0.31473851203918457\n",
            "step 1342: generator_loss=1.363600254058838, discriminator_loss=0.31218260526657104\n",
            "step 1343: generator_loss=1.3828554153442383, discriminator_loss=0.30887699127197266\n",
            "step 1344: generator_loss=1.3961882591247559, discriminator_loss=0.3066689968109131\n",
            "step 1345: generator_loss=1.4022595882415771, discriminator_loss=0.3053467571735382\n",
            "step 1346: generator_loss=1.4029247760772705, discriminator_loss=0.3048386871814728\n",
            "step 1347: generator_loss=1.3984169960021973, discriminator_loss=0.305210679769516\n",
            "step 1348: generator_loss=1.3901917934417725, discriminator_loss=0.30615270137786865\n",
            "step 1349: generator_loss=1.3743793964385986, discriminator_loss=0.3086148500442505\n",
            "step 1350: generator_loss=1.3578968048095703, discriminator_loss=0.3113745152950287\n",
            "step 1351: generator_loss=1.3376119136810303, discriminator_loss=0.3149360716342926\n",
            "step 1352: generator_loss=1.3220566511154175, discriminator_loss=0.3179051876068115\n",
            "step 1353: generator_loss=1.3033101558685303, discriminator_loss=0.3213953673839569\n",
            "step 1354: generator_loss=1.2865030765533447, discriminator_loss=0.32476115226745605\n",
            "step 1355: generator_loss=1.280343770980835, discriminator_loss=0.3259972929954529\n",
            "step 1356: generator_loss=1.275219202041626, discriminator_loss=0.32707011699676514\n",
            "step 1357: generator_loss=1.2819041013717651, discriminator_loss=0.3260241746902466\n",
            "step 1358: generator_loss=1.2965598106384277, discriminator_loss=0.3232553005218506\n",
            "step 1359: generator_loss=1.319983959197998, discriminator_loss=0.31888675689697266\n",
            "step 1360: generator_loss=1.3436700105667114, discriminator_loss=0.314680278301239\n",
            "step 1361: generator_loss=1.3796011209487915, discriminator_loss=0.30864202976226807\n",
            "step 1362: generator_loss=1.4082939624786377, discriminator_loss=0.3037380874156952\n",
            "step 1363: generator_loss=1.4414310455322266, discriminator_loss=0.2982938885688782\n",
            "step 1364: generator_loss=1.4780895709991455, discriminator_loss=0.29240190982818604\n",
            "step 1365: generator_loss=1.5195964574813843, discriminator_loss=0.28625375032424927\n",
            "step 1366: generator_loss=1.5517594814300537, discriminator_loss=0.28162717819213867\n",
            "step 1367: generator_loss=1.571296215057373, discriminator_loss=0.2785434424877167\n",
            "step 1368: generator_loss=1.5797570943832397, discriminator_loss=0.27703583240509033\n",
            "step 1369: generator_loss=1.604933738708496, discriminator_loss=0.27333885431289673\n",
            "step 1370: generator_loss=1.62221097946167, discriminator_loss=0.2707705497741699\n",
            "step 1371: generator_loss=1.6422255039215088, discriminator_loss=0.26782792806625366\n",
            "step 1372: generator_loss=1.6571142673492432, discriminator_loss=0.26567551493644714\n",
            "step 1373: generator_loss=1.6655343770980835, discriminator_loss=0.26436179876327515\n",
            "step 1374: generator_loss=1.6665375232696533, discriminator_loss=0.26377665996551514\n",
            "step 1375: generator_loss=1.6598622798919678, discriminator_loss=0.26427239179611206\n",
            "step 1376: generator_loss=1.6462721824645996, discriminator_loss=0.26553186774253845\n",
            "step 1377: generator_loss=1.6266734600067139, discriminator_loss=0.26744359731674194\n",
            "step 1378: generator_loss=1.6047070026397705, discriminator_loss=0.2696688771247864\n",
            "step 1379: generator_loss=1.578450322151184, discriminator_loss=0.2725892663002014\n",
            "step 1380: generator_loss=1.5466387271881104, discriminator_loss=0.27616727352142334\n",
            "step 1381: generator_loss=1.5139989852905273, discriminator_loss=0.2802686095237732\n",
            "step 1382: generator_loss=1.4721662998199463, discriminator_loss=0.2859577536582947\n",
            "step 1383: generator_loss=1.434172511100769, discriminator_loss=0.29144513607025146\n",
            "step 1384: generator_loss=1.4002145528793335, discriminator_loss=0.2965790629386902\n",
            "step 1385: generator_loss=1.3735377788543701, discriminator_loss=0.3007827401161194\n",
            "step 1386: generator_loss=1.3516247272491455, discriminator_loss=0.30440637469291687\n",
            "step 1387: generator_loss=1.32953679561615, discriminator_loss=0.3083685040473938\n",
            "step 1388: generator_loss=1.3094351291656494, discriminator_loss=0.3122347593307495\n",
            "step 1389: generator_loss=1.29345703125, discriminator_loss=0.315292090177536\n",
            "step 1390: generator_loss=1.2733771800994873, discriminator_loss=0.31918978691101074\n",
            "step 1391: generator_loss=1.2494275569915771, discriminator_loss=0.3243125081062317\n",
            "step 1392: generator_loss=1.230931043624878, discriminator_loss=0.3283705711364746\n",
            "step 1393: generator_loss=1.2222273349761963, discriminator_loss=0.33083993196487427\n",
            "step 1394: generator_loss=1.2239041328430176, discriminator_loss=0.33116722106933594\n",
            "step 1395: generator_loss=1.227964997291565, discriminator_loss=0.33105915784835815\n",
            "step 1396: generator_loss=1.2508213520050049, discriminator_loss=0.3271338641643524\n",
            "step 1397: generator_loss=1.2764661312103271, discriminator_loss=0.32264551520347595\n",
            "step 1398: generator_loss=1.2967051267623901, discriminator_loss=0.3195376992225647\n",
            "step 1399: generator_loss=1.3092496395111084, discriminator_loss=0.31784218549728394\n",
            "step 1400: generator_loss=1.3169807195663452, discriminator_loss=0.3170596957206726\n",
            "step 1401: generator_loss=1.3219484090805054, discriminator_loss=0.31689709424972534\n",
            "step 1402: generator_loss=1.3316307067871094, discriminator_loss=0.3157516121864319\n",
            "step 1403: generator_loss=1.3523595333099365, discriminator_loss=0.312730610370636\n",
            "step 1404: generator_loss=1.3767969608306885, discriminator_loss=0.3091045022010803\n",
            "step 1405: generator_loss=1.4017572402954102, discriminator_loss=0.305320143699646\n",
            "step 1406: generator_loss=1.428244709968567, discriminator_loss=0.3013741075992584\n",
            "step 1407: generator_loss=1.4476515054702759, discriminator_loss=0.2983880341053009\n",
            "step 1408: generator_loss=1.4613032341003418, discriminator_loss=0.2963753640651703\n",
            "step 1409: generator_loss=1.4744716882705688, discriminator_loss=0.2942548990249634\n",
            "step 1410: generator_loss=1.4859979152679443, discriminator_loss=0.2922508716583252\n",
            "step 1411: generator_loss=1.4899671077728271, discriminator_loss=0.29141247272491455\n",
            "step 1412: generator_loss=1.4849998950958252, discriminator_loss=0.2920805513858795\n",
            "step 1413: generator_loss=1.4791852235794067, discriminator_loss=0.2927713990211487\n",
            "step 1414: generator_loss=1.471569299697876, discriminator_loss=0.2939278185367584\n",
            "step 1415: generator_loss=1.4566665887832642, discriminator_loss=0.2962512969970703\n",
            "step 1416: generator_loss=1.4388831853866577, discriminator_loss=0.2989997863769531\n",
            "step 1417: generator_loss=1.426133394241333, discriminator_loss=0.30109119415283203\n",
            "step 1418: generator_loss=1.4145692586898804, discriminator_loss=0.30292457342147827\n",
            "step 1419: generator_loss=1.420506477355957, discriminator_loss=0.3018776774406433\n",
            "step 1420: generator_loss=1.4357085227966309, discriminator_loss=0.299088716506958\n",
            "step 1421: generator_loss=1.44838285446167, discriminator_loss=0.2968105673789978\n",
            "step 1422: generator_loss=1.4649484157562256, discriminator_loss=0.29379719495773315\n",
            "step 1423: generator_loss=1.4763562679290771, discriminator_loss=0.29142680764198303\n",
            "step 1424: generator_loss=1.4946246147155762, discriminator_loss=0.2881239056587219\n",
            "step 1425: generator_loss=1.5169548988342285, discriminator_loss=0.28425610065460205\n",
            "step 1426: generator_loss=1.5408306121826172, discriminator_loss=0.28024226427078247\n",
            "step 1427: generator_loss=1.5600090026855469, discriminator_loss=0.2769497334957123\n",
            "step 1428: generator_loss=1.5713257789611816, discriminator_loss=0.2746652364730835\n",
            "step 1429: generator_loss=1.5829977989196777, discriminator_loss=0.2722119092941284\n",
            "step 1430: generator_loss=1.5906383991241455, discriminator_loss=0.2704501152038574\n",
            "step 1431: generator_loss=1.586531639099121, discriminator_loss=0.2700628638267517\n",
            "step 1432: generator_loss=1.5768711566925049, discriminator_loss=0.2703733742237091\n",
            "step 1433: generator_loss=1.5621651411056519, discriminator_loss=0.2714751958847046\n",
            "step 1434: generator_loss=1.5442678928375244, discriminator_loss=0.273002028465271\n",
            "step 1435: generator_loss=1.5235304832458496, discriminator_loss=0.27512991428375244\n",
            "step 1436: generator_loss=1.4995757341384888, discriminator_loss=0.2778017818927765\n",
            "step 1437: generator_loss=1.4739959239959717, discriminator_loss=0.28097641468048096\n",
            "step 1438: generator_loss=1.453251838684082, discriminator_loss=0.2835272550582886\n",
            "step 1439: generator_loss=1.430861234664917, discriminator_loss=0.2865346074104309\n",
            "step 1440: generator_loss=1.4070777893066406, discriminator_loss=0.2899205684661865\n",
            "step 1441: generator_loss=1.3830137252807617, discriminator_loss=0.2936611771583557\n",
            "step 1442: generator_loss=1.3593199253082275, discriminator_loss=0.2975214123725891\n",
            "step 1443: generator_loss=1.335935115814209, discriminator_loss=0.3015071749687195\n",
            "step 1444: generator_loss=1.3126788139343262, discriminator_loss=0.3055226802825928\n",
            "step 1445: generator_loss=1.2904094457626343, discriminator_loss=0.30956757068634033\n",
            "step 1446: generator_loss=1.2738454341888428, discriminator_loss=0.31274840235710144\n",
            "step 1447: generator_loss=1.258707046508789, discriminator_loss=0.3157675266265869\n",
            "step 1448: generator_loss=1.2478276491165161, discriminator_loss=0.3180989623069763\n",
            "step 1449: generator_loss=1.2472316026687622, discriminator_loss=0.3182425796985626\n",
            "step 1450: generator_loss=1.2503724098205566, discriminator_loss=0.3179166316986084\n",
            "step 1451: generator_loss=1.2560932636260986, discriminator_loss=0.3171544373035431\n",
            "step 1452: generator_loss=1.2634155750274658, discriminator_loss=0.3160494565963745\n",
            "step 1453: generator_loss=1.269669532775879, discriminator_loss=0.3151649236679077\n",
            "step 1454: generator_loss=1.2747178077697754, discriminator_loss=0.3145293593406677\n",
            "step 1455: generator_loss=1.278257966041565, discriminator_loss=0.3142045736312866\n",
            "step 1456: generator_loss=1.2792203426361084, discriminator_loss=0.31431639194488525\n",
            "step 1457: generator_loss=1.2778948545455933, discriminator_loss=0.31511953473091125\n",
            "step 1458: generator_loss=1.2771426439285278, discriminator_loss=0.3158600926399231\n",
            "step 1459: generator_loss=1.2749884128570557, discriminator_loss=0.31672438979148865\n",
            "step 1460: generator_loss=1.2794067859649658, discriminator_loss=0.3164916932582855\n",
            "step 1461: generator_loss=1.2829142808914185, discriminator_loss=0.31618738174438477\n",
            "step 1462: generator_loss=1.2903704643249512, discriminator_loss=0.3152924180030823\n",
            "step 1463: generator_loss=1.297769546508789, discriminator_loss=0.314475953578949\n",
            "step 1464: generator_loss=1.3185220956802368, discriminator_loss=0.31109941005706787\n",
            "step 1465: generator_loss=1.335357904434204, discriminator_loss=0.3086928129196167\n",
            "step 1466: generator_loss=1.3631956577301025, discriminator_loss=0.30409497022628784\n",
            "step 1467: generator_loss=1.385583758354187, discriminator_loss=0.30065444111824036\n",
            "step 1468: generator_loss=1.405031681060791, discriminator_loss=0.29772841930389404\n",
            "step 1469: generator_loss=1.4414751529693604, discriminator_loss=0.29219168424606323\n",
            "step 1470: generator_loss=1.4746408462524414, discriminator_loss=0.28739818930625916\n",
            "step 1471: generator_loss=1.5163068771362305, discriminator_loss=0.2813682556152344\n",
            "step 1472: generator_loss=1.5606399774551392, discriminator_loss=0.27570414543151855\n",
            "step 1473: generator_loss=1.5869245529174805, discriminator_loss=0.27178874611854553\n",
            "step 1474: generator_loss=1.6282286643981934, discriminator_loss=0.2673853635787964\n",
            "step 1475: generator_loss=1.6379117965698242, discriminator_loss=0.2651546597480774\n",
            "step 1476: generator_loss=1.6570332050323486, discriminator_loss=0.2624094784259796\n",
            "step 1477: generator_loss=1.6827783584594727, discriminator_loss=0.25926560163497925\n",
            "step 1478: generator_loss=1.704278826713562, discriminator_loss=0.2570127248764038\n",
            "step 1479: generator_loss=1.6905962228775024, discriminator_loss=0.257251113653183\n",
            "step 1480: generator_loss=1.6975390911102295, discriminator_loss=0.25625601410865784\n",
            "step 1481: generator_loss=1.685019612312317, discriminator_loss=0.25677821040153503\n",
            "step 1482: generator_loss=1.6726839542388916, discriminator_loss=0.2573566734790802\n",
            "step 1483: generator_loss=1.6593482494354248, discriminator_loss=0.2579302191734314\n",
            "step 1484: generator_loss=1.6471154689788818, discriminator_loss=0.25846076011657715\n",
            "step 1485: generator_loss=1.6383063793182373, discriminator_loss=0.25866562128067017\n",
            "step 1486: generator_loss=1.6161683797836304, discriminator_loss=0.2605586647987366\n",
            "step 1487: generator_loss=1.6026471853256226, discriminator_loss=0.2615445852279663\n",
            "step 1488: generator_loss=1.5793323516845703, discriminator_loss=0.26379215717315674\n",
            "step 1489: generator_loss=1.5573105812072754, discriminator_loss=0.2661631405353546\n",
            "step 1490: generator_loss=1.5360760688781738, discriminator_loss=0.2683921754360199\n",
            "step 1491: generator_loss=1.517966866493225, discriminator_loss=0.2703278064727783\n",
            "step 1492: generator_loss=1.5085912942886353, discriminator_loss=0.27115902304649353\n",
            "step 1493: generator_loss=1.4833884239196777, discriminator_loss=0.2742508053779602\n",
            "step 1494: generator_loss=1.4829888343811035, discriminator_loss=0.2741861343383789\n",
            "step 1495: generator_loss=1.491422414779663, discriminator_loss=0.2724579870700836\n",
            "step 1496: generator_loss=1.4873945713043213, discriminator_loss=0.27197903394699097\n",
            "step 1497: generator_loss=1.5077261924743652, discriminator_loss=0.2686123549938202\n",
            "step 1498: generator_loss=1.5427448749542236, discriminator_loss=0.2635977864265442\n",
            "step 1499: generator_loss=1.5615348815917969, discriminator_loss=0.2606329321861267\n",
            "step 1500: generator_loss=1.585237741470337, discriminator_loss=0.2570292353630066\n",
            "step 1501: generator_loss=1.6052618026733398, discriminator_loss=0.25422170758247375\n",
            "step 1502: generator_loss=1.6244118213653564, discriminator_loss=0.2517229914665222\n",
            "step 1503: generator_loss=1.6383020877838135, discriminator_loss=0.24997416138648987\n",
            "step 1504: generator_loss=1.6434863805770874, discriminator_loss=0.2492542564868927\n",
            "step 1505: generator_loss=1.6487561464309692, discriminator_loss=0.24859261512756348\n",
            "step 1506: generator_loss=1.6481318473815918, discriminator_loss=0.2486587017774582\n",
            "step 1507: generator_loss=1.6481014490127563, discriminator_loss=0.2486516237258911\n",
            "step 1508: generator_loss=1.640621304512024, discriminator_loss=0.24942921102046967\n",
            "step 1509: generator_loss=1.6264119148254395, discriminator_loss=0.25116732716560364\n",
            "step 1510: generator_loss=1.6088416576385498, discriminator_loss=0.253174364566803\n",
            "step 1511: generator_loss=1.5832301378250122, discriminator_loss=0.2563585042953491\n",
            "step 1512: generator_loss=1.5777077674865723, discriminator_loss=0.2569352686405182\n",
            "step 1513: generator_loss=1.5554137229919434, discriminator_loss=0.2596849799156189\n",
            "step 1514: generator_loss=1.52899169921875, discriminator_loss=0.2632324993610382\n",
            "step 1515: generator_loss=1.5149704217910767, discriminator_loss=0.2649684548377991\n",
            "step 1516: generator_loss=1.4926719665527344, discriminator_loss=0.2679632306098938\n",
            "step 1517: generator_loss=1.4800268411636353, discriminator_loss=0.2697647213935852\n",
            "step 1518: generator_loss=1.454681158065796, discriminator_loss=0.27326929569244385\n",
            "step 1519: generator_loss=1.4458250999450684, discriminator_loss=0.274537593126297\n",
            "step 1520: generator_loss=1.4484126567840576, discriminator_loss=0.27401480078697205\n",
            "step 1521: generator_loss=1.4357497692108154, discriminator_loss=0.27603715658187866\n",
            "step 1522: generator_loss=1.4373414516448975, discriminator_loss=0.2756160497665405\n",
            "step 1523: generator_loss=1.4420373439788818, discriminator_loss=0.27506452798843384\n",
            "step 1524: generator_loss=1.4258875846862793, discriminator_loss=0.27769050002098083\n",
            "step 1525: generator_loss=1.4218919277191162, discriminator_loss=0.2783120274543762\n",
            "step 1526: generator_loss=1.401239037513733, discriminator_loss=0.28207364678382874\n",
            "step 1527: generator_loss=1.4050099849700928, discriminator_loss=0.2816331684589386\n",
            "step 1528: generator_loss=1.3953431844711304, discriminator_loss=0.28354382514953613\n",
            "step 1529: generator_loss=1.3845099210739136, discriminator_loss=0.2860047221183777\n",
            "step 1530: generator_loss=1.392073154449463, discriminator_loss=0.28455138206481934\n",
            "step 1531: generator_loss=1.3845434188842773, discriminator_loss=0.28602153062820435\n",
            "step 1532: generator_loss=1.383697271347046, discriminator_loss=0.2865331172943115\n",
            "step 1533: generator_loss=1.4028851985931396, discriminator_loss=0.2831646203994751\n",
            "step 1534: generator_loss=1.3992115259170532, discriminator_loss=0.28438088297843933\n",
            "step 1535: generator_loss=1.4018466472625732, discriminator_loss=0.28420722484588623\n",
            "step 1536: generator_loss=1.4245049953460693, discriminator_loss=0.2806260585784912\n",
            "step 1537: generator_loss=1.4310234785079956, discriminator_loss=0.27986305952072144\n",
            "step 1538: generator_loss=1.445948839187622, discriminator_loss=0.27752095460891724\n",
            "step 1539: generator_loss=1.458413004875183, discriminator_loss=0.27556493878364563\n",
            "step 1540: generator_loss=1.4804699420928955, discriminator_loss=0.2726064920425415\n",
            "step 1541: generator_loss=1.501619577407837, discriminator_loss=0.26974159479141235\n",
            "step 1542: generator_loss=1.5337584018707275, discriminator_loss=0.26541396975517273\n",
            "step 1543: generator_loss=1.5452237129211426, discriminator_loss=0.2638283371925354\n",
            "step 1544: generator_loss=1.5781922340393066, discriminator_loss=0.2594883441925049\n",
            "step 1545: generator_loss=1.593957781791687, discriminator_loss=0.2578972578048706\n",
            "step 1546: generator_loss=1.5855296850204468, discriminator_loss=0.25885501503944397\n",
            "step 1547: generator_loss=1.5908945798873901, discriminator_loss=0.25802475214004517\n",
            "step 1548: generator_loss=1.582147479057312, discriminator_loss=0.2594246566295624\n",
            "step 1549: generator_loss=1.596577763557434, discriminator_loss=0.2573612630367279\n",
            "step 1550: generator_loss=1.591854214668274, discriminator_loss=0.25798553228378296\n",
            "step 1551: generator_loss=1.585514783859253, discriminator_loss=0.25885578989982605\n",
            "step 1552: generator_loss=1.575799822807312, discriminator_loss=0.25970277190208435\n",
            "step 1553: generator_loss=1.5693657398223877, discriminator_loss=0.26049694418907166\n",
            "step 1554: generator_loss=1.5585132837295532, discriminator_loss=0.26196956634521484\n",
            "step 1555: generator_loss=1.5702272653579712, discriminator_loss=0.2605877220630646\n",
            "step 1556: generator_loss=1.5775244235992432, discriminator_loss=0.2592376470565796\n",
            "step 1557: generator_loss=1.5481867790222168, discriminator_loss=0.26314854621887207\n",
            "step 1558: generator_loss=1.5606322288513184, discriminator_loss=0.2614377737045288\n",
            "step 1559: generator_loss=1.553924322128296, discriminator_loss=0.2620151937007904\n",
            "step 1560: generator_loss=1.5534169673919678, discriminator_loss=0.2620288133621216\n",
            "step 1561: generator_loss=1.5321083068847656, discriminator_loss=0.26489728689193726\n",
            "step 1562: generator_loss=1.528695821762085, discriminator_loss=0.26509881019592285\n",
            "step 1563: generator_loss=1.5419684648513794, discriminator_loss=0.2631528377532959\n",
            "step 1564: generator_loss=1.5298370122909546, discriminator_loss=0.26460421085357666\n",
            "step 1565: generator_loss=1.532615303993225, discriminator_loss=0.2639123201370239\n",
            "step 1566: generator_loss=1.5027483701705933, discriminator_loss=0.267994225025177\n",
            "step 1567: generator_loss=1.5003695487976074, discriminator_loss=0.26850205659866333\n",
            "step 1568: generator_loss=1.4937105178833008, discriminator_loss=0.26912930607795715\n",
            "step 1569: generator_loss=1.488381266593933, discriminator_loss=0.2695643901824951\n",
            "step 1570: generator_loss=1.4852465391159058, discriminator_loss=0.2699343264102936\n",
            "step 1571: generator_loss=1.476845145225525, discriminator_loss=0.27117329835891724\n",
            "step 1572: generator_loss=1.48122239112854, discriminator_loss=0.27074164152145386\n",
            "step 1573: generator_loss=1.493187427520752, discriminator_loss=0.26901137828826904\n",
            "step 1574: generator_loss=1.4905871152877808, discriminator_loss=0.2692564129829407\n",
            "step 1575: generator_loss=1.4950422048568726, discriminator_loss=0.2684893310070038\n",
            "step 1576: generator_loss=1.4843212366104126, discriminator_loss=0.2698938250541687\n",
            "step 1577: generator_loss=1.4798715114593506, discriminator_loss=0.27037250995635986\n",
            "step 1578: generator_loss=1.4777507781982422, discriminator_loss=0.27044981718063354\n",
            "step 1579: generator_loss=1.4666199684143066, discriminator_loss=0.27182602882385254\n",
            "step 1580: generator_loss=1.4492201805114746, discriminator_loss=0.27418607473373413\n",
            "step 1581: generator_loss=1.4351708889007568, discriminator_loss=0.27608826756477356\n",
            "step 1582: generator_loss=1.4247876405715942, discriminator_loss=0.2776273190975189\n",
            "step 1583: generator_loss=1.4153369665145874, discriminator_loss=0.2790679633617401\n",
            "step 1584: generator_loss=1.4130408763885498, discriminator_loss=0.2794284224510193\n",
            "step 1585: generator_loss=1.4154648780822754, discriminator_loss=0.2790681719779968\n",
            "step 1586: generator_loss=1.4226964712142944, discriminator_loss=0.2779526114463806\n",
            "step 1587: generator_loss=1.4400570392608643, discriminator_loss=0.2753063440322876\n",
            "step 1588: generator_loss=1.4537827968597412, discriminator_loss=0.27328604459762573\n",
            "step 1589: generator_loss=1.4724609851837158, discriminator_loss=0.2704147696495056\n",
            "step 1590: generator_loss=1.4808201789855957, discriminator_loss=0.2691844701766968\n",
            "step 1591: generator_loss=1.4874227046966553, discriminator_loss=0.2683008313179016\n",
            "step 1592: generator_loss=1.4917606115341187, discriminator_loss=0.2677176594734192\n",
            "step 1593: generator_loss=1.4951860904693604, discriminator_loss=0.26727205514907837\n",
            "step 1594: generator_loss=1.4916212558746338, discriminator_loss=0.2679222822189331\n",
            "step 1595: generator_loss=1.4814379215240479, discriminator_loss=0.2695135474205017\n",
            "step 1596: generator_loss=1.4643807411193848, discriminator_loss=0.27231305837631226\n",
            "step 1597: generator_loss=1.4724295139312744, discriminator_loss=0.2712719738483429\n",
            "step 1598: generator_loss=1.4804368019104004, discriminator_loss=0.2702008783817291\n",
            "step 1599: generator_loss=1.4855197668075562, discriminator_loss=0.2695814371109009\n",
            "step 1600: generator_loss=1.4861228466033936, discriminator_loss=0.26950323581695557\n",
            "step 1601: generator_loss=1.4831334352493286, discriminator_loss=0.2700396180152893\n",
            "step 1602: generator_loss=1.4778039455413818, discriminator_loss=0.2709294557571411\n",
            "step 1603: generator_loss=1.462134838104248, discriminator_loss=0.27345800399780273\n",
            "step 1604: generator_loss=1.4563078880310059, discriminator_loss=0.2746261954307556\n",
            "step 1605: generator_loss=1.4459229707717896, discriminator_loss=0.2762775719165802\n",
            "step 1606: generator_loss=1.4391825199127197, discriminator_loss=0.27751263976097107\n",
            "step 1607: generator_loss=1.436769723892212, discriminator_loss=0.2780749797821045\n",
            "step 1608: generator_loss=1.4436547756195068, discriminator_loss=0.27717819809913635\n",
            "step 1609: generator_loss=1.4643478393554688, discriminator_loss=0.27421486377716064\n",
            "step 1610: generator_loss=1.4924468994140625, discriminator_loss=0.2702692151069641\n",
            "step 1611: generator_loss=1.515146255493164, discriminator_loss=0.2672291696071625\n",
            "step 1612: generator_loss=1.5316894054412842, discriminator_loss=0.2650804817676544\n",
            "step 1613: generator_loss=1.5421290397644043, discriminator_loss=0.26383429765701294\n",
            "step 1614: generator_loss=1.5518066883087158, discriminator_loss=0.26258987188339233\n",
            "step 1615: generator_loss=1.5539000034332275, discriminator_loss=0.26228928565979004\n",
            "step 1616: generator_loss=1.554316520690918, discriminator_loss=0.2622467577457428\n",
            "step 1617: generator_loss=1.5620120763778687, discriminator_loss=0.261194109916687\n",
            "step 1618: generator_loss=1.5730465650558472, discriminator_loss=0.259728342294693\n",
            "step 1619: generator_loss=1.5798563957214355, discriminator_loss=0.2588750123977661\n",
            "step 1620: generator_loss=1.5790040493011475, discriminator_loss=0.25888168811798096\n",
            "step 1621: generator_loss=1.5711781978607178, discriminator_loss=0.2600017786026001\n",
            "step 1622: generator_loss=1.558293104171753, discriminator_loss=0.2616179287433624\n",
            "step 1623: generator_loss=1.5398902893066406, discriminator_loss=0.264009952545166\n",
            "step 1624: generator_loss=1.5119690895080566, discriminator_loss=0.2677990198135376\n",
            "step 1625: generator_loss=1.4757111072540283, discriminator_loss=0.2729578912258148\n",
            "step 1626: generator_loss=1.4345176219940186, discriminator_loss=0.2792730927467346\n",
            "step 1627: generator_loss=1.4018824100494385, discriminator_loss=0.2846142649650574\n",
            "step 1628: generator_loss=1.377229928970337, discriminator_loss=0.28899142146110535\n",
            "step 1629: generator_loss=1.3563182353973389, discriminator_loss=0.29300540685653687\n",
            "step 1630: generator_loss=1.3471022844314575, discriminator_loss=0.2950167655944824\n",
            "step 1631: generator_loss=1.3420131206512451, discriminator_loss=0.29645031690597534\n",
            "step 1632: generator_loss=1.3449811935424805, discriminator_loss=0.2965371012687683\n",
            "step 1633: generator_loss=1.3497064113616943, discriminator_loss=0.29623082280158997\n",
            "step 1634: generator_loss=1.3712081909179688, discriminator_loss=0.293174147605896\n",
            "step 1635: generator_loss=1.4133410453796387, discriminator_loss=0.2866820693016052\n",
            "step 1636: generator_loss=1.4541049003601074, discriminator_loss=0.28062957525253296\n",
            "step 1637: generator_loss=1.491401195526123, discriminator_loss=0.2753514051437378\n",
            "step 1638: generator_loss=1.5330756902694702, discriminator_loss=0.2695571482181549\n",
            "step 1639: generator_loss=1.5710821151733398, discriminator_loss=0.26430878043174744\n",
            "step 1640: generator_loss=1.606084942817688, discriminator_loss=0.2597047984600067\n",
            "step 1641: generator_loss=1.6422709226608276, discriminator_loss=0.255291610956192\n",
            "step 1642: generator_loss=1.6686725616455078, discriminator_loss=0.2518618106842041\n",
            "step 1643: generator_loss=1.6864787340164185, discriminator_loss=0.24956965446472168\n",
            "step 1644: generator_loss=1.6979477405548096, discriminator_loss=0.2480563521385193\n",
            "step 1645: generator_loss=1.69417142868042, discriminator_loss=0.24789494276046753\n",
            "step 1646: generator_loss=1.6858354806900024, discriminator_loss=0.2481861412525177\n",
            "step 1647: generator_loss=1.6726093292236328, discriminator_loss=0.24908192455768585\n",
            "step 1648: generator_loss=1.6410561800003052, discriminator_loss=0.2520923614501953\n",
            "step 1649: generator_loss=1.6114023923873901, discriminator_loss=0.255016028881073\n",
            "step 1650: generator_loss=1.5756199359893799, discriminator_loss=0.2589578330516815\n",
            "step 1651: generator_loss=1.5345706939697266, discriminator_loss=0.26375043392181396\n",
            "step 1652: generator_loss=1.5016961097717285, discriminator_loss=0.2678491473197937\n",
            "step 1653: generator_loss=1.4710885286331177, discriminator_loss=0.27192223072052\n",
            "step 1654: generator_loss=1.4460415840148926, discriminator_loss=0.27547499537467957\n",
            "step 1655: generator_loss=1.4253408908843994, discriminator_loss=0.2785528302192688\n",
            "step 1656: generator_loss=1.4036730527877808, discriminator_loss=0.2819734811782837\n",
            "step 1657: generator_loss=1.3805105686187744, discriminator_loss=0.28577572107315063\n",
            "step 1658: generator_loss=1.3636316061019897, discriminator_loss=0.28882351517677307\n",
            "step 1659: generator_loss=1.3567066192626953, discriminator_loss=0.29020124673843384\n",
            "step 1660: generator_loss=1.3548071384429932, discriminator_loss=0.2907099723815918\n",
            "step 1661: generator_loss=1.3631881475448608, discriminator_loss=0.28949645161628723\n",
            "step 1662: generator_loss=1.371886968612671, discriminator_loss=0.28819894790649414\n",
            "step 1663: generator_loss=1.3884949684143066, discriminator_loss=0.2857307195663452\n",
            "step 1664: generator_loss=1.40571928024292, discriminator_loss=0.28323128819465637\n",
            "step 1665: generator_loss=1.4237254858016968, discriminator_loss=0.28078770637512207\n",
            "step 1666: generator_loss=1.4345215559005737, discriminator_loss=0.2795068323612213\n",
            "step 1667: generator_loss=1.4452457427978516, discriminator_loss=0.2780812382698059\n",
            "step 1668: generator_loss=1.4616594314575195, discriminator_loss=0.27601468563079834\n",
            "step 1669: generator_loss=1.4749164581298828, discriminator_loss=0.27437859773635864\n",
            "step 1670: generator_loss=1.482094168663025, discriminator_loss=0.27357733249664307\n",
            "step 1671: generator_loss=1.4906706809997559, discriminator_loss=0.2727145850658417\n",
            "step 1672: generator_loss=1.5021557807922363, discriminator_loss=0.2714432179927826\n",
            "step 1673: generator_loss=1.5112605094909668, discriminator_loss=0.27054691314697266\n",
            "step 1674: generator_loss=1.5140794515609741, discriminator_loss=0.27052512764930725\n",
            "step 1675: generator_loss=1.5155396461486816, discriminator_loss=0.2703632712364197\n",
            "step 1676: generator_loss=1.5212898254394531, discriminator_loss=0.26954221725463867\n",
            "step 1677: generator_loss=1.5338363647460938, discriminator_loss=0.2676289677619934\n",
            "step 1678: generator_loss=1.5460853576660156, discriminator_loss=0.26578372716903687\n",
            "step 1679: generator_loss=1.5580354928970337, discriminator_loss=0.26391318440437317\n",
            "step 1680: generator_loss=1.5718438625335693, discriminator_loss=0.2619059085845947\n",
            "step 1681: generator_loss=1.5854823589324951, discriminator_loss=0.259931743144989\n",
            "step 1682: generator_loss=1.5691848993301392, discriminator_loss=0.26177117228507996\n",
            "step 1683: generator_loss=1.5671378374099731, discriminator_loss=0.2617088258266449\n",
            "step 1684: generator_loss=1.5626269578933716, discriminator_loss=0.26207852363586426\n",
            "step 1685: generator_loss=1.5557446479797363, discriminator_loss=0.26265186071395874\n",
            "step 1686: generator_loss=1.5457963943481445, discriminator_loss=0.2636782228946686\n",
            "step 1687: generator_loss=1.5293771028518677, discriminator_loss=0.2655029594898224\n",
            "step 1688: generator_loss=1.5162601470947266, discriminator_loss=0.2668461203575134\n",
            "step 1689: generator_loss=1.4936621189117432, discriminator_loss=0.2699013948440552\n",
            "step 1690: generator_loss=1.4914188385009766, discriminator_loss=0.2703717052936554\n",
            "step 1691: generator_loss=1.4669221639633179, discriminator_loss=0.27313846349716187\n",
            "step 1692: generator_loss=1.4577791690826416, discriminator_loss=0.27397841215133667\n",
            "step 1693: generator_loss=1.4413714408874512, discriminator_loss=0.27610257267951965\n",
            "step 1694: generator_loss=1.4263478517532349, discriminator_loss=0.2782307267189026\n",
            "step 1695: generator_loss=1.4177734851837158, discriminator_loss=0.2796263098716736\n",
            "step 1696: generator_loss=1.4105194807052612, discriminator_loss=0.2801728844642639\n",
            "step 1697: generator_loss=1.4150769710540771, discriminator_loss=0.2792724668979645\n",
            "step 1698: generator_loss=1.42020583152771, discriminator_loss=0.27837106585502625\n",
            "step 1699: generator_loss=1.444040298461914, discriminator_loss=0.2745409309864044\n",
            "step 1700: generator_loss=1.459782361984253, discriminator_loss=0.2721189260482788\n",
            "step 1701: generator_loss=1.4931669235229492, discriminator_loss=0.26715004444122314\n",
            "step 1702: generator_loss=1.5373852252960205, discriminator_loss=0.2608093023300171\n",
            "step 1703: generator_loss=1.5798184871673584, discriminator_loss=0.2551022171974182\n",
            "step 1704: generator_loss=1.653379201889038, discriminator_loss=0.24583932757377625\n",
            "step 1705: generator_loss=1.6950327157974243, discriminator_loss=0.24097415804862976\n",
            "step 1706: generator_loss=1.752564549446106, discriminator_loss=0.23444177210330963\n",
            "step 1707: generator_loss=1.7502613067626953, discriminator_loss=0.23459969460964203\n",
            "step 1708: generator_loss=1.7612321376800537, discriminator_loss=0.23320603370666504\n",
            "step 1709: generator_loss=1.7485718727111816, discriminator_loss=0.2342304289340973\n",
            "step 1710: generator_loss=1.7600040435791016, discriminator_loss=0.23236584663391113\n",
            "step 1711: generator_loss=1.7676693201065063, discriminator_loss=0.23108243942260742\n",
            "step 1712: generator_loss=1.7262459993362427, discriminator_loss=0.2350202351808548\n",
            "step 1713: generator_loss=1.674041509628296, discriminator_loss=0.24113108217716217\n",
            "step 1714: generator_loss=1.5981826782226562, discriminator_loss=0.2506556510925293\n",
            "step 1715: generator_loss=1.5796657800674438, discriminator_loss=0.25306856632232666\n",
            "step 1716: generator_loss=1.5008889436721802, discriminator_loss=0.2643636465072632\n",
            "step 1717: generator_loss=1.5555338859558105, discriminator_loss=0.2555103302001953\n",
            "step 1718: generator_loss=1.4704042673110962, discriminator_loss=0.26840412616729736\n",
            "step 1719: generator_loss=1.453894853591919, discriminator_loss=0.2703629732131958\n",
            "step 1720: generator_loss=1.4267940521240234, discriminator_loss=0.2754526734352112\n",
            "step 1721: generator_loss=1.4521963596343994, discriminator_loss=0.2708711624145508\n",
            "step 1722: generator_loss=1.4348022937774658, discriminator_loss=0.2738554775714874\n",
            "step 1723: generator_loss=1.4467289447784424, discriminator_loss=0.2718043625354767\n",
            "step 1724: generator_loss=1.4496054649353027, discriminator_loss=0.2709087133407593\n",
            "step 1725: generator_loss=1.5140618085861206, discriminator_loss=0.26096734404563904\n",
            "step 1726: generator_loss=1.5069624185562134, discriminator_loss=0.2624223232269287\n",
            "step 1727: generator_loss=1.5741666555404663, discriminator_loss=0.25254589319229126\n",
            "step 1728: generator_loss=1.5509063005447388, discriminator_loss=0.2565785050392151\n",
            "step 1729: generator_loss=1.5706559419631958, discriminator_loss=0.2545546293258667\n",
            "step 1730: generator_loss=1.6299803256988525, discriminator_loss=0.24702811241149902\n",
            "step 1731: generator_loss=1.6167876720428467, discriminator_loss=0.24948431551456451\n",
            "step 1732: generator_loss=1.6503686904907227, discriminator_loss=0.2453286051750183\n",
            "step 1733: generator_loss=1.655442476272583, discriminator_loss=0.24478326737880707\n",
            "step 1734: generator_loss=1.6957261562347412, discriminator_loss=0.24067683517932892\n",
            "step 1735: generator_loss=1.6993924379348755, discriminator_loss=0.24079757928848267\n",
            "step 1736: generator_loss=1.6513140201568604, discriminator_loss=0.24661396443843842\n",
            "step 1737: generator_loss=1.6696512699127197, discriminator_loss=0.24384848773479462\n",
            "step 1738: generator_loss=1.6364047527313232, discriminator_loss=0.24828171730041504\n",
            "step 1739: generator_loss=1.591118335723877, discriminator_loss=0.25434911251068115\n",
            "step 1740: generator_loss=1.6283531188964844, discriminator_loss=0.24898311495780945\n",
            "step 1741: generator_loss=1.6127309799194336, discriminator_loss=0.2506198287010193\n",
            "step 1742: generator_loss=1.6221860647201538, discriminator_loss=0.24854359030723572\n",
            "step 1743: generator_loss=1.6145987510681152, discriminator_loss=0.24953623116016388\n",
            "step 1744: generator_loss=1.6383397579193115, discriminator_loss=0.2453036904335022\n",
            "step 1745: generator_loss=1.6061997413635254, discriminator_loss=0.2497919499874115\n",
            "step 1746: generator_loss=1.6152987480163574, discriminator_loss=0.24780559539794922\n",
            "step 1747: generator_loss=1.6538407802581787, discriminator_loss=0.2435283064842224\n",
            "step 1748: generator_loss=1.6151962280273438, discriminator_loss=0.24716705083847046\n",
            "step 1749: generator_loss=1.617837905883789, discriminator_loss=0.24666240811347961\n",
            "step 1750: generator_loss=1.6634180545806885, discriminator_loss=0.24419933557510376\n",
            "step 1751: generator_loss=1.6561102867126465, discriminator_loss=0.24304647743701935\n",
            "step 1752: generator_loss=1.658027172088623, discriminator_loss=0.24264128506183624\n",
            "step 1753: generator_loss=1.6474049091339111, discriminator_loss=0.24396169185638428\n",
            "step 1754: generator_loss=1.6207767724990845, discriminator_loss=0.2458617091178894\n",
            "step 1755: generator_loss=1.59982430934906, discriminator_loss=0.24806693196296692\n",
            "step 1756: generator_loss=1.600498914718628, discriminator_loss=0.24846643209457397\n",
            "step 1757: generator_loss=1.579220175743103, discriminator_loss=0.25187164545059204\n",
            "step 1758: generator_loss=1.5901910066604614, discriminator_loss=0.2516905665397644\n",
            "step 1759: generator_loss=1.5760819911956787, discriminator_loss=0.2520432472229004\n",
            "step 1760: generator_loss=1.537226676940918, discriminator_loss=0.2587728500366211\n",
            "step 1761: generator_loss=1.5714731216430664, discriminator_loss=0.2542262077331543\n",
            "step 1762: generator_loss=1.4925932884216309, discriminator_loss=0.26875144243240356\n",
            "step 1763: generator_loss=1.5467723608016968, discriminator_loss=0.26043957471847534\n",
            "step 1764: generator_loss=1.5266437530517578, discriminator_loss=0.2646961808204651\n",
            "step 1765: generator_loss=1.5301543474197388, discriminator_loss=0.2676698863506317\n",
            "step 1766: generator_loss=1.5414502620697021, discriminator_loss=0.2671397924423218\n",
            "step 1767: generator_loss=1.537742018699646, discriminator_loss=0.2704775929450989\n",
            "step 1768: generator_loss=1.5241825580596924, discriminator_loss=0.2723233103752136\n",
            "step 1769: generator_loss=1.5194538831710815, discriminator_loss=0.27670368552207947\n",
            "step 1770: generator_loss=1.4297329187393188, discriminator_loss=0.29350852966308594\n",
            "step 1771: generator_loss=1.47792387008667, discriminator_loss=0.2867451608181\n",
            "step 1772: generator_loss=1.4818713665008545, discriminator_loss=0.28579723834991455\n",
            "step 1773: generator_loss=1.446542739868164, discriminator_loss=0.29515182971954346\n",
            "step 1774: generator_loss=1.4320929050445557, discriminator_loss=0.29355335235595703\n",
            "step 1775: generator_loss=1.4436957836151123, discriminator_loss=0.29508236050605774\n",
            "step 1776: generator_loss=1.4433737993240356, discriminator_loss=0.29064273834228516\n",
            "step 1777: generator_loss=1.4937515258789062, discriminator_loss=0.28155025839805603\n",
            "step 1778: generator_loss=1.4053889513015747, discriminator_loss=0.29413264989852905\n",
            "step 1779: generator_loss=1.531109094619751, discriminator_loss=0.27161675691604614\n",
            "step 1780: generator_loss=1.5486112833023071, discriminator_loss=0.26653873920440674\n",
            "step 1781: generator_loss=1.600733995437622, discriminator_loss=0.2595570981502533\n",
            "step 1782: generator_loss=1.5767014026641846, discriminator_loss=0.2608530521392822\n",
            "step 1783: generator_loss=1.5388853549957275, discriminator_loss=0.2653639614582062\n",
            "step 1784: generator_loss=1.5876070261001587, discriminator_loss=0.2585380971431732\n",
            "step 1785: generator_loss=1.615317702293396, discriminator_loss=0.25506484508514404\n",
            "step 1786: generator_loss=1.5882117748260498, discriminator_loss=0.25669804215431213\n",
            "step 1787: generator_loss=1.6074692010879517, discriminator_loss=0.25425007939338684\n",
            "step 1788: generator_loss=1.6405279636383057, discriminator_loss=0.2497802972793579\n",
            "step 1789: generator_loss=1.6941018104553223, discriminator_loss=0.2427818775177002\n",
            "step 1790: generator_loss=1.6359937191009521, discriminator_loss=0.24986183643341064\n",
            "step 1791: generator_loss=1.6541941165924072, discriminator_loss=0.2464510202407837\n",
            "step 1792: generator_loss=1.6608717441558838, discriminator_loss=0.2457110434770584\n",
            "step 1793: generator_loss=1.6516718864440918, discriminator_loss=0.2465973198413849\n",
            "step 1794: generator_loss=1.6352375745773315, discriminator_loss=0.24709005653858185\n",
            "step 1795: generator_loss=1.6084246635437012, discriminator_loss=0.2503238618373871\n",
            "step 1796: generator_loss=1.5610331296920776, discriminator_loss=0.25678348541259766\n",
            "step 1797: generator_loss=1.5271183252334595, discriminator_loss=0.26064246892929077\n",
            "step 1798: generator_loss=1.4938843250274658, discriminator_loss=0.264599084854126\n",
            "step 1799: generator_loss=1.4734604358673096, discriminator_loss=0.26777738332748413\n",
            "step 1800: generator_loss=1.4597139358520508, discriminator_loss=0.27056002616882324\n",
            "step 1801: generator_loss=1.4649468660354614, discriminator_loss=0.26942670345306396\n",
            "step 1802: generator_loss=1.4181654453277588, discriminator_loss=0.2745268940925598\n",
            "step 1803: generator_loss=1.4196133613586426, discriminator_loss=0.2741876542568207\n",
            "step 1804: generator_loss=1.4065247774124146, discriminator_loss=0.2771751582622528\n",
            "step 1805: generator_loss=1.4193521738052368, discriminator_loss=0.2747465968132019\n",
            "step 1806: generator_loss=1.3867743015289307, discriminator_loss=0.2790250778198242\n",
            "step 1807: generator_loss=1.4020140171051025, discriminator_loss=0.2766895294189453\n",
            "step 1808: generator_loss=1.394596815109253, discriminator_loss=0.2777154743671417\n",
            "step 1809: generator_loss=1.4213008880615234, discriminator_loss=0.27348625659942627\n",
            "step 1810: generator_loss=1.435110092163086, discriminator_loss=0.27164745330810547\n",
            "step 1811: generator_loss=1.4446702003479004, discriminator_loss=0.270464688539505\n",
            "step 1812: generator_loss=1.4595942497253418, discriminator_loss=0.26870468258857727\n",
            "step 1813: generator_loss=1.4645862579345703, discriminator_loss=0.2685852646827698\n",
            "step 1814: generator_loss=1.4634013175964355, discriminator_loss=0.26983189582824707\n",
            "step 1815: generator_loss=1.4550777673721313, discriminator_loss=0.27124157547950745\n",
            "step 1816: generator_loss=1.4325917959213257, discriminator_loss=0.2761523127555847\n",
            "step 1817: generator_loss=1.4608187675476074, discriminator_loss=0.2731103301048279\n",
            "step 1818: generator_loss=1.3778424263000488, discriminator_loss=0.28597116470336914\n",
            "step 1819: generator_loss=1.4050333499908447, discriminator_loss=0.2827661633491516\n",
            "step 1820: generator_loss=1.3168096542358398, discriminator_loss=0.2979818880558014\n",
            "step 1821: generator_loss=1.2992525100708008, discriminator_loss=0.3016400933265686\n",
            "step 1822: generator_loss=1.267582893371582, discriminator_loss=0.3082776963710785\n",
            "step 1823: generator_loss=1.2125885486602783, discriminator_loss=0.31957054138183594\n",
            "step 1824: generator_loss=1.176325798034668, discriminator_loss=0.3275109827518463\n",
            "step 1825: generator_loss=1.117321491241455, discriminator_loss=0.3401486873626709\n",
            "step 1826: generator_loss=1.174682855606079, discriminator_loss=0.3293851912021637\n",
            "step 1827: generator_loss=1.1198968887329102, discriminator_loss=0.3428463935852051\n",
            "step 1828: generator_loss=1.13022780418396, discriminator_loss=0.34112346172332764\n",
            "step 1829: generator_loss=1.1129050254821777, discriminator_loss=0.3453541398048401\n",
            "step 1830: generator_loss=1.0850625038146973, discriminator_loss=0.3519902229309082\n",
            "step 1831: generator_loss=1.0934786796569824, discriminator_loss=0.35167038440704346\n",
            "step 1832: generator_loss=1.0739846229553223, discriminator_loss=0.356561541557312\n",
            "step 1833: generator_loss=1.08375883102417, discriminator_loss=0.3546909689903259\n",
            "step 1834: generator_loss=1.1152422428131104, discriminator_loss=0.34799712896347046\n",
            "step 1835: generator_loss=1.1354303359985352, discriminator_loss=0.34409812092781067\n",
            "step 1836: generator_loss=1.165755033493042, discriminator_loss=0.3380955159664154\n",
            "step 1837: generator_loss=1.2120718955993652, discriminator_loss=0.32951366901397705\n",
            "step 1838: generator_loss=1.2170019149780273, discriminator_loss=0.3291237950325012\n",
            "step 1839: generator_loss=1.2514533996582031, discriminator_loss=0.32327139377593994\n",
            "step 1840: generator_loss=1.2635421752929688, discriminator_loss=0.321975976228714\n",
            "step 1841: generator_loss=1.2920509576797485, discriminator_loss=0.3176230192184448\n",
            "step 1842: generator_loss=1.3364664316177368, discriminator_loss=0.310261607170105\n",
            "step 1843: generator_loss=1.3931858539581299, discriminator_loss=0.30098870396614075\n",
            "step 1844: generator_loss=1.445907473564148, discriminator_loss=0.2930712103843689\n",
            "step 1845: generator_loss=1.4886085987091064, discriminator_loss=0.28674477338790894\n",
            "step 1846: generator_loss=1.53082275390625, discriminator_loss=0.2808821201324463\n",
            "step 1847: generator_loss=1.5648629665374756, discriminator_loss=0.27628466486930847\n",
            "step 1848: generator_loss=1.5762300491333008, discriminator_loss=0.2743784785270691\n",
            "step 1849: generator_loss=1.5817879438400269, discriminator_loss=0.27311134338378906\n",
            "step 1850: generator_loss=1.566038966178894, discriminator_loss=0.27456045150756836\n",
            "step 1851: generator_loss=1.553039312362671, discriminator_loss=0.27525168657302856\n",
            "step 1852: generator_loss=1.5245875120162964, discriminator_loss=0.27827945351600647\n",
            "step 1853: generator_loss=1.5017467737197876, discriminator_loss=0.2806287705898285\n",
            "step 1854: generator_loss=1.480259656906128, discriminator_loss=0.28306248784065247\n",
            "step 1855: generator_loss=1.460310935974121, discriminator_loss=0.2852332293987274\n",
            "step 1856: generator_loss=1.4529948234558105, discriminator_loss=0.28555136919021606\n",
            "step 1857: generator_loss=1.443455696105957, discriminator_loss=0.2860773205757141\n",
            "step 1858: generator_loss=1.440476655960083, discriminator_loss=0.2857745885848999\n",
            "step 1859: generator_loss=1.4316589832305908, discriminator_loss=0.28631648421287537\n",
            "step 1860: generator_loss=1.4373573064804077, discriminator_loss=0.28474971652030945\n",
            "step 1861: generator_loss=1.437983751296997, discriminator_loss=0.2839812934398651\n",
            "step 1862: generator_loss=1.4458332061767578, discriminator_loss=0.28198161721229553\n",
            "step 1863: generator_loss=1.4461002349853516, discriminator_loss=0.281147837638855\n",
            "step 1864: generator_loss=1.4490749835968018, discriminator_loss=0.27982380986213684\n",
            "step 1865: generator_loss=1.4493334293365479, discriminator_loss=0.2790490984916687\n",
            "step 1866: generator_loss=1.458683729171753, discriminator_loss=0.2767966389656067\n",
            "step 1867: generator_loss=1.4646129608154297, discriminator_loss=0.27508634328842163\n",
            "step 1868: generator_loss=1.4754611253738403, discriminator_loss=0.27250319719314575\n",
            "step 1869: generator_loss=1.478632926940918, discriminator_loss=0.27113619446754456\n",
            "step 1870: generator_loss=1.4812171459197998, discriminator_loss=0.269930899143219\n",
            "step 1871: generator_loss=1.4835844039916992, discriminator_loss=0.26897290349006653\n",
            "step 1872: generator_loss=1.4853825569152832, discriminator_loss=0.2679349184036255\n",
            "step 1873: generator_loss=1.4865161180496216, discriminator_loss=0.267418771982193\n",
            "step 1874: generator_loss=1.4833825826644897, discriminator_loss=0.26745593547821045\n",
            "step 1875: generator_loss=1.477031946182251, discriminator_loss=0.26801005005836487\n",
            "step 1876: generator_loss=1.4674142599105835, discriminator_loss=0.2691460847854614\n",
            "step 1877: generator_loss=1.4514119625091553, discriminator_loss=0.2711827754974365\n",
            "step 1878: generator_loss=1.437893271446228, discriminator_loss=0.27305668592453003\n",
            "step 1879: generator_loss=1.4243824481964111, discriminator_loss=0.2750474810600281\n",
            "step 1880: generator_loss=1.4111709594726562, discriminator_loss=0.2769051790237427\n",
            "step 1881: generator_loss=1.3992060422897339, discriminator_loss=0.27864497900009155\n",
            "step 1882: generator_loss=1.3982062339782715, discriminator_loss=0.2788175940513611\n",
            "step 1883: generator_loss=1.4087655544281006, discriminator_loss=0.27708661556243896\n",
            "step 1884: generator_loss=1.4158868789672852, discriminator_loss=0.275798499584198\n",
            "step 1885: generator_loss=1.4207370281219482, discriminator_loss=0.2751004695892334\n",
            "step 1886: generator_loss=1.4224324226379395, discriminator_loss=0.27476441860198975\n",
            "step 1887: generator_loss=1.4394943714141846, discriminator_loss=0.2721322774887085\n",
            "step 1888: generator_loss=1.4578386545181274, discriminator_loss=0.269237756729126\n",
            "step 1889: generator_loss=1.4721602201461792, discriminator_loss=0.26706022024154663\n",
            "step 1890: generator_loss=1.4824833869934082, discriminator_loss=0.26559215784072876\n",
            "step 1891: generator_loss=1.4943246841430664, discriminator_loss=0.2639572322368622\n",
            "step 1892: generator_loss=1.5166571140289307, discriminator_loss=0.2610354423522949\n",
            "step 1893: generator_loss=1.5473891496658325, discriminator_loss=0.2569279074668884\n",
            "step 1894: generator_loss=1.5738425254821777, discriminator_loss=0.25352638959884644\n",
            "step 1895: generator_loss=1.594710111618042, discriminator_loss=0.25079280138015747\n",
            "step 1896: generator_loss=1.6051610708236694, discriminator_loss=0.24941270053386688\n",
            "step 1897: generator_loss=1.6131566762924194, discriminator_loss=0.2484472393989563\n",
            "step 1898: generator_loss=1.6216543912887573, discriminator_loss=0.24729914963245392\n",
            "step 1899: generator_loss=1.6257352828979492, discriminator_loss=0.2465658038854599\n",
            "step 1900: generator_loss=1.632169485092163, discriminator_loss=0.24548491835594177\n",
            "step 1901: generator_loss=1.632944107055664, discriminator_loss=0.24518007040023804\n",
            "step 1902: generator_loss=1.631164312362671, discriminator_loss=0.24496781826019287\n",
            "step 1903: generator_loss=1.6287596225738525, discriminator_loss=0.24476441740989685\n",
            "step 1904: generator_loss=1.6310285329818726, discriminator_loss=0.24405957758426666\n",
            "step 1905: generator_loss=1.6387699842453003, discriminator_loss=0.24281743168830872\n",
            "step 1906: generator_loss=1.6424601078033447, discriminator_loss=0.2420138418674469\n",
            "step 1907: generator_loss=1.645361304283142, discriminator_loss=0.2413056641817093\n",
            "step 1908: generator_loss=1.645655870437622, discriminator_loss=0.24088159203529358\n",
            "step 1909: generator_loss=1.6505900621414185, discriminator_loss=0.2398405820131302\n",
            "step 1910: generator_loss=1.6598035097122192, discriminator_loss=0.23822012543678284\n",
            "step 1911: generator_loss=1.6687188148498535, discriminator_loss=0.2368248701095581\n",
            "step 1912: generator_loss=1.6745576858520508, discriminator_loss=0.23593977093696594\n",
            "step 1913: generator_loss=1.677123785018921, discriminator_loss=0.23536819219589233\n",
            "step 1914: generator_loss=1.6709280014038086, discriminator_loss=0.23577529191970825\n",
            "step 1915: generator_loss=1.6581618785858154, discriminator_loss=0.23698481917381287\n",
            "step 1916: generator_loss=1.6504969596862793, discriminator_loss=0.2376999855041504\n",
            "step 1917: generator_loss=1.6395156383514404, discriminator_loss=0.23882746696472168\n",
            "step 1918: generator_loss=1.6437915563583374, discriminator_loss=0.23804901540279388\n",
            "step 1919: generator_loss=1.6456655263900757, discriminator_loss=0.23773637413978577\n",
            "step 1920: generator_loss=1.657086730003357, discriminator_loss=0.23612292110919952\n",
            "step 1921: generator_loss=1.6798475980758667, discriminator_loss=0.23329445719718933\n",
            "step 1922: generator_loss=1.7051377296447754, discriminator_loss=0.2302069514989853\n",
            "step 1923: generator_loss=1.7244830131530762, discriminator_loss=0.22778834402561188\n",
            "step 1924: generator_loss=1.7399096488952637, discriminator_loss=0.22597545385360718\n",
            "step 1925: generator_loss=1.749050259590149, discriminator_loss=0.2247905731201172\n",
            "step 1926: generator_loss=1.7515895366668701, discriminator_loss=0.22432246804237366\n",
            "step 1927: generator_loss=1.7482883930206299, discriminator_loss=0.22460471093654633\n",
            "step 1928: generator_loss=1.7394521236419678, discriminator_loss=0.2254105508327484\n",
            "step 1929: generator_loss=1.7217519283294678, discriminator_loss=0.22724705934524536\n",
            "step 1930: generator_loss=1.7014997005462646, discriminator_loss=0.22938847541809082\n",
            "step 1931: generator_loss=1.6761138439178467, discriminator_loss=0.2320740520954132\n",
            "step 1932: generator_loss=1.647463083267212, discriminator_loss=0.2353857159614563\n",
            "step 1933: generator_loss=1.6272709369659424, discriminator_loss=0.23779109120368958\n",
            "step 1934: generator_loss=1.6122978925704956, discriminator_loss=0.23961935937404633\n",
            "step 1935: generator_loss=1.5972309112548828, discriminator_loss=0.24153146147727966\n",
            "step 1936: generator_loss=1.5775396823883057, discriminator_loss=0.24410861730575562\n",
            "step 1937: generator_loss=1.5567439794540405, discriminator_loss=0.24689263105392456\n",
            "step 1938: generator_loss=1.5418823957443237, discriminator_loss=0.2489696592092514\n",
            "step 1939: generator_loss=1.5371243953704834, discriminator_loss=0.2499069720506668\n",
            "step 1940: generator_loss=1.532320261001587, discriminator_loss=0.25101345777511597\n",
            "step 1941: generator_loss=1.5302233695983887, discriminator_loss=0.25162339210510254\n",
            "step 1942: generator_loss=1.5283409357070923, discriminator_loss=0.2522360682487488\n",
            "step 1943: generator_loss=1.5209681987762451, discriminator_loss=0.25338608026504517\n",
            "step 1944: generator_loss=1.5241246223449707, discriminator_loss=0.25328513979911804\n",
            "step 1945: generator_loss=1.5290875434875488, discriminator_loss=0.2527851462364197\n",
            "step 1946: generator_loss=1.5450142621994019, discriminator_loss=0.25094228982925415\n",
            "step 1947: generator_loss=1.5628820657730103, discriminator_loss=0.2487800419330597\n",
            "step 1948: generator_loss=1.5854599475860596, discriminator_loss=0.246080219745636\n",
            "step 1949: generator_loss=1.6086935997009277, discriminator_loss=0.24333786964416504\n",
            "step 1950: generator_loss=1.638656735420227, discriminator_loss=0.23991379141807556\n",
            "step 1951: generator_loss=1.6529642343521118, discriminator_loss=0.23818765580654144\n",
            "step 1952: generator_loss=1.6675868034362793, discriminator_loss=0.23631879687309265\n",
            "step 1953: generator_loss=1.6829278469085693, discriminator_loss=0.23446616530418396\n",
            "step 1954: generator_loss=1.7246434688568115, discriminator_loss=0.22966428101062775\n",
            "step 1955: generator_loss=1.7849571704864502, discriminator_loss=0.2229950726032257\n",
            "step 1956: generator_loss=1.8355848789215088, discriminator_loss=0.21761566400527954\n",
            "step 1957: generator_loss=1.876391887664795, discriminator_loss=0.2133302092552185\n",
            "step 1958: generator_loss=1.9046134948730469, discriminator_loss=0.21012087166309357\n",
            "step 1959: generator_loss=1.9194313287734985, discriminator_loss=0.20828519761562347\n",
            "step 1960: generator_loss=1.9192978143692017, discriminator_loss=0.20763587951660156\n",
            "step 1961: generator_loss=1.9108283519744873, discriminator_loss=0.20762231945991516\n",
            "step 1962: generator_loss=1.8963085412979126, discriminator_loss=0.20829425752162933\n",
            "step 1963: generator_loss=1.8773424625396729, discriminator_loss=0.20946848392486572\n",
            "step 1964: generator_loss=1.8672354221343994, discriminator_loss=0.2099587470293045\n",
            "step 1965: generator_loss=1.8625752925872803, discriminator_loss=0.20998099446296692\n",
            "step 1966: generator_loss=1.856746792793274, discriminator_loss=0.209957093000412\n",
            "step 1967: generator_loss=1.845821499824524, discriminator_loss=0.2105758786201477\n",
            "step 1968: generator_loss=1.8375602960586548, discriminator_loss=0.2109844982624054\n",
            "step 1969: generator_loss=1.837965965270996, discriminator_loss=0.210482656955719\n",
            "step 1970: generator_loss=1.8436076641082764, discriminator_loss=0.2095736563205719\n",
            "step 1971: generator_loss=1.8490400314331055, discriminator_loss=0.20894408226013184\n",
            "step 1972: generator_loss=1.8554869890213013, discriminator_loss=0.2082308679819107\n",
            "step 1973: generator_loss=1.8528861999511719, discriminator_loss=0.20821252465248108\n",
            "step 1974: generator_loss=1.8389248847961426, discriminator_loss=0.2094445526599884\n",
            "step 1975: generator_loss=1.8199635744094849, discriminator_loss=0.21114620566368103\n",
            "step 1976: generator_loss=1.7931212186813354, discriminator_loss=0.2136189341545105\n",
            "step 1977: generator_loss=1.7552664279937744, discriminator_loss=0.21730700135231018\n",
            "step 1978: generator_loss=1.7166721820831299, discriminator_loss=0.2213842123746872\n",
            "step 1979: generator_loss=1.6847972869873047, discriminator_loss=0.22488215565681458\n",
            "step 1980: generator_loss=1.669578194618225, discriminator_loss=0.22655294835567474\n",
            "step 1981: generator_loss=1.6574289798736572, discriminator_loss=0.2279457002878189\n",
            "step 1982: generator_loss=1.6488351821899414, discriminator_loss=0.22910037636756897\n",
            "step 1983: generator_loss=1.6300244331359863, discriminator_loss=0.2315172553062439\n",
            "step 1984: generator_loss=1.6133770942687988, discriminator_loss=0.2339068353176117\n",
            "step 1985: generator_loss=1.5866515636444092, discriminator_loss=0.2376723289489746\n",
            "step 1986: generator_loss=1.5503145456314087, discriminator_loss=0.24311180412769318\n",
            "step 1987: generator_loss=1.532935619354248, discriminator_loss=0.24589493870735168\n",
            "step 1988: generator_loss=1.5033693313598633, discriminator_loss=0.25063809752464294\n",
            "step 1989: generator_loss=1.4785630702972412, discriminator_loss=0.25490695238113403\n",
            "step 1990: generator_loss=1.4585479497909546, discriminator_loss=0.25931066274642944\n",
            "step 1991: generator_loss=1.4441184997558594, discriminator_loss=0.2616623044013977\n",
            "step 1992: generator_loss=1.4406569004058838, discriminator_loss=0.26326000690460205\n",
            "step 1993: generator_loss=1.4270392656326294, discriminator_loss=0.2664216458797455\n",
            "step 1994: generator_loss=1.4415521621704102, discriminator_loss=0.2650128901004791\n",
            "step 1995: generator_loss=1.429445505142212, discriminator_loss=0.2676706612110138\n",
            "step 1996: generator_loss=1.4338172674179077, discriminator_loss=0.26834702491760254\n",
            "step 1997: generator_loss=1.4286887645721436, discriminator_loss=0.26943936944007874\n",
            "step 1998: generator_loss=1.4015752077102661, discriminator_loss=0.2739655375480652\n",
            "step 1999: generator_loss=1.3870644569396973, discriminator_loss=0.27692893147468567\n",
            "step 2000: generator_loss=1.405534267425537, discriminator_loss=0.2743494510650635\n",
            "step 2001: generator_loss=1.39347505569458, discriminator_loss=0.2774086892604828\n",
            "step 2002: generator_loss=1.4261895418167114, discriminator_loss=0.2724735736846924\n",
            "step 2003: generator_loss=1.417501449584961, discriminator_loss=0.27410420775413513\n",
            "step 2004: generator_loss=1.443516731262207, discriminator_loss=0.2706935405731201\n",
            "step 2005: generator_loss=1.4722145795822144, discriminator_loss=0.2663884162902832\n",
            "step 2006: generator_loss=1.5283399820327759, discriminator_loss=0.2590281665325165\n",
            "step 2007: generator_loss=1.5498124361038208, discriminator_loss=0.2556365132331848\n",
            "step 2008: generator_loss=1.6028902530670166, discriminator_loss=0.24891187250614166\n",
            "step 2009: generator_loss=1.6508395671844482, discriminator_loss=0.242852121591568\n",
            "step 2010: generator_loss=1.7064567804336548, discriminator_loss=0.236026331782341\n",
            "step 2011: generator_loss=1.7596435546875, discriminator_loss=0.23002077639102936\n",
            "step 2012: generator_loss=1.827622413635254, discriminator_loss=0.22232288122177124\n",
            "step 2013: generator_loss=1.8547444343566895, discriminator_loss=0.21872328221797943\n",
            "step 2014: generator_loss=1.9008524417877197, discriminator_loss=0.21356303989887238\n",
            "step 2015: generator_loss=1.914332389831543, discriminator_loss=0.21152879297733307\n",
            "step 2016: generator_loss=1.9376729726791382, discriminator_loss=0.20875348150730133\n",
            "step 2017: generator_loss=1.9381773471832275, discriminator_loss=0.208039790391922\n",
            "step 2018: generator_loss=1.9346057176589966, discriminator_loss=0.20790541172027588\n",
            "step 2019: generator_loss=1.8763866424560547, discriminator_loss=0.21263915300369263\n",
            "step 2020: generator_loss=1.8856244087219238, discriminator_loss=0.21142664551734924\n",
            "step 2021: generator_loss=1.8441903591156006, discriminator_loss=0.21485747396945953\n",
            "step 2022: generator_loss=1.8286659717559814, discriminator_loss=0.2158052921295166\n",
            "step 2023: generator_loss=1.7630317211151123, discriminator_loss=0.22198718786239624\n",
            "step 2024: generator_loss=1.7627030611038208, discriminator_loss=0.22210639715194702\n",
            "step 2025: generator_loss=1.6868995428085327, discriminator_loss=0.22968201339244843\n",
            "step 2026: generator_loss=1.6338011026382446, discriminator_loss=0.23549999296665192\n",
            "step 2027: generator_loss=1.6414128541946411, discriminator_loss=0.2346148043870926\n",
            "step 2028: generator_loss=1.593745231628418, discriminator_loss=0.23974651098251343\n",
            "step 2029: generator_loss=1.6027541160583496, discriminator_loss=0.2377040684223175\n",
            "step 2030: generator_loss=1.5846140384674072, discriminator_loss=0.23924390971660614\n",
            "step 2031: generator_loss=1.5842325687408447, discriminator_loss=0.23844939470291138\n",
            "step 2032: generator_loss=1.5635404586791992, discriminator_loss=0.2400054782629013\n",
            "step 2033: generator_loss=1.5718367099761963, discriminator_loss=0.23835328221321106\n",
            "step 2034: generator_loss=1.5927079916000366, discriminator_loss=0.23510906100273132\n",
            "step 2035: generator_loss=1.6069890260696411, discriminator_loss=0.23286770284175873\n",
            "step 2036: generator_loss=1.61356520652771, discriminator_loss=0.23151804506778717\n",
            "step 2037: generator_loss=1.649073600769043, discriminator_loss=0.2271910309791565\n",
            "step 2038: generator_loss=1.662939190864563, discriminator_loss=0.22547973692417145\n",
            "step 2039: generator_loss=1.6731129884719849, discriminator_loss=0.22433045506477356\n",
            "step 2040: generator_loss=1.6798064708709717, discriminator_loss=0.2237563133239746\n",
            "step 2041: generator_loss=1.6712888479232788, discriminator_loss=0.2245989441871643\n",
            "step 2042: generator_loss=1.6694384813308716, discriminator_loss=0.2250221073627472\n",
            "step 2043: generator_loss=1.646592617034912, discriminator_loss=0.2276630848646164\n",
            "step 2044: generator_loss=1.624314546585083, discriminator_loss=0.23064561188220978\n",
            "step 2045: generator_loss=1.596480369567871, discriminator_loss=0.23434996604919434\n",
            "step 2046: generator_loss=1.5753730535507202, discriminator_loss=0.23735181987285614\n",
            "step 2047: generator_loss=1.5441503524780273, discriminator_loss=0.2416011542081833\n",
            "step 2048: generator_loss=1.5244790315628052, discriminator_loss=0.24450139701366425\n",
            "step 2049: generator_loss=1.5189851522445679, discriminator_loss=0.2454233169555664\n",
            "step 2050: generator_loss=1.5301257371902466, discriminator_loss=0.24414882063865662\n",
            "step 2051: generator_loss=1.5273334980010986, discriminator_loss=0.24477340281009674\n",
            "step 2052: generator_loss=1.528165340423584, discriminator_loss=0.2451561987400055\n",
            "step 2053: generator_loss=1.5337395668029785, discriminator_loss=0.2447253167629242\n",
            "step 2054: generator_loss=1.5411896705627441, discriminator_loss=0.24410219490528107\n",
            "step 2055: generator_loss=1.5575411319732666, discriminator_loss=0.2424004077911377\n",
            "step 2056: generator_loss=1.5721765756607056, discriminator_loss=0.24093016982078552\n",
            "step 2057: generator_loss=1.6155506372451782, discriminator_loss=0.23583252727985382\n",
            "step 2058: generator_loss=1.6689529418945312, discriminator_loss=0.2300068587064743\n",
            "step 2059: generator_loss=1.739739179611206, discriminator_loss=0.22247587144374847\n",
            "step 2060: generator_loss=1.8372094631195068, discriminator_loss=0.21314886212348938\n",
            "step 2061: generator_loss=1.9536863565444946, discriminator_loss=0.20305819809436798\n",
            "step 2062: generator_loss=2.0634052753448486, discriminator_loss=0.19475224614143372\n",
            "step 2063: generator_loss=2.176835060119629, discriminator_loss=0.18692725896835327\n",
            "step 2064: generator_loss=2.2500970363616943, discriminator_loss=0.18215905129909515\n",
            "step 2065: generator_loss=2.31561279296875, discriminator_loss=0.1780451089143753\n",
            "step 2066: generator_loss=2.33345365524292, discriminator_loss=0.17645128071308136\n",
            "step 2067: generator_loss=2.3221688270568848, discriminator_loss=0.1765134036540985\n",
            "step 2068: generator_loss=2.273017644882202, discriminator_loss=0.17843717336654663\n",
            "step 2069: generator_loss=2.249181032180786, discriminator_loss=0.17906919121742249\n",
            "step 2070: generator_loss=2.244858980178833, discriminator_loss=0.17855750024318695\n",
            "step 2071: generator_loss=2.221510410308838, discriminator_loss=0.17900966107845306\n",
            "step 2072: generator_loss=2.1938934326171875, discriminator_loss=0.17989234626293182\n",
            "step 2073: generator_loss=2.140115737915039, discriminator_loss=0.1823236644268036\n",
            "step 2074: generator_loss=2.054537296295166, discriminator_loss=0.18746110796928406\n",
            "step 2075: generator_loss=1.971958875656128, discriminator_loss=0.19317124783992767\n",
            "step 2076: generator_loss=1.9249756336212158, discriminator_loss=0.19690313935279846\n",
            "step 2077: generator_loss=1.908340334892273, discriminator_loss=0.19792254269123077\n",
            "step 2078: generator_loss=1.9078326225280762, discriminator_loss=0.1977595090866089\n",
            "step 2079: generator_loss=1.8876107931137085, discriminator_loss=0.19898056983947754\n",
            "step 2080: generator_loss=1.8917961120605469, discriminator_loss=0.19906049966812134\n",
            "step 2081: generator_loss=1.8564698696136475, discriminator_loss=0.2017785906791687\n",
            "step 2082: generator_loss=1.7942893505096436, discriminator_loss=0.20734615623950958\n",
            "step 2083: generator_loss=1.770350456237793, discriminator_loss=0.21054428815841675\n",
            "step 2084: generator_loss=1.73063325881958, discriminator_loss=0.21506839990615845\n",
            "step 2085: generator_loss=1.7184792757034302, discriminator_loss=0.2170698046684265\n",
            "step 2086: generator_loss=1.7128753662109375, discriminator_loss=0.21878871321678162\n",
            "step 2087: generator_loss=1.7172406911849976, discriminator_loss=0.21953830122947693\n",
            "step 2088: generator_loss=1.6786856651306152, discriminator_loss=0.22378352284431458\n",
            "step 2089: generator_loss=1.6736490726470947, discriminator_loss=0.2234833836555481\n",
            "step 2090: generator_loss=1.6436192989349365, discriminator_loss=0.22868692874908447\n",
            "step 2091: generator_loss=1.6727192401885986, discriminator_loss=0.2262071967124939\n",
            "step 2092: generator_loss=1.6567142009735107, discriminator_loss=0.22819149494171143\n",
            "step 2093: generator_loss=1.687806248664856, discriminator_loss=0.226322740316391\n",
            "step 2094: generator_loss=1.6812796592712402, discriminator_loss=0.22668984532356262\n",
            "step 2095: generator_loss=1.677344560623169, discriminator_loss=0.22921231389045715\n",
            "step 2096: generator_loss=1.605626106262207, discriminator_loss=0.23710867762565613\n",
            "step 2097: generator_loss=1.5713711977005005, discriminator_loss=0.2404528707265854\n",
            "step 2098: generator_loss=1.5660107135772705, discriminator_loss=0.24128639698028564\n",
            "step 2099: generator_loss=1.5495460033416748, discriminator_loss=0.2436160445213318\n",
            "step 2100: generator_loss=1.5033683776855469, discriminator_loss=0.24978692829608917\n",
            "step 2101: generator_loss=1.4940125942230225, discriminator_loss=0.2508765757083893\n",
            "step 2102: generator_loss=1.459995985031128, discriminator_loss=0.2551669478416443\n",
            "step 2103: generator_loss=1.45574951171875, discriminator_loss=0.25572043657302856\n",
            "step 2104: generator_loss=1.4422962665557861, discriminator_loss=0.25708603858947754\n",
            "step 2105: generator_loss=1.4978474378585815, discriminator_loss=0.24962395429611206\n",
            "step 2106: generator_loss=1.5342755317687988, discriminator_loss=0.24491527676582336\n",
            "step 2107: generator_loss=1.570960521697998, discriminator_loss=0.2392924726009369\n",
            "step 2108: generator_loss=1.657804250717163, discriminator_loss=0.22934174537658691\n",
            "step 2109: generator_loss=1.690190076828003, discriminator_loss=0.22502227127552032\n",
            "step 2110: generator_loss=1.7552605867385864, discriminator_loss=0.21782776713371277\n",
            "step 2111: generator_loss=1.7907077074050903, discriminator_loss=0.2136998176574707\n",
            "step 2112: generator_loss=1.8246915340423584, discriminator_loss=0.21009935438632965\n",
            "step 2113: generator_loss=1.8605632781982422, discriminator_loss=0.20656463503837585\n",
            "step 2114: generator_loss=1.86606764793396, discriminator_loss=0.2053799331188202\n",
            "step 2115: generator_loss=1.8666102886199951, discriminator_loss=0.20483627915382385\n",
            "step 2116: generator_loss=1.863134741783142, discriminator_loss=0.20470881462097168\n",
            "step 2117: generator_loss=1.8445789813995361, discriminator_loss=0.20574162900447845\n",
            "step 2118: generator_loss=1.833286166191101, discriminator_loss=0.20621851086616516\n",
            "step 2119: generator_loss=1.8256605863571167, discriminator_loss=0.20647889375686646\n",
            "step 2120: generator_loss=1.8157899379730225, discriminator_loss=0.2068508416414261\n",
            "step 2121: generator_loss=1.8080940246582031, discriminator_loss=0.20693817734718323\n",
            "step 2122: generator_loss=1.8006970882415771, discriminator_loss=0.20706447958946228\n",
            "step 2123: generator_loss=1.8020665645599365, discriminator_loss=0.2062961906194687\n",
            "step 2124: generator_loss=1.8111647367477417, discriminator_loss=0.20491430163383484\n",
            "step 2125: generator_loss=1.8195316791534424, discriminator_loss=0.20347699522972107\n",
            "step 2126: generator_loss=1.8549716472625732, discriminator_loss=0.19973985850811005\n",
            "step 2127: generator_loss=1.8782546520233154, discriminator_loss=0.19715921580791473\n",
            "step 2128: generator_loss=1.928842306137085, discriminator_loss=0.1922774761915207\n",
            "step 2129: generator_loss=1.9676767587661743, discriminator_loss=0.18858566880226135\n",
            "step 2130: generator_loss=2.0116775035858154, discriminator_loss=0.18478035926818848\n",
            "step 2131: generator_loss=2.0417914390563965, discriminator_loss=0.1820267140865326\n",
            "step 2132: generator_loss=2.0616331100463867, discriminator_loss=0.18003033101558685\n",
            "step 2133: generator_loss=2.0758190155029297, discriminator_loss=0.178470179438591\n",
            "step 2134: generator_loss=2.0922484397888184, discriminator_loss=0.1767820119857788\n",
            "step 2135: generator_loss=2.1034533977508545, discriminator_loss=0.1754116266965866\n",
            "step 2136: generator_loss=2.108860969543457, discriminator_loss=0.17454524338245392\n",
            "step 2137: generator_loss=2.0970892906188965, discriminator_loss=0.17491066455841064\n",
            "step 2138: generator_loss=2.0933799743652344, discriminator_loss=0.17491239309310913\n",
            "step 2139: generator_loss=2.076836347579956, discriminator_loss=0.17576798796653748\n",
            "step 2140: generator_loss=2.0686237812042236, discriminator_loss=0.17613506317138672\n",
            "step 2141: generator_loss=2.0634546279907227, discriminator_loss=0.17636238038539886\n",
            "step 2142: generator_loss=2.0444233417510986, discriminator_loss=0.17756955325603485\n",
            "step 2143: generator_loss=2.030193328857422, discriminator_loss=0.17851173877716064\n",
            "step 2144: generator_loss=2.0009636878967285, discriminator_loss=0.1806429922580719\n",
            "step 2145: generator_loss=1.9611554145812988, discriminator_loss=0.1836162507534027\n",
            "step 2146: generator_loss=1.93180513381958, discriminator_loss=0.18590447306632996\n",
            "step 2147: generator_loss=1.9086195230484009, discriminator_loss=0.18781563639640808\n",
            "step 2148: generator_loss=1.8786609172821045, discriminator_loss=0.19039060175418854\n",
            "step 2149: generator_loss=1.8470162153244019, discriminator_loss=0.1932395100593567\n",
            "step 2150: generator_loss=1.8067848682403564, discriminator_loss=0.19729602336883545\n",
            "step 2151: generator_loss=1.7680174112319946, discriminator_loss=0.2012735903263092\n",
            "step 2152: generator_loss=1.7597126960754395, discriminator_loss=0.20230159163475037\n",
            "step 2153: generator_loss=1.7217488288879395, discriminator_loss=0.20678916573524475\n",
            "step 2154: generator_loss=1.7058489322662354, discriminator_loss=0.2094242423772812\n",
            "step 2155: generator_loss=1.7004722356796265, discriminator_loss=0.21039777994155884\n",
            "step 2156: generator_loss=1.6600046157836914, discriminator_loss=0.21577906608581543\n",
            "step 2157: generator_loss=1.6400752067565918, discriminator_loss=0.21980777382850647\n",
            "step 2158: generator_loss=1.6292163133621216, discriminator_loss=0.22170186042785645\n",
            "step 2159: generator_loss=1.6222108602523804, discriminator_loss=0.22493241727352142\n",
            "step 2160: generator_loss=1.620747447013855, discriminator_loss=0.224786177277565\n",
            "step 2161: generator_loss=1.5569140911102295, discriminator_loss=0.23629671335220337\n",
            "step 2162: generator_loss=1.599509358406067, discriminator_loss=0.23274174332618713\n",
            "step 2163: generator_loss=1.590956211090088, discriminator_loss=0.23936080932617188\n",
            "step 2164: generator_loss=1.6193969249725342, discriminator_loss=0.23542146384716034\n",
            "step 2165: generator_loss=1.566914677619934, discriminator_loss=0.24769681692123413\n",
            "step 2166: generator_loss=1.5564041137695312, discriminator_loss=0.2488775998353958\n",
            "step 2167: generator_loss=1.5914041996002197, discriminator_loss=0.24435116350650787\n",
            "step 2168: generator_loss=1.5369845628738403, discriminator_loss=0.25537627935409546\n",
            "step 2169: generator_loss=1.5671643018722534, discriminator_loss=0.2524423599243164\n",
            "step 2170: generator_loss=1.5480618476867676, discriminator_loss=0.25841110944747925\n",
            "step 2171: generator_loss=1.574462890625, discriminator_loss=0.253696084022522\n",
            "step 2172: generator_loss=1.7680593729019165, discriminator_loss=0.22630856931209564\n",
            "step 2173: generator_loss=1.8356317281723022, discriminator_loss=0.21598342061042786\n",
            "step 2174: generator_loss=1.8475611209869385, discriminator_loss=0.2155991494655609\n",
            "step 2175: generator_loss=2.0202410221099854, discriminator_loss=0.19506517052650452\n",
            "step 2176: generator_loss=2.005476713180542, discriminator_loss=0.19553935527801514\n",
            "step 2177: generator_loss=2.251326322555542, discriminator_loss=0.17258620262145996\n",
            "step 2178: generator_loss=2.13411283493042, discriminator_loss=0.18120026588439941\n",
            "step 2179: generator_loss=2.186429023742676, discriminator_loss=0.17593832314014435\n",
            "step 2180: generator_loss=2.2792062759399414, discriminator_loss=0.17189599573612213\n",
            "step 2181: generator_loss=2.2924938201904297, discriminator_loss=0.16757088899612427\n",
            "step 2182: generator_loss=2.3231329917907715, discriminator_loss=0.16550111770629883\n",
            "step 2183: generator_loss=2.2821261882781982, discriminator_loss=0.16596415638923645\n",
            "step 2184: generator_loss=2.244579553604126, discriminator_loss=0.1665397584438324\n",
            "step 2185: generator_loss=2.2167513370513916, discriminator_loss=0.16817989945411682\n",
            "step 2186: generator_loss=2.1419148445129395, discriminator_loss=0.17208240926265717\n",
            "step 2187: generator_loss=2.1922767162323, discriminator_loss=0.16787107288837433\n",
            "step 2188: generator_loss=2.1444382667541504, discriminator_loss=0.17088696360588074\n",
            "step 2189: generator_loss=2.1112725734710693, discriminator_loss=0.17282411456108093\n",
            "step 2190: generator_loss=2.1129491329193115, discriminator_loss=0.1716538965702057\n",
            "step 2191: generator_loss=2.0690765380859375, discriminator_loss=0.17425444722175598\n",
            "step 2192: generator_loss=2.011112928390503, discriminator_loss=0.17817650735378265\n",
            "step 2193: generator_loss=1.9225351810455322, discriminator_loss=0.18505896627902985\n",
            "step 2194: generator_loss=1.8876636028289795, discriminator_loss=0.18728026747703552\n",
            "step 2195: generator_loss=1.8921457529067993, discriminator_loss=0.18640094995498657\n",
            "step 2196: generator_loss=1.8581762313842773, discriminator_loss=0.18900690972805023\n",
            "step 2197: generator_loss=1.8372828960418701, discriminator_loss=0.19043409824371338\n",
            "step 2198: generator_loss=1.8257083892822266, discriminator_loss=0.19094052910804749\n",
            "step 2199: generator_loss=1.7969636917114258, discriminator_loss=0.193521186709404\n",
            "step 2200: generator_loss=1.7912213802337646, discriminator_loss=0.1936475932598114\n",
            "step 2201: generator_loss=1.7925422191619873, discriminator_loss=0.19320376217365265\n",
            "step 2202: generator_loss=1.8161132335662842, discriminator_loss=0.19096936285495758\n",
            "step 2203: generator_loss=1.7989001274108887, discriminator_loss=0.1922982931137085\n",
            "step 2204: generator_loss=1.8103125095367432, discriminator_loss=0.19114099442958832\n",
            "step 2205: generator_loss=1.816815733909607, discriminator_loss=0.19047221541404724\n",
            "step 2206: generator_loss=1.8170225620269775, discriminator_loss=0.1903538703918457\n",
            "step 2207: generator_loss=1.8202641010284424, discriminator_loss=0.190064936876297\n",
            "step 2208: generator_loss=1.8160938024520874, discriminator_loss=0.19050072133541107\n",
            "step 2209: generator_loss=1.809419870376587, discriminator_loss=0.19114390015602112\n",
            "step 2210: generator_loss=1.7951328754425049, discriminator_loss=0.19252149760723114\n",
            "step 2211: generator_loss=1.7823729515075684, discriminator_loss=0.19386552274227142\n",
            "step 2212: generator_loss=1.785477876663208, discriminator_loss=0.19364327192306519\n",
            "step 2213: generator_loss=1.7957035303115845, discriminator_loss=0.1926604062318802\n",
            "step 2214: generator_loss=1.802026391029358, discriminator_loss=0.19218893349170685\n",
            "step 2215: generator_loss=1.8054094314575195, discriminator_loss=0.19186073541641235\n",
            "step 2216: generator_loss=1.8190358877182007, discriminator_loss=0.1906100958585739\n",
            "step 2217: generator_loss=1.8246212005615234, discriminator_loss=0.19014820456504822\n",
            "step 2218: generator_loss=1.8066859245300293, discriminator_loss=0.19172658026218414\n",
            "step 2219: generator_loss=1.8110030889511108, discriminator_loss=0.19141831994056702\n",
            "step 2220: generator_loss=1.7904731035232544, discriminator_loss=0.1936229169368744\n",
            "step 2221: generator_loss=1.778450608253479, discriminator_loss=0.1949341595172882\n",
            "step 2222: generator_loss=1.7464218139648438, discriminator_loss=0.19866198301315308\n",
            "step 2223: generator_loss=1.744347333908081, discriminator_loss=0.1991214007139206\n",
            "step 2224: generator_loss=1.723994255065918, discriminator_loss=0.20186161994934082\n",
            "step 2225: generator_loss=1.7252265214920044, discriminator_loss=0.2018052488565445\n",
            "step 2226: generator_loss=1.7063311338424683, discriminator_loss=0.20403873920440674\n",
            "step 2227: generator_loss=1.7028770446777344, discriminator_loss=0.20476742088794708\n",
            "step 2228: generator_loss=1.6860016584396362, discriminator_loss=0.20670709013938904\n",
            "step 2229: generator_loss=1.6995172500610352, discriminator_loss=0.20519587397575378\n",
            "step 2230: generator_loss=1.7200520038604736, discriminator_loss=0.2032943069934845\n",
            "step 2231: generator_loss=1.744070053100586, discriminator_loss=0.20063617825508118\n",
            "step 2232: generator_loss=1.7319964170455933, discriminator_loss=0.201582133769989\n",
            "step 2233: generator_loss=1.7426328659057617, discriminator_loss=0.20059964060783386\n",
            "step 2234: generator_loss=1.7012414932250977, discriminator_loss=0.20460164546966553\n",
            "step 2235: generator_loss=1.672452449798584, discriminator_loss=0.2076227217912674\n",
            "step 2236: generator_loss=1.6391170024871826, discriminator_loss=0.21166884899139404\n",
            "step 2237: generator_loss=1.5906598567962646, discriminator_loss=0.21784496307373047\n",
            "step 2238: generator_loss=1.5615965127944946, discriminator_loss=0.2216772437095642\n",
            "step 2239: generator_loss=1.5618467330932617, discriminator_loss=0.22207745909690857\n",
            "step 2240: generator_loss=1.5569422245025635, discriminator_loss=0.22275957465171814\n",
            "step 2241: generator_loss=1.5812137126922607, discriminator_loss=0.21997672319412231\n",
            "step 2242: generator_loss=1.629685401916504, discriminator_loss=0.21402040123939514\n",
            "step 2243: generator_loss=1.6818137168884277, discriminator_loss=0.20805564522743225\n",
            "step 2244: generator_loss=1.7357842922210693, discriminator_loss=0.20208024978637695\n",
            "step 2245: generator_loss=1.8095386028289795, discriminator_loss=0.1945442408323288\n",
            "step 2246: generator_loss=1.867277979850769, discriminator_loss=0.18900242447853088\n",
            "step 2247: generator_loss=1.9105303287506104, discriminator_loss=0.18497046828269958\n",
            "step 2248: generator_loss=1.9362914562225342, discriminator_loss=0.18261586129665375\n",
            "step 2249: generator_loss=1.9338222742080688, discriminator_loss=0.18237125873565674\n",
            "step 2250: generator_loss=1.9153133630752563, discriminator_loss=0.18367639183998108\n",
            "step 2251: generator_loss=1.8884508609771729, discriminator_loss=0.18565189838409424\n",
            "step 2252: generator_loss=1.8482965230941772, discriminator_loss=0.18902000784873962\n",
            "step 2253: generator_loss=1.80646550655365, discriminator_loss=0.19286511838436127\n",
            "step 2254: generator_loss=1.7972874641418457, discriminator_loss=0.19364067912101746\n",
            "step 2255: generator_loss=1.8052458763122559, discriminator_loss=0.19280165433883667\n",
            "step 2256: generator_loss=1.8076422214508057, discriminator_loss=0.192356139421463\n",
            "step 2257: generator_loss=1.8008618354797363, discriminator_loss=0.19284547865390778\n",
            "step 2258: generator_loss=1.7885576486587524, discriminator_loss=0.19393882155418396\n",
            "step 2259: generator_loss=1.7672414779663086, discriminator_loss=0.19588536024093628\n",
            "step 2260: generator_loss=1.7402186393737793, discriminator_loss=0.1985251009464264\n",
            "step 2261: generator_loss=1.7138196229934692, discriminator_loss=0.2013760805130005\n",
            "step 2262: generator_loss=1.6852600574493408, discriminator_loss=0.20454153418540955\n",
            "step 2263: generator_loss=1.6528171300888062, discriminator_loss=0.20841318368911743\n",
            "step 2264: generator_loss=1.6243946552276611, discriminator_loss=0.211942657828331\n",
            "step 2265: generator_loss=1.6004530191421509, discriminator_loss=0.2152390033006668\n",
            "step 2266: generator_loss=1.5738170146942139, discriminator_loss=0.2189023792743683\n",
            "step 2267: generator_loss=1.5455363988876343, discriminator_loss=0.22292503714561462\n",
            "step 2268: generator_loss=1.532450795173645, discriminator_loss=0.22525563836097717\n",
            "step 2269: generator_loss=1.5481431484222412, discriminator_loss=0.22343634068965912\n",
            "step 2270: generator_loss=1.5598938465118408, discriminator_loss=0.22231990098953247\n",
            "step 2271: generator_loss=1.5746426582336426, discriminator_loss=0.22086530923843384\n",
            "step 2272: generator_loss=1.5866529941558838, discriminator_loss=0.21966595947742462\n",
            "step 2273: generator_loss=1.609503984451294, discriminator_loss=0.21699568629264832\n",
            "step 2274: generator_loss=1.641597867012024, discriminator_loss=0.2134896218776703\n",
            "step 2275: generator_loss=1.6889771223068237, discriminator_loss=0.20831778645515442\n",
            "step 2276: generator_loss=1.740563154220581, discriminator_loss=0.2031158208847046\n",
            "step 2277: generator_loss=1.8033527135849, discriminator_loss=0.19693610072135925\n",
            "step 2278: generator_loss=1.8634109497070312, discriminator_loss=0.1914019137620926\n",
            "step 2279: generator_loss=1.9134905338287354, discriminator_loss=0.18715396523475647\n",
            "step 2280: generator_loss=1.9495751857757568, discriminator_loss=0.1839621365070343\n",
            "step 2281: generator_loss=1.96830153465271, discriminator_loss=0.18229497969150543\n",
            "step 2282: generator_loss=1.9708338975906372, discriminator_loss=0.1818021833896637\n",
            "step 2283: generator_loss=1.974471092224121, discriminator_loss=0.1812422275543213\n",
            "step 2284: generator_loss=1.9697353839874268, discriminator_loss=0.18120697140693665\n",
            "step 2285: generator_loss=1.9517998695373535, discriminator_loss=0.1822524219751358\n",
            "step 2286: generator_loss=1.9226903915405273, discriminator_loss=0.18408524990081787\n",
            "step 2287: generator_loss=1.8936095237731934, discriminator_loss=0.18609188497066498\n",
            "step 2288: generator_loss=1.8715578317642212, discriminator_loss=0.18758583068847656\n",
            "step 2289: generator_loss=1.848402738571167, discriminator_loss=0.1894262135028839\n",
            "step 2290: generator_loss=1.825286626815796, discriminator_loss=0.19114243984222412\n",
            "step 2291: generator_loss=1.7969602346420288, discriminator_loss=0.1936427354812622\n",
            "step 2292: generator_loss=1.767726182937622, discriminator_loss=0.196316659450531\n",
            "step 2293: generator_loss=1.7407701015472412, discriminator_loss=0.19887369871139526\n",
            "step 2294: generator_loss=1.7088608741760254, discriminator_loss=0.20209214091300964\n",
            "step 2295: generator_loss=1.6875927448272705, discriminator_loss=0.2043476700782776\n",
            "step 2296: generator_loss=1.6807031631469727, discriminator_loss=0.20513561367988586\n",
            "step 2297: generator_loss=1.6937048435211182, discriminator_loss=0.2036924660205841\n",
            "step 2298: generator_loss=1.7108781337738037, discriminator_loss=0.2020786702632904\n",
            "step 2299: generator_loss=1.7407134771347046, discriminator_loss=0.19923365116119385\n",
            "step 2300: generator_loss=1.7861475944519043, discriminator_loss=0.19487535953521729\n",
            "step 2301: generator_loss=1.8430309295654297, discriminator_loss=0.1895931363105774\n",
            "step 2302: generator_loss=1.8962681293487549, discriminator_loss=0.1848982870578766\n",
            "step 2303: generator_loss=1.9443426132202148, discriminator_loss=0.18091130256652832\n",
            "step 2304: generator_loss=1.9799665212631226, discriminator_loss=0.1780426800251007\n",
            "step 2305: generator_loss=2.0032901763916016, discriminator_loss=0.17614732682704926\n",
            "step 2306: generator_loss=2.0092344284057617, discriminator_loss=0.17549219727516174\n",
            "step 2307: generator_loss=2.0052895545959473, discriminator_loss=0.1756683588027954\n",
            "step 2308: generator_loss=1.959792971611023, discriminator_loss=0.17956653237342834\n",
            "step 2309: generator_loss=1.9038050174713135, discriminator_loss=0.18492399156093597\n",
            "step 2310: generator_loss=1.8549656867980957, discriminator_loss=0.19024479389190674\n",
            "step 2311: generator_loss=1.776427984237671, discriminator_loss=0.2010081708431244\n",
            "step 2312: generator_loss=1.6610255241394043, discriminator_loss=0.21852940320968628\n",
            "step 2313: generator_loss=1.6501410007476807, discriminator_loss=0.22596475481987\n",
            "step 2314: generator_loss=1.5436865091323853, discriminator_loss=0.2526220679283142\n",
            "step 2315: generator_loss=1.5739092826843262, discriminator_loss=0.24988408386707306\n",
            "step 2316: generator_loss=1.4889395236968994, discriminator_loss=0.2732726037502289\n",
            "step 2317: generator_loss=1.5014913082122803, discriminator_loss=0.27694547176361084\n",
            "step 2318: generator_loss=1.540757656097412, discriminator_loss=0.2737468481063843\n",
            "step 2319: generator_loss=1.5954564809799194, discriminator_loss=0.26586008071899414\n",
            "step 2320: generator_loss=1.6538163423538208, discriminator_loss=0.2556992471218109\n",
            "step 2321: generator_loss=1.5265756845474243, discriminator_loss=0.2848646342754364\n",
            "step 2322: generator_loss=1.7149152755737305, discriminator_loss=0.24952134490013123\n",
            "step 2323: generator_loss=1.6916413307189941, discriminator_loss=0.25413456559181213\n",
            "step 2324: generator_loss=1.8920485973358154, discriminator_loss=0.22294320166110992\n",
            "step 2325: generator_loss=1.9013620615005493, discriminator_loss=0.2254464328289032\n",
            "step 2326: generator_loss=2.1315903663635254, discriminator_loss=0.1933182179927826\n",
            "step 2327: generator_loss=2.046213150024414, discriminator_loss=0.20244061946868896\n",
            "step 2328: generator_loss=2.2106595039367676, discriminator_loss=0.18685144186019897\n",
            "step 2329: generator_loss=2.132176399230957, discriminator_loss=0.19102151691913605\n",
            "step 2330: generator_loss=2.3450803756713867, discriminator_loss=0.17385095357894897\n",
            "step 2331: generator_loss=2.3739311695098877, discriminator_loss=0.1697535663843155\n",
            "step 2332: generator_loss=2.3731305599212646, discriminator_loss=0.16637031733989716\n",
            "step 2333: generator_loss=2.4767189025878906, discriminator_loss=0.15988346934318542\n",
            "step 2334: generator_loss=2.4771957397460938, discriminator_loss=0.15843212604522705\n",
            "step 2335: generator_loss=2.4718029499053955, discriminator_loss=0.15729519724845886\n",
            "step 2336: generator_loss=2.4568448066711426, discriminator_loss=0.15768486261367798\n",
            "step 2337: generator_loss=2.5146493911743164, discriminator_loss=0.15278290212154388\n",
            "step 2338: generator_loss=2.3044705390930176, discriminator_loss=0.16520079970359802\n",
            "step 2339: generator_loss=2.4360830783843994, discriminator_loss=0.1562788486480713\n",
            "step 2340: generator_loss=2.312987804412842, discriminator_loss=0.16443905234336853\n",
            "step 2341: generator_loss=2.3145651817321777, discriminator_loss=0.16276969015598297\n",
            "step 2342: generator_loss=2.2590174674987793, discriminator_loss=0.16582414507865906\n",
            "step 2343: generator_loss=2.2704248428344727, discriminator_loss=0.16566181182861328\n",
            "step 2344: generator_loss=2.1656205654144287, discriminator_loss=0.17222777009010315\n",
            "step 2345: generator_loss=2.1500284671783447, discriminator_loss=0.17323103547096252\n",
            "step 2346: generator_loss=2.161128044128418, discriminator_loss=0.17033281922340393\n",
            "step 2347: generator_loss=2.1613473892211914, discriminator_loss=0.1717294454574585\n",
            "step 2348: generator_loss=2.1357662677764893, discriminator_loss=0.1730109304189682\n",
            "step 2349: generator_loss=2.1789021492004395, discriminator_loss=0.16819454729557037\n",
            "step 2350: generator_loss=2.1025328636169434, discriminator_loss=0.1728677600622177\n",
            "step 2351: generator_loss=2.1468803882598877, discriminator_loss=0.16869516670703888\n",
            "step 2352: generator_loss=2.152423858642578, discriminator_loss=0.16729633510112762\n",
            "step 2353: generator_loss=2.220594644546509, discriminator_loss=0.1612178385257721\n",
            "step 2354: generator_loss=2.1372976303100586, discriminator_loss=0.165816068649292\n",
            "step 2355: generator_loss=2.148399829864502, discriminator_loss=0.16500651836395264\n",
            "step 2356: generator_loss=2.1924920082092285, discriminator_loss=0.1588015854358673\n",
            "step 2357: generator_loss=2.1460037231445312, discriminator_loss=0.16307251155376434\n",
            "step 2358: generator_loss=2.0859432220458984, discriminator_loss=0.16605648398399353\n",
            "step 2359: generator_loss=2.0016210079193115, discriminator_loss=0.17105910181999207\n",
            "step 2360: generator_loss=2.0304019451141357, discriminator_loss=0.17197629809379578\n",
            "step 2361: generator_loss=1.9514939785003662, discriminator_loss=0.17516760528087616\n",
            "step 2362: generator_loss=1.9067566394805908, discriminator_loss=0.17964333295822144\n",
            "step 2363: generator_loss=1.86802339553833, discriminator_loss=0.18084797263145447\n",
            "step 2364: generator_loss=1.8760895729064941, discriminator_loss=0.18413913249969482\n",
            "step 2365: generator_loss=1.823791265487671, discriminator_loss=0.18621742725372314\n",
            "step 2366: generator_loss=1.8193371295928955, discriminator_loss=0.1861066371202469\n",
            "step 2367: generator_loss=1.8505468368530273, discriminator_loss=0.18359139561653137\n",
            "step 2368: generator_loss=1.8195072412490845, discriminator_loss=0.18802492320537567\n",
            "step 2369: generator_loss=1.7904462814331055, discriminator_loss=0.19025371968746185\n",
            "step 2370: generator_loss=1.814737319946289, discriminator_loss=0.18879754841327667\n",
            "step 2371: generator_loss=1.77157461643219, discriminator_loss=0.19202522933483124\n",
            "step 2372: generator_loss=1.8274085521697998, discriminator_loss=0.18908637762069702\n",
            "step 2373: generator_loss=1.7458351850509644, discriminator_loss=0.19429543614387512\n",
            "step 2374: generator_loss=1.8037017583847046, discriminator_loss=0.1935855895280838\n",
            "step 2375: generator_loss=1.7085368633270264, discriminator_loss=0.19990122318267822\n",
            "step 2376: generator_loss=1.696218729019165, discriminator_loss=0.2016022503376007\n",
            "step 2377: generator_loss=1.6245536804199219, discriminator_loss=0.20721375942230225\n",
            "step 2378: generator_loss=1.6355774402618408, discriminator_loss=0.20753982663154602\n",
            "step 2379: generator_loss=1.604131817817688, discriminator_loss=0.21123704314231873\n",
            "step 2380: generator_loss=1.5767322778701782, discriminator_loss=0.21484869718551636\n",
            "step 2381: generator_loss=1.558791995048523, discriminator_loss=0.2180662602186203\n",
            "step 2382: generator_loss=1.5092322826385498, discriminator_loss=0.2253228724002838\n",
            "step 2383: generator_loss=1.4767770767211914, discriminator_loss=0.23031000792980194\n",
            "step 2384: generator_loss=1.4157605171203613, discriminator_loss=0.24022257328033447\n",
            "step 2385: generator_loss=1.4154871702194214, discriminator_loss=0.24191069602966309\n",
            "step 2386: generator_loss=1.4174554347991943, discriminator_loss=0.2431626319885254\n",
            "step 2387: generator_loss=1.3586809635162354, discriminator_loss=0.25350117683410645\n",
            "step 2388: generator_loss=1.3581650257110596, discriminator_loss=0.255676805973053\n",
            "step 2389: generator_loss=1.365347146987915, discriminator_loss=0.25677332282066345\n",
            "step 2390: generator_loss=1.3875839710235596, discriminator_loss=0.25577351450920105\n",
            "step 2391: generator_loss=1.3452709913253784, discriminator_loss=0.26274508237838745\n",
            "step 2392: generator_loss=1.3675298690795898, discriminator_loss=0.26026394963264465\n",
            "step 2393: generator_loss=1.3705857992172241, discriminator_loss=0.2612088918685913\n",
            "step 2394: generator_loss=1.3948612213134766, discriminator_loss=0.25805342197418213\n",
            "step 2395: generator_loss=1.462756633758545, discriminator_loss=0.24922756850719452\n",
            "step 2396: generator_loss=1.5394420623779297, discriminator_loss=0.23974356055259705\n",
            "step 2397: generator_loss=1.6310380697250366, discriminator_loss=0.22950896620750427\n",
            "step 2398: generator_loss=1.7165722846984863, discriminator_loss=0.22130413353443146\n",
            "step 2399: generator_loss=1.8009090423583984, discriminator_loss=0.21430648863315582\n",
            "step 2400: generator_loss=1.8590192794799805, discriminator_loss=0.21058370172977448\n",
            "step 2401: generator_loss=1.8565014600753784, discriminator_loss=0.21108217537403107\n",
            "step 2402: generator_loss=1.8743281364440918, discriminator_loss=0.2104097604751587\n",
            "step 2403: generator_loss=1.8939863443374634, discriminator_loss=0.20972256362438202\n",
            "step 2404: generator_loss=1.872961401939392, discriminator_loss=0.21172210574150085\n",
            "step 2405: generator_loss=1.8553996086120605, discriminator_loss=0.21342787146568298\n",
            "step 2406: generator_loss=1.82505202293396, discriminator_loss=0.2158219814300537\n",
            "step 2407: generator_loss=1.794772982597351, discriminator_loss=0.21793970465660095\n",
            "step 2408: generator_loss=1.7883031368255615, discriminator_loss=0.21744436025619507\n",
            "step 2409: generator_loss=1.7702758312225342, discriminator_loss=0.2176988422870636\n",
            "step 2410: generator_loss=1.7557488679885864, discriminator_loss=0.2174130380153656\n",
            "step 2411: generator_loss=1.7330143451690674, discriminator_loss=0.21799640357494354\n",
            "step 2412: generator_loss=1.7062737941741943, discriminator_loss=0.21909606456756592\n",
            "step 2413: generator_loss=1.6883665323257446, discriminator_loss=0.21936511993408203\n",
            "step 2414: generator_loss=1.6619688272476196, discriminator_loss=0.2207944095134735\n",
            "step 2415: generator_loss=1.6557579040527344, discriminator_loss=0.2201620638370514\n",
            "step 2416: generator_loss=1.6474535465240479, discriminator_loss=0.22016417980194092\n",
            "step 2417: generator_loss=1.6588623523712158, discriminator_loss=0.2183561474084854\n",
            "step 2418: generator_loss=1.656653881072998, discriminator_loss=0.2182547152042389\n",
            "step 2419: generator_loss=1.6756618022918701, discriminator_loss=0.215131476521492\n",
            "step 2420: generator_loss=1.6625134944915771, discriminator_loss=0.21823248267173767\n",
            "step 2421: generator_loss=1.666503667831421, discriminator_loss=0.2183057814836502\n",
            "step 2422: generator_loss=1.6533093452453613, discriminator_loss=0.22326502203941345\n",
            "step 2423: generator_loss=1.705974817276001, discriminator_loss=0.21795785427093506\n",
            "step 2424: generator_loss=1.6439909934997559, discriminator_loss=0.22984027862548828\n",
            "step 2425: generator_loss=1.7178034782409668, discriminator_loss=0.2218962013721466\n",
            "step 2426: generator_loss=1.801117181777954, discriminator_loss=0.2130868136882782\n",
            "step 2427: generator_loss=1.8916609287261963, discriminator_loss=0.20618703961372375\n",
            "step 2428: generator_loss=1.8991531133651733, discriminator_loss=0.2064163088798523\n",
            "step 2429: generator_loss=1.7278419733047485, discriminator_loss=0.23540736734867096\n",
            "step 2430: generator_loss=2.001887798309326, discriminator_loss=0.20571771264076233\n",
            "step 2431: generator_loss=2.0707123279571533, discriminator_loss=0.1971830278635025\n",
            "step 2432: generator_loss=1.9930933713912964, discriminator_loss=0.20826208591461182\n",
            "step 2433: generator_loss=2.0538954734802246, discriminator_loss=0.19933798909187317\n",
            "step 2434: generator_loss=2.221449851989746, discriminator_loss=0.18534180521965027\n",
            "step 2435: generator_loss=2.140787124633789, discriminator_loss=0.19104281067848206\n",
            "step 2436: generator_loss=2.1958673000335693, discriminator_loss=0.1870933175086975\n",
            "step 2437: generator_loss=2.2624688148498535, discriminator_loss=0.18046234548091888\n",
            "step 2438: generator_loss=2.293252944946289, discriminator_loss=0.18001596629619598\n",
            "step 2439: generator_loss=2.345322847366333, discriminator_loss=0.17358535528182983\n",
            "step 2440: generator_loss=2.5612049102783203, discriminator_loss=0.1594797670841217\n",
            "step 2441: generator_loss=2.5763089656829834, discriminator_loss=0.1538982391357422\n",
            "step 2442: generator_loss=2.475106954574585, discriminator_loss=0.15998558700084686\n",
            "step 2443: generator_loss=2.5193097591400146, discriminator_loss=0.1566876471042633\n",
            "step 2444: generator_loss=2.4705681800842285, discriminator_loss=0.15688389539718628\n",
            "step 2445: generator_loss=2.532012462615967, discriminator_loss=0.15305015444755554\n",
            "step 2446: generator_loss=2.617135524749756, discriminator_loss=0.14934846758842468\n",
            "step 2447: generator_loss=2.461440324783325, discriminator_loss=0.15665209293365479\n",
            "step 2448: generator_loss=2.4711408615112305, discriminator_loss=0.15223270654678345\n",
            "step 2449: generator_loss=2.567310094833374, discriminator_loss=0.14803698658943176\n",
            "step 2450: generator_loss=2.4184539318084717, discriminator_loss=0.1531696915626526\n",
            "step 2451: generator_loss=2.437007427215576, discriminator_loss=0.1511809378862381\n",
            "step 2452: generator_loss=2.4350156784057617, discriminator_loss=0.14912532269954681\n",
            "step 2453: generator_loss=2.431018829345703, discriminator_loss=0.14890700578689575\n",
            "step 2454: generator_loss=2.2911031246185303, discriminator_loss=0.15610888600349426\n",
            "step 2455: generator_loss=2.3146474361419678, discriminator_loss=0.15427237749099731\n",
            "step 2456: generator_loss=2.175913095474243, discriminator_loss=0.1601364016532898\n",
            "step 2457: generator_loss=2.1289525032043457, discriminator_loss=0.16240859031677246\n",
            "step 2458: generator_loss=2.1605849266052246, discriminator_loss=0.16237185895442963\n",
            "step 2459: generator_loss=2.0392837524414062, discriminator_loss=0.1676434576511383\n",
            "step 2460: generator_loss=2.0409421920776367, discriminator_loss=0.16989222168922424\n",
            "step 2461: generator_loss=1.9010608196258545, discriminator_loss=0.17666129767894745\n",
            "step 2462: generator_loss=1.9839825630187988, discriminator_loss=0.1736442893743515\n",
            "step 2463: generator_loss=1.913332223892212, discriminator_loss=0.18056625127792358\n",
            "step 2464: generator_loss=1.8748209476470947, discriminator_loss=0.1836511492729187\n",
            "step 2465: generator_loss=1.7693108320236206, discriminator_loss=0.1905832588672638\n",
            "step 2466: generator_loss=1.7653416395187378, discriminator_loss=0.19402912259101868\n",
            "step 2467: generator_loss=1.8182562589645386, discriminator_loss=0.19576218724250793\n",
            "step 2468: generator_loss=1.7692307233810425, discriminator_loss=0.2025618553161621\n",
            "step 2469: generator_loss=1.8543221950531006, discriminator_loss=0.19728818535804749\n",
            "step 2470: generator_loss=1.786969780921936, discriminator_loss=0.20487338304519653\n",
            "step 2471: generator_loss=1.6135923862457275, discriminator_loss=0.2170577198266983\n",
            "step 2472: generator_loss=1.6672438383102417, discriminator_loss=0.21610090136528015\n",
            "step 2473: generator_loss=1.61000394821167, discriminator_loss=0.22183926403522491\n",
            "step 2474: generator_loss=1.584201693534851, discriminator_loss=0.22344623506069183\n",
            "step 2475: generator_loss=1.5398693084716797, discriminator_loss=0.22765257954597473\n",
            "step 2476: generator_loss=1.5746469497680664, discriminator_loss=0.22845515608787537\n",
            "step 2477: generator_loss=1.5575878620147705, discriminator_loss=0.2281275987625122\n",
            "step 2478: generator_loss=1.4889609813690186, discriminator_loss=0.23492364585399628\n",
            "step 2479: generator_loss=1.5282235145568848, discriminator_loss=0.23156389594078064\n",
            "step 2480: generator_loss=1.521535873413086, discriminator_loss=0.23255448043346405\n",
            "step 2481: generator_loss=1.4547276496887207, discriminator_loss=0.241621732711792\n",
            "step 2482: generator_loss=1.5205028057098389, discriminator_loss=0.23144081234931946\n",
            "step 2483: generator_loss=1.55613374710083, discriminator_loss=0.22834239900112152\n",
            "step 2484: generator_loss=1.5753114223480225, discriminator_loss=0.22494566440582275\n",
            "step 2485: generator_loss=1.6125911474227905, discriminator_loss=0.2210673689842224\n",
            "step 2486: generator_loss=1.619025707244873, discriminator_loss=0.22093109786510468\n",
            "step 2487: generator_loss=1.667548418045044, discriminator_loss=0.2169371396303177\n",
            "step 2488: generator_loss=1.6703121662139893, discriminator_loss=0.2180616408586502\n",
            "step 2489: generator_loss=1.7409008741378784, discriminator_loss=0.21174314618110657\n",
            "step 2490: generator_loss=1.7143168449401855, discriminator_loss=0.21372675895690918\n",
            "step 2491: generator_loss=1.6782348155975342, discriminator_loss=0.21867170929908752\n",
            "step 2492: generator_loss=1.6258291006088257, discriminator_loss=0.22614817321300507\n",
            "step 2493: generator_loss=1.575242042541504, discriminator_loss=0.23495356738567352\n",
            "step 2494: generator_loss=1.5680317878723145, discriminator_loss=0.2366950511932373\n",
            "step 2495: generator_loss=1.6048252582550049, discriminator_loss=0.23391395807266235\n",
            "step 2496: generator_loss=1.5648701190948486, discriminator_loss=0.24092333018779755\n",
            "step 2497: generator_loss=1.5471402406692505, discriminator_loss=0.24492444097995758\n",
            "step 2498: generator_loss=1.5574896335601807, discriminator_loss=0.24456322193145752\n",
            "step 2499: generator_loss=1.5826630592346191, discriminator_loss=0.2460828721523285\n",
            "step 2500: generator_loss=1.5613999366760254, discriminator_loss=0.25045254826545715\n",
            "step 2501: generator_loss=1.5589042901992798, discriminator_loss=0.2548726201057434\n",
            "step 2502: generator_loss=1.5505661964416504, discriminator_loss=0.2609207332134247\n",
            "step 2503: generator_loss=1.6292171478271484, discriminator_loss=0.2530112862586975\n",
            "step 2504: generator_loss=1.6668574810028076, discriminator_loss=0.25188255310058594\n",
            "step 2505: generator_loss=1.6771092414855957, discriminator_loss=0.25094467401504517\n",
            "step 2506: generator_loss=1.8166898488998413, discriminator_loss=0.23469841480255127\n",
            "step 2507: generator_loss=1.7489535808563232, discriminator_loss=0.23979830741882324\n",
            "step 2508: generator_loss=1.7757498025894165, discriminator_loss=0.2420327067375183\n",
            "step 2509: generator_loss=1.7640388011932373, discriminator_loss=0.24096283316612244\n",
            "step 2510: generator_loss=1.7502343654632568, discriminator_loss=0.2431247979402542\n",
            "step 2511: generator_loss=1.8300318717956543, discriminator_loss=0.23171210289001465\n",
            "step 2512: generator_loss=1.8649182319641113, discriminator_loss=0.22623014450073242\n",
            "step 2513: generator_loss=1.8636014461517334, discriminator_loss=0.22346320748329163\n",
            "step 2514: generator_loss=1.857863187789917, discriminator_loss=0.21971821784973145\n",
            "step 2515: generator_loss=1.9092175960540771, discriminator_loss=0.2126549482345581\n",
            "step 2516: generator_loss=1.9690812826156616, discriminator_loss=0.20542433857917786\n",
            "step 2517: generator_loss=1.9809225797653198, discriminator_loss=0.20101501047611237\n",
            "step 2518: generator_loss=2.0587172508239746, discriminator_loss=0.19366058707237244\n",
            "step 2519: generator_loss=2.056711196899414, discriminator_loss=0.1922726035118103\n",
            "step 2520: generator_loss=2.0898237228393555, discriminator_loss=0.1873575747013092\n",
            "step 2521: generator_loss=2.1368370056152344, discriminator_loss=0.18264064192771912\n",
            "step 2522: generator_loss=2.0701284408569336, discriminator_loss=0.18584084510803223\n",
            "step 2523: generator_loss=2.0969271659851074, discriminator_loss=0.18214288353919983\n",
            "step 2524: generator_loss=2.0907602310180664, discriminator_loss=0.18091191351413727\n",
            "step 2525: generator_loss=2.0971434116363525, discriminator_loss=0.17915822565555573\n",
            "step 2526: generator_loss=2.0964465141296387, discriminator_loss=0.17772802710533142\n",
            "step 2527: generator_loss=2.0670971870422363, discriminator_loss=0.17832908034324646\n",
            "step 2528: generator_loss=2.0517477989196777, discriminator_loss=0.17822211980819702\n",
            "step 2529: generator_loss=1.9955990314483643, discriminator_loss=0.1809164583683014\n",
            "step 2530: generator_loss=1.9539778232574463, discriminator_loss=0.18331274390220642\n",
            "step 2531: generator_loss=1.9326542615890503, discriminator_loss=0.18364989757537842\n",
            "step 2532: generator_loss=1.8988540172576904, discriminator_loss=0.18541006743907928\n",
            "step 2533: generator_loss=1.8562724590301514, discriminator_loss=0.18816961348056793\n",
            "step 2534: generator_loss=1.8452608585357666, discriminator_loss=0.1884121149778366\n",
            "step 2535: generator_loss=1.8492052555084229, discriminator_loss=0.18718594312667847\n",
            "step 2536: generator_loss=1.8387947082519531, discriminator_loss=0.18760210275650024\n",
            "step 2537: generator_loss=1.8503974676132202, discriminator_loss=0.18605275452136993\n",
            "step 2538: generator_loss=1.856744408607483, discriminator_loss=0.18519511818885803\n",
            "step 2539: generator_loss=1.8386542797088623, discriminator_loss=0.18684667348861694\n",
            "step 2540: generator_loss=1.8490865230560303, discriminator_loss=0.18610459566116333\n",
            "step 2541: generator_loss=1.820826768875122, discriminator_loss=0.1886160671710968\n",
            "step 2542: generator_loss=1.8133794069290161, discriminator_loss=0.18932320177555084\n",
            "step 2543: generator_loss=1.831676959991455, discriminator_loss=0.18826055526733398\n",
            "step 2544: generator_loss=1.8463860750198364, discriminator_loss=0.18686003983020782\n",
            "step 2545: generator_loss=1.8270527124404907, discriminator_loss=0.1889508068561554\n",
            "step 2546: generator_loss=1.8169397115707397, discriminator_loss=0.18983539938926697\n",
            "step 2547: generator_loss=1.7920515537261963, discriminator_loss=0.19313809275627136\n",
            "step 2548: generator_loss=1.8043568134307861, discriminator_loss=0.1923639178276062\n",
            "step 2549: generator_loss=1.7296628952026367, discriminator_loss=0.1991540640592575\n",
            "step 2550: generator_loss=1.6944677829742432, discriminator_loss=0.2025321125984192\n",
            "step 2551: generator_loss=1.641231656074524, discriminator_loss=0.20889756083488464\n",
            "step 2552: generator_loss=1.6482658386230469, discriminator_loss=0.20879535377025604\n",
            "step 2553: generator_loss=1.6659135818481445, discriminator_loss=0.20713627338409424\n",
            "step 2554: generator_loss=1.662947654724121, discriminator_loss=0.20788279175758362\n",
            "step 2555: generator_loss=1.7365310192108154, discriminator_loss=0.20001205801963806\n",
            "step 2556: generator_loss=1.710956335067749, discriminator_loss=0.20390541851520538\n",
            "step 2557: generator_loss=1.75544011592865, discriminator_loss=0.19963404536247253\n",
            "step 2558: generator_loss=1.7761352062225342, discriminator_loss=0.19866834580898285\n",
            "step 2559: generator_loss=1.7740525007247925, discriminator_loss=0.1987249255180359\n",
            "step 2560: generator_loss=1.7294353246688843, discriminator_loss=0.20460617542266846\n",
            "step 2561: generator_loss=1.6930744647979736, discriminator_loss=0.20884579420089722\n",
            "step 2562: generator_loss=1.7156054973602295, discriminator_loss=0.2104015350341797\n",
            "step 2563: generator_loss=1.6999564170837402, discriminator_loss=0.2128385305404663\n",
            "step 2564: generator_loss=1.7445939779281616, discriminator_loss=0.20720891654491425\n",
            "step 2565: generator_loss=1.7803698778152466, discriminator_loss=0.2059856355190277\n",
            "step 2566: generator_loss=1.7671302556991577, discriminator_loss=0.20718443393707275\n",
            "step 2567: generator_loss=1.7401865720748901, discriminator_loss=0.2092488408088684\n",
            "step 2568: generator_loss=1.7457504272460938, discriminator_loss=0.20855897665023804\n",
            "step 2569: generator_loss=1.7920247316360474, discriminator_loss=0.20364177227020264\n",
            "step 2570: generator_loss=1.7867988348007202, discriminator_loss=0.20202751457691193\n",
            "step 2571: generator_loss=1.8311505317687988, discriminator_loss=0.1975395530462265\n",
            "step 2572: generator_loss=1.891169786453247, discriminator_loss=0.19170549511909485\n",
            "step 2573: generator_loss=1.9326996803283691, discriminator_loss=0.188076913356781\n",
            "step 2574: generator_loss=2.032658338546753, discriminator_loss=0.18001560866832733\n",
            "step 2575: generator_loss=2.1092891693115234, discriminator_loss=0.1738966405391693\n",
            "step 2576: generator_loss=2.182570457458496, discriminator_loss=0.168610081076622\n",
            "step 2577: generator_loss=2.2234387397766113, discriminator_loss=0.16538235545158386\n",
            "step 2578: generator_loss=2.270285129547119, discriminator_loss=0.16176831722259521\n",
            "step 2579: generator_loss=2.2930803298950195, discriminator_loss=0.1598203480243683\n",
            "step 2580: generator_loss=2.2886276245117188, discriminator_loss=0.159236878156662\n",
            "step 2581: generator_loss=2.290947675704956, discriminator_loss=0.15822410583496094\n",
            "step 2582: generator_loss=2.255476713180542, discriminator_loss=0.1593770980834961\n",
            "step 2583: generator_loss=2.231865167617798, discriminator_loss=0.16019989550113678\n",
            "step 2584: generator_loss=2.180488109588623, discriminator_loss=0.16224288940429688\n",
            "step 2585: generator_loss=2.1530752182006836, discriminator_loss=0.16327789425849915\n",
            "step 2586: generator_loss=2.0722527503967285, discriminator_loss=0.16834263503551483\n",
            "step 2587: generator_loss=2.0007433891296387, discriminator_loss=0.17353501915931702\n",
            "step 2588: generator_loss=1.9677281379699707, discriminator_loss=0.1762530505657196\n",
            "step 2589: generator_loss=1.8879375457763672, discriminator_loss=0.18415208160877228\n",
            "step 2590: generator_loss=1.8181921243667603, discriminator_loss=0.19351044297218323\n",
            "step 2591: generator_loss=1.7365926504135132, discriminator_loss=0.20673751831054688\n",
            "step 2592: generator_loss=1.7012617588043213, discriminator_loss=0.2180350422859192\n",
            "step 2593: generator_loss=1.6312572956085205, discriminator_loss=0.23840297758579254\n",
            "step 2594: generator_loss=1.7222130298614502, discriminator_loss=0.22250065207481384\n",
            "step 2595: generator_loss=1.6614124774932861, discriminator_loss=0.23940765857696533\n",
            "step 2596: generator_loss=1.6619597673416138, discriminator_loss=0.24144603312015533\n",
            "step 2597: generator_loss=1.7421355247497559, discriminator_loss=0.2297654002904892\n",
            "step 2598: generator_loss=1.6865029335021973, discriminator_loss=0.23957763612270355\n",
            "step 2599: generator_loss=1.7748193740844727, discriminator_loss=0.23113203048706055\n",
            "step 2600: generator_loss=1.7820849418640137, discriminator_loss=0.23069438338279724\n",
            "step 2601: generator_loss=1.9400067329406738, discriminator_loss=0.20662672817707062\n",
            "step 2602: generator_loss=1.9671547412872314, discriminator_loss=0.20361962914466858\n",
            "step 2603: generator_loss=2.0743799209594727, discriminator_loss=0.1868506371974945\n",
            "step 2604: generator_loss=2.163173198699951, discriminator_loss=0.17634254693984985\n",
            "step 2605: generator_loss=2.1647472381591797, discriminator_loss=0.1756860911846161\n",
            "step 2606: generator_loss=2.333789587020874, discriminator_loss=0.16280843317508698\n",
            "step 2607: generator_loss=2.3767409324645996, discriminator_loss=0.155747652053833\n",
            "step 2608: generator_loss=2.4679553508758545, discriminator_loss=0.14998501539230347\n",
            "step 2609: generator_loss=2.4774296283721924, discriminator_loss=0.14934268593788147\n",
            "step 2610: generator_loss=2.5124714374542236, discriminator_loss=0.14623713493347168\n",
            "step 2611: generator_loss=2.490793228149414, discriminator_loss=0.14588218927383423\n",
            "step 2612: generator_loss=2.5023984909057617, discriminator_loss=0.14680567383766174\n",
            "step 2613: generator_loss=2.3843297958374023, discriminator_loss=0.15110550820827484\n",
            "step 2614: generator_loss=2.3778834342956543, discriminator_loss=0.15373940765857697\n",
            "step 2615: generator_loss=2.2211976051330566, discriminator_loss=0.1603013277053833\n",
            "step 2616: generator_loss=2.1311426162719727, discriminator_loss=0.16818514466285706\n",
            "step 2617: generator_loss=2.059940814971924, discriminator_loss=0.17379409074783325\n",
            "step 2618: generator_loss=1.884962797164917, discriminator_loss=0.19638803601264954\n",
            "step 2619: generator_loss=1.7605384588241577, discriminator_loss=0.21945233643054962\n",
            "step 2620: generator_loss=1.9399349689483643, discriminator_loss=0.19850027561187744\n",
            "step 2621: generator_loss=1.783372163772583, discriminator_loss=0.21898820996284485\n",
            "step 2622: generator_loss=1.6414794921875, discriminator_loss=0.23921306431293488\n",
            "step 2623: generator_loss=1.697793960571289, discriminator_loss=0.23576809465885162\n",
            "step 2624: generator_loss=1.7260639667510986, discriminator_loss=0.23627980053424835\n",
            "step 2625: generator_loss=1.7219219207763672, discriminator_loss=0.23627497255802155\n",
            "step 2626: generator_loss=1.8338830471038818, discriminator_loss=0.21404048800468445\n",
            "step 2627: generator_loss=1.9012290239334106, discriminator_loss=0.20456290245056152\n",
            "step 2628: generator_loss=1.8623762130737305, discriminator_loss=0.21713116765022278\n",
            "step 2629: generator_loss=1.8465709686279297, discriminator_loss=0.21907122433185577\n",
            "step 2630: generator_loss=1.8363662958145142, discriminator_loss=0.22326257824897766\n",
            "step 2631: generator_loss=1.829355001449585, discriminator_loss=0.2204996645450592\n",
            "step 2632: generator_loss=1.864182472229004, discriminator_loss=0.2179037183523178\n",
            "step 2633: generator_loss=1.8539714813232422, discriminator_loss=0.22385020554065704\n",
            "step 2634: generator_loss=2.0727949142456055, discriminator_loss=0.1970730721950531\n",
            "step 2635: generator_loss=2.0551462173461914, discriminator_loss=0.20251153409481049\n",
            "step 2636: generator_loss=2.088008403778076, discriminator_loss=0.1961258351802826\n",
            "step 2637: generator_loss=1.9564182758331299, discriminator_loss=0.20642995834350586\n",
            "step 2638: generator_loss=2.102189540863037, discriminator_loss=0.19299760460853577\n",
            "step 2639: generator_loss=2.1596827507019043, discriminator_loss=0.18727198243141174\n",
            "step 2640: generator_loss=2.074328899383545, discriminator_loss=0.18981939554214478\n",
            "step 2641: generator_loss=2.125704288482666, discriminator_loss=0.18473118543624878\n",
            "step 2642: generator_loss=2.0795533657073975, discriminator_loss=0.1862688511610031\n",
            "step 2643: generator_loss=2.1577258110046387, discriminator_loss=0.17432014644145966\n",
            "step 2644: generator_loss=2.133016347885132, discriminator_loss=0.17352423071861267\n",
            "step 2645: generator_loss=2.030240535736084, discriminator_loss=0.17710904777050018\n",
            "step 2646: generator_loss=2.03515362739563, discriminator_loss=0.1752648949623108\n",
            "step 2647: generator_loss=2.1117584705352783, discriminator_loss=0.17028869688510895\n",
            "step 2648: generator_loss=1.9820948839187622, discriminator_loss=0.17712469398975372\n",
            "step 2649: generator_loss=2.0107922554016113, discriminator_loss=0.17426162958145142\n",
            "step 2650: generator_loss=1.9818246364593506, discriminator_loss=0.17578157782554626\n",
            "step 2651: generator_loss=1.9456360340118408, discriminator_loss=0.17797665297985077\n",
            "step 2652: generator_loss=1.9533915519714355, discriminator_loss=0.18028730154037476\n",
            "step 2653: generator_loss=1.889906883239746, discriminator_loss=0.18442153930664062\n",
            "step 2654: generator_loss=1.8679357767105103, discriminator_loss=0.18808507919311523\n",
            "step 2655: generator_loss=1.7842705249786377, discriminator_loss=0.19465002417564392\n",
            "step 2656: generator_loss=1.8613414764404297, discriminator_loss=0.18999718129634857\n",
            "step 2657: generator_loss=1.69183349609375, discriminator_loss=0.20085583627223969\n",
            "step 2658: generator_loss=1.6798455715179443, discriminator_loss=0.2036578357219696\n",
            "step 2659: generator_loss=1.7398204803466797, discriminator_loss=0.20652887225151062\n",
            "step 2660: generator_loss=1.6555331945419312, discriminator_loss=0.21433159708976746\n",
            "step 2661: generator_loss=1.5842807292938232, discriminator_loss=0.21621914207935333\n",
            "step 2662: generator_loss=1.6783040761947632, discriminator_loss=0.20943346619606018\n",
            "step 2663: generator_loss=1.684157371520996, discriminator_loss=0.21211519837379456\n",
            "step 2664: generator_loss=1.5881426334381104, discriminator_loss=0.21581053733825684\n",
            "step 2665: generator_loss=1.6740999221801758, discriminator_loss=0.20670056343078613\n",
            "step 2666: generator_loss=1.6522222757339478, discriminator_loss=0.2074533998966217\n",
            "step 2667: generator_loss=1.6956323385238647, discriminator_loss=0.20507271587848663\n",
            "step 2668: generator_loss=1.7553831338882446, discriminator_loss=0.1999860554933548\n",
            "step 2669: generator_loss=1.6795074939727783, discriminator_loss=0.20651251077651978\n",
            "step 2670: generator_loss=1.668736457824707, discriminator_loss=0.20807158946990967\n",
            "step 2671: generator_loss=1.5884132385253906, discriminator_loss=0.21697504818439484\n",
            "step 2672: generator_loss=1.6089383363723755, discriminator_loss=0.21604172885417938\n",
            "step 2673: generator_loss=1.5414217710494995, discriminator_loss=0.22474506497383118\n",
            "step 2674: generator_loss=1.5202157497406006, discriminator_loss=0.22901412844657898\n",
            "step 2675: generator_loss=1.5267138481140137, discriminator_loss=0.2298421859741211\n",
            "step 2676: generator_loss=1.543269157409668, discriminator_loss=0.22958716750144958\n",
            "step 2677: generator_loss=1.5567448139190674, discriminator_loss=0.229539155960083\n",
            "step 2678: generator_loss=1.5915826559066772, discriminator_loss=0.22739921510219574\n",
            "step 2679: generator_loss=1.6123483180999756, discriminator_loss=0.22632426023483276\n",
            "step 2680: generator_loss=1.582815408706665, discriminator_loss=0.2300538420677185\n",
            "step 2681: generator_loss=1.5869780778884888, discriminator_loss=0.2302718162536621\n",
            "step 2682: generator_loss=1.5882306098937988, discriminator_loss=0.23071949183940887\n",
            "step 2683: generator_loss=1.6017600297927856, discriminator_loss=0.23091906309127808\n",
            "step 2684: generator_loss=1.5467958450317383, discriminator_loss=0.2373729944229126\n",
            "step 2685: generator_loss=1.5550975799560547, discriminator_loss=0.23722237348556519\n",
            "step 2686: generator_loss=1.5596492290496826, discriminator_loss=0.23720714449882507\n",
            "step 2687: generator_loss=1.5737054347991943, discriminator_loss=0.23624935746192932\n",
            "step 2688: generator_loss=1.5645415782928467, discriminator_loss=0.23789963126182556\n",
            "step 2689: generator_loss=1.5643296241760254, discriminator_loss=0.2386174499988556\n",
            "step 2690: generator_loss=1.5940864086151123, discriminator_loss=0.23581069707870483\n",
            "step 2691: generator_loss=1.6173653602600098, discriminator_loss=0.233305424451828\n",
            "step 2692: generator_loss=1.6622238159179688, discriminator_loss=0.22847653925418854\n",
            "step 2693: generator_loss=1.7210259437561035, discriminator_loss=0.22231903672218323\n",
            "step 2694: generator_loss=1.7645013332366943, discriminator_loss=0.21788884699344635\n",
            "step 2695: generator_loss=1.8071479797363281, discriminator_loss=0.21354907751083374\n",
            "step 2696: generator_loss=1.8549998998641968, discriminator_loss=0.20876622200012207\n",
            "step 2697: generator_loss=1.8990049362182617, discriminator_loss=0.20418071746826172\n",
            "step 2698: generator_loss=1.9157066345214844, discriminator_loss=0.20239412784576416\n",
            "step 2699: generator_loss=1.911086082458496, discriminator_loss=0.20227399468421936\n",
            "step 2700: generator_loss=1.8828601837158203, discriminator_loss=0.2041696310043335\n",
            "step 2701: generator_loss=1.852280616760254, discriminator_loss=0.2062196433544159\n",
            "step 2702: generator_loss=1.8104617595672607, discriminator_loss=0.20941013097763062\n",
            "step 2703: generator_loss=1.7699767351150513, discriminator_loss=0.21261534094810486\n",
            "step 2704: generator_loss=1.7243821620941162, discriminator_loss=0.21638816595077515\n",
            "step 2705: generator_loss=1.6778299808502197, discriminator_loss=0.2202509492635727\n",
            "step 2706: generator_loss=1.6432586908340454, discriminator_loss=0.22297050058841705\n",
            "step 2707: generator_loss=1.6291773319244385, discriminator_loss=0.2234622836112976\n",
            "step 2708: generator_loss=1.6130597591400146, discriminator_loss=0.22414222359657288\n",
            "step 2709: generator_loss=1.6028423309326172, discriminator_loss=0.22431230545043945\n",
            "step 2710: generator_loss=1.611017107963562, discriminator_loss=0.22237828373908997\n",
            "step 2711: generator_loss=1.6266775131225586, discriminator_loss=0.21987487375736237\n",
            "step 2712: generator_loss=1.6486847400665283, discriminator_loss=0.2166656106710434\n",
            "step 2713: generator_loss=1.6702512502670288, discriminator_loss=0.21351054310798645\n",
            "step 2714: generator_loss=1.6916602849960327, discriminator_loss=0.2107202410697937\n",
            "step 2715: generator_loss=1.743462085723877, discriminator_loss=0.2046302855014801\n",
            "step 2716: generator_loss=1.8155547380447388, discriminator_loss=0.19684448838233948\n",
            "step 2717: generator_loss=1.8910056352615356, discriminator_loss=0.18941333889961243\n",
            "step 2718: generator_loss=1.9532945156097412, discriminator_loss=0.18374869227409363\n",
            "step 2719: generator_loss=1.9995994567871094, discriminator_loss=0.1794302612543106\n",
            "step 2720: generator_loss=2.0245862007141113, discriminator_loss=0.17676782608032227\n",
            "step 2721: generator_loss=2.034203052520752, discriminator_loss=0.1752118617296219\n",
            "step 2722: generator_loss=2.024860382080078, discriminator_loss=0.17506030201911926\n",
            "step 2723: generator_loss=1.998057246208191, discriminator_loss=0.1762617528438568\n",
            "step 2724: generator_loss=1.9612184762954712, discriminator_loss=0.17844364047050476\n",
            "step 2725: generator_loss=1.9096989631652832, discriminator_loss=0.18197816610336304\n",
            "step 2726: generator_loss=1.8486050367355347, discriminator_loss=0.18669769167900085\n",
            "step 2727: generator_loss=1.7891582250595093, discriminator_loss=0.19174161553382874\n",
            "step 2728: generator_loss=1.7335071563720703, discriminator_loss=0.1970151960849762\n",
            "step 2729: generator_loss=1.6832208633422852, discriminator_loss=0.20208817720413208\n",
            "step 2730: generator_loss=1.6580142974853516, discriminator_loss=0.2047751545906067\n",
            "step 2731: generator_loss=1.6467057466506958, discriminator_loss=0.20590254664421082\n",
            "step 2732: generator_loss=1.6514647006988525, discriminator_loss=0.20513460040092468\n",
            "step 2733: generator_loss=1.6720993518829346, discriminator_loss=0.20276284217834473\n",
            "step 2734: generator_loss=1.691802978515625, discriminator_loss=0.2005334198474884\n",
            "step 2735: generator_loss=1.7170228958129883, discriminator_loss=0.19784536957740784\n",
            "step 2736: generator_loss=1.7621634006500244, discriminator_loss=0.19294531643390656\n",
            "step 2737: generator_loss=1.8103809356689453, discriminator_loss=0.1879938542842865\n",
            "step 2738: generator_loss=1.8568845987319946, discriminator_loss=0.183567076921463\n",
            "step 2739: generator_loss=1.8959805965423584, discriminator_loss=0.1799401044845581\n",
            "step 2740: generator_loss=1.9307730197906494, discriminator_loss=0.1770276427268982\n",
            "step 2741: generator_loss=1.950064778327942, discriminator_loss=0.17520837485790253\n",
            "step 2742: generator_loss=1.9574964046478271, discriminator_loss=0.17434433102607727\n",
            "step 2743: generator_loss=1.9733604192733765, discriminator_loss=0.17280061542987823\n",
            "step 2744: generator_loss=2.0041720867156982, discriminator_loss=0.1700441539287567\n",
            "step 2745: generator_loss=2.0321767330169678, discriminator_loss=0.1675102561712265\n",
            "step 2746: generator_loss=2.051705837249756, discriminator_loss=0.1656351089477539\n",
            "step 2747: generator_loss=2.063291072845459, discriminator_loss=0.16436031460762024\n",
            "step 2748: generator_loss=2.057711124420166, discriminator_loss=0.16425156593322754\n",
            "step 2749: generator_loss=2.0409276485443115, discriminator_loss=0.16505083441734314\n",
            "step 2750: generator_loss=2.0288214683532715, discriminator_loss=0.16542664170265198\n",
            "step 2751: generator_loss=2.016641139984131, discriminator_loss=0.16590186953544617\n",
            "step 2752: generator_loss=2.03153920173645, discriminator_loss=0.16421492397785187\n",
            "step 2753: generator_loss=2.050751209259033, discriminator_loss=0.16242024302482605\n",
            "step 2754: generator_loss=2.0577383041381836, discriminator_loss=0.1613132804632187\n",
            "step 2755: generator_loss=2.066333770751953, discriminator_loss=0.16021476686000824\n",
            "step 2756: generator_loss=2.083223342895508, discriminator_loss=0.158500075340271\n",
            "step 2757: generator_loss=2.0955007076263428, discriminator_loss=0.15712830424308777\n",
            "step 2758: generator_loss=2.0998804569244385, discriminator_loss=0.15650951862335205\n",
            "step 2759: generator_loss=2.0939998626708984, discriminator_loss=0.1564752608537674\n",
            "step 2760: generator_loss=2.066617488861084, discriminator_loss=0.1580735146999359\n",
            "step 2761: generator_loss=2.0523500442504883, discriminator_loss=0.15877360105514526\n",
            "step 2762: generator_loss=2.0438294410705566, discriminator_loss=0.1590903103351593\n",
            "step 2763: generator_loss=2.029423236846924, discriminator_loss=0.1600521355867386\n",
            "step 2764: generator_loss=2.0146889686584473, discriminator_loss=0.16109120845794678\n",
            "step 2765: generator_loss=1.9990276098251343, discriminator_loss=0.16255858540534973\n",
            "step 2766: generator_loss=2.035684585571289, discriminator_loss=0.15957671403884888\n",
            "step 2767: generator_loss=2.046319007873535, discriminator_loss=0.1588657647371292\n",
            "step 2768: generator_loss=2.049691677093506, discriminator_loss=0.15912432968616486\n",
            "step 2769: generator_loss=2.063383102416992, discriminator_loss=0.1588379293680191\n",
            "step 2770: generator_loss=2.073244094848633, discriminator_loss=0.15987065434455872\n",
            "step 2771: generator_loss=2.072457790374756, discriminator_loss=0.1625647395849228\n",
            "step 2772: generator_loss=2.0936594009399414, discriminator_loss=0.1642889678478241\n",
            "step 2773: generator_loss=2.0704212188720703, discriminator_loss=0.17144565284252167\n",
            "step 2774: generator_loss=2.054363489151001, discriminator_loss=0.17795664072036743\n",
            "step 2775: generator_loss=2.137723922729492, discriminator_loss=0.16924068331718445\n",
            "step 2776: generator_loss=2.1155805587768555, discriminator_loss=0.1765337437391281\n",
            "step 2777: generator_loss=2.1315953731536865, discriminator_loss=0.17697279155254364\n",
            "step 2778: generator_loss=2.1786420345306396, discriminator_loss=0.17991438508033752\n",
            "step 2779: generator_loss=2.1207141876220703, discriminator_loss=0.18942606449127197\n",
            "step 2780: generator_loss=2.221259593963623, discriminator_loss=0.1851443350315094\n",
            "step 2781: generator_loss=2.1630096435546875, discriminator_loss=0.19730469584465027\n",
            "step 2782: generator_loss=2.05191707611084, discriminator_loss=0.21152332425117493\n",
            "step 2783: generator_loss=2.2219276428222656, discriminator_loss=0.20076262950897217\n",
            "step 2784: generator_loss=2.2621750831604004, discriminator_loss=0.19590085744857788\n",
            "step 2785: generator_loss=2.2226924896240234, discriminator_loss=0.19692480564117432\n",
            "step 2786: generator_loss=2.309798240661621, discriminator_loss=0.18882517516613007\n",
            "step 2787: generator_loss=2.5298399925231934, discriminator_loss=0.16795706748962402\n",
            "step 2788: generator_loss=2.7076759338378906, discriminator_loss=0.15454626083374023\n",
            "step 2789: generator_loss=2.3666152954101562, discriminator_loss=0.17907071113586426\n",
            "step 2790: generator_loss=2.6988887786865234, discriminator_loss=0.15359213948249817\n",
            "step 2791: generator_loss=2.682319164276123, discriminator_loss=0.15157103538513184\n",
            "step 2792: generator_loss=2.9727587699890137, discriminator_loss=0.13777564465999603\n",
            "step 2793: generator_loss=2.6591336727142334, discriminator_loss=0.1490403711795807\n",
            "step 2794: generator_loss=2.8121845722198486, discriminator_loss=0.14140626788139343\n",
            "step 2795: generator_loss=2.901730537414551, discriminator_loss=0.13587512075901031\n",
            "step 2796: generator_loss=2.6914572715759277, discriminator_loss=0.14382490515708923\n",
            "step 2797: generator_loss=2.8500852584838867, discriminator_loss=0.1336607038974762\n",
            "step 2798: generator_loss=2.8577167987823486, discriminator_loss=0.13379380106925964\n",
            "step 2799: generator_loss=2.8380656242370605, discriminator_loss=0.1337025761604309\n",
            "step 2800: generator_loss=2.831190586090088, discriminator_loss=0.13215795159339905\n",
            "step 2801: generator_loss=2.893026828765869, discriminator_loss=0.12880751490592957\n",
            "step 2802: generator_loss=2.8627028465270996, discriminator_loss=0.12960153818130493\n",
            "step 2803: generator_loss=2.743411064147949, discriminator_loss=0.13236631453037262\n",
            "step 2804: generator_loss=2.768738269805908, discriminator_loss=0.13339032232761383\n",
            "step 2805: generator_loss=2.6190433502197266, discriminator_loss=0.13639068603515625\n",
            "step 2806: generator_loss=2.477330207824707, discriminator_loss=0.13925838470458984\n",
            "step 2807: generator_loss=2.3880224227905273, discriminator_loss=0.14325737953186035\n",
            "step 2808: generator_loss=2.3247287273406982, discriminator_loss=0.14839690923690796\n",
            "step 2809: generator_loss=2.2530245780944824, discriminator_loss=0.15074147284030914\n",
            "step 2810: generator_loss=2.1070666313171387, discriminator_loss=0.15799137949943542\n",
            "step 2811: generator_loss=2.1355459690093994, discriminator_loss=0.1621934324502945\n",
            "step 2812: generator_loss=2.0582218170166016, discriminator_loss=0.1655830442905426\n",
            "step 2813: generator_loss=1.9617266654968262, discriminator_loss=0.1691901832818985\n",
            "step 2814: generator_loss=1.8667138814926147, discriminator_loss=0.17763344943523407\n",
            "step 2815: generator_loss=1.8691987991333008, discriminator_loss=0.175266295671463\n",
            "step 2816: generator_loss=1.7451285123825073, discriminator_loss=0.18473432958126068\n",
            "step 2817: generator_loss=1.7950928211212158, discriminator_loss=0.1840941607952118\n",
            "step 2818: generator_loss=1.805528163909912, discriminator_loss=0.18236088752746582\n",
            "step 2819: generator_loss=1.8275471925735474, discriminator_loss=0.1843058466911316\n",
            "step 2820: generator_loss=1.8557324409484863, discriminator_loss=0.18328312039375305\n",
            "step 2821: generator_loss=1.7395145893096924, discriminator_loss=0.18875469267368317\n",
            "step 2822: generator_loss=1.786064624786377, discriminator_loss=0.186237633228302\n",
            "step 2823: generator_loss=1.8907921314239502, discriminator_loss=0.17996472120285034\n",
            "step 2824: generator_loss=1.857893705368042, discriminator_loss=0.181467205286026\n",
            "step 2825: generator_loss=1.8120524883270264, discriminator_loss=0.1842685043811798\n",
            "step 2826: generator_loss=1.8350967168807983, discriminator_loss=0.18486499786376953\n",
            "step 2827: generator_loss=1.796332597732544, discriminator_loss=0.18595537543296814\n",
            "step 2828: generator_loss=1.8389148712158203, discriminator_loss=0.1854179948568344\n",
            "step 2829: generator_loss=1.8143928050994873, discriminator_loss=0.18698439002037048\n",
            "step 2830: generator_loss=1.7567901611328125, discriminator_loss=0.19040809571743011\n",
            "step 2831: generator_loss=1.7747700214385986, discriminator_loss=0.1899431347846985\n",
            "step 2832: generator_loss=1.7949284315109253, discriminator_loss=0.1885402798652649\n",
            "step 2833: generator_loss=1.7932703495025635, discriminator_loss=0.1867268979549408\n",
            "step 2834: generator_loss=1.7771966457366943, discriminator_loss=0.18738584220409393\n",
            "step 2835: generator_loss=1.8188250064849854, discriminator_loss=0.1835097372531891\n",
            "step 2836: generator_loss=1.8171041011810303, discriminator_loss=0.1833377480506897\n",
            "step 2837: generator_loss=1.8329288959503174, discriminator_loss=0.18190008401870728\n",
            "step 2838: generator_loss=1.852516770362854, discriminator_loss=0.18052349984645844\n",
            "step 2839: generator_loss=1.8408421277999878, discriminator_loss=0.18146473169326782\n",
            "step 2840: generator_loss=1.8439009189605713, discriminator_loss=0.18134205043315887\n",
            "step 2841: generator_loss=1.862716794013977, discriminator_loss=0.17986954748630524\n",
            "step 2842: generator_loss=1.862442135810852, discriminator_loss=0.17982281744480133\n",
            "step 2843: generator_loss=1.8493469953536987, discriminator_loss=0.1814655065536499\n",
            "step 2844: generator_loss=1.8303565979003906, discriminator_loss=0.18353106081485748\n",
            "step 2845: generator_loss=1.83562433719635, discriminator_loss=0.18329685926437378\n",
            "step 2846: generator_loss=1.8549726009368896, discriminator_loss=0.18232445418834686\n",
            "step 2847: generator_loss=1.8561726808547974, discriminator_loss=0.18193209171295166\n",
            "step 2848: generator_loss=1.8520030975341797, discriminator_loss=0.18135255575180054\n",
            "step 2849: generator_loss=1.9022293090820312, discriminator_loss=0.17666305601596832\n",
            "step 2850: generator_loss=1.957397699356079, discriminator_loss=0.17192429304122925\n",
            "step 2851: generator_loss=1.9802948236465454, discriminator_loss=0.16973356902599335\n",
            "step 2852: generator_loss=1.996532678604126, discriminator_loss=0.16818666458129883\n",
            "step 2853: generator_loss=2.0287399291992188, discriminator_loss=0.16556225717067719\n",
            "step 2854: generator_loss=2.0607070922851562, discriminator_loss=0.16294339299201965\n",
            "step 2855: generator_loss=2.0951855182647705, discriminator_loss=0.1599717140197754\n",
            "step 2856: generator_loss=2.10685658454895, discriminator_loss=0.1585586816072464\n",
            "step 2857: generator_loss=2.097177505493164, discriminator_loss=0.15874189138412476\n",
            "step 2858: generator_loss=2.0748090744018555, discriminator_loss=0.15954458713531494\n",
            "step 2859: generator_loss=2.0537126064300537, discriminator_loss=0.16070836782455444\n",
            "step 2860: generator_loss=2.04414439201355, discriminator_loss=0.16081607341766357\n",
            "step 2861: generator_loss=2.03419828414917, discriminator_loss=0.16087166965007782\n",
            "step 2862: generator_loss=2.0242509841918945, discriminator_loss=0.16107523441314697\n",
            "step 2863: generator_loss=2.008699417114258, discriminator_loss=0.16159850358963013\n",
            "step 2864: generator_loss=2.0127573013305664, discriminator_loss=0.16070525348186493\n",
            "step 2865: generator_loss=2.018454074859619, discriminator_loss=0.15965545177459717\n",
            "step 2866: generator_loss=2.0260562896728516, discriminator_loss=0.15843284130096436\n",
            "step 2867: generator_loss=2.0412282943725586, discriminator_loss=0.1566295623779297\n",
            "step 2868: generator_loss=2.059084415435791, discriminator_loss=0.15476056933403015\n",
            "step 2869: generator_loss=2.0676960945129395, discriminator_loss=0.1535009741783142\n",
            "step 2870: generator_loss=2.0693602561950684, discriminator_loss=0.15313471853733063\n",
            "step 2871: generator_loss=2.0731253623962402, discriminator_loss=0.15251260995864868\n",
            "step 2872: generator_loss=2.091196060180664, discriminator_loss=0.15101942420005798\n",
            "step 2873: generator_loss=2.1140246391296387, discriminator_loss=0.14927522838115692\n",
            "step 2874: generator_loss=2.1404824256896973, discriminator_loss=0.14735147356987\n",
            "step 2875: generator_loss=2.176939010620117, discriminator_loss=0.1447363793849945\n",
            "step 2876: generator_loss=2.2056479454040527, discriminator_loss=0.14265164732933044\n",
            "step 2877: generator_loss=2.2213234901428223, discriminator_loss=0.14159345626831055\n",
            "step 2878: generator_loss=2.2185211181640625, discriminator_loss=0.14159539341926575\n",
            "step 2879: generator_loss=2.1922855377197266, discriminator_loss=0.14285175502300262\n",
            "step 2880: generator_loss=2.1671581268310547, discriminator_loss=0.14416638016700745\n",
            "step 2881: generator_loss=2.1454110145568848, discriminator_loss=0.14530298113822937\n",
            "step 2882: generator_loss=2.1228785514831543, discriminator_loss=0.14645570516586304\n",
            "step 2883: generator_loss=2.1060235500335693, discriminator_loss=0.14734399318695068\n",
            "step 2884: generator_loss=2.097931146621704, discriminator_loss=0.14766880869865417\n",
            "step 2885: generator_loss=2.086181402206421, discriminator_loss=0.14815816283226013\n",
            "step 2886: generator_loss=2.073033332824707, discriminator_loss=0.14898061752319336\n",
            "step 2887: generator_loss=2.059230327606201, discriminator_loss=0.14981096982955933\n",
            "step 2888: generator_loss=2.0500032901763916, discriminator_loss=0.15033534169197083\n",
            "step 2889: generator_loss=2.0393402576446533, discriminator_loss=0.15102994441986084\n",
            "step 2890: generator_loss=2.0235650539398193, discriminator_loss=0.15209710597991943\n",
            "step 2891: generator_loss=2.0006606578826904, discriminator_loss=0.15379184484481812\n",
            "step 2892: generator_loss=1.9742728471755981, discriminator_loss=0.15585988759994507\n",
            "step 2893: generator_loss=1.9497207403182983, discriminator_loss=0.1578361839056015\n",
            "step 2894: generator_loss=1.922044277191162, discriminator_loss=0.16019776463508606\n",
            "step 2895: generator_loss=1.9025533199310303, discriminator_loss=0.16198331117630005\n",
            "step 2896: generator_loss=1.8814629316329956, discriminator_loss=0.16402873396873474\n",
            "step 2897: generator_loss=1.8664402961730957, discriminator_loss=0.16562891006469727\n",
            "step 2898: generator_loss=1.873410701751709, discriminator_loss=0.16524451971054077\n",
            "step 2899: generator_loss=1.8936989307403564, discriminator_loss=0.16361352801322937\n",
            "step 2900: generator_loss=1.9368882179260254, discriminator_loss=0.15997663140296936\n",
            "step 2901: generator_loss=1.9853242635726929, discriminator_loss=0.15614007413387299\n",
            "step 2902: generator_loss=2.022909164428711, discriminator_loss=0.15328660607337952\n",
            "step 2903: generator_loss=2.0656354427337646, discriminator_loss=0.15013271570205688\n",
            "step 2904: generator_loss=2.115111827850342, discriminator_loss=0.14658531546592712\n",
            "step 2905: generator_loss=2.152712821960449, discriminator_loss=0.14396971464157104\n",
            "step 2906: generator_loss=2.1667120456695557, discriminator_loss=0.14292633533477783\n",
            "step 2907: generator_loss=2.1662468910217285, discriminator_loss=0.1427374929189682\n",
            "step 2908: generator_loss=2.1568429470062256, discriminator_loss=0.14300279319286346\n",
            "step 2909: generator_loss=2.1395819187164307, discriminator_loss=0.14393064379692078\n",
            "step 2910: generator_loss=2.116636276245117, discriminator_loss=0.14515116810798645\n",
            "step 2911: generator_loss=2.087611198425293, discriminator_loss=0.14682060480117798\n",
            "step 2912: generator_loss=2.0564656257629395, discriminator_loss=0.14877328276634216\n",
            "step 2913: generator_loss=2.020996570587158, discriminator_loss=0.1512400209903717\n",
            "step 2914: generator_loss=1.9791769981384277, discriminator_loss=0.15425479412078857\n",
            "step 2915: generator_loss=1.940152645111084, discriminator_loss=0.15735256671905518\n",
            "step 2916: generator_loss=1.9223822355270386, discriminator_loss=0.1587747037410736\n",
            "step 2917: generator_loss=1.9186091423034668, discriminator_loss=0.15906664729118347\n",
            "step 2918: generator_loss=1.926548719406128, discriminator_loss=0.15840351581573486\n",
            "step 2919: generator_loss=1.9635682106018066, discriminator_loss=0.1552533209323883\n",
            "step 2920: generator_loss=2.008794069290161, discriminator_loss=0.15178850293159485\n",
            "step 2921: generator_loss=2.054872512817383, discriminator_loss=0.14845438301563263\n",
            "step 2922: generator_loss=2.092975616455078, discriminator_loss=0.14573830366134644\n",
            "step 2923: generator_loss=2.1172990798950195, discriminator_loss=0.14424782991409302\n",
            "step 2924: generator_loss=2.141599178314209, discriminator_loss=0.14262188971042633\n",
            "step 2925: generator_loss=2.1542532444000244, discriminator_loss=0.14189225435256958\n",
            "step 2926: generator_loss=2.1591763496398926, discriminator_loss=0.14151152968406677\n",
            "step 2927: generator_loss=2.1590826511383057, discriminator_loss=0.14142325520515442\n",
            "step 2928: generator_loss=2.1640114784240723, discriminator_loss=0.1410035490989685\n",
            "step 2929: generator_loss=2.1693742275238037, discriminator_loss=0.14044831693172455\n",
            "step 2930: generator_loss=2.172694444656372, discriminator_loss=0.14001011848449707\n",
            "step 2931: generator_loss=2.175049304962158, discriminator_loss=0.13958606123924255\n",
            "step 2932: generator_loss=2.1759133338928223, discriminator_loss=0.13921304047107697\n",
            "step 2933: generator_loss=2.1644840240478516, discriminator_loss=0.1396322399377823\n",
            "step 2934: generator_loss=2.1633002758026123, discriminator_loss=0.1395300328731537\n",
            "step 2935: generator_loss=2.1608681678771973, discriminator_loss=0.13948430120944977\n",
            "step 2936: generator_loss=2.1766374111175537, discriminator_loss=0.13821130990982056\n",
            "step 2937: generator_loss=2.1981234550476074, discriminator_loss=0.13670268654823303\n",
            "step 2938: generator_loss=2.2162656784057617, discriminator_loss=0.1354626715183258\n",
            "step 2939: generator_loss=2.2252397537231445, discriminator_loss=0.1346905678510666\n",
            "step 2940: generator_loss=2.2424275875091553, discriminator_loss=0.1334344744682312\n",
            "step 2941: generator_loss=2.2534425258636475, discriminator_loss=0.13272500038146973\n",
            "step 2942: generator_loss=2.2498676776885986, discriminator_loss=0.13273224234580994\n",
            "step 2943: generator_loss=2.232090473175049, discriminator_loss=0.13370192050933838\n",
            "step 2944: generator_loss=2.2095072269439697, discriminator_loss=0.13515418767929077\n",
            "step 2945: generator_loss=2.176875591278076, discriminator_loss=0.1372261941432953\n",
            "step 2946: generator_loss=2.140324831008911, discriminator_loss=0.1397046446800232\n",
            "step 2947: generator_loss=2.109821081161499, discriminator_loss=0.1418635994195938\n",
            "step 2948: generator_loss=2.0905094146728516, discriminator_loss=0.14324599504470825\n",
            "step 2949: generator_loss=2.068964958190918, discriminator_loss=0.1448473334312439\n",
            "step 2950: generator_loss=2.0512759685516357, discriminator_loss=0.14624741673469543\n",
            "step 2951: generator_loss=2.0343546867370605, discriminator_loss=0.147609680891037\n",
            "step 2952: generator_loss=2.0504915714263916, discriminator_loss=0.14671045541763306\n",
            "step 2953: generator_loss=2.0818710327148438, discriminator_loss=0.14464625716209412\n",
            "step 2954: generator_loss=2.134378433227539, discriminator_loss=0.14116071164608002\n",
            "step 2955: generator_loss=2.1787309646606445, discriminator_loss=0.1385515332221985\n",
            "step 2956: generator_loss=2.211778163909912, discriminator_loss=0.1365603357553482\n",
            "step 2957: generator_loss=2.2314672470092773, discriminator_loss=0.13547274470329285\n",
            "step 2958: generator_loss=2.2712440490722656, discriminator_loss=0.13308613002300262\n",
            "step 2959: generator_loss=2.296787977218628, discriminator_loss=0.1315493881702423\n",
            "step 2960: generator_loss=2.3121747970581055, discriminator_loss=0.13043484091758728\n",
            "step 2961: generator_loss=2.331305742263794, discriminator_loss=0.12902627885341644\n",
            "step 2962: generator_loss=2.340219020843506, discriminator_loss=0.12809737026691437\n",
            "step 2963: generator_loss=2.331605911254883, discriminator_loss=0.1280122995376587\n",
            "step 2964: generator_loss=2.3075380325317383, discriminator_loss=0.1289103776216507\n",
            "step 2965: generator_loss=2.269516944885254, discriminator_loss=0.1307247132062912\n",
            "step 2966: generator_loss=2.219432830810547, discriminator_loss=0.13334232568740845\n",
            "step 2967: generator_loss=2.1698524951934814, discriminator_loss=0.136216938495636\n",
            "step 2968: generator_loss=2.1304049491882324, discriminator_loss=0.138605996966362\n",
            "step 2969: generator_loss=2.1052701473236084, discriminator_loss=0.14019179344177246\n",
            "step 2970: generator_loss=2.077401638031006, discriminator_loss=0.14204341173171997\n",
            "step 2971: generator_loss=2.057520627975464, discriminator_loss=0.14336836338043213\n",
            "step 2972: generator_loss=2.0521321296691895, discriminator_loss=0.14372298121452332\n",
            "step 2973: generator_loss=2.042393445968628, discriminator_loss=0.14438334107398987\n",
            "step 2974: generator_loss=2.026658058166504, discriminator_loss=0.14552269876003265\n",
            "step 2975: generator_loss=2.006103277206421, discriminator_loss=0.14706462621688843\n",
            "step 2976: generator_loss=1.985557198524475, discriminator_loss=0.14871282875537872\n",
            "step 2977: generator_loss=2.0029897689819336, discriminator_loss=0.14743824303150177\n",
            "step 2978: generator_loss=2.0227556228637695, discriminator_loss=0.14595571160316467\n",
            "step 2979: generator_loss=2.0360589027404785, discriminator_loss=0.1450951099395752\n",
            "step 2980: generator_loss=2.0407228469848633, discriminator_loss=0.14494222402572632\n",
            "step 2981: generator_loss=2.044830560684204, discriminator_loss=0.14473748207092285\n",
            "step 2982: generator_loss=2.0366768836975098, discriminator_loss=0.1456415057182312\n",
            "step 2983: generator_loss=2.018404483795166, discriminator_loss=0.14721107482910156\n",
            "step 2984: generator_loss=1.9978018999099731, discriminator_loss=0.14911052584648132\n",
            "step 2985: generator_loss=1.9854334592819214, discriminator_loss=0.15034882724285126\n",
            "step 2986: generator_loss=1.9796710014343262, discriminator_loss=0.15103618800640106\n",
            "step 2987: generator_loss=1.9836435317993164, discriminator_loss=0.1510169804096222\n",
            "step 2988: generator_loss=2.0001444816589355, discriminator_loss=0.14997604489326477\n",
            "step 2989: generator_loss=2.0171751976013184, discriminator_loss=0.1490471363067627\n",
            "step 2990: generator_loss=2.034837007522583, discriminator_loss=0.1480782926082611\n",
            "step 2991: generator_loss=2.040493965148926, discriminator_loss=0.14814728498458862\n",
            "step 2992: generator_loss=2.0423460006713867, discriminator_loss=0.14819592237472534\n",
            "step 2993: generator_loss=2.0368309020996094, discriminator_loss=0.148810476064682\n",
            "step 2994: generator_loss=2.0211892127990723, discriminator_loss=0.15021564066410065\n",
            "step 2995: generator_loss=2.027322769165039, discriminator_loss=0.14991672337055206\n",
            "step 2996: generator_loss=2.0399389266967773, discriminator_loss=0.14914824068546295\n",
            "step 2997: generator_loss=2.06288480758667, discriminator_loss=0.14757147431373596\n",
            "step 2998: generator_loss=2.081789016723633, discriminator_loss=0.14624060690402985\n",
            "step 2999: generator_loss=2.0945279598236084, discriminator_loss=0.1452779769897461\n",
            "step 3000: generator_loss=2.087733745574951, discriminator_loss=0.14572805166244507\n",
            "step 3001: generator_loss=2.069744110107422, discriminator_loss=0.14672720432281494\n",
            "step 3002: generator_loss=2.04764986038208, discriminator_loss=0.14812670648097992\n",
            "step 3003: generator_loss=2.0192809104919434, discriminator_loss=0.14997249841690063\n",
            "step 3004: generator_loss=1.9795022010803223, discriminator_loss=0.15299610793590546\n",
            "step 3005: generator_loss=1.9684133529663086, discriminator_loss=0.15388858318328857\n",
            "step 3006: generator_loss=1.968158483505249, discriminator_loss=0.15388181805610657\n",
            "step 3007: generator_loss=1.972113847732544, discriminator_loss=0.15349222719669342\n",
            "step 3008: generator_loss=1.98020339012146, discriminator_loss=0.15275584161281586\n",
            "step 3009: generator_loss=2.0034327507019043, discriminator_loss=0.1510540246963501\n",
            "step 3010: generator_loss=2.055776596069336, discriminator_loss=0.14711619913578033\n",
            "step 3011: generator_loss=2.1180531978607178, discriminator_loss=0.14269563555717468\n",
            "step 3012: generator_loss=2.166205406188965, discriminator_loss=0.13946616649627686\n",
            "step 3013: generator_loss=2.1969399452209473, discriminator_loss=0.1374102085828781\n",
            "step 3014: generator_loss=2.2182908058166504, discriminator_loss=0.13606488704681396\n",
            "step 3015: generator_loss=2.2441115379333496, discriminator_loss=0.13417498767375946\n",
            "step 3016: generator_loss=2.2934601306915283, discriminator_loss=0.1311212033033371\n",
            "step 3017: generator_loss=2.3341619968414307, discriminator_loss=0.1285940557718277\n",
            "step 3018: generator_loss=2.3547685146331787, discriminator_loss=0.1270047277212143\n",
            "step 3019: generator_loss=2.3545851707458496, discriminator_loss=0.12663760781288147\n",
            "step 3020: generator_loss=2.360741376876831, discriminator_loss=0.12576957046985626\n",
            "step 3021: generator_loss=2.348768472671509, discriminator_loss=0.12580657005310059\n",
            "step 3022: generator_loss=2.3550400733947754, discriminator_loss=0.12489110231399536\n",
            "step 3023: generator_loss=2.366468906402588, discriminator_loss=0.12375620752573013\n",
            "step 3024: generator_loss=2.3603081703186035, discriminator_loss=0.12340810894966125\n",
            "step 3025: generator_loss=2.3463759422302246, discriminator_loss=0.12349751591682434\n",
            "step 3026: generator_loss=2.3288393020629883, discriminator_loss=0.1239517405629158\n",
            "step 3027: generator_loss=2.2989084720611572, discriminator_loss=0.1250588297843933\n",
            "step 3028: generator_loss=2.270991325378418, discriminator_loss=0.126126229763031\n",
            "step 3029: generator_loss=2.2333717346191406, discriminator_loss=0.12795321643352509\n",
            "step 3030: generator_loss=2.2065484523773193, discriminator_loss=0.1292305439710617\n",
            "step 3031: generator_loss=2.1867918968200684, discriminator_loss=0.13016119599342346\n",
            "step 3032: generator_loss=2.1614537239074707, discriminator_loss=0.1314694583415985\n",
            "step 3033: generator_loss=2.12626051902771, discriminator_loss=0.13357505202293396\n",
            "step 3034: generator_loss=2.085041046142578, discriminator_loss=0.1362568885087967\n",
            "step 3035: generator_loss=2.041522979736328, discriminator_loss=0.1392851173877716\n",
            "step 3036: generator_loss=2.011573076248169, discriminator_loss=0.14139941334724426\n",
            "step 3037: generator_loss=1.9785995483398438, discriminator_loss=0.14399737119674683\n",
            "step 3038: generator_loss=1.9516575336456299, discriminator_loss=0.14627504348754883\n",
            "step 3039: generator_loss=1.9267361164093018, discriminator_loss=0.1484813094139099\n",
            "step 3040: generator_loss=1.9188652038574219, discriminator_loss=0.14941835403442383\n",
            "step 3041: generator_loss=1.9247221946716309, discriminator_loss=0.149268239736557\n",
            "step 3042: generator_loss=1.9395813941955566, discriminator_loss=0.14847546815872192\n",
            "step 3043: generator_loss=1.9590203762054443, discriminator_loss=0.1474299430847168\n",
            "step 3044: generator_loss=1.9710915088653564, discriminator_loss=0.14692528545856476\n",
            "step 3045: generator_loss=1.9787676334381104, discriminator_loss=0.1470368206501007\n",
            "step 3046: generator_loss=1.9851733446121216, discriminator_loss=0.14720359444618225\n",
            "step 3047: generator_loss=2.025599718093872, discriminator_loss=0.14468087255954742\n",
            "step 3048: generator_loss=2.102052688598633, discriminator_loss=0.13970613479614258\n",
            "step 3049: generator_loss=2.1779026985168457, discriminator_loss=0.13509172201156616\n",
            "step 3050: generator_loss=2.2361416816711426, discriminator_loss=0.13179096579551697\n",
            "step 3051: generator_loss=2.279475450515747, discriminator_loss=0.12940062582492828\n",
            "step 3052: generator_loss=2.303614616394043, discriminator_loss=0.12818071246147156\n",
            "step 3053: generator_loss=2.3063058853149414, discriminator_loss=0.12801812589168549\n",
            "step 3054: generator_loss=2.3084352016448975, discriminator_loss=0.12771964073181152\n",
            "step 3055: generator_loss=2.305039405822754, discriminator_loss=0.12771938741207123\n",
            "step 3056: generator_loss=2.3041176795959473, discriminator_loss=0.12725159525871277\n",
            "step 3057: generator_loss=2.317509174346924, discriminator_loss=0.12615275382995605\n",
            "step 3058: generator_loss=2.3249974250793457, discriminator_loss=0.12528793513774872\n",
            "step 3059: generator_loss=2.3409388065338135, discriminator_loss=0.12400652468204498\n",
            "step 3060: generator_loss=2.3453352451324463, discriminator_loss=0.123445063829422\n",
            "step 3061: generator_loss=2.3432559967041016, discriminator_loss=0.12306123226881027\n",
            "step 3062: generator_loss=2.3238959312438965, discriminator_loss=0.12382388859987259\n",
            "step 3063: generator_loss=2.317781448364258, discriminator_loss=0.12377506494522095\n",
            "step 3064: generator_loss=2.3007235527038574, discriminator_loss=0.1243874579668045\n",
            "step 3065: generator_loss=2.2802860736846924, discriminator_loss=0.125130295753479\n",
            "step 3066: generator_loss=2.299891233444214, discriminator_loss=0.12369529902935028\n",
            "step 3067: generator_loss=2.3349742889404297, discriminator_loss=0.12147478759288788\n",
            "step 3068: generator_loss=2.369879961013794, discriminator_loss=0.11935320496559143\n",
            "step 3069: generator_loss=2.403911828994751, discriminator_loss=0.11740364134311676\n",
            "step 3070: generator_loss=2.423602819442749, discriminator_loss=0.11616809666156769\n",
            "step 3071: generator_loss=2.437324047088623, discriminator_loss=0.11525609344244003\n",
            "step 3072: generator_loss=2.446126937866211, discriminator_loss=0.11466936767101288\n",
            "step 3073: generator_loss=2.4513752460479736, discriminator_loss=0.11422279477119446\n",
            "step 3074: generator_loss=2.4773831367492676, discriminator_loss=0.11279461532831192\n",
            "step 3075: generator_loss=2.5100598335266113, discriminator_loss=0.11100146919488907\n",
            "step 3076: generator_loss=2.5280494689941406, discriminator_loss=0.10994505882263184\n",
            "step 3077: generator_loss=2.5253942012786865, discriminator_loss=0.10981228947639465\n",
            "step 3078: generator_loss=2.522012710571289, discriminator_loss=0.10956158488988876\n",
            "step 3079: generator_loss=2.516655921936035, discriminator_loss=0.10935837775468826\n",
            "step 3080: generator_loss=2.534958600997925, discriminator_loss=0.10811007022857666\n",
            "step 3081: generator_loss=2.555737018585205, discriminator_loss=0.106877900660038\n",
            "step 3082: generator_loss=2.5600576400756836, discriminator_loss=0.10625305771827698\n",
            "step 3083: generator_loss=2.555349111557007, discriminator_loss=0.10606750845909119\n",
            "step 3084: generator_loss=2.538114547729492, discriminator_loss=0.10646864026784897\n",
            "step 3085: generator_loss=2.509035110473633, discriminator_loss=0.10731607675552368\n",
            "step 3086: generator_loss=2.4765663146972656, discriminator_loss=0.10842718929052353\n",
            "step 3087: generator_loss=2.433518409729004, discriminator_loss=0.1101207509636879\n",
            "step 3088: generator_loss=2.383357524871826, discriminator_loss=0.11239594966173172\n",
            "step 3089: generator_loss=2.3352012634277344, discriminator_loss=0.11471632122993469\n",
            "step 3090: generator_loss=2.3059463500976562, discriminator_loss=0.11621053516864777\n",
            "step 3091: generator_loss=2.2954459190368652, discriminator_loss=0.11678136140108109\n",
            "step 3092: generator_loss=2.3291141986846924, discriminator_loss=0.11498357355594635\n",
            "step 3093: generator_loss=2.3781025409698486, discriminator_loss=0.11253973841667175\n",
            "step 3094: generator_loss=2.4399874210357666, discriminator_loss=0.10962021350860596\n",
            "step 3095: generator_loss=2.502056360244751, discriminator_loss=0.10678297281265259\n",
            "step 3096: generator_loss=2.548736095428467, discriminator_loss=0.10485365986824036\n",
            "step 3097: generator_loss=2.593686580657959, discriminator_loss=0.10299713164567947\n",
            "step 3098: generator_loss=2.6383185386657715, discriminator_loss=0.10110202431678772\n",
            "step 3099: generator_loss=2.6835885047912598, discriminator_loss=0.09936945140361786\n",
            "step 3100: generator_loss=2.6985230445861816, discriminator_loss=0.09866209328174591\n",
            "step 3101: generator_loss=2.6867053508758545, discriminator_loss=0.09881439805030823\n",
            "step 3102: generator_loss=2.678104877471924, discriminator_loss=0.09890146553516388\n",
            "step 3103: generator_loss=2.6800758838653564, discriminator_loss=0.09855661541223526\n",
            "step 3104: generator_loss=2.702545404434204, discriminator_loss=0.09737543761730194\n",
            "step 3105: generator_loss=2.7088286876678467, discriminator_loss=0.09690698236227036\n",
            "step 3106: generator_loss=2.716045618057251, discriminator_loss=0.09625788033008575\n",
            "step 3107: generator_loss=2.713775157928467, discriminator_loss=0.09607252478599548\n",
            "step 3108: generator_loss=2.6951141357421875, discriminator_loss=0.09646598994731903\n",
            "step 3109: generator_loss=2.6634843349456787, discriminator_loss=0.09735685586929321\n",
            "step 3110: generator_loss=2.640827178955078, discriminator_loss=0.09792610257863998\n",
            "step 3111: generator_loss=2.6283111572265625, discriminator_loss=0.09816049039363861\n",
            "step 3112: generator_loss=2.623002052307129, discriminator_loss=0.09814095497131348\n",
            "step 3113: generator_loss=2.634127140045166, discriminator_loss=0.09745605289936066\n",
            "step 3114: generator_loss=2.664809226989746, discriminator_loss=0.09603135287761688\n",
            "step 3115: generator_loss=2.691133975982666, discriminator_loss=0.09486795961856842\n",
            "step 3116: generator_loss=2.7026522159576416, discriminator_loss=0.09432616829872131\n",
            "step 3117: generator_loss=2.696981906890869, discriminator_loss=0.09438972920179367\n",
            "step 3118: generator_loss=2.6746926307678223, discriminator_loss=0.09511368721723557\n",
            "step 3119: generator_loss=2.659832239151001, discriminator_loss=0.09552637487649918\n",
            "step 3120: generator_loss=2.651209831237793, discriminator_loss=0.09582532942295074\n",
            "step 3121: generator_loss=2.624837875366211, discriminator_loss=0.0966687723994255\n",
            "step 3122: generator_loss=2.5831797122955322, discriminator_loss=0.09821861237287521\n",
            "step 3123: generator_loss=2.5438718795776367, discriminator_loss=0.09980300068855286\n",
            "step 3124: generator_loss=2.5253348350524902, discriminator_loss=0.10049480944871902\n",
            "step 3125: generator_loss=2.5058350563049316, discriminator_loss=0.10124185681343079\n",
            "step 3126: generator_loss=2.4951729774475098, discriminator_loss=0.10168422758579254\n",
            "step 3127: generator_loss=2.492567777633667, discriminator_loss=0.10168334096670151\n",
            "step 3128: generator_loss=2.497408866882324, discriminator_loss=0.10143312811851501\n",
            "step 3129: generator_loss=2.4998388290405273, discriminator_loss=0.10122086107730865\n",
            "step 3130: generator_loss=2.5099902153015137, discriminator_loss=0.10070423781871796\n",
            "step 3131: generator_loss=2.5331437587738037, discriminator_loss=0.09953445196151733\n",
            "step 3132: generator_loss=2.551058769226074, discriminator_loss=0.09866447746753693\n",
            "step 3133: generator_loss=2.5679516792297363, discriminator_loss=0.09780966490507126\n",
            "step 3134: generator_loss=2.581486701965332, discriminator_loss=0.09710130840539932\n",
            "step 3135: generator_loss=2.5891575813293457, discriminator_loss=0.09651944786310196\n",
            "step 3136: generator_loss=2.581936836242676, discriminator_loss=0.09658773988485336\n",
            "step 3137: generator_loss=2.561134099960327, discriminator_loss=0.09724032133817673\n",
            "step 3138: generator_loss=2.535254955291748, discriminator_loss=0.09810996055603027\n",
            "step 3139: generator_loss=2.5021796226501465, discriminator_loss=0.09941299259662628\n",
            "step 3140: generator_loss=2.4628586769104004, discriminator_loss=0.10100185871124268\n",
            "step 3141: generator_loss=2.432425022125244, discriminator_loss=0.10226427018642426\n",
            "step 3142: generator_loss=2.399867534637451, discriminator_loss=0.10374114662408829\n",
            "step 3143: generator_loss=2.3772315979003906, discriminator_loss=0.10477888584136963\n",
            "step 3144: generator_loss=2.3711581230163574, discriminator_loss=0.10507450252771378\n",
            "step 3145: generator_loss=2.3747975826263428, discriminator_loss=0.10480044782161713\n",
            "step 3146: generator_loss=2.378772258758545, discriminator_loss=0.10459621250629425\n",
            "step 3147: generator_loss=2.3810930252075195, discriminator_loss=0.10449616611003876\n",
            "step 3148: generator_loss=2.3854215145111084, discriminator_loss=0.10446412861347198\n",
            "step 3149: generator_loss=2.394207000732422, discriminator_loss=0.10466539114713669\n",
            "step 3150: generator_loss=2.4046897888183594, discriminator_loss=0.1042003259062767\n",
            "step 3151: generator_loss=2.378655433654785, discriminator_loss=0.10644786059856415\n",
            "step 3152: generator_loss=2.2843198776245117, discriminator_loss=0.11361587047576904\n",
            "step 3153: generator_loss=2.2401318550109863, discriminator_loss=0.11802853643894196\n",
            "step 3154: generator_loss=2.1657826900482178, discriminator_loss=0.12734836339950562\n",
            "step 3155: generator_loss=2.072031021118164, discriminator_loss=0.13874593377113342\n",
            "step 3156: generator_loss=2.056164026260376, discriminator_loss=0.14786039292812347\n",
            "step 3157: generator_loss=1.9001202583312988, discriminator_loss=0.18210861086845398\n",
            "step 3158: generator_loss=1.8975260257720947, discriminator_loss=0.1824427843093872\n",
            "step 3159: generator_loss=1.8079986572265625, discriminator_loss=0.21106195449829102\n",
            "step 3160: generator_loss=2.0750608444213867, discriminator_loss=0.18598875403404236\n",
            "step 3161: generator_loss=1.880730390548706, discriminator_loss=0.21614617109298706\n",
            "step 3162: generator_loss=1.5759954452514648, discriminator_loss=0.27607664465904236\n",
            "step 3163: generator_loss=1.9755750894546509, discriminator_loss=0.21776065230369568\n",
            "step 3164: generator_loss=2.068448305130005, discriminator_loss=0.20299267768859863\n",
            "step 3165: generator_loss=2.030646562576294, discriminator_loss=0.1893339455127716\n",
            "step 3166: generator_loss=2.276658535003662, discriminator_loss=0.17059090733528137\n",
            "step 3167: generator_loss=2.342862129211426, discriminator_loss=0.1587958186864853\n",
            "step 3168: generator_loss=2.187746524810791, discriminator_loss=0.15081748366355896\n",
            "step 3169: generator_loss=2.522578001022339, discriminator_loss=0.13774356245994568\n",
            "step 3170: generator_loss=2.652521848678589, discriminator_loss=0.12940192222595215\n",
            "step 3171: generator_loss=2.9971351623535156, discriminator_loss=0.11940205097198486\n",
            "step 3172: generator_loss=2.794541835784912, discriminator_loss=0.12495572865009308\n",
            "step 3173: generator_loss=2.6223607063293457, discriminator_loss=0.12436458468437195\n",
            "step 3174: generator_loss=3.088531970977783, discriminator_loss=0.11613182723522186\n",
            "step 3175: generator_loss=2.5102078914642334, discriminator_loss=0.12940409779548645\n",
            "step 3176: generator_loss=2.8270974159240723, discriminator_loss=0.11975326389074326\n",
            "step 3177: generator_loss=2.7081809043884277, discriminator_loss=0.12498639523983002\n",
            "step 3178: generator_loss=2.790454864501953, discriminator_loss=0.12201572954654694\n",
            "step 3179: generator_loss=2.6677818298339844, discriminator_loss=0.12065008282661438\n",
            "step 3180: generator_loss=2.65647554397583, discriminator_loss=0.12082824110984802\n",
            "step 3181: generator_loss=2.6670844554901123, discriminator_loss=0.11907891929149628\n",
            "step 3182: generator_loss=2.636448383331299, discriminator_loss=0.12050164490938187\n",
            "step 3183: generator_loss=2.5667757987976074, discriminator_loss=0.11943319439888\n",
            "step 3184: generator_loss=2.535841703414917, discriminator_loss=0.12368692457675934\n",
            "step 3185: generator_loss=2.4492921829223633, discriminator_loss=0.12335805594921112\n",
            "step 3186: generator_loss=2.5171642303466797, discriminator_loss=0.11827602237462997\n",
            "step 3187: generator_loss=2.5261921882629395, discriminator_loss=0.11604668945074081\n",
            "step 3188: generator_loss=2.7571771144866943, discriminator_loss=0.10709277540445328\n",
            "step 3189: generator_loss=2.855409860610962, discriminator_loss=0.10309482365846634\n",
            "step 3190: generator_loss=2.7612123489379883, discriminator_loss=0.1057167574763298\n",
            "step 3191: generator_loss=2.804558277130127, discriminator_loss=0.10204219818115234\n",
            "step 3192: generator_loss=2.768127679824829, discriminator_loss=0.10388556122779846\n",
            "step 3193: generator_loss=2.7566683292388916, discriminator_loss=0.10283659398555756\n",
            "step 3194: generator_loss=2.7850022315979004, discriminator_loss=0.10239376127719879\n",
            "step 3195: generator_loss=2.7001662254333496, discriminator_loss=0.10411449521780014\n",
            "step 3196: generator_loss=2.589972496032715, discriminator_loss=0.10933432728052139\n",
            "step 3197: generator_loss=2.622744083404541, discriminator_loss=0.10777536034584045\n",
            "step 3198: generator_loss=2.452669143676758, discriminator_loss=0.11478453129529953\n",
            "step 3199: generator_loss=2.529989242553711, discriminator_loss=0.11213243007659912\n",
            "step 3200: generator_loss=2.6753158569335938, discriminator_loss=0.1082550585269928\n",
            "step 3201: generator_loss=2.6527538299560547, discriminator_loss=0.1100882738828659\n",
            "step 3202: generator_loss=2.700554609298706, discriminator_loss=0.10624130070209503\n",
            "step 3203: generator_loss=2.704488754272461, discriminator_loss=0.10476943850517273\n",
            "step 3204: generator_loss=2.839231014251709, discriminator_loss=0.10127885639667511\n",
            "step 3205: generator_loss=2.8640894889831543, discriminator_loss=0.09890975058078766\n",
            "step 3206: generator_loss=2.814708709716797, discriminator_loss=0.10229828953742981\n",
            "step 3207: generator_loss=2.7908220291137695, discriminator_loss=0.10146670043468475\n",
            "step 3208: generator_loss=2.7484612464904785, discriminator_loss=0.10478154569864273\n",
            "step 3209: generator_loss=2.7916862964630127, discriminator_loss=0.10437087714672089\n",
            "step 3210: generator_loss=2.729381799697876, discriminator_loss=0.10474439710378647\n",
            "step 3211: generator_loss=2.6144025325775146, discriminator_loss=0.10958831012248993\n",
            "step 3212: generator_loss=2.6184487342834473, discriminator_loss=0.11316395550966263\n",
            "step 3213: generator_loss=2.4805712699890137, discriminator_loss=0.11490648984909058\n",
            "step 3214: generator_loss=2.259087562561035, discriminator_loss=0.12734739482402802\n",
            "step 3215: generator_loss=2.371140480041504, discriminator_loss=0.12354174256324768\n",
            "step 3216: generator_loss=2.2710278034210205, discriminator_loss=0.12872529029846191\n",
            "step 3217: generator_loss=2.189683437347412, discriminator_loss=0.1336694210767746\n",
            "step 3218: generator_loss=2.196481227874756, discriminator_loss=0.1321268528699875\n",
            "step 3219: generator_loss=2.0493264198303223, discriminator_loss=0.13964968919754028\n",
            "step 3220: generator_loss=2.039083242416382, discriminator_loss=0.1390235722064972\n",
            "step 3221: generator_loss=1.9454931020736694, discriminator_loss=0.14268627762794495\n",
            "step 3222: generator_loss=2.0428991317749023, discriminator_loss=0.135477215051651\n",
            "step 3223: generator_loss=2.1983399391174316, discriminator_loss=0.12597395479679108\n",
            "step 3224: generator_loss=2.1959049701690674, discriminator_loss=0.1262633353471756\n",
            "step 3225: generator_loss=2.378938913345337, discriminator_loss=0.11719542741775513\n",
            "step 3226: generator_loss=2.400026798248291, discriminator_loss=0.11587921530008316\n",
            "step 3227: generator_loss=2.3476762771606445, discriminator_loss=0.11763732880353928\n",
            "step 3228: generator_loss=2.3585352897644043, discriminator_loss=0.11630527675151825\n",
            "step 3229: generator_loss=2.3487510681152344, discriminator_loss=0.11755293607711792\n",
            "step 3230: generator_loss=2.2907230854034424, discriminator_loss=0.12011463940143585\n",
            "step 3231: generator_loss=2.2442572116851807, discriminator_loss=0.12291806191205978\n",
            "step 3232: generator_loss=2.18942928314209, discriminator_loss=0.12619802355766296\n",
            "step 3233: generator_loss=2.152693271636963, discriminator_loss=0.12919750809669495\n",
            "step 3234: generator_loss=2.0796382427215576, discriminator_loss=0.13413488864898682\n",
            "step 3235: generator_loss=2.0330469608306885, discriminator_loss=0.13819944858551025\n",
            "step 3236: generator_loss=1.922106385231018, discriminator_loss=0.14605768024921417\n",
            "step 3237: generator_loss=1.890976071357727, discriminator_loss=0.14965932071208954\n",
            "step 3238: generator_loss=1.8520417213439941, discriminator_loss=0.1535874903202057\n",
            "step 3239: generator_loss=1.8252536058425903, discriminator_loss=0.15740840137004852\n",
            "step 3240: generator_loss=1.7919557094573975, discriminator_loss=0.16025826334953308\n",
            "step 3241: generator_loss=1.782907247543335, discriminator_loss=0.16176573932170868\n",
            "step 3242: generator_loss=1.7732930183410645, discriminator_loss=0.1629215031862259\n",
            "step 3243: generator_loss=1.7394287586212158, discriminator_loss=0.16682296991348267\n",
            "step 3244: generator_loss=1.7350589036941528, discriminator_loss=0.1684310883283615\n",
            "step 3245: generator_loss=1.7078676223754883, discriminator_loss=0.1722453236579895\n",
            "step 3246: generator_loss=1.7144383192062378, discriminator_loss=0.17290696501731873\n",
            "step 3247: generator_loss=1.7077875137329102, discriminator_loss=0.17502737045288086\n",
            "step 3248: generator_loss=1.714714765548706, discriminator_loss=0.1758514642715454\n",
            "step 3249: generator_loss=1.7093851566314697, discriminator_loss=0.17781156301498413\n",
            "step 3250: generator_loss=1.6992034912109375, discriminator_loss=0.18018856644630432\n",
            "step 3251: generator_loss=1.7173361778259277, discriminator_loss=0.17979401350021362\n",
            "step 3252: generator_loss=1.7335944175720215, discriminator_loss=0.179457426071167\n",
            "step 3253: generator_loss=1.7483335733413696, discriminator_loss=0.17942401766777039\n",
            "step 3254: generator_loss=1.7544338703155518, discriminator_loss=0.18040812015533447\n",
            "step 3255: generator_loss=1.7835960388183594, discriminator_loss=0.17903873324394226\n",
            "step 3256: generator_loss=1.808870553970337, discriminator_loss=0.17778193950653076\n",
            "step 3257: generator_loss=1.846869945526123, discriminator_loss=0.1755576729774475\n",
            "step 3258: generator_loss=1.9026761054992676, discriminator_loss=0.17143592238426208\n",
            "step 3259: generator_loss=1.964165210723877, discriminator_loss=0.16723574697971344\n",
            "step 3260: generator_loss=2.0198512077331543, discriminator_loss=0.16331592202186584\n",
            "step 3261: generator_loss=2.0624704360961914, discriminator_loss=0.16050580143928528\n",
            "step 3262: generator_loss=2.0812830924987793, discriminator_loss=0.1593017280101776\n",
            "step 3263: generator_loss=2.0595836639404297, discriminator_loss=0.1602083444595337\n",
            "step 3264: generator_loss=2.033491849899292, discriminator_loss=0.16143259406089783\n",
            "step 3265: generator_loss=2.0037856101989746, discriminator_loss=0.16279059648513794\n",
            "step 3266: generator_loss=1.9845894575119019, discriminator_loss=0.1632656455039978\n",
            "step 3267: generator_loss=1.9568486213684082, discriminator_loss=0.16437003016471863\n",
            "step 3268: generator_loss=1.926133155822754, discriminator_loss=0.1656179428100586\n",
            "step 3269: generator_loss=1.888200044631958, discriminator_loss=0.16756924986839294\n",
            "step 3270: generator_loss=1.8447508811950684, discriminator_loss=0.17012205719947815\n",
            "step 3271: generator_loss=1.818110466003418, discriminator_loss=0.1714477241039276\n",
            "step 3272: generator_loss=1.8002042770385742, discriminator_loss=0.17208406329154968\n",
            "step 3273: generator_loss=1.815351963043213, discriminator_loss=0.1695246398448944\n",
            "step 3274: generator_loss=1.8558746576309204, discriminator_loss=0.164573073387146\n",
            "step 3275: generator_loss=1.8906000852584839, discriminator_loss=0.16042178869247437\n",
            "step 3276: generator_loss=1.9178229570388794, discriminator_loss=0.1571453958749771\n",
            "step 3277: generator_loss=1.9480271339416504, discriminator_loss=0.1544160693883896\n",
            "step 3278: generator_loss=1.9453831911087036, discriminator_loss=0.1532343327999115\n",
            "step 3279: generator_loss=1.9298958778381348, discriminator_loss=0.15353891253471375\n",
            "step 3280: generator_loss=1.9270498752593994, discriminator_loss=0.1536620855331421\n",
            "step 3281: generator_loss=1.8931403160095215, discriminator_loss=0.15646132826805115\n",
            "step 3282: generator_loss=1.893126130104065, discriminator_loss=0.15666227042675018\n",
            "step 3283: generator_loss=1.8610073328018188, discriminator_loss=0.1599414050579071\n",
            "step 3284: generator_loss=1.8696978092193604, discriminator_loss=0.16022394597530365\n",
            "step 3285: generator_loss=1.7801848649978638, discriminator_loss=0.17232802510261536\n",
            "step 3286: generator_loss=1.8406801223754883, discriminator_loss=0.16828936338424683\n",
            "step 3287: generator_loss=1.7521848678588867, discriminator_loss=0.18180346488952637\n",
            "step 3288: generator_loss=1.66517972946167, discriminator_loss=0.19807152450084686\n",
            "step 3289: generator_loss=1.6524412631988525, discriminator_loss=0.20549385249614716\n",
            "step 3290: generator_loss=1.7846653461456299, discriminator_loss=0.19170477986335754\n",
            "step 3291: generator_loss=1.6085333824157715, discriminator_loss=0.22165167331695557\n",
            "step 3292: generator_loss=1.6996562480926514, discriminator_loss=0.21330435574054718\n",
            "step 3293: generator_loss=1.6052502393722534, discriminator_loss=0.22154462337493896\n",
            "step 3294: generator_loss=1.7231413125991821, discriminator_loss=0.21637263894081116\n",
            "step 3295: generator_loss=1.7248286008834839, discriminator_loss=0.21815833449363708\n",
            "step 3296: generator_loss=1.9849045276641846, discriminator_loss=0.19810841977596283\n",
            "step 3297: generator_loss=2.105306625366211, discriminator_loss=0.1978369653224945\n",
            "step 3298: generator_loss=2.407719612121582, discriminator_loss=0.17156696319580078\n",
            "step 3299: generator_loss=2.40095591545105, discriminator_loss=0.15980814397335052\n",
            "step 3300: generator_loss=2.509120464324951, discriminator_loss=0.16177354753017426\n",
            "step 3301: generator_loss=2.5332937240600586, discriminator_loss=0.15509150922298431\n",
            "step 3302: generator_loss=2.7169814109802246, discriminator_loss=0.1470859944820404\n",
            "step 3303: generator_loss=2.6145811080932617, discriminator_loss=0.15048936009407043\n",
            "step 3304: generator_loss=2.4869182109832764, discriminator_loss=0.14734667539596558\n",
            "step 3305: generator_loss=2.8193318843841553, discriminator_loss=0.13377127051353455\n",
            "step 3306: generator_loss=2.836050033569336, discriminator_loss=0.13458400964736938\n",
            "step 3307: generator_loss=2.8656368255615234, discriminator_loss=0.1349373459815979\n",
            "step 3308: generator_loss=2.670797348022461, discriminator_loss=0.14110000431537628\n",
            "step 3309: generator_loss=2.6392111778259277, discriminator_loss=0.14465457201004028\n",
            "step 3310: generator_loss=2.5964205265045166, discriminator_loss=0.1432962417602539\n",
            "step 3311: generator_loss=2.610032081604004, discriminator_loss=0.13470245897769928\n",
            "step 3312: generator_loss=2.514781951904297, discriminator_loss=0.1391521692276001\n",
            "step 3313: generator_loss=2.770791530609131, discriminator_loss=0.12609252333641052\n",
            "step 3314: generator_loss=2.649582624435425, discriminator_loss=0.13154473900794983\n",
            "step 3315: generator_loss=2.634521484375, discriminator_loss=0.1295134276151657\n",
            "step 3316: generator_loss=2.60139536857605, discriminator_loss=0.12936216592788696\n",
            "step 3317: generator_loss=2.789348840713501, discriminator_loss=0.12197137624025345\n",
            "step 3318: generator_loss=2.526024103164673, discriminator_loss=0.12823492288589478\n",
            "step 3319: generator_loss=2.7183313369750977, discriminator_loss=0.12034259736537933\n",
            "step 3320: generator_loss=2.6505508422851562, discriminator_loss=0.11972174793481827\n",
            "step 3321: generator_loss=2.860807418823242, discriminator_loss=0.11290740966796875\n",
            "step 3322: generator_loss=2.719029426574707, discriminator_loss=0.11522030830383301\n",
            "step 3323: generator_loss=2.9953970909118652, discriminator_loss=0.11197194457054138\n",
            "step 3324: generator_loss=2.772702693939209, discriminator_loss=0.11605580896139145\n",
            "step 3325: generator_loss=2.6768500804901123, discriminator_loss=0.11853542178869247\n",
            "step 3326: generator_loss=2.621614933013916, discriminator_loss=0.11776066571474075\n",
            "step 3327: generator_loss=2.500666379928589, discriminator_loss=0.12321172654628754\n",
            "step 3328: generator_loss=2.671309471130371, discriminator_loss=0.12136099487543106\n",
            "step 3329: generator_loss=2.4687740802764893, discriminator_loss=0.12635748088359833\n",
            "step 3330: generator_loss=2.352804183959961, discriminator_loss=0.12820877134799957\n",
            "step 3331: generator_loss=2.4849905967712402, discriminator_loss=0.1249222457408905\n",
            "step 3332: generator_loss=2.5917587280273438, discriminator_loss=0.1255493462085724\n",
            "step 3333: generator_loss=2.518134117126465, discriminator_loss=0.12509313225746155\n",
            "step 3334: generator_loss=2.5451509952545166, discriminator_loss=0.12798088788986206\n",
            "step 3335: generator_loss=2.5177650451660156, discriminator_loss=0.13012465834617615\n",
            "step 3336: generator_loss=2.479586601257324, discriminator_loss=0.13286367058753967\n",
            "step 3337: generator_loss=2.3885679244995117, discriminator_loss=0.13191580772399902\n",
            "step 3338: generator_loss=2.523230791091919, discriminator_loss=0.12461210042238235\n",
            "step 3339: generator_loss=2.5789525508880615, discriminator_loss=0.12640732526779175\n",
            "step 3340: generator_loss=2.2658309936523438, discriminator_loss=0.13699540495872498\n",
            "step 3341: generator_loss=2.3549559116363525, discriminator_loss=0.13450022041797638\n",
            "step 3342: generator_loss=2.2999894618988037, discriminator_loss=0.13893479108810425\n",
            "step 3343: generator_loss=2.248016834259033, discriminator_loss=0.1456279158592224\n",
            "step 3344: generator_loss=2.0918185710906982, discriminator_loss=0.1501738727092743\n",
            "step 3345: generator_loss=1.9779798984527588, discriminator_loss=0.15487562119960785\n",
            "step 3346: generator_loss=1.9387931823730469, discriminator_loss=0.15884819626808167\n",
            "step 3347: generator_loss=1.8959639072418213, discriminator_loss=0.15978185832500458\n",
            "step 3348: generator_loss=1.854240894317627, discriminator_loss=0.16600677371025085\n",
            "step 3349: generator_loss=1.8242467641830444, discriminator_loss=0.16611970961093903\n",
            "step 3350: generator_loss=1.882481575012207, discriminator_loss=0.16292807459831238\n",
            "step 3351: generator_loss=1.781043291091919, discriminator_loss=0.17303672432899475\n",
            "step 3352: generator_loss=1.805593490600586, discriminator_loss=0.1722830832004547\n",
            "step 3353: generator_loss=1.7738053798675537, discriminator_loss=0.17605385184288025\n",
            "step 3354: generator_loss=1.6788771152496338, discriminator_loss=0.18742713332176208\n",
            "step 3355: generator_loss=1.672965407371521, discriminator_loss=0.18762463331222534\n",
            "step 3356: generator_loss=1.6831691265106201, discriminator_loss=0.19009239971637726\n",
            "step 3357: generator_loss=1.5914740562438965, discriminator_loss=0.20635220408439636\n",
            "step 3358: generator_loss=1.5485882759094238, discriminator_loss=0.22269514203071594\n",
            "step 3359: generator_loss=1.5234980583190918, discriminator_loss=0.23254238069057465\n",
            "step 3360: generator_loss=1.5784109830856323, discriminator_loss=0.23380409181118011\n",
            "step 3361: generator_loss=1.5008643865585327, discriminator_loss=0.25163164734840393\n",
            "step 3362: generator_loss=1.421633243560791, discriminator_loss=0.28145769238471985\n",
            "step 3363: generator_loss=1.4878292083740234, discriminator_loss=0.27389711141586304\n",
            "step 3364: generator_loss=1.4170081615447998, discriminator_loss=0.2855709195137024\n",
            "step 3365: generator_loss=1.640049695968628, discriminator_loss=0.2517314851284027\n",
            "step 3366: generator_loss=1.482323169708252, discriminator_loss=0.2727731466293335\n",
            "step 3367: generator_loss=1.6919528245925903, discriminator_loss=0.2392634153366089\n",
            "step 3368: generator_loss=1.7115325927734375, discriminator_loss=0.22565039992332458\n",
            "step 3369: generator_loss=1.7618441581726074, discriminator_loss=0.22095859050750732\n",
            "step 3370: generator_loss=1.8409550189971924, discriminator_loss=0.20719236135482788\n",
            "step 3371: generator_loss=1.8654502630233765, discriminator_loss=0.2047319859266281\n",
            "step 3372: generator_loss=1.892953872680664, discriminator_loss=0.20150437951087952\n",
            "step 3373: generator_loss=2.056711435317993, discriminator_loss=0.18797525763511658\n",
            "step 3374: generator_loss=2.056069850921631, discriminator_loss=0.1887488216161728\n",
            "step 3375: generator_loss=2.107569932937622, discriminator_loss=0.18849016726016998\n",
            "step 3376: generator_loss=2.2844746112823486, discriminator_loss=0.17732322216033936\n",
            "step 3377: generator_loss=2.077378034591675, discriminator_loss=0.18886107206344604\n",
            "step 3378: generator_loss=2.1974403858184814, discriminator_loss=0.1852327138185501\n",
            "step 3379: generator_loss=2.328084945678711, discriminator_loss=0.17057770490646362\n",
            "step 3380: generator_loss=2.1126883029937744, discriminator_loss=0.19103378057479858\n",
            "step 3381: generator_loss=2.0610740184783936, discriminator_loss=0.20027396082878113\n",
            "step 3382: generator_loss=2.1718907356262207, discriminator_loss=0.18509671092033386\n",
            "step 3383: generator_loss=2.0950372219085693, discriminator_loss=0.18735839426517487\n",
            "step 3384: generator_loss=2.0704541206359863, discriminator_loss=0.18843892216682434\n",
            "step 3385: generator_loss=2.0360169410705566, discriminator_loss=0.19833745062351227\n",
            "step 3386: generator_loss=2.144329309463501, discriminator_loss=0.18283727765083313\n",
            "step 3387: generator_loss=2.057792901992798, discriminator_loss=0.185380756855011\n",
            "step 3388: generator_loss=2.21866774559021, discriminator_loss=0.17750920355319977\n",
            "step 3389: generator_loss=2.2330245971679688, discriminator_loss=0.16939619183540344\n",
            "step 3390: generator_loss=2.2830650806427, discriminator_loss=0.16206395626068115\n",
            "step 3391: generator_loss=2.352003812789917, discriminator_loss=0.15636232495307922\n",
            "step 3392: generator_loss=2.3468503952026367, discriminator_loss=0.15870848298072815\n",
            "step 3393: generator_loss=2.5133185386657715, discriminator_loss=0.15136456489562988\n",
            "step 3394: generator_loss=2.6252264976501465, discriminator_loss=0.14580026268959045\n",
            "step 3395: generator_loss=2.463758945465088, discriminator_loss=0.14722280204296112\n",
            "step 3396: generator_loss=2.2911975383758545, discriminator_loss=0.15586069226264954\n",
            "step 3397: generator_loss=2.267540454864502, discriminator_loss=0.15476959943771362\n",
            "step 3398: generator_loss=2.2369232177734375, discriminator_loss=0.15782266855239868\n",
            "step 3399: generator_loss=2.2271127700805664, discriminator_loss=0.1618383526802063\n",
            "step 3400: generator_loss=2.020481586456299, discriminator_loss=0.1672464907169342\n",
            "step 3401: generator_loss=2.006080389022827, discriminator_loss=0.1698605716228485\n",
            "step 3402: generator_loss=1.7973670959472656, discriminator_loss=0.18122220039367676\n",
            "step 3403: generator_loss=1.9769365787506104, discriminator_loss=0.16888216137886047\n",
            "step 3404: generator_loss=2.1160945892333984, discriminator_loss=0.15738004446029663\n",
            "step 3405: generator_loss=2.0032920837402344, discriminator_loss=0.16252075135707855\n",
            "step 3406: generator_loss=1.9610086679458618, discriminator_loss=0.16402505338191986\n",
            "step 3407: generator_loss=2.0790483951568604, discriminator_loss=0.1574782431125641\n",
            "step 3408: generator_loss=2.1249513626098633, discriminator_loss=0.15417996048927307\n",
            "step 3409: generator_loss=2.035529136657715, discriminator_loss=0.1596425324678421\n",
            "step 3410: generator_loss=1.9814670085906982, discriminator_loss=0.16373278200626373\n",
            "step 3411: generator_loss=1.9088774919509888, discriminator_loss=0.16683489084243774\n",
            "step 3412: generator_loss=1.8872597217559814, discriminator_loss=0.1683962643146515\n",
            "step 3413: generator_loss=1.8301323652267456, discriminator_loss=0.17431603372097015\n",
            "step 3414: generator_loss=1.7533912658691406, discriminator_loss=0.18026867508888245\n",
            "step 3415: generator_loss=1.6988519430160522, discriminator_loss=0.18668730556964874\n",
            "step 3416: generator_loss=1.6814351081848145, discriminator_loss=0.18829208612442017\n",
            "step 3417: generator_loss=1.6461536884307861, discriminator_loss=0.19277843832969666\n",
            "step 3418: generator_loss=1.654703140258789, discriminator_loss=0.19185557961463928\n",
            "step 3419: generator_loss=1.6392230987548828, discriminator_loss=0.19393476843833923\n",
            "step 3420: generator_loss=1.6250872611999512, discriminator_loss=0.19640833139419556\n",
            "step 3421: generator_loss=1.6234179735183716, discriminator_loss=0.19747775793075562\n",
            "step 3422: generator_loss=1.6649014949798584, discriminator_loss=0.19423484802246094\n",
            "step 3423: generator_loss=1.6466304063796997, discriminator_loss=0.19820736348628998\n",
            "step 3424: generator_loss=1.647807002067566, discriminator_loss=0.1996818333864212\n",
            "step 3425: generator_loss=1.6475155353546143, discriminator_loss=0.20006074011325836\n",
            "step 3426: generator_loss=1.5971691608428955, discriminator_loss=0.20744431018829346\n",
            "step 3427: generator_loss=1.6268277168273926, discriminator_loss=0.20582064986228943\n",
            "step 3428: generator_loss=1.6417016983032227, discriminator_loss=0.20552510023117065\n",
            "step 3429: generator_loss=1.6028317213058472, discriminator_loss=0.21075738966464996\n",
            "step 3430: generator_loss=1.550694465637207, discriminator_loss=0.22044850885868073\n",
            "step 3431: generator_loss=1.559549331665039, discriminator_loss=0.21937918663024902\n",
            "step 3432: generator_loss=1.6278049945831299, discriminator_loss=0.2139209806919098\n",
            "step 3433: generator_loss=1.5917428731918335, discriminator_loss=0.21894367039203644\n",
            "step 3434: generator_loss=1.672656774520874, discriminator_loss=0.21172527968883514\n",
            "step 3435: generator_loss=1.6483182907104492, discriminator_loss=0.21546459197998047\n",
            "step 3436: generator_loss=1.6490488052368164, discriminator_loss=0.2151537835597992\n",
            "step 3437: generator_loss=1.6810336112976074, discriminator_loss=0.2127811461687088\n",
            "step 3438: generator_loss=1.6987882852554321, discriminator_loss=0.211018905043602\n",
            "step 3439: generator_loss=1.744387149810791, discriminator_loss=0.20661503076553345\n",
            "step 3440: generator_loss=1.775700569152832, discriminator_loss=0.203774094581604\n",
            "step 3441: generator_loss=1.8140703439712524, discriminator_loss=0.1998554915189743\n",
            "step 3442: generator_loss=1.8360769748687744, discriminator_loss=0.19736993312835693\n",
            "step 3443: generator_loss=1.858452320098877, discriminator_loss=0.19498905539512634\n",
            "step 3444: generator_loss=1.8904975652694702, discriminator_loss=0.1912587285041809\n",
            "step 3445: generator_loss=1.9024033546447754, discriminator_loss=0.18943700194358826\n",
            "step 3446: generator_loss=1.895763874053955, discriminator_loss=0.18880221247673035\n",
            "step 3447: generator_loss=1.872105598449707, discriminator_loss=0.18989160656929016\n",
            "step 3448: generator_loss=1.8577865362167358, discriminator_loss=0.1908871829509735\n",
            "step 3449: generator_loss=1.7928061485290527, discriminator_loss=0.1951712965965271\n",
            "step 3450: generator_loss=1.742512583732605, discriminator_loss=0.19921636581420898\n",
            "step 3451: generator_loss=1.7044785022735596, discriminator_loss=0.20226237177848816\n",
            "step 3452: generator_loss=1.6664221286773682, discriminator_loss=0.205538809299469\n",
            "step 3453: generator_loss=1.6273369789123535, discriminator_loss=0.20951366424560547\n",
            "step 3454: generator_loss=1.6038812398910522, discriminator_loss=0.2122078239917755\n",
            "step 3455: generator_loss=1.5957444906234741, discriminator_loss=0.21334108710289001\n",
            "step 3456: generator_loss=1.5937029123306274, discriminator_loss=0.2136102318763733\n",
            "step 3457: generator_loss=1.6101692914962769, discriminator_loss=0.2118518352508545\n",
            "step 3458: generator_loss=1.6210750341415405, discriminator_loss=0.21088942885398865\n",
            "step 3459: generator_loss=1.6379456520080566, discriminator_loss=0.20946067571640015\n",
            "step 3460: generator_loss=1.6456289291381836, discriminator_loss=0.20865756273269653\n",
            "step 3461: generator_loss=1.6392114162445068, discriminator_loss=0.20970571041107178\n",
            "step 3462: generator_loss=1.642235517501831, discriminator_loss=0.2097710818052292\n",
            "step 3463: generator_loss=1.6397911310195923, discriminator_loss=0.21070796251296997\n",
            "step 3464: generator_loss=1.6236761808395386, discriminator_loss=0.21347305178642273\n",
            "step 3465: generator_loss=1.5988203287124634, discriminator_loss=0.21758006513118744\n",
            "step 3466: generator_loss=1.5664081573486328, discriminator_loss=0.22312632203102112\n",
            "step 3467: generator_loss=1.529111623764038, discriminator_loss=0.22963351011276245\n",
            "step 3468: generator_loss=1.5060317516326904, discriminator_loss=0.23442545533180237\n",
            "step 3469: generator_loss=1.5001463890075684, discriminator_loss=0.2377576231956482\n",
            "step 3470: generator_loss=1.4736275672912598, discriminator_loss=0.24210159480571747\n",
            "step 3471: generator_loss=1.4703783988952637, discriminator_loss=0.24393227696418762\n",
            "step 3472: generator_loss=1.4756653308868408, discriminator_loss=0.24499326944351196\n",
            "step 3473: generator_loss=1.4918464422225952, discriminator_loss=0.24452657997608185\n",
            "step 3474: generator_loss=1.505725383758545, discriminator_loss=0.24347683787345886\n",
            "step 3475: generator_loss=1.5387358665466309, discriminator_loss=0.2396862804889679\n",
            "step 3476: generator_loss=1.5866601467132568, discriminator_loss=0.23394441604614258\n",
            "step 3477: generator_loss=1.6305410861968994, discriminator_loss=0.22929885983467102\n",
            "step 3478: generator_loss=1.6502931118011475, discriminator_loss=0.2269875407218933\n",
            "step 3479: generator_loss=1.6901695728302002, discriminator_loss=0.2229616343975067\n",
            "step 3480: generator_loss=1.694582462310791, discriminator_loss=0.22197020053863525\n",
            "step 3481: generator_loss=1.7166156768798828, discriminator_loss=0.2188914716243744\n",
            "step 3482: generator_loss=1.7300055027008057, discriminator_loss=0.21700608730316162\n",
            "step 3483: generator_loss=1.7561805248260498, discriminator_loss=0.2136002480983734\n",
            "step 3484: generator_loss=1.7888412475585938, discriminator_loss=0.20960447192192078\n",
            "step 3485: generator_loss=1.8677833080291748, discriminator_loss=0.20108681917190552\n",
            "step 3486: generator_loss=1.948291301727295, discriminator_loss=0.1935933530330658\n",
            "step 3487: generator_loss=2.02066707611084, discriminator_loss=0.18794909119606018\n",
            "step 3488: generator_loss=2.056668281555176, discriminator_loss=0.18588528037071228\n",
            "step 3489: generator_loss=2.110016107559204, discriminator_loss=0.18389157950878143\n",
            "step 3490: generator_loss=2.1842141151428223, discriminator_loss=0.1774037778377533\n",
            "step 3491: generator_loss=2.1427180767059326, discriminator_loss=0.18552368879318237\n",
            "step 3492: generator_loss=2.078251838684082, discriminator_loss=0.1960073858499527\n",
            "step 3493: generator_loss=2.238048791885376, discriminator_loss=0.18293020129203796\n",
            "step 3494: generator_loss=2.3354310989379883, discriminator_loss=0.17643779516220093\n",
            "step 3495: generator_loss=2.2976322174072266, discriminator_loss=0.1794109344482422\n",
            "step 3496: generator_loss=2.397024393081665, discriminator_loss=0.17499662935733795\n",
            "step 3497: generator_loss=2.6533854007720947, discriminator_loss=0.1565142124891281\n",
            "step 3498: generator_loss=2.668954610824585, discriminator_loss=0.15745967626571655\n",
            "step 3499: generator_loss=2.741652488708496, discriminator_loss=0.1522739827632904\n",
            "step 3500: generator_loss=2.672451972961426, discriminator_loss=0.1585652381181717\n",
            "step 3501: generator_loss=2.8107423782348633, discriminator_loss=0.14826825261116028\n",
            "step 3502: generator_loss=2.860776901245117, discriminator_loss=0.1481282263994217\n",
            "step 3503: generator_loss=2.8946921825408936, discriminator_loss=0.14493328332901\n",
            "step 3504: generator_loss=2.9123713970184326, discriminator_loss=0.14261141419410706\n",
            "step 3505: generator_loss=2.8893256187438965, discriminator_loss=0.14015763998031616\n",
            "step 3506: generator_loss=2.911550998687744, discriminator_loss=0.13549673557281494\n",
            "step 3507: generator_loss=3.1029348373413086, discriminator_loss=0.12408384680747986\n",
            "step 3508: generator_loss=2.9534127712249756, discriminator_loss=0.13169890642166138\n",
            "step 3509: generator_loss=3.028777599334717, discriminator_loss=0.12591609358787537\n",
            "step 3510: generator_loss=2.919729232788086, discriminator_loss=0.12471410632133484\n",
            "step 3511: generator_loss=2.9443507194519043, discriminator_loss=0.1227225512266159\n",
            "step 3512: generator_loss=3.049776554107666, discriminator_loss=0.1174565777182579\n",
            "step 3513: generator_loss=3.118053436279297, discriminator_loss=0.11434580385684967\n",
            "step 3514: generator_loss=3.003519058227539, discriminator_loss=0.116979219019413\n",
            "step 3515: generator_loss=2.95395827293396, discriminator_loss=0.11500173062086105\n",
            "step 3516: generator_loss=2.9893529415130615, discriminator_loss=0.11314447969198227\n",
            "step 3517: generator_loss=2.969217538833618, discriminator_loss=0.11279462277889252\n",
            "step 3518: generator_loss=2.9680533409118652, discriminator_loss=0.1111576035618782\n",
            "step 3519: generator_loss=2.7688679695129395, discriminator_loss=0.11839050054550171\n",
            "step 3520: generator_loss=2.5828559398651123, discriminator_loss=0.12379823625087738\n",
            "step 3521: generator_loss=2.762650489807129, discriminator_loss=0.11732123792171478\n",
            "step 3522: generator_loss=2.5572926998138428, discriminator_loss=0.12274837493896484\n",
            "step 3523: generator_loss=2.4589195251464844, discriminator_loss=0.12636736035346985\n",
            "step 3524: generator_loss=2.3861567974090576, discriminator_loss=0.13095203042030334\n",
            "step 3525: generator_loss=2.353534698486328, discriminator_loss=0.13200530409812927\n",
            "step 3526: generator_loss=2.2208056449890137, discriminator_loss=0.139203280210495\n",
            "step 3527: generator_loss=2.093618392944336, discriminator_loss=0.1442302167415619\n",
            "step 3528: generator_loss=2.0480566024780273, discriminator_loss=0.14919471740722656\n",
            "step 3529: generator_loss=2.0770320892333984, discriminator_loss=0.14752818644046783\n",
            "step 3530: generator_loss=2.1965463161468506, discriminator_loss=0.14685383439064026\n",
            "step 3531: generator_loss=2.185297966003418, discriminator_loss=0.14838320016860962\n",
            "step 3532: generator_loss=1.9916064739227295, discriminator_loss=0.15729361772537231\n",
            "step 3533: generator_loss=2.038529396057129, discriminator_loss=0.15634268522262573\n",
            "step 3534: generator_loss=1.9942704439163208, discriminator_loss=0.15828275680541992\n",
            "step 3535: generator_loss=2.04079008102417, discriminator_loss=0.15732696652412415\n",
            "step 3536: generator_loss=1.903267502784729, discriminator_loss=0.16363659501075745\n",
            "step 3537: generator_loss=1.9065966606140137, discriminator_loss=0.16413575410842896\n",
            "step 3538: generator_loss=1.9160741567611694, discriminator_loss=0.16640663146972656\n",
            "step 3539: generator_loss=1.8199008703231812, discriminator_loss=0.17139406502246857\n",
            "step 3540: generator_loss=1.7984585762023926, discriminator_loss=0.17607396841049194\n",
            "step 3541: generator_loss=1.810570240020752, discriminator_loss=0.17485766112804413\n",
            "step 3542: generator_loss=1.8846427202224731, discriminator_loss=0.17109186947345734\n",
            "step 3543: generator_loss=1.825448751449585, discriminator_loss=0.17467881739139557\n",
            "step 3544: generator_loss=1.7689366340637207, discriminator_loss=0.17737284302711487\n",
            "step 3545: generator_loss=1.7833101749420166, discriminator_loss=0.17753589153289795\n",
            "step 3546: generator_loss=1.7524769306182861, discriminator_loss=0.18114274740219116\n",
            "step 3547: generator_loss=1.7427632808685303, discriminator_loss=0.18366549909114838\n",
            "step 3548: generator_loss=1.6830790042877197, discriminator_loss=0.18981227278709412\n",
            "step 3549: generator_loss=1.6388919353485107, discriminator_loss=0.19549520313739777\n",
            "step 3550: generator_loss=1.6115310192108154, discriminator_loss=0.20020660758018494\n",
            "step 3551: generator_loss=1.564103603363037, discriminator_loss=0.20660951733589172\n",
            "step 3552: generator_loss=1.5607093572616577, discriminator_loss=0.20879799127578735\n",
            "step 3553: generator_loss=1.5679137706756592, discriminator_loss=0.2097652554512024\n",
            "step 3554: generator_loss=1.5846145153045654, discriminator_loss=0.20962998270988464\n",
            "step 3555: generator_loss=1.626427173614502, discriminator_loss=0.2068081945180893\n",
            "step 3556: generator_loss=1.6856361627578735, discriminator_loss=0.20195260643959045\n",
            "step 3557: generator_loss=1.7520805597305298, discriminator_loss=0.19763866066932678\n",
            "step 3558: generator_loss=1.7979918718338013, discriminator_loss=0.19365069270133972\n",
            "step 3559: generator_loss=1.8950424194335938, discriminator_loss=0.18655240535736084\n",
            "step 3560: generator_loss=2.000837802886963, discriminator_loss=0.17934992909431458\n",
            "step 3561: generator_loss=2.0489819049835205, discriminator_loss=0.1768445074558258\n",
            "step 3562: generator_loss=2.0525569915771484, discriminator_loss=0.17604395747184753\n",
            "step 3563: generator_loss=2.0650641918182373, discriminator_loss=0.1752026081085205\n",
            "step 3564: generator_loss=2.0711123943328857, discriminator_loss=0.17453497648239136\n",
            "step 3565: generator_loss=2.069070816040039, discriminator_loss=0.17413559556007385\n",
            "step 3566: generator_loss=2.066600799560547, discriminator_loss=0.17349782586097717\n",
            "step 3567: generator_loss=2.055340051651001, discriminator_loss=0.1733531951904297\n",
            "step 3568: generator_loss=2.039569854736328, discriminator_loss=0.17345592379570007\n",
            "step 3569: generator_loss=2.0243990421295166, discriminator_loss=0.17361006140708923\n",
            "step 3570: generator_loss=2.013477087020874, discriminator_loss=0.17335988581180573\n",
            "step 3571: generator_loss=1.9914082288742065, discriminator_loss=0.17375808954238892\n",
            "step 3572: generator_loss=1.9587812423706055, discriminator_loss=0.1750349998474121\n",
            "step 3573: generator_loss=1.9283545017242432, discriminator_loss=0.17647084593772888\n",
            "step 3574: generator_loss=1.9002214670181274, discriminator_loss=0.17760056257247925\n",
            "step 3575: generator_loss=1.8709537982940674, discriminator_loss=0.1793576180934906\n",
            "step 3576: generator_loss=1.8707829713821411, discriminator_loss=0.1785014271736145\n",
            "step 3577: generator_loss=1.8795573711395264, discriminator_loss=0.17681440711021423\n",
            "step 3578: generator_loss=1.8860089778900146, discriminator_loss=0.17557434737682343\n",
            "step 3579: generator_loss=1.8794257640838623, discriminator_loss=0.17593427002429962\n",
            "step 3580: generator_loss=1.8575608730316162, discriminator_loss=0.17762015759944916\n",
            "step 3581: generator_loss=1.8379870653152466, discriminator_loss=0.17924758791923523\n",
            "step 3582: generator_loss=1.8345465660095215, discriminator_loss=0.17942534387111664\n",
            "step 3583: generator_loss=1.8356409072875977, discriminator_loss=0.17905764281749725\n",
            "step 3584: generator_loss=1.8433390855789185, discriminator_loss=0.178096204996109\n",
            "step 3585: generator_loss=1.8422245979309082, discriminator_loss=0.17806144058704376\n",
            "step 3586: generator_loss=1.8504972457885742, discriminator_loss=0.1771569848060608\n",
            "step 3587: generator_loss=1.8744635581970215, discriminator_loss=0.17466548085212708\n",
            "step 3588: generator_loss=1.9135398864746094, discriminator_loss=0.17103484272956848\n",
            "step 3589: generator_loss=1.981523036956787, discriminator_loss=0.16530129313468933\n",
            "step 3590: generator_loss=2.0805177688598633, discriminator_loss=0.15746498107910156\n",
            "step 3591: generator_loss=2.1819634437561035, discriminator_loss=0.15042628347873688\n",
            "step 3592: generator_loss=2.266636371612549, discriminator_loss=0.14490073919296265\n",
            "step 3593: generator_loss=2.319181203842163, discriminator_loss=0.14127367734909058\n",
            "step 3594: generator_loss=2.3252034187316895, discriminator_loss=0.14000320434570312\n",
            "step 3595: generator_loss=2.3177971839904785, discriminator_loss=0.13939644396305084\n",
            "step 3596: generator_loss=2.301492214202881, discriminator_loss=0.1389048993587494\n",
            "step 3597: generator_loss=2.2591071128845215, discriminator_loss=0.14007976651191711\n",
            "step 3598: generator_loss=2.202145576477051, discriminator_loss=0.14246898889541626\n",
            "step 3599: generator_loss=2.1325860023498535, discriminator_loss=0.1455974578857422\n",
            "step 3600: generator_loss=2.0677330493927, discriminator_loss=0.14880385994911194\n",
            "step 3601: generator_loss=2.0196356773376465, discriminator_loss=0.15127524733543396\n",
            "step 3602: generator_loss=1.9984441995620728, discriminator_loss=0.15164753794670105\n",
            "step 3603: generator_loss=1.9820044040679932, discriminator_loss=0.15204598009586334\n",
            "step 3604: generator_loss=1.9606854915618896, discriminator_loss=0.15307927131652832\n",
            "step 3605: generator_loss=1.933840036392212, discriminator_loss=0.1549150049686432\n",
            "step 3606: generator_loss=1.9033875465393066, discriminator_loss=0.15711668133735657\n",
            "step 3607: generator_loss=1.9164035320281982, discriminator_loss=0.15581093728542328\n",
            "step 3608: generator_loss=1.9288378953933716, discriminator_loss=0.15476027131080627\n",
            "step 3609: generator_loss=1.9484612941741943, discriminator_loss=0.15353654325008392\n",
            "step 3610: generator_loss=1.9643458127975464, discriminator_loss=0.1524462103843689\n",
            "step 3611: generator_loss=1.9934017658233643, discriminator_loss=0.1504327654838562\n",
            "step 3612: generator_loss=2.0505518913269043, discriminator_loss=0.14661838114261627\n",
            "step 3613: generator_loss=2.139336585998535, discriminator_loss=0.14057734608650208\n",
            "step 3614: generator_loss=2.225435733795166, discriminator_loss=0.1353401094675064\n",
            "step 3615: generator_loss=2.301334857940674, discriminator_loss=0.13093219697475433\n",
            "step 3616: generator_loss=2.3517191410064697, discriminator_loss=0.12812836468219757\n",
            "step 3617: generator_loss=2.4051008224487305, discriminator_loss=0.12504468858242035\n",
            "step 3618: generator_loss=2.436591148376465, discriminator_loss=0.12299484014511108\n",
            "step 3619: generator_loss=2.44268798828125, discriminator_loss=0.12214332073926926\n",
            "step 3620: generator_loss=2.445462465286255, discriminator_loss=0.12122723460197449\n",
            "step 3621: generator_loss=2.4512972831726074, discriminator_loss=0.1201445460319519\n",
            "step 3622: generator_loss=2.4531519412994385, discriminator_loss=0.11909220367670059\n",
            "step 3623: generator_loss=2.444514751434326, discriminator_loss=0.11836870014667511\n",
            "step 3624: generator_loss=2.410951614379883, discriminator_loss=0.11893198639154434\n",
            "step 3625: generator_loss=2.363537073135376, discriminator_loss=0.12025531381368637\n",
            "step 3626: generator_loss=2.296184539794922, discriminator_loss=0.12270058691501617\n",
            "step 3627: generator_loss=2.2321605682373047, discriminator_loss=0.12543830275535583\n",
            "step 3628: generator_loss=2.183082342147827, discriminator_loss=0.1276901513338089\n",
            "step 3629: generator_loss=2.131436824798584, discriminator_loss=0.13035430014133453\n",
            "step 3630: generator_loss=2.0961153507232666, discriminator_loss=0.13215932250022888\n",
            "step 3631: generator_loss=2.0796871185302734, discriminator_loss=0.13287265598773956\n",
            "step 3632: generator_loss=2.0760245323181152, discriminator_loss=0.1327086091041565\n",
            "step 3633: generator_loss=2.0973610877990723, discriminator_loss=0.13096538186073303\n",
            "step 3634: generator_loss=2.123627185821533, discriminator_loss=0.1292419284582138\n",
            "step 3635: generator_loss=2.134958505630493, discriminator_loss=0.12807774543762207\n",
            "step 3636: generator_loss=2.150843620300293, discriminator_loss=0.1269664764404297\n",
            "step 3637: generator_loss=2.1671204566955566, discriminator_loss=0.12587036192417145\n",
            "step 3638: generator_loss=2.189774990081787, discriminator_loss=0.12445496022701263\n",
            "step 3639: generator_loss=2.219938039779663, discriminator_loss=0.12266547232866287\n",
            "step 3640: generator_loss=2.247913122177124, discriminator_loss=0.12099789083003998\n",
            "step 3641: generator_loss=2.2724227905273438, discriminator_loss=0.11954708397388458\n",
            "step 3642: generator_loss=2.276607036590576, discriminator_loss=0.11942391097545624\n",
            "step 3643: generator_loss=2.261685371398926, discriminator_loss=0.1202925443649292\n",
            "step 3644: generator_loss=2.2405176162719727, discriminator_loss=0.12153726816177368\n",
            "step 3645: generator_loss=2.2052650451660156, discriminator_loss=0.12375390529632568\n",
            "step 3646: generator_loss=2.1586971282958984, discriminator_loss=0.12670886516571045\n",
            "step 3647: generator_loss=2.1041274070739746, discriminator_loss=0.13043688237667084\n",
            "step 3648: generator_loss=2.044121265411377, discriminator_loss=0.13473060727119446\n",
            "step 3649: generator_loss=1.9926207065582275, discriminator_loss=0.13872109353542328\n",
            "step 3650: generator_loss=1.9743022918701172, discriminator_loss=0.14039045572280884\n",
            "step 3651: generator_loss=1.987428903579712, discriminator_loss=0.13952848315238953\n",
            "step 3652: generator_loss=2.0530574321746826, discriminator_loss=0.1347230076789856\n",
            "step 3653: generator_loss=2.1530051231384277, discriminator_loss=0.1280101090669632\n",
            "step 3654: generator_loss=2.2516283988952637, discriminator_loss=0.12219229340553284\n",
            "step 3655: generator_loss=2.3322644233703613, discriminator_loss=0.11772347241640091\n",
            "step 3656: generator_loss=2.3887102603912354, discriminator_loss=0.11499130725860596\n",
            "step 3657: generator_loss=2.4363880157470703, discriminator_loss=0.1124783307313919\n",
            "step 3658: generator_loss=2.503631591796875, discriminator_loss=0.10929994285106659\n",
            "step 3659: generator_loss=2.5646262168884277, discriminator_loss=0.10638082027435303\n",
            "step 3660: generator_loss=2.593273401260376, discriminator_loss=0.10484382510185242\n",
            "step 3661: generator_loss=2.5870800018310547, discriminator_loss=0.1046585962176323\n",
            "step 3662: generator_loss=2.5560338497161865, discriminator_loss=0.10536706447601318\n",
            "step 3663: generator_loss=2.5132408142089844, discriminator_loss=0.10669295489788055\n",
            "step 3664: generator_loss=2.4616811275482178, discriminator_loss=0.10851568728685379\n",
            "step 3665: generator_loss=2.4075145721435547, discriminator_loss=0.11040392518043518\n",
            "step 3666: generator_loss=2.3491878509521484, discriminator_loss=0.11286379396915436\n",
            "step 3667: generator_loss=2.29948091506958, discriminator_loss=0.11510720103979111\n",
            "step 3668: generator_loss=2.263993263244629, discriminator_loss=0.11671080440282822\n",
            "step 3669: generator_loss=2.248455047607422, discriminator_loss=0.11735881865024567\n",
            "step 3670: generator_loss=2.2579336166381836, discriminator_loss=0.11661629378795624\n",
            "step 3671: generator_loss=2.274250030517578, discriminator_loss=0.11558864265680313\n",
            "step 3672: generator_loss=2.293935775756836, discriminator_loss=0.11448036134243011\n",
            "step 3673: generator_loss=2.313260316848755, discriminator_loss=0.11340436339378357\n",
            "step 3674: generator_loss=2.333237648010254, discriminator_loss=0.11240220069885254\n",
            "step 3675: generator_loss=2.3517215251922607, discriminator_loss=0.11147075146436691\n",
            "step 3676: generator_loss=2.370182991027832, discriminator_loss=0.11041200160980225\n",
            "step 3677: generator_loss=2.3755030632019043, discriminator_loss=0.11012448370456696\n",
            "step 3678: generator_loss=2.364823818206787, discriminator_loss=0.11055529117584229\n",
            "step 3679: generator_loss=2.3436851501464844, discriminator_loss=0.11156653612852097\n",
            "step 3680: generator_loss=2.3139991760253906, discriminator_loss=0.11297763139009476\n",
            "step 3681: generator_loss=2.2904107570648193, discriminator_loss=0.11428824812173843\n",
            "step 3682: generator_loss=2.2762036323547363, discriminator_loss=0.11503957211971283\n",
            "step 3683: generator_loss=2.2570338249206543, discriminator_loss=0.1161578893661499\n",
            "step 3684: generator_loss=2.2505836486816406, discriminator_loss=0.116591677069664\n",
            "step 3685: generator_loss=2.2501564025878906, discriminator_loss=0.11659714579582214\n",
            "step 3686: generator_loss=2.251516819000244, discriminator_loss=0.11658633500337601\n",
            "step 3687: generator_loss=2.2434093952178955, discriminator_loss=0.11710654199123383\n",
            "step 3688: generator_loss=2.231069564819336, discriminator_loss=0.11787433922290802\n",
            "step 3689: generator_loss=2.2185769081115723, discriminator_loss=0.11870241165161133\n",
            "step 3690: generator_loss=2.207003116607666, discriminator_loss=0.1194421648979187\n",
            "step 3691: generator_loss=2.2065303325653076, discriminator_loss=0.11949223279953003\n",
            "step 3692: generator_loss=2.2222886085510254, discriminator_loss=0.11848139017820358\n",
            "step 3693: generator_loss=2.247187614440918, discriminator_loss=0.11702042818069458\n",
            "step 3694: generator_loss=2.265915632247925, discriminator_loss=0.11590857058763504\n",
            "step 3695: generator_loss=2.280414581298828, discriminator_loss=0.11500687152147293\n",
            "step 3696: generator_loss=2.277400493621826, discriminator_loss=0.11523469537496567\n",
            "step 3697: generator_loss=2.270188331604004, discriminator_loss=0.11561518162488937\n",
            "step 3698: generator_loss=2.2609002590179443, discriminator_loss=0.1161520779132843\n",
            "step 3699: generator_loss=2.2371253967285156, discriminator_loss=0.11751671135425568\n",
            "step 3700: generator_loss=2.2171237468719482, discriminator_loss=0.1187376081943512\n",
            "step 3701: generator_loss=2.198948383331299, discriminator_loss=0.11980091035366058\n",
            "step 3702: generator_loss=2.187929153442383, discriminator_loss=0.12054562568664551\n",
            "step 3703: generator_loss=2.1870648860931396, discriminator_loss=0.12057127058506012\n",
            "step 3704: generator_loss=2.2034997940063477, discriminator_loss=0.11957511305809021\n",
            "step 3705: generator_loss=2.234372615814209, discriminator_loss=0.11779601871967316\n",
            "step 3706: generator_loss=2.277651309967041, discriminator_loss=0.11537529528141022\n",
            "step 3707: generator_loss=2.3281540870666504, discriminator_loss=0.11265663057565689\n",
            "step 3708: generator_loss=2.3744375705718994, discriminator_loss=0.11028201878070831\n",
            "step 3709: generator_loss=2.402256965637207, discriminator_loss=0.10899944603443146\n",
            "step 3710: generator_loss=2.4185590744018555, discriminator_loss=0.10819709300994873\n",
            "step 3711: generator_loss=2.4235405921936035, discriminator_loss=0.1079854816198349\n",
            "step 3712: generator_loss=2.4225656986236572, discriminator_loss=0.10801488906145096\n",
            "step 3713: generator_loss=2.4047226905822754, discriminator_loss=0.10871593654155731\n",
            "step 3714: generator_loss=2.367001533508301, discriminator_loss=0.11037782579660416\n",
            "step 3715: generator_loss=2.337963819503784, discriminator_loss=0.11177044361829758\n",
            "step 3716: generator_loss=2.303424835205078, discriminator_loss=0.11357125639915466\n",
            "step 3717: generator_loss=2.260976552963257, discriminator_loss=0.11578444391489029\n",
            "step 3718: generator_loss=2.2202281951904297, discriminator_loss=0.11805939674377441\n",
            "step 3719: generator_loss=2.1982333660125732, discriminator_loss=0.11935316771268845\n",
            "step 3720: generator_loss=2.186429023742676, discriminator_loss=0.12014175951480865\n",
            "step 3721: generator_loss=2.1768078804016113, discriminator_loss=0.12090844660997391\n",
            "step 3722: generator_loss=2.164198398590088, discriminator_loss=0.12185928970575333\n",
            "step 3723: generator_loss=2.157700777053833, discriminator_loss=0.12244675308465958\n",
            "step 3724: generator_loss=2.1727919578552246, discriminator_loss=0.12170610576868057\n",
            "step 3725: generator_loss=2.187835693359375, discriminator_loss=0.12080851197242737\n",
            "step 3726: generator_loss=2.200521469116211, discriminator_loss=0.12010654807090759\n",
            "step 3727: generator_loss=2.221637010574341, discriminator_loss=0.11896499991416931\n",
            "step 3728: generator_loss=2.2496914863586426, discriminator_loss=0.11734002083539963\n",
            "step 3729: generator_loss=2.2539405822753906, discriminator_loss=0.11729308217763901\n",
            "step 3730: generator_loss=2.25626802444458, discriminator_loss=0.11732534319162369\n",
            "step 3731: generator_loss=2.2292473316192627, discriminator_loss=0.11922453343868256\n",
            "step 3732: generator_loss=2.201931953430176, discriminator_loss=0.12095290422439575\n",
            "step 3733: generator_loss=2.1743311882019043, discriminator_loss=0.12307056784629822\n",
            "step 3734: generator_loss=2.181225061416626, discriminator_loss=0.12276770174503326\n",
            "step 3735: generator_loss=2.156090497970581, discriminator_loss=0.12501534819602966\n",
            "step 3736: generator_loss=2.178274631500244, discriminator_loss=0.1242513656616211\n",
            "step 3737: generator_loss=2.2095489501953125, discriminator_loss=0.12316104769706726\n",
            "step 3738: generator_loss=2.127241611480713, discriminator_loss=0.1295211911201477\n",
            "step 3739: generator_loss=2.217148780822754, discriminator_loss=0.1229875385761261\n",
            "step 3740: generator_loss=2.1951727867126465, discriminator_loss=0.12772150337696075\n",
            "step 3741: generator_loss=2.203267812728882, discriminator_loss=0.12840953469276428\n",
            "step 3742: generator_loss=2.2796692848205566, discriminator_loss=0.1249922439455986\n",
            "step 3743: generator_loss=2.4620628356933594, discriminator_loss=0.11617739498615265\n",
            "step 3744: generator_loss=2.449277877807617, discriminator_loss=0.12044109404087067\n",
            "step 3745: generator_loss=2.4773173332214355, discriminator_loss=0.12564411759376526\n",
            "step 3746: generator_loss=2.5052852630615234, discriminator_loss=0.12354937195777893\n",
            "step 3747: generator_loss=2.5304534435272217, discriminator_loss=0.12646979093551636\n",
            "step 3748: generator_loss=2.6725568771362305, discriminator_loss=0.11924076825380325\n",
            "step 3749: generator_loss=2.5964126586914062, discriminator_loss=0.12117810547351837\n",
            "step 3750: generator_loss=2.6310009956359863, discriminator_loss=0.11765796691179276\n",
            "step 3751: generator_loss=2.7614734172821045, discriminator_loss=0.11642131209373474\n",
            "step 3752: generator_loss=2.658069133758545, discriminator_loss=0.1201193779706955\n",
            "step 3753: generator_loss=2.621082067489624, discriminator_loss=0.12274451553821564\n",
            "step 3754: generator_loss=2.619370937347412, discriminator_loss=0.12433872371912003\n",
            "step 3755: generator_loss=2.660885810852051, discriminator_loss=0.12329010665416718\n",
            "step 3756: generator_loss=2.7743332386016846, discriminator_loss=0.12199413776397705\n",
            "step 3757: generator_loss=2.7199013233184814, discriminator_loss=0.12935718894004822\n",
            "step 3758: generator_loss=2.6009433269500732, discriminator_loss=0.13189761340618134\n",
            "step 3759: generator_loss=2.583576202392578, discriminator_loss=0.13426300883293152\n",
            "step 3760: generator_loss=2.6643199920654297, discriminator_loss=0.1344393640756607\n",
            "step 3761: generator_loss=2.4522337913513184, discriminator_loss=0.136989027261734\n",
            "step 3762: generator_loss=3.0268394947052, discriminator_loss=0.11009345203638077\n",
            "step 3763: generator_loss=2.8287198543548584, discriminator_loss=0.11535344272851944\n",
            "step 3764: generator_loss=3.021446704864502, discriminator_loss=0.10299316793680191\n",
            "step 3765: generator_loss=3.0561599731445312, discriminator_loss=0.10054431110620499\n",
            "step 3766: generator_loss=3.0203638076782227, discriminator_loss=0.09761925041675568\n",
            "step 3767: generator_loss=3.28295636177063, discriminator_loss=0.09073017537593842\n",
            "step 3768: generator_loss=3.357495069503784, discriminator_loss=0.08701597154140472\n",
            "step 3769: generator_loss=3.6863908767700195, discriminator_loss=0.08372245728969574\n",
            "step 3770: generator_loss=3.454998016357422, discriminator_loss=0.08340218663215637\n",
            "step 3771: generator_loss=3.55310320854187, discriminator_loss=0.08419395983219147\n",
            "step 3772: generator_loss=3.535330295562744, discriminator_loss=0.0831434354186058\n",
            "step 3773: generator_loss=3.2417807579040527, discriminator_loss=0.08480724692344666\n",
            "step 3774: generator_loss=3.3393383026123047, discriminator_loss=0.08477618545293808\n",
            "step 3775: generator_loss=3.3693370819091797, discriminator_loss=0.08452406525611877\n",
            "step 3776: generator_loss=3.056774616241455, discriminator_loss=0.08703821897506714\n",
            "step 3777: generator_loss=3.0509822368621826, discriminator_loss=0.09383273124694824\n",
            "step 3778: generator_loss=2.8892080783843994, discriminator_loss=0.09552126377820969\n",
            "step 3779: generator_loss=2.8335485458374023, discriminator_loss=0.10172224044799805\n",
            "step 3780: generator_loss=2.787938356399536, discriminator_loss=0.10888859629631042\n",
            "step 3781: generator_loss=2.895453453063965, discriminator_loss=0.10981149226427078\n",
            "step 3782: generator_loss=3.0024337768554688, discriminator_loss=0.11205818504095078\n",
            "step 3783: generator_loss=3.013171434402466, discriminator_loss=0.12233923375606537\n",
            "step 3784: generator_loss=2.9762277603149414, discriminator_loss=0.12264548242092133\n",
            "step 3785: generator_loss=3.2706592082977295, discriminator_loss=0.1210995465517044\n",
            "step 3786: generator_loss=3.4142723083496094, discriminator_loss=0.10929735004901886\n",
            "step 3787: generator_loss=3.533379077911377, discriminator_loss=0.12746214866638184\n",
            "step 3788: generator_loss=3.7708473205566406, discriminator_loss=0.11454437673091888\n",
            "step 3789: generator_loss=3.9265055656433105, discriminator_loss=0.08848400413990021\n",
            "step 3790: generator_loss=3.865297794342041, discriminator_loss=0.10417604446411133\n",
            "step 3791: generator_loss=4.014253616333008, discriminator_loss=0.09003062546253204\n",
            "step 3792: generator_loss=4.572897434234619, discriminator_loss=0.07626973092556\n",
            "step 3793: generator_loss=4.282696723937988, discriminator_loss=0.08996756374835968\n",
            "step 3794: generator_loss=4.108819484710693, discriminator_loss=0.08919555693864822\n",
            "step 3795: generator_loss=3.73588490486145, discriminator_loss=0.09571198374032974\n",
            "step 3796: generator_loss=4.2300872802734375, discriminator_loss=0.0884322077035904\n",
            "step 3797: generator_loss=4.268939971923828, discriminator_loss=0.07903134077787399\n",
            "step 3798: generator_loss=4.052422523498535, discriminator_loss=0.07573193311691284\n",
            "step 3799: generator_loss=3.930026054382324, discriminator_loss=0.07877958565950394\n",
            "step 3800: generator_loss=3.5724782943725586, discriminator_loss=0.08317708224058151\n",
            "step 3801: generator_loss=3.6197996139526367, discriminator_loss=0.08091191947460175\n",
            "step 3802: generator_loss=3.5167460441589355, discriminator_loss=0.08419293910264969\n",
            "step 3803: generator_loss=3.1448516845703125, discriminator_loss=0.08920584619045258\n",
            "step 3804: generator_loss=2.853661060333252, discriminator_loss=0.09404172003269196\n",
            "step 3805: generator_loss=2.685680389404297, discriminator_loss=0.0992506891489029\n",
            "step 3806: generator_loss=2.646442413330078, discriminator_loss=0.10380075126886368\n",
            "step 3807: generator_loss=2.5674805641174316, discriminator_loss=0.10124486684799194\n",
            "step 3808: generator_loss=2.462376832962036, discriminator_loss=0.1039985716342926\n",
            "step 3809: generator_loss=2.3990907669067383, discriminator_loss=0.10741917788982391\n",
            "step 3810: generator_loss=2.318708896636963, discriminator_loss=0.11231622099876404\n",
            "step 3811: generator_loss=2.372424840927124, discriminator_loss=0.10910642147064209\n",
            "step 3812: generator_loss=2.3613193035125732, discriminator_loss=0.10909698903560638\n",
            "step 3813: generator_loss=2.582609176635742, discriminator_loss=0.10064657032489777\n",
            "step 3814: generator_loss=2.5257210731506348, discriminator_loss=0.10233235359191895\n",
            "step 3815: generator_loss=2.7220845222473145, discriminator_loss=0.09737862646579742\n",
            "step 3816: generator_loss=2.728949546813965, discriminator_loss=0.0986567884683609\n",
            "step 3817: generator_loss=2.7003073692321777, discriminator_loss=0.09862970560789108\n",
            "step 3818: generator_loss=2.68184757232666, discriminator_loss=0.096553273499012\n",
            "step 3819: generator_loss=2.7997546195983887, discriminator_loss=0.09852384030818939\n",
            "step 3820: generator_loss=2.669999122619629, discriminator_loss=0.10283239185810089\n",
            "step 3821: generator_loss=2.7559385299682617, discriminator_loss=0.10114145278930664\n",
            "step 3822: generator_loss=2.627652168273926, discriminator_loss=0.10364498198032379\n",
            "step 3823: generator_loss=2.5225377082824707, discriminator_loss=0.10535459965467453\n",
            "step 3824: generator_loss=2.6419625282287598, discriminator_loss=0.10559068620204926\n",
            "step 3825: generator_loss=2.571218252182007, discriminator_loss=0.11024122685194016\n",
            "step 3826: generator_loss=2.422657012939453, discriminator_loss=0.11404263973236084\n",
            "step 3827: generator_loss=2.537177562713623, discriminator_loss=0.11620597541332245\n",
            "step 3828: generator_loss=2.1815967559814453, discriminator_loss=0.12468081712722778\n",
            "step 3829: generator_loss=2.2603061199188232, discriminator_loss=0.12473106384277344\n",
            "step 3830: generator_loss=2.2571218013763428, discriminator_loss=0.12880226969718933\n",
            "step 3831: generator_loss=2.0213263034820557, discriminator_loss=0.13743332028388977\n",
            "step 3832: generator_loss=1.9108703136444092, discriminator_loss=0.14407645165920258\n",
            "step 3833: generator_loss=1.9674105644226074, discriminator_loss=0.14633303880691528\n",
            "step 3834: generator_loss=1.8286364078521729, discriminator_loss=0.15158379077911377\n",
            "step 3835: generator_loss=1.7960454225540161, discriminator_loss=0.1548582911491394\n",
            "step 3836: generator_loss=1.822004795074463, discriminator_loss=0.1572631448507309\n",
            "step 3837: generator_loss=1.778419852256775, discriminator_loss=0.15847784280776978\n",
            "step 3838: generator_loss=1.7961713075637817, discriminator_loss=0.15724721550941467\n",
            "step 3839: generator_loss=1.8197362422943115, discriminator_loss=0.1563147008419037\n",
            "step 3840: generator_loss=1.8682687282562256, discriminator_loss=0.15353262424468994\n",
            "step 3841: generator_loss=1.9786007404327393, discriminator_loss=0.14835327863693237\n",
            "step 3842: generator_loss=1.996938705444336, discriminator_loss=0.14686289429664612\n",
            "step 3843: generator_loss=1.9991438388824463, discriminator_loss=0.14698529243469238\n",
            "step 3844: generator_loss=2.025724411010742, discriminator_loss=0.14652693271636963\n",
            "step 3845: generator_loss=2.0190327167510986, discriminator_loss=0.14777959883213043\n",
            "step 3846: generator_loss=2.0366666316986084, discriminator_loss=0.14736482501029968\n",
            "step 3847: generator_loss=2.06199312210083, discriminator_loss=0.14608818292617798\n",
            "step 3848: generator_loss=2.0687758922576904, discriminator_loss=0.1461365967988968\n",
            "step 3849: generator_loss=2.062448024749756, discriminator_loss=0.14698565006256104\n",
            "step 3850: generator_loss=2.0572428703308105, discriminator_loss=0.14774948358535767\n",
            "step 3851: generator_loss=2.035663366317749, discriminator_loss=0.14929839968681335\n",
            "step 3852: generator_loss=2.0360469818115234, discriminator_loss=0.14929968118667603\n",
            "step 3853: generator_loss=2.017502784729004, discriminator_loss=0.15042337775230408\n",
            "step 3854: generator_loss=2.0093743801116943, discriminator_loss=0.15099281072616577\n",
            "step 3855: generator_loss=2.015871047973633, discriminator_loss=0.15008148550987244\n",
            "step 3856: generator_loss=2.022458076477051, discriminator_loss=0.14924213290214539\n",
            "step 3857: generator_loss=2.011570930480957, discriminator_loss=0.1495717167854309\n",
            "step 3858: generator_loss=2.0015010833740234, discriminator_loss=0.14961174130439758\n",
            "step 3859: generator_loss=1.9967031478881836, discriminator_loss=0.14932569861412048\n",
            "step 3860: generator_loss=2.002232551574707, discriminator_loss=0.1482084095478058\n",
            "step 3861: generator_loss=2.0020482540130615, discriminator_loss=0.14736898243427277\n",
            "step 3862: generator_loss=2.003474712371826, discriminator_loss=0.14652128517627716\n",
            "step 3863: generator_loss=2.009939670562744, discriminator_loss=0.14548981189727783\n",
            "step 3864: generator_loss=2.020278215408325, discriminator_loss=0.14414754509925842\n",
            "step 3865: generator_loss=2.050575017929077, discriminator_loss=0.14140944182872772\n",
            "step 3866: generator_loss=2.0824666023254395, discriminator_loss=0.13865603506565094\n",
            "step 3867: generator_loss=2.125014305114746, discriminator_loss=0.13531169295310974\n",
            "step 3868: generator_loss=2.15274715423584, discriminator_loss=0.13284868001937866\n",
            "step 3869: generator_loss=2.1755051612854004, discriminator_loss=0.13083431124687195\n",
            "step 3870: generator_loss=2.1778018474578857, discriminator_loss=0.12999475002288818\n",
            "step 3871: generator_loss=2.173081398010254, discriminator_loss=0.12940296530723572\n",
            "step 3872: generator_loss=2.166361093521118, discriminator_loss=0.12891626358032227\n",
            "step 3873: generator_loss=2.168870210647583, discriminator_loss=0.12794743478298187\n",
            "step 3874: generator_loss=2.160944938659668, discriminator_loss=0.12754705548286438\n",
            "step 3875: generator_loss=2.155784845352173, discriminator_loss=0.12716849148273468\n",
            "step 3876: generator_loss=2.168724536895752, discriminator_loss=0.1258438676595688\n",
            "step 3877: generator_loss=2.1732053756713867, discriminator_loss=0.12506982684135437\n",
            "step 3878: generator_loss=2.1680006980895996, discriminator_loss=0.12497319281101227\n",
            "step 3879: generator_loss=2.1730897426605225, discriminator_loss=0.1243034303188324\n",
            "step 3880: generator_loss=2.1734542846679688, discriminator_loss=0.12394587695598602\n",
            "step 3881: generator_loss=2.1630730628967285, discriminator_loss=0.12429188191890717\n",
            "step 3882: generator_loss=2.157623291015625, discriminator_loss=0.12446887791156769\n",
            "step 3883: generator_loss=2.154755115509033, discriminator_loss=0.12443149089813232\n",
            "step 3884: generator_loss=2.1499476432800293, discriminator_loss=0.1246643215417862\n",
            "step 3885: generator_loss=2.1363863945007324, discriminator_loss=0.1255732625722885\n",
            "step 3886: generator_loss=2.113373279571533, discriminator_loss=0.12708596885204315\n",
            "step 3887: generator_loss=2.0803284645080566, discriminator_loss=0.12935695052146912\n",
            "step 3888: generator_loss=2.0440001487731934, discriminator_loss=0.1322544515132904\n",
            "step 3889: generator_loss=1.9997897148132324, discriminator_loss=0.13568615913391113\n",
            "step 3890: generator_loss=1.962907314300537, discriminator_loss=0.1391811966896057\n",
            "step 3891: generator_loss=1.9388091564178467, discriminator_loss=0.14163771271705627\n",
            "step 3892: generator_loss=1.9268629550933838, discriminator_loss=0.14317119121551514\n",
            "step 3893: generator_loss=1.9309625625610352, discriminator_loss=0.14339789748191833\n",
            "step 3894: generator_loss=1.938807487487793, discriminator_loss=0.14325177669525146\n",
            "step 3895: generator_loss=1.9420387744903564, discriminator_loss=0.14361479878425598\n",
            "step 3896: generator_loss=1.9431413412094116, discriminator_loss=0.14393773674964905\n",
            "step 3897: generator_loss=1.9446693658828735, discriminator_loss=0.1445300579071045\n",
            "step 3898: generator_loss=1.954056978225708, discriminator_loss=0.14435788989067078\n",
            "step 3899: generator_loss=1.9829373359680176, discriminator_loss=0.14268311858177185\n",
            "step 3900: generator_loss=2.0146143436431885, discriminator_loss=0.1408727467060089\n",
            "step 3901: generator_loss=2.0325279235839844, discriminator_loss=0.1401534080505371\n",
            "step 3902: generator_loss=2.03741192817688, discriminator_loss=0.1406077742576599\n",
            "step 3903: generator_loss=2.028653144836426, discriminator_loss=0.14178819954395294\n",
            "step 3904: generator_loss=2.010140895843506, discriminator_loss=0.14406605064868927\n",
            "step 3905: generator_loss=2.007707118988037, discriminator_loss=0.14502805471420288\n",
            "step 3906: generator_loss=2.051273822784424, discriminator_loss=0.142449289560318\n",
            "step 3907: generator_loss=2.1555256843566895, discriminator_loss=0.13562992215156555\n",
            "step 3908: generator_loss=2.2810702323913574, discriminator_loss=0.12838347256183624\n",
            "step 3909: generator_loss=2.3622374534606934, discriminator_loss=0.1241123229265213\n",
            "step 3910: generator_loss=2.4350905418395996, discriminator_loss=0.12024123221635818\n",
            "step 3911: generator_loss=2.451575517654419, discriminator_loss=0.11935442686080933\n",
            "step 3912: generator_loss=2.472504138946533, discriminator_loss=0.11748316138982773\n",
            "step 3913: generator_loss=2.421088695526123, discriminator_loss=0.1196984276175499\n",
            "step 3914: generator_loss=2.424043893814087, discriminator_loss=0.1183253675699234\n",
            "step 3915: generator_loss=2.3477988243103027, discriminator_loss=0.1224023848772049\n",
            "step 3916: generator_loss=2.3657774925231934, discriminator_loss=0.11930302530527115\n",
            "step 3917: generator_loss=2.292264461517334, discriminator_loss=0.12444881349802017\n",
            "step 3918: generator_loss=2.173048257827759, discriminator_loss=0.1353999674320221\n",
            "step 3919: generator_loss=2.084822177886963, discriminator_loss=0.1480419635772705\n",
            "step 3920: generator_loss=2.0477895736694336, discriminator_loss=0.16007167100906372\n",
            "step 3921: generator_loss=2.1257286071777344, discriminator_loss=0.15896417200565338\n",
            "step 3922: generator_loss=2.075045347213745, discriminator_loss=0.17927882075309753\n",
            "step 3923: generator_loss=2.1775736808776855, discriminator_loss=0.1768677681684494\n",
            "step 3924: generator_loss=2.2223544120788574, discriminator_loss=0.19473186135292053\n",
            "step 3925: generator_loss=2.3079938888549805, discriminator_loss=0.18599560856819153\n",
            "step 3926: generator_loss=2.4007067680358887, discriminator_loss=0.18640904128551483\n",
            "step 3927: generator_loss=2.697545289993286, discriminator_loss=0.1563412994146347\n",
            "step 3928: generator_loss=2.5739192962646484, discriminator_loss=0.16838717460632324\n",
            "step 3929: generator_loss=2.6266329288482666, discriminator_loss=0.17639294266700745\n",
            "step 3930: generator_loss=2.6937179565429688, discriminator_loss=0.1688128411769867\n",
            "step 3931: generator_loss=2.824793815612793, discriminator_loss=0.14758998155593872\n",
            "step 3932: generator_loss=2.933703899383545, discriminator_loss=0.1354776918888092\n",
            "step 3933: generator_loss=2.7294199466705322, discriminator_loss=0.15126129984855652\n",
            "step 3934: generator_loss=2.9082953929901123, discriminator_loss=0.1323722004890442\n",
            "step 3935: generator_loss=3.0050911903381348, discriminator_loss=0.11631462723016739\n",
            "step 3936: generator_loss=3.1094157695770264, discriminator_loss=0.10882346332073212\n",
            "step 3937: generator_loss=3.3634204864501953, discriminator_loss=0.09748659282922745\n",
            "step 3938: generator_loss=3.195660352706909, discriminator_loss=0.09909211099147797\n",
            "step 3939: generator_loss=3.2395739555358887, discriminator_loss=0.09632159769535065\n",
            "step 3940: generator_loss=3.297153949737549, discriminator_loss=0.09192994982004166\n",
            "step 3941: generator_loss=3.2297396659851074, discriminator_loss=0.09276578575372696\n",
            "step 3942: generator_loss=3.214989423751831, discriminator_loss=0.0916765108704567\n",
            "step 3943: generator_loss=3.135633707046509, discriminator_loss=0.09237947314977646\n",
            "step 3944: generator_loss=2.997626781463623, discriminator_loss=0.09655722230672836\n",
            "step 3945: generator_loss=3.1121983528137207, discriminator_loss=0.09285728633403778\n",
            "step 3946: generator_loss=2.8463528156280518, discriminator_loss=0.10322951525449753\n",
            "step 3947: generator_loss=2.881516695022583, discriminator_loss=0.1000688225030899\n",
            "step 3948: generator_loss=2.7016148567199707, discriminator_loss=0.10788309574127197\n",
            "step 3949: generator_loss=2.6924362182617188, discriminator_loss=0.10634525120258331\n",
            "step 3950: generator_loss=2.6667537689208984, discriminator_loss=0.10713519155979156\n",
            "step 3951: generator_loss=2.5515594482421875, discriminator_loss=0.11187741160392761\n",
            "step 3952: generator_loss=2.562232732772827, discriminator_loss=0.11143700778484344\n",
            "step 3953: generator_loss=2.6236071586608887, discriminator_loss=0.10574504733085632\n",
            "step 3954: generator_loss=2.4203948974609375, discriminator_loss=0.11288751661777496\n",
            "step 3955: generator_loss=2.4405412673950195, discriminator_loss=0.1124778538942337\n",
            "step 3956: generator_loss=2.402317523956299, discriminator_loss=0.11311571300029755\n",
            "step 3957: generator_loss=2.3279707431793213, discriminator_loss=0.11636847257614136\n",
            "step 3958: generator_loss=2.2604310512542725, discriminator_loss=0.11973536014556885\n",
            "step 3959: generator_loss=2.314295530319214, discriminator_loss=0.1160128116607666\n",
            "step 3960: generator_loss=2.277034044265747, discriminator_loss=0.11794444173574448\n",
            "step 3961: generator_loss=2.3732128143310547, discriminator_loss=0.11205583810806274\n",
            "step 3962: generator_loss=2.1965675354003906, discriminator_loss=0.12152431905269623\n",
            "step 3963: generator_loss=2.2205405235290527, discriminator_loss=0.1193450316786766\n",
            "step 3964: generator_loss=2.3718390464782715, discriminator_loss=0.11646457761526108\n",
            "step 3965: generator_loss=2.264744281768799, discriminator_loss=0.11531754583120346\n",
            "step 3966: generator_loss=2.3523683547973633, discriminator_loss=0.11299379169940948\n",
            "step 3967: generator_loss=2.3165066242218018, discriminator_loss=0.11540703475475311\n",
            "step 3968: generator_loss=2.242839813232422, discriminator_loss=0.11667881906032562\n",
            "step 3969: generator_loss=2.2611494064331055, discriminator_loss=0.11635160446166992\n",
            "step 3970: generator_loss=2.258820056915283, discriminator_loss=0.11676165461540222\n",
            "step 3971: generator_loss=2.345778226852417, discriminator_loss=0.11541885137557983\n",
            "step 3972: generator_loss=2.2669782638549805, discriminator_loss=0.11741544306278229\n",
            "step 3973: generator_loss=2.18761944770813, discriminator_loss=0.12068019062280655\n",
            "step 3974: generator_loss=2.2081005573272705, discriminator_loss=0.12028852850198746\n",
            "step 3975: generator_loss=2.1872920989990234, discriminator_loss=0.1216123104095459\n",
            "step 3976: generator_loss=2.167072296142578, discriminator_loss=0.123999685049057\n",
            "step 3977: generator_loss=2.1174254417419434, discriminator_loss=0.1264411062002182\n",
            "step 3978: generator_loss=2.0426230430603027, discriminator_loss=0.13069231808185577\n",
            "step 3979: generator_loss=2.067589282989502, discriminator_loss=0.13066691160202026\n",
            "step 3980: generator_loss=1.9906996488571167, discriminator_loss=0.13472217321395874\n",
            "step 3981: generator_loss=1.9436169862747192, discriminator_loss=0.13788729906082153\n",
            "step 3982: generator_loss=1.941352367401123, discriminator_loss=0.13887393474578857\n",
            "step 3983: generator_loss=1.9528236389160156, discriminator_loss=0.1392498016357422\n",
            "step 3984: generator_loss=1.9287469387054443, discriminator_loss=0.14141002297401428\n",
            "step 3985: generator_loss=1.915146827697754, discriminator_loss=0.14312130212783813\n",
            "step 3986: generator_loss=1.9792548418045044, discriminator_loss=0.14141955971717834\n",
            "step 3987: generator_loss=1.8959083557128906, discriminator_loss=0.1465606987476349\n",
            "step 3988: generator_loss=1.9090386629104614, discriminator_loss=0.14667809009552002\n",
            "step 3989: generator_loss=1.8452726602554321, discriminator_loss=0.15203046798706055\n",
            "step 3990: generator_loss=1.8652012348175049, discriminator_loss=0.1524587869644165\n",
            "step 3991: generator_loss=1.8633058071136475, discriminator_loss=0.15332752466201782\n",
            "step 3992: generator_loss=1.8366626501083374, discriminator_loss=0.1563398540019989\n",
            "step 3993: generator_loss=1.8507386445999146, discriminator_loss=0.1575627326965332\n",
            "step 3994: generator_loss=1.796521782875061, discriminator_loss=0.16270792484283447\n",
            "step 3995: generator_loss=1.7739170789718628, discriminator_loss=0.16654840111732483\n",
            "step 3996: generator_loss=1.7537798881530762, discriminator_loss=0.1699429452419281\n",
            "step 3997: generator_loss=1.7754229307174683, discriminator_loss=0.1697959005832672\n",
            "step 3998: generator_loss=1.7695319652557373, discriminator_loss=0.17136302590370178\n",
            "step 3999: generator_loss=1.8018137216567993, discriminator_loss=0.16983124613761902\n",
            "step 4000: generator_loss=1.8653064966201782, discriminator_loss=0.16546911001205444\n",
            "step 4001: generator_loss=1.9387229681015015, discriminator_loss=0.16062785685062408\n",
            "step 4002: generator_loss=2.052110195159912, discriminator_loss=0.15312540531158447\n",
            "step 4003: generator_loss=2.213043212890625, discriminator_loss=0.1435665786266327\n",
            "step 4004: generator_loss=2.3586978912353516, discriminator_loss=0.1358972191810608\n",
            "step 4005: generator_loss=2.5314390659332275, discriminator_loss=0.12814182043075562\n",
            "step 4006: generator_loss=2.6605491638183594, discriminator_loss=0.1224629133939743\n",
            "step 4007: generator_loss=2.737945556640625, discriminator_loss=0.11910289525985718\n",
            "step 4008: generator_loss=2.7702231407165527, discriminator_loss=0.11644704639911652\n",
            "step 4009: generator_loss=2.781938314437866, discriminator_loss=0.11456470191478729\n",
            "step 4010: generator_loss=2.7896018028259277, discriminator_loss=0.11220524460077286\n",
            "step 4011: generator_loss=2.807858467102051, discriminator_loss=0.10960071533918381\n",
            "step 4012: generator_loss=2.7966206073760986, discriminator_loss=0.10767411440610886\n",
            "step 4013: generator_loss=2.752228260040283, discriminator_loss=0.106813944876194\n",
            "step 4014: generator_loss=2.702709436416626, discriminator_loss=0.10633479803800583\n",
            "step 4015: generator_loss=2.646135091781616, discriminator_loss=0.10635553300380707\n",
            "step 4016: generator_loss=2.5627102851867676, discriminator_loss=0.10755503177642822\n",
            "step 4017: generator_loss=2.4839887619018555, discriminator_loss=0.10906124860048294\n",
            "step 4018: generator_loss=2.39333438873291, discriminator_loss=0.11174501478672028\n",
            "step 4019: generator_loss=2.2961206436157227, discriminator_loss=0.11540952324867249\n",
            "step 4020: generator_loss=2.205432891845703, discriminator_loss=0.11954578012228012\n",
            "step 4021: generator_loss=2.1392898559570312, discriminator_loss=0.12292258441448212\n",
            "step 4022: generator_loss=2.1103248596191406, discriminator_loss=0.12419228255748749\n",
            "step 4023: generator_loss=2.096712350845337, discriminator_loss=0.12476258724927902\n",
            "step 4024: generator_loss=2.092891216278076, discriminator_loss=0.12491729855537415\n",
            "step 4025: generator_loss=2.095881938934326, discriminator_loss=0.12482647597789764\n",
            "step 4026: generator_loss=2.09366774559021, discriminator_loss=0.12511885166168213\n",
            "step 4027: generator_loss=2.0910444259643555, discriminator_loss=0.12551480531692505\n",
            "step 4028: generator_loss=2.0942859649658203, discriminator_loss=0.1257593333721161\n",
            "step 4029: generator_loss=2.11474347114563, discriminator_loss=0.12476050853729248\n",
            "step 4030: generator_loss=2.131685733795166, discriminator_loss=0.12425456196069717\n",
            "step 4031: generator_loss=2.1556918621063232, discriminator_loss=0.12330188602209091\n",
            "step 4032: generator_loss=2.1538467407226562, discriminator_loss=0.12397105991840363\n",
            "step 4033: generator_loss=2.147660255432129, discriminator_loss=0.1249898299574852\n",
            "step 4034: generator_loss=2.1375885009765625, discriminator_loss=0.12640246748924255\n",
            "step 4035: generator_loss=2.1184945106506348, discriminator_loss=0.12839283049106598\n",
            "step 4036: generator_loss=2.0853030681610107, discriminator_loss=0.13136757910251617\n",
            "step 4037: generator_loss=2.0592033863067627, discriminator_loss=0.13380348682403564\n",
            "step 4038: generator_loss=2.04899525642395, discriminator_loss=0.13515588641166687\n",
            "step 4039: generator_loss=2.072654962539673, discriminator_loss=0.1339138001203537\n",
            "step 4040: generator_loss=2.1008753776550293, discriminator_loss=0.13255661725997925\n",
            "step 4041: generator_loss=2.1577401161193848, discriminator_loss=0.12939786911010742\n",
            "step 4042: generator_loss=2.2248072624206543, discriminator_loss=0.12561199069023132\n",
            "step 4043: generator_loss=2.2668285369873047, discriminator_loss=0.12369448691606522\n",
            "step 4044: generator_loss=2.2989020347595215, discriminator_loss=0.12215854972600937\n",
            "step 4045: generator_loss=2.315138816833496, discriminator_loss=0.12163028120994568\n",
            "step 4046: generator_loss=2.3117241859436035, discriminator_loss=0.12203231453895569\n",
            "step 4047: generator_loss=2.2849442958831787, discriminator_loss=0.12367768585681915\n",
            "step 4048: generator_loss=2.232607364654541, discriminator_loss=0.12674033641815186\n",
            "step 4049: generator_loss=2.1662473678588867, discriminator_loss=0.13080158829689026\n",
            "step 4050: generator_loss=2.1034388542175293, discriminator_loss=0.13478101789951324\n",
            "step 4051: generator_loss=2.091437578201294, discriminator_loss=0.1355561912059784\n",
            "step 4052: generator_loss=2.1110165119171143, discriminator_loss=0.1342751681804657\n",
            "step 4053: generator_loss=2.1739611625671387, discriminator_loss=0.13001617789268494\n",
            "step 4054: generator_loss=2.2693750858306885, discriminator_loss=0.1240023821592331\n",
            "step 4055: generator_loss=2.3520846366882324, discriminator_loss=0.11914296448230743\n",
            "step 4056: generator_loss=2.4069247245788574, discriminator_loss=0.11611749976873398\n",
            "step 4057: generator_loss=2.4425134658813477, discriminator_loss=0.11390596628189087\n",
            "step 4058: generator_loss=2.46543025970459, discriminator_loss=0.11222000420093536\n",
            "step 4059: generator_loss=2.4853386878967285, discriminator_loss=0.11064957082271576\n",
            "step 4060: generator_loss=2.5085368156433105, discriminator_loss=0.10876668989658356\n",
            "step 4061: generator_loss=2.5198135375976562, discriminator_loss=0.10742668807506561\n",
            "step 4062: generator_loss=2.5268187522888184, discriminator_loss=0.10616748780012131\n",
            "step 4063: generator_loss=2.5253193378448486, discriminator_loss=0.10519522428512573\n",
            "step 4064: generator_loss=2.5115034580230713, discriminator_loss=0.10506100952625275\n",
            "step 4065: generator_loss=2.4785642623901367, discriminator_loss=0.10570384562015533\n",
            "step 4066: generator_loss=2.4198503494262695, discriminator_loss=0.10779879242181778\n",
            "step 4067: generator_loss=2.382567882537842, discriminator_loss=0.1090853214263916\n",
            "step 4068: generator_loss=2.350032329559326, discriminator_loss=0.11018656194210052\n",
            "step 4069: generator_loss=2.340440273284912, discriminator_loss=0.11021122336387634\n",
            "step 4070: generator_loss=2.348703384399414, discriminator_loss=0.10930763185024261\n",
            "step 4071: generator_loss=2.3380894660949707, discriminator_loss=0.1099872887134552\n",
            "step 4072: generator_loss=2.366136074066162, discriminator_loss=0.10815692692995071\n",
            "step 4073: generator_loss=2.347738742828369, discriminator_loss=0.1096634641289711\n",
            "step 4074: generator_loss=2.3552608489990234, discriminator_loss=0.10974081605672836\n",
            "step 4075: generator_loss=2.302490711212158, discriminator_loss=0.11654184758663177\n",
            "step 4076: generator_loss=2.3317058086395264, discriminator_loss=0.11729378998279572\n",
            "step 4077: generator_loss=2.3838629722595215, discriminator_loss=0.11942669004201889\n",
            "step 4078: generator_loss=2.4219281673431396, discriminator_loss=0.12297782301902771\n",
            "step 4079: generator_loss=2.383544921875, discriminator_loss=0.14063102006912231\n",
            "step 4080: generator_loss=2.415015697479248, discriminator_loss=0.16010743379592896\n",
            "step 4081: generator_loss=2.2738137245178223, discriminator_loss=0.19396555423736572\n",
            "step 4082: generator_loss=2.5166707038879395, discriminator_loss=0.1666431725025177\n",
            "step 4083: generator_loss=2.3999102115631104, discriminator_loss=0.20512446761131287\n",
            "step 4084: generator_loss=2.686405897140503, discriminator_loss=0.17094244062900543\n",
            "step 4085: generator_loss=2.7780826091766357, discriminator_loss=0.16590271890163422\n",
            "step 4086: generator_loss=2.812439441680908, discriminator_loss=0.16813533008098602\n",
            "step 4087: generator_loss=2.690343141555786, discriminator_loss=0.16984868049621582\n",
            "step 4088: generator_loss=3.0048182010650635, discriminator_loss=0.14163243770599365\n",
            "step 4089: generator_loss=3.4779233932495117, discriminator_loss=0.10930975526571274\n",
            "step 4090: generator_loss=3.540217876434326, discriminator_loss=0.09585969150066376\n",
            "step 4091: generator_loss=3.73919415473938, discriminator_loss=0.08983805775642395\n",
            "step 4092: generator_loss=3.910447120666504, discriminator_loss=0.08372209221124649\n",
            "step 4093: generator_loss=3.971954584121704, discriminator_loss=0.0811455175280571\n",
            "step 4094: generator_loss=4.172607898712158, discriminator_loss=0.07702349126338959\n",
            "step 4095: generator_loss=4.050167083740234, discriminator_loss=0.07922592759132385\n",
            "step 4096: generator_loss=4.111318588256836, discriminator_loss=0.07727114856243134\n",
            "step 4097: generator_loss=3.781372308731079, discriminator_loss=0.08021435141563416\n",
            "step 4098: generator_loss=3.774721145629883, discriminator_loss=0.08094090968370438\n",
            "step 4099: generator_loss=3.552473545074463, discriminator_loss=0.08426539599895477\n",
            "step 4100: generator_loss=3.4869775772094727, discriminator_loss=0.0834280401468277\n",
            "step 4101: generator_loss=3.1982154846191406, discriminator_loss=0.09368366003036499\n",
            "step 4102: generator_loss=3.1938576698303223, discriminator_loss=0.0913035124540329\n",
            "step 4103: generator_loss=2.95701265335083, discriminator_loss=0.09829595685005188\n",
            "step 4104: generator_loss=2.793952703475952, discriminator_loss=0.1069832593202591\n",
            "step 4105: generator_loss=2.758552074432373, discriminator_loss=0.10374210774898529\n",
            "step 4106: generator_loss=2.637850522994995, discriminator_loss=0.10313795506954193\n",
            "step 4107: generator_loss=2.7600901126861572, discriminator_loss=0.10168416053056717\n",
            "step 4108: generator_loss=2.626783847808838, discriminator_loss=0.10332980006933212\n",
            "step 4109: generator_loss=2.455254554748535, discriminator_loss=0.10658562183380127\n",
            "step 4110: generator_loss=2.412155866622925, discriminator_loss=0.10441799461841583\n",
            "step 4111: generator_loss=2.4211277961730957, discriminator_loss=0.10586726665496826\n",
            "step 4112: generator_loss=2.4342384338378906, discriminator_loss=0.10422112792730331\n",
            "step 4113: generator_loss=2.3700056076049805, discriminator_loss=0.10909278690814972\n",
            "step 4114: generator_loss=2.212461471557617, discriminator_loss=0.11482210457324982\n",
            "step 4115: generator_loss=2.18668270111084, discriminator_loss=0.11606001853942871\n",
            "step 4116: generator_loss=2.183607816696167, discriminator_loss=0.11575638502836227\n",
            "step 4117: generator_loss=2.1720523834228516, discriminator_loss=0.11736181378364563\n",
            "step 4118: generator_loss=2.2537689208984375, discriminator_loss=0.11537298560142517\n",
            "step 4119: generator_loss=2.2274246215820312, discriminator_loss=0.1161736398935318\n",
            "step 4120: generator_loss=2.3748574256896973, discriminator_loss=0.1123526319861412\n",
            "step 4121: generator_loss=2.2451906204223633, discriminator_loss=0.11643701046705246\n",
            "step 4122: generator_loss=2.286151647567749, discriminator_loss=0.11587648093700409\n",
            "step 4123: generator_loss=2.245541572570801, discriminator_loss=0.11787615716457367\n",
            "step 4124: generator_loss=2.1906063556671143, discriminator_loss=0.12042940407991409\n",
            "step 4125: generator_loss=2.1804962158203125, discriminator_loss=0.12147868424654007\n",
            "step 4126: generator_loss=2.142559289932251, discriminator_loss=0.12334524095058441\n",
            "step 4127: generator_loss=2.1195969581604004, discriminator_loss=0.12462083995342255\n",
            "step 4128: generator_loss=2.040175437927246, discriminator_loss=0.12951546907424927\n",
            "step 4129: generator_loss=2.0319037437438965, discriminator_loss=0.13036563992500305\n",
            "step 4130: generator_loss=2.015589714050293, discriminator_loss=0.13261494040489197\n",
            "step 4131: generator_loss=1.9854683876037598, discriminator_loss=0.13409483432769775\n",
            "step 4132: generator_loss=1.9869773387908936, discriminator_loss=0.13479045033454895\n",
            "step 4133: generator_loss=1.9857795238494873, discriminator_loss=0.13438208401203156\n",
            "step 4134: generator_loss=1.9615530967712402, discriminator_loss=0.13624553382396698\n",
            "step 4135: generator_loss=1.9819562435150146, discriminator_loss=0.13527947664260864\n",
            "step 4136: generator_loss=1.988917589187622, discriminator_loss=0.13512378931045532\n",
            "step 4137: generator_loss=1.996179223060608, discriminator_loss=0.13499803841114044\n",
            "step 4138: generator_loss=1.9774808883666992, discriminator_loss=0.13703006505966187\n",
            "step 4139: generator_loss=1.986835241317749, discriminator_loss=0.13710972666740417\n",
            "step 4140: generator_loss=2.041459560394287, discriminator_loss=0.13347157835960388\n",
            "step 4141: generator_loss=2.120694160461426, discriminator_loss=0.12853918969631195\n",
            "step 4142: generator_loss=2.212890148162842, discriminator_loss=0.12331528961658478\n",
            "step 4143: generator_loss=2.313823699951172, discriminator_loss=0.11839012801647186\n",
            "step 4144: generator_loss=2.3537917137145996, discriminator_loss=0.11627375334501266\n",
            "step 4145: generator_loss=2.3799777030944824, discriminator_loss=0.1152307540178299\n",
            "step 4146: generator_loss=2.396130323410034, discriminator_loss=0.11433159559965134\n",
            "step 4147: generator_loss=2.41632080078125, discriminator_loss=0.11383510380983353\n",
            "step 4148: generator_loss=2.401294469833374, discriminator_loss=0.113745778799057\n",
            "step 4149: generator_loss=2.3968558311462402, discriminator_loss=0.11370065808296204\n",
            "step 4150: generator_loss=2.3895070552825928, discriminator_loss=0.11343534290790558\n",
            "step 4151: generator_loss=2.366992950439453, discriminator_loss=0.1140715479850769\n",
            "step 4152: generator_loss=2.370728015899658, discriminator_loss=0.11337241530418396\n",
            "step 4153: generator_loss=2.3731579780578613, discriminator_loss=0.1127934679389\n",
            "step 4154: generator_loss=2.358306884765625, discriminator_loss=0.11294261366128922\n",
            "step 4155: generator_loss=2.3618383407592773, discriminator_loss=0.11219558864831924\n",
            "step 4156: generator_loss=2.361621379852295, discriminator_loss=0.11174209415912628\n",
            "step 4157: generator_loss=2.3426718711853027, discriminator_loss=0.11238057911396027\n",
            "step 4158: generator_loss=2.3227179050445557, discriminator_loss=0.1129620224237442\n",
            "step 4159: generator_loss=2.30766224861145, discriminator_loss=0.11340685933828354\n",
            "step 4160: generator_loss=2.2780232429504395, discriminator_loss=0.11450451612472534\n",
            "step 4161: generator_loss=2.2667078971862793, discriminator_loss=0.11482037603855133\n",
            "step 4162: generator_loss=2.291977882385254, discriminator_loss=0.11309303343296051\n",
            "step 4163: generator_loss=2.292649984359741, discriminator_loss=0.11280485987663269\n",
            "step 4164: generator_loss=2.276857852935791, discriminator_loss=0.11342541873455048\n",
            "step 4165: generator_loss=2.245506525039673, discriminator_loss=0.11526671051979065\n",
            "step 4166: generator_loss=2.2076897621154785, discriminator_loss=0.11758163571357727\n",
            "step 4167: generator_loss=2.180349588394165, discriminator_loss=0.11955808848142624\n",
            "step 4168: generator_loss=2.184277296066284, discriminator_loss=0.11949902772903442\n",
            "step 4169: generator_loss=2.20175838470459, discriminator_loss=0.11899194121360779\n",
            "step 4170: generator_loss=2.2258362770080566, discriminator_loss=0.11786897480487823\n",
            "step 4171: generator_loss=2.2399959564208984, discriminator_loss=0.1174144372344017\n",
            "step 4172: generator_loss=2.2539520263671875, discriminator_loss=0.11715728044509888\n",
            "step 4173: generator_loss=2.2515251636505127, discriminator_loss=0.11760741472244263\n",
            "step 4174: generator_loss=2.2427120208740234, discriminator_loss=0.11855797469615936\n",
            "step 4175: generator_loss=2.2322630882263184, discriminator_loss=0.11966830492019653\n",
            "step 4176: generator_loss=2.208137035369873, discriminator_loss=0.12150764465332031\n",
            "step 4177: generator_loss=2.175522804260254, discriminator_loss=0.12379536032676697\n",
            "step 4178: generator_loss=2.1382570266723633, discriminator_loss=0.12633636593818665\n",
            "step 4179: generator_loss=2.098177433013916, discriminator_loss=0.12911224365234375\n",
            "step 4180: generator_loss=2.0608067512512207, discriminator_loss=0.13180437684059143\n",
            "step 4181: generator_loss=2.0584185123443604, discriminator_loss=0.13210821151733398\n",
            "step 4182: generator_loss=2.091603994369507, discriminator_loss=0.12968966364860535\n",
            "step 4183: generator_loss=2.1332192420959473, discriminator_loss=0.1270211935043335\n",
            "step 4184: generator_loss=2.183346748352051, discriminator_loss=0.12364286184310913\n",
            "step 4185: generator_loss=2.2249584197998047, discriminator_loss=0.12111279368400574\n",
            "step 4186: generator_loss=2.2630043029785156, discriminator_loss=0.11868122220039368\n",
            "step 4187: generator_loss=2.2906155586242676, discriminator_loss=0.11703554540872574\n",
            "step 4188: generator_loss=2.322638988494873, discriminator_loss=0.11502307653427124\n",
            "step 4189: generator_loss=2.3404650688171387, discriminator_loss=0.11379780620336533\n",
            "step 4190: generator_loss=2.3424668312072754, discriminator_loss=0.1136242002248764\n",
            "step 4191: generator_loss=2.359010696411133, discriminator_loss=0.11250288784503937\n",
            "step 4192: generator_loss=2.3771371841430664, discriminator_loss=0.11139091849327087\n",
            "step 4193: generator_loss=2.381120443344116, discriminator_loss=0.11083093285560608\n",
            "step 4194: generator_loss=2.3682618141174316, discriminator_loss=0.11129781603813171\n",
            "step 4195: generator_loss=2.3488762378692627, discriminator_loss=0.1119346097111702\n",
            "step 4196: generator_loss=2.3501899242401123, discriminator_loss=0.11145289987325668\n",
            "step 4197: generator_loss=2.371070384979248, discriminator_loss=0.11002091318368912\n",
            "step 4198: generator_loss=2.4022796154022217, discriminator_loss=0.10803765058517456\n",
            "step 4199: generator_loss=2.4298601150512695, discriminator_loss=0.10634031891822815\n",
            "step 4200: generator_loss=2.463751792907715, discriminator_loss=0.10429637134075165\n",
            "step 4201: generator_loss=2.4916210174560547, discriminator_loss=0.10250533372163773\n",
            "step 4202: generator_loss=2.5159506797790527, discriminator_loss=0.1009950190782547\n",
            "step 4203: generator_loss=2.52571964263916, discriminator_loss=0.10011927783489227\n",
            "step 4204: generator_loss=2.5450448989868164, discriminator_loss=0.09865880012512207\n",
            "step 4205: generator_loss=2.581540107727051, discriminator_loss=0.09660886228084564\n",
            "step 4206: generator_loss=2.625049352645874, discriminator_loss=0.09437521547079086\n",
            "step 4207: generator_loss=2.6801600456237793, discriminator_loss=0.09164425730705261\n",
            "step 4208: generator_loss=2.7313809394836426, discriminator_loss=0.08914855122566223\n",
            "step 4209: generator_loss=2.7631752490997314, discriminator_loss=0.0874786376953125\n",
            "step 4210: generator_loss=2.7827162742614746, discriminator_loss=0.0860370546579361\n",
            "step 4211: generator_loss=2.775630474090576, discriminator_loss=0.08574976772069931\n",
            "step 4212: generator_loss=2.7468323707580566, discriminator_loss=0.08604299277067184\n",
            "step 4213: generator_loss=2.709433078765869, discriminator_loss=0.08668109774589539\n",
            "step 4214: generator_loss=2.684556007385254, discriminator_loss=0.08688879758119583\n",
            "step 4215: generator_loss=2.654995918273926, discriminator_loss=0.08737485110759735\n",
            "step 4216: generator_loss=2.6317853927612305, discriminator_loss=0.08780762553215027\n",
            "step 4217: generator_loss=2.622159004211426, discriminator_loss=0.08769246935844421\n",
            "step 4218: generator_loss=2.6146440505981445, discriminator_loss=0.0875401645898819\n",
            "step 4219: generator_loss=2.6065025329589844, discriminator_loss=0.0874759778380394\n",
            "step 4220: generator_loss=2.5993146896362305, discriminator_loss=0.08737653493881226\n",
            "step 4221: generator_loss=2.584463119506836, discriminator_loss=0.0877581536769867\n",
            "step 4222: generator_loss=2.570312738418579, discriminator_loss=0.08800004422664642\n",
            "step 4223: generator_loss=2.5508809089660645, discriminator_loss=0.08860699832439423\n",
            "step 4224: generator_loss=2.533116340637207, discriminator_loss=0.089229054749012\n",
            "step 4225: generator_loss=2.536630630493164, discriminator_loss=0.08897767961025238\n",
            "step 4226: generator_loss=2.563136577606201, discriminator_loss=0.08780967444181442\n",
            "step 4227: generator_loss=2.5994577407836914, discriminator_loss=0.08637697249650955\n",
            "step 4228: generator_loss=2.6322743892669678, discriminator_loss=0.08510904014110565\n",
            "step 4229: generator_loss=2.665245532989502, discriminator_loss=0.08391577750444412\n",
            "step 4230: generator_loss=2.6895623207092285, discriminator_loss=0.08300640434026718\n",
            "step 4231: generator_loss=2.7154457569122314, discriminator_loss=0.08204320073127747\n",
            "step 4232: generator_loss=2.7296175956726074, discriminator_loss=0.08151863515377045\n",
            "step 4233: generator_loss=2.735521078109741, discriminator_loss=0.08123394846916199\n",
            "step 4234: generator_loss=2.7513790130615234, discriminator_loss=0.0806371420621872\n",
            "step 4235: generator_loss=2.757347583770752, discriminator_loss=0.08035551011562347\n",
            "step 4236: generator_loss=2.7728638648986816, discriminator_loss=0.07978422939777374\n",
            "step 4237: generator_loss=2.771697759628296, discriminator_loss=0.07977299392223358\n",
            "step 4238: generator_loss=2.7861578464508057, discriminator_loss=0.07916420698165894\n",
            "step 4239: generator_loss=2.795065402984619, discriminator_loss=0.0787355825304985\n",
            "step 4240: generator_loss=2.790426254272461, discriminator_loss=0.07876282185316086\n",
            "step 4241: generator_loss=2.7831902503967285, discriminator_loss=0.07886868715286255\n",
            "step 4242: generator_loss=2.788698196411133, discriminator_loss=0.07848532497882843\n",
            "step 4243: generator_loss=2.8054661750793457, discriminator_loss=0.07780168205499649\n",
            "step 4244: generator_loss=2.8097479343414307, discriminator_loss=0.07744661718606949\n",
            "step 4245: generator_loss=2.8037424087524414, discriminator_loss=0.07746585458517075\n",
            "step 4246: generator_loss=2.7821245193481445, discriminator_loss=0.07799462229013443\n",
            "step 4247: generator_loss=2.7600631713867188, discriminator_loss=0.07851086556911469\n",
            "step 4248: generator_loss=2.766522169113159, discriminator_loss=0.07819079607725143\n",
            "step 4249: generator_loss=2.752324342727661, discriminator_loss=0.07848771661520004\n",
            "step 4250: generator_loss=2.7327003479003906, discriminator_loss=0.0790313184261322\n",
            "step 4251: generator_loss=2.723745107650757, discriminator_loss=0.07917293161153793\n",
            "step 4252: generator_loss=2.7099685668945312, discriminator_loss=0.07952126860618591\n",
            "step 4253: generator_loss=2.7106735706329346, discriminator_loss=0.0794433057308197\n",
            "step 4254: generator_loss=2.7079663276672363, discriminator_loss=0.07938829064369202\n",
            "step 4255: generator_loss=2.702359914779663, discriminator_loss=0.07950666546821594\n",
            "step 4256: generator_loss=2.724346399307251, discriminator_loss=0.07862695306539536\n",
            "step 4257: generator_loss=2.7418038845062256, discriminator_loss=0.07789098471403122\n",
            "step 4258: generator_loss=2.757389545440674, discriminator_loss=0.07728536427021027\n",
            "step 4259: generator_loss=2.786041736602783, discriminator_loss=0.07622594386339188\n",
            "step 4260: generator_loss=2.8460166454315186, discriminator_loss=0.07416532933712006\n",
            "step 4261: generator_loss=2.900564670562744, discriminator_loss=0.07241977751255035\n",
            "step 4262: generator_loss=2.982879877090454, discriminator_loss=0.07000607997179031\n",
            "step 4263: generator_loss=3.0535595417022705, discriminator_loss=0.06802279502153397\n",
            "step 4264: generator_loss=3.085585117340088, discriminator_loss=0.06716037541627884\n",
            "step 4265: generator_loss=3.0828168392181396, discriminator_loss=0.06704257428646088\n",
            "step 4266: generator_loss=3.0458672046661377, discriminator_loss=0.06770443916320801\n",
            "step 4267: generator_loss=3.017446517944336, discriminator_loss=0.06824667751789093\n",
            "step 4268: generator_loss=2.978961944580078, discriminator_loss=0.06903713196516037\n",
            "step 4269: generator_loss=2.9302966594696045, discriminator_loss=0.07015226781368256\n",
            "step 4270: generator_loss=2.912301540374756, discriminator_loss=0.07047303020954132\n",
            "step 4271: generator_loss=2.90323805809021, discriminator_loss=0.0704849511384964\n",
            "step 4272: generator_loss=2.8960390090942383, discriminator_loss=0.07051465660333633\n",
            "step 4273: generator_loss=2.8735764026641846, discriminator_loss=0.07097189128398895\n",
            "step 4274: generator_loss=2.8512048721313477, discriminator_loss=0.0715358704328537\n",
            "step 4275: generator_loss=2.816513776779175, discriminator_loss=0.07241073250770569\n",
            "step 4276: generator_loss=2.783290147781372, discriminator_loss=0.07339571416378021\n",
            "step 4277: generator_loss=2.78426194190979, discriminator_loss=0.07325087487697601\n",
            "step 4278: generator_loss=2.789105176925659, discriminator_loss=0.07307262718677521\n",
            "step 4279: generator_loss=2.78298020362854, discriminator_loss=0.0731213390827179\n",
            "step 4280: generator_loss=2.780839204788208, discriminator_loss=0.07307900488376617\n",
            "step 4281: generator_loss=2.7856884002685547, discriminator_loss=0.07284233719110489\n",
            "step 4282: generator_loss=2.8002865314483643, discriminator_loss=0.07228045165538788\n",
            "step 4283: generator_loss=2.8599042892456055, discriminator_loss=0.07032590359449387\n",
            "step 4284: generator_loss=2.917266368865967, discriminator_loss=0.06854735314846039\n",
            "step 4285: generator_loss=2.9552295207977295, discriminator_loss=0.06740057468414307\n",
            "step 4286: generator_loss=2.985731840133667, discriminator_loss=0.06647894531488419\n",
            "step 4287: generator_loss=2.990618944168091, discriminator_loss=0.06626991927623749\n",
            "step 4288: generator_loss=2.97192645072937, discriminator_loss=0.06669024378061295\n",
            "step 4289: generator_loss=2.9457790851593018, discriminator_loss=0.06724948436021805\n",
            "step 4290: generator_loss=2.907945156097412, discriminator_loss=0.06817828863859177\n",
            "step 4291: generator_loss=2.8547682762145996, discriminator_loss=0.0696321427822113\n",
            "step 4292: generator_loss=2.8022289276123047, discriminator_loss=0.07109680026769638\n",
            "step 4293: generator_loss=2.7490134239196777, discriminator_loss=0.07275721430778503\n",
            "step 4294: generator_loss=2.707887649536133, discriminator_loss=0.0740753561258316\n",
            "step 4295: generator_loss=2.66916561126709, discriminator_loss=0.07543345540761948\n",
            "step 4296: generator_loss=2.6458308696746826, discriminator_loss=0.0762416273355484\n",
            "step 4297: generator_loss=2.661651134490967, discriminator_loss=0.07559479027986526\n",
            "step 4298: generator_loss=2.7001001834869385, discriminator_loss=0.07415211200714111\n",
            "step 4299: generator_loss=2.7236075401306152, discriminator_loss=0.07325795292854309\n",
            "step 4300: generator_loss=2.730083465576172, discriminator_loss=0.07304873317480087\n",
            "step 4301: generator_loss=2.717421531677246, discriminator_loss=0.07349439710378647\n",
            "step 4302: generator_loss=2.688059091567993, discriminator_loss=0.07461749017238617\n",
            "step 4303: generator_loss=2.6795074939727783, discriminator_loss=0.07495541870594025\n",
            "step 4304: generator_loss=2.6771154403686523, discriminator_loss=0.07509838044643402\n",
            "step 4305: generator_loss=2.6680054664611816, discriminator_loss=0.07553128898143768\n",
            "step 4306: generator_loss=2.657496452331543, discriminator_loss=0.07597919553518295\n",
            "step 4307: generator_loss=2.665945291519165, discriminator_loss=0.07582160830497742\n",
            "step 4308: generator_loss=2.665304183959961, discriminator_loss=0.0759502649307251\n",
            "step 4309: generator_loss=2.657106876373291, discriminator_loss=0.07635192573070526\n",
            "step 4310: generator_loss=2.6366982460021973, discriminator_loss=0.07713820040225983\n",
            "step 4311: generator_loss=2.619030714035034, discriminator_loss=0.07784251868724823\n",
            "step 4312: generator_loss=2.602935314178467, discriminator_loss=0.07839837670326233\n",
            "step 4313: generator_loss=2.570166826248169, discriminator_loss=0.07979264855384827\n",
            "step 4314: generator_loss=2.547003746032715, discriminator_loss=0.0807207003235817\n",
            "step 4315: generator_loss=2.535491466522217, discriminator_loss=0.08112543821334839\n",
            "step 4316: generator_loss=2.547837018966675, discriminator_loss=0.08055785298347473\n",
            "step 4317: generator_loss=2.5728604793548584, discriminator_loss=0.07952127605676651\n",
            "step 4318: generator_loss=2.598148822784424, discriminator_loss=0.0785338431596756\n",
            "step 4319: generator_loss=2.6078171730041504, discriminator_loss=0.07811659574508667\n",
            "step 4320: generator_loss=2.6306257247924805, discriminator_loss=0.07720968127250671\n",
            "step 4321: generator_loss=2.6598315238952637, discriminator_loss=0.07605317234992981\n",
            "step 4322: generator_loss=2.674082040786743, discriminator_loss=0.07559078931808472\n",
            "step 4323: generator_loss=2.673078775405884, discriminator_loss=0.07570146024227142\n",
            "step 4324: generator_loss=2.6859781742095947, discriminator_loss=0.0752091184258461\n",
            "step 4325: generator_loss=2.6918697357177734, discriminator_loss=0.07498881220817566\n",
            "step 4326: generator_loss=2.6845712661743164, discriminator_loss=0.0752796158194542\n",
            "step 4327: generator_loss=2.674039363861084, discriminator_loss=0.07574018836021423\n",
            "step 4328: generator_loss=2.6611201763153076, discriminator_loss=0.0762186348438263\n",
            "step 4329: generator_loss=2.6351187229156494, discriminator_loss=0.07728744298219681\n",
            "step 4330: generator_loss=2.5905842781066895, discriminator_loss=0.07900966703891754\n",
            "step 4331: generator_loss=2.558586597442627, discriminator_loss=0.08043905347585678\n",
            "step 4332: generator_loss=2.5282981395721436, discriminator_loss=0.08178199827671051\n",
            "step 4333: generator_loss=2.4899635314941406, discriminator_loss=0.08353909105062485\n",
            "step 4334: generator_loss=2.4479198455810547, discriminator_loss=0.08555205166339874\n",
            "step 4335: generator_loss=2.424002170562744, discriminator_loss=0.08673462271690369\n",
            "step 4336: generator_loss=2.4413976669311523, discriminator_loss=0.08601678907871246\n",
            "step 4337: generator_loss=2.4805829524993896, discriminator_loss=0.08429531753063202\n",
            "step 4338: generator_loss=2.5111083984375, discriminator_loss=0.08308064192533493\n",
            "step 4339: generator_loss=2.5435290336608887, discriminator_loss=0.08179739117622375\n",
            "step 4340: generator_loss=2.5779671669006348, discriminator_loss=0.08055635541677475\n",
            "step 4341: generator_loss=2.6000466346740723, discriminator_loss=0.07993318140506744\n",
            "step 4342: generator_loss=2.6166458129882812, discriminator_loss=0.07944485545158386\n",
            "step 4343: generator_loss=2.6115899085998535, discriminator_loss=0.07984428852796555\n",
            "step 4344: generator_loss=2.5858259201049805, discriminator_loss=0.08094356954097748\n",
            "step 4345: generator_loss=2.5498719215393066, discriminator_loss=0.08259332925081253\n",
            "step 4346: generator_loss=2.5075106620788574, discriminator_loss=0.08459217846393585\n",
            "step 4347: generator_loss=2.4802074432373047, discriminator_loss=0.08583928644657135\n",
            "step 4348: generator_loss=2.457434892654419, discriminator_loss=0.08696393668651581\n",
            "step 4349: generator_loss=2.4486775398254395, discriminator_loss=0.08750664442777634\n",
            "step 4350: generator_loss=2.4277420043945312, discriminator_loss=0.08870431780815125\n",
            "step 4351: generator_loss=2.3992247581481934, discriminator_loss=0.090200275182724\n",
            "step 4352: generator_loss=2.38728666305542, discriminator_loss=0.09093154966831207\n",
            "step 4353: generator_loss=2.369474411010742, discriminator_loss=0.09208008646965027\n",
            "step 4354: generator_loss=2.3638756275177, discriminator_loss=0.09253201633691788\n",
            "step 4355: generator_loss=2.3535730838775635, discriminator_loss=0.09331370890140533\n",
            "step 4356: generator_loss=2.336280584335327, discriminator_loss=0.09449975192546844\n",
            "step 4357: generator_loss=2.357473850250244, discriminator_loss=0.09363946318626404\n",
            "step 4358: generator_loss=2.4607174396514893, discriminator_loss=0.08872410655021667\n",
            "step 4359: generator_loss=2.5822224617004395, discriminator_loss=0.08348135650157928\n",
            "step 4360: generator_loss=2.7076685428619385, discriminator_loss=0.07897280156612396\n",
            "step 4361: generator_loss=2.7974281311035156, discriminator_loss=0.07591596245765686\n",
            "step 4362: generator_loss=2.874001979827881, discriminator_loss=0.07349704205989838\n",
            "step 4363: generator_loss=2.932175636291504, discriminator_loss=0.07176201045513153\n",
            "step 4364: generator_loss=2.966662645339966, discriminator_loss=0.07067688554525375\n",
            "step 4365: generator_loss=2.9608213901519775, discriminator_loss=0.07061867415904999\n",
            "step 4366: generator_loss=2.9334969520568848, discriminator_loss=0.07107929140329361\n",
            "step 4367: generator_loss=2.927286148071289, discriminator_loss=0.07090619206428528\n",
            "step 4368: generator_loss=2.895583152770996, discriminator_loss=0.07149018347263336\n",
            "step 4369: generator_loss=2.8374974727630615, discriminator_loss=0.07281474024057388\n",
            "step 4370: generator_loss=2.7589211463928223, discriminator_loss=0.07493605464696884\n",
            "step 4371: generator_loss=2.6790738105773926, discriminator_loss=0.07747168838977814\n",
            "step 4372: generator_loss=2.6093077659606934, discriminator_loss=0.07979331910610199\n",
            "step 4373: generator_loss=2.5746657848358154, discriminator_loss=0.0808752253651619\n",
            "step 4374: generator_loss=2.573315382003784, discriminator_loss=0.08072777092456818\n",
            "step 4375: generator_loss=2.5943551063537598, discriminator_loss=0.07977136224508286\n",
            "step 4376: generator_loss=2.629136085510254, discriminator_loss=0.07826772332191467\n",
            "step 4377: generator_loss=2.649552583694458, discriminator_loss=0.07744327187538147\n",
            "step 4378: generator_loss=2.6649539470672607, discriminator_loss=0.07671504467725754\n",
            "step 4379: generator_loss=2.705219268798828, discriminator_loss=0.0751376673579216\n",
            "step 4380: generator_loss=2.7738332748413086, discriminator_loss=0.07276006788015366\n",
            "step 4381: generator_loss=2.8285961151123047, discriminator_loss=0.07084397971630096\n",
            "step 4382: generator_loss=2.8638827800750732, discriminator_loss=0.06960787624120712\n",
            "step 4383: generator_loss=2.8944835662841797, discriminator_loss=0.06858759373426437\n",
            "step 4384: generator_loss=2.8802690505981445, discriminator_loss=0.06880607455968857\n",
            "step 4385: generator_loss=2.8672428131103516, discriminator_loss=0.06915108859539032\n",
            "step 4386: generator_loss=2.833167552947998, discriminator_loss=0.07001309841871262\n",
            "step 4387: generator_loss=2.788478374481201, discriminator_loss=0.07139184325933456\n",
            "step 4388: generator_loss=2.777975082397461, discriminator_loss=0.07165402173995972\n",
            "step 4389: generator_loss=2.758634567260742, discriminator_loss=0.07250680774450302\n",
            "step 4390: generator_loss=2.7288174629211426, discriminator_loss=0.07427182793617249\n",
            "step 4391: generator_loss=2.740234375, discriminator_loss=0.07529771327972412\n",
            "step 4392: generator_loss=2.6438848972320557, discriminator_loss=0.08237363398075104\n",
            "step 4393: generator_loss=2.622293710708618, discriminator_loss=0.08753490447998047\n",
            "step 4394: generator_loss=2.578909397125244, discriminator_loss=0.0991906225681305\n",
            "step 4395: generator_loss=2.5584750175476074, discriminator_loss=0.11481505632400513\n",
            "step 4396: generator_loss=2.5115487575531006, discriminator_loss=0.13518530130386353\n",
            "step 4397: generator_loss=2.554161310195923, discriminator_loss=0.15828225016593933\n",
            "step 4398: generator_loss=2.7406697273254395, discriminator_loss=0.15440452098846436\n",
            "step 4399: generator_loss=2.5333199501037598, discriminator_loss=0.1935996413230896\n",
            "step 4400: generator_loss=2.733778953552246, discriminator_loss=0.18086180090904236\n",
            "step 4401: generator_loss=3.357520580291748, discriminator_loss=0.11705251038074493\n",
            "step 4402: generator_loss=3.2345871925354004, discriminator_loss=0.13460350036621094\n",
            "step 4403: generator_loss=3.1438088417053223, discriminator_loss=0.12722977995872498\n",
            "step 4404: generator_loss=3.3959343433380127, discriminator_loss=0.11093329638242722\n",
            "step 4405: generator_loss=3.6422183513641357, discriminator_loss=0.09762541204690933\n",
            "step 4406: generator_loss=3.9147751331329346, discriminator_loss=0.08424930274486542\n",
            "step 4407: generator_loss=3.988577365875244, discriminator_loss=0.07598497718572617\n",
            "step 4408: generator_loss=4.143856048583984, discriminator_loss=0.07009220123291016\n",
            "step 4409: generator_loss=4.433774471282959, discriminator_loss=0.06384864449501038\n",
            "step 4410: generator_loss=4.396613121032715, discriminator_loss=0.06155335158109665\n",
            "step 4411: generator_loss=4.554781436920166, discriminator_loss=0.05986706539988518\n",
            "step 4412: generator_loss=4.557697296142578, discriminator_loss=0.05843684822320938\n",
            "step 4413: generator_loss=4.350696086883545, discriminator_loss=0.06122449412941933\n",
            "step 4414: generator_loss=4.134925365447998, discriminator_loss=0.06257089972496033\n",
            "step 4415: generator_loss=3.8218934535980225, discriminator_loss=0.06720392405986786\n",
            "step 4416: generator_loss=3.872753381729126, discriminator_loss=0.06668879091739655\n",
            "step 4417: generator_loss=3.8512117862701416, discriminator_loss=0.06575925648212433\n",
            "step 4418: generator_loss=3.456723213195801, discriminator_loss=0.07545410096645355\n",
            "step 4419: generator_loss=3.5874013900756836, discriminator_loss=0.07329057902097702\n",
            "step 4420: generator_loss=3.4092514514923096, discriminator_loss=0.0806930810213089\n",
            "step 4421: generator_loss=3.1084065437316895, discriminator_loss=0.09056507050991058\n",
            "step 4422: generator_loss=3.030411720275879, discriminator_loss=0.08958891779184341\n",
            "step 4423: generator_loss=3.0935006141662598, discriminator_loss=0.08563908189535141\n",
            "step 4424: generator_loss=2.952324390411377, discriminator_loss=0.08667105436325073\n",
            "step 4425: generator_loss=3.040334701538086, discriminator_loss=0.08012273162603378\n",
            "step 4426: generator_loss=2.9119749069213867, discriminator_loss=0.08182591944932938\n",
            "step 4427: generator_loss=3.056464672088623, discriminator_loss=0.07471472024917603\n",
            "step 4428: generator_loss=3.1681313514709473, discriminator_loss=0.07060955464839935\n",
            "step 4429: generator_loss=3.070889949798584, discriminator_loss=0.07242915779352188\n",
            "step 4430: generator_loss=3.2133166790008545, discriminator_loss=0.06877048313617706\n",
            "step 4431: generator_loss=3.0383400917053223, discriminator_loss=0.07167623192071915\n",
            "step 4432: generator_loss=3.090144157409668, discriminator_loss=0.06875090301036835\n",
            "step 4433: generator_loss=3.01737380027771, discriminator_loss=0.06922642886638641\n",
            "step 4434: generator_loss=2.999112606048584, discriminator_loss=0.06892527639865875\n",
            "step 4435: generator_loss=2.864628791809082, discriminator_loss=0.07256346940994263\n",
            "step 4436: generator_loss=2.9178977012634277, discriminator_loss=0.07219818234443665\n",
            "step 4437: generator_loss=2.755319118499756, discriminator_loss=0.07519517838954926\n",
            "step 4438: generator_loss=2.647484540939331, discriminator_loss=0.07890814542770386\n",
            "step 4439: generator_loss=2.5670409202575684, discriminator_loss=0.08155696094036102\n",
            "step 4440: generator_loss=2.507589817047119, discriminator_loss=0.08382520079612732\n",
            "step 4441: generator_loss=2.4236927032470703, discriminator_loss=0.0870257169008255\n",
            "step 4442: generator_loss=2.3523502349853516, discriminator_loss=0.09050498902797699\n",
            "step 4443: generator_loss=2.2785351276397705, discriminator_loss=0.09391424059867859\n",
            "step 4444: generator_loss=2.2923827171325684, discriminator_loss=0.0931316688656807\n",
            "step 4445: generator_loss=2.2687718868255615, discriminator_loss=0.0944068431854248\n",
            "step 4446: generator_loss=2.241328716278076, discriminator_loss=0.0957726314663887\n",
            "step 4447: generator_loss=2.246408700942993, discriminator_loss=0.09581099450588226\n",
            "step 4448: generator_loss=2.278109550476074, discriminator_loss=0.09425605833530426\n",
            "step 4449: generator_loss=2.2861135005950928, discriminator_loss=0.09438782930374146\n",
            "step 4450: generator_loss=2.2843077182769775, discriminator_loss=0.09503807127475739\n",
            "step 4451: generator_loss=2.3856632709503174, discriminator_loss=0.09085345268249512\n",
            "step 4452: generator_loss=2.3910794258117676, discriminator_loss=0.09136362373828888\n",
            "step 4453: generator_loss=2.422715187072754, discriminator_loss=0.09112977236509323\n",
            "step 4454: generator_loss=2.4660587310791016, discriminator_loss=0.09038213640451431\n",
            "step 4455: generator_loss=2.456343173980713, discriminator_loss=0.09175987541675568\n",
            "step 4456: generator_loss=2.463240385055542, discriminator_loss=0.09149342030286789\n",
            "step 4457: generator_loss=2.3494162559509277, discriminator_loss=0.0967738926410675\n",
            "step 4458: generator_loss=2.279594898223877, discriminator_loss=0.10002745687961578\n",
            "step 4459: generator_loss=2.215653419494629, discriminator_loss=0.10449367761611938\n",
            "step 4460: generator_loss=2.1667261123657227, discriminator_loss=0.10739010572433472\n",
            "step 4461: generator_loss=2.165205955505371, discriminator_loss=0.10816985368728638\n",
            "step 4462: generator_loss=2.1218156814575195, discriminator_loss=0.1108865737915039\n",
            "step 4463: generator_loss=2.0901248455047607, discriminator_loss=0.11324279755353928\n",
            "step 4464: generator_loss=2.0912837982177734, discriminator_loss=0.11451417207717896\n",
            "step 4465: generator_loss=2.042353630065918, discriminator_loss=0.11842872947454453\n",
            "step 4466: generator_loss=2.0057373046875, discriminator_loss=0.12257800996303558\n",
            "step 4467: generator_loss=1.9662811756134033, discriminator_loss=0.12664657831192017\n",
            "step 4468: generator_loss=1.9359924793243408, discriminator_loss=0.1298852562904358\n",
            "step 4469: generator_loss=1.979345679283142, discriminator_loss=0.1283721923828125\n",
            "step 4470: generator_loss=1.9722256660461426, discriminator_loss=0.12849846482276917\n",
            "step 4471: generator_loss=2.048048257827759, discriminator_loss=0.12475539743900299\n",
            "step 4472: generator_loss=2.080270290374756, discriminator_loss=0.12285961955785751\n",
            "step 4473: generator_loss=2.1406970024108887, discriminator_loss=0.12000636756420135\n",
            "step 4474: generator_loss=2.1778149604797363, discriminator_loss=0.11842106282711029\n",
            "step 4475: generator_loss=2.1931824684143066, discriminator_loss=0.11859451234340668\n",
            "step 4476: generator_loss=2.2292773723602295, discriminator_loss=0.11727986484766006\n",
            "step 4477: generator_loss=2.2420787811279297, discriminator_loss=0.1170613169670105\n",
            "step 4478: generator_loss=2.239917755126953, discriminator_loss=0.11728940159082413\n",
            "step 4479: generator_loss=2.264457941055298, discriminator_loss=0.1161661148071289\n",
            "step 4480: generator_loss=2.3048293590545654, discriminator_loss=0.11429141461849213\n",
            "step 4481: generator_loss=2.3424172401428223, discriminator_loss=0.11273963004350662\n",
            "step 4482: generator_loss=2.384675979614258, discriminator_loss=0.11103637516498566\n",
            "step 4483: generator_loss=2.403573513031006, discriminator_loss=0.11011803150177002\n",
            "step 4484: generator_loss=2.4128713607788086, discriminator_loss=0.10943762212991714\n",
            "step 4485: generator_loss=2.439380168914795, discriminator_loss=0.10824034363031387\n",
            "step 4486: generator_loss=2.4574098587036133, discriminator_loss=0.10724550485610962\n",
            "step 4487: generator_loss=2.4609484672546387, discriminator_loss=0.10674456506967545\n",
            "step 4488: generator_loss=2.4562528133392334, discriminator_loss=0.10667281597852707\n",
            "step 4489: generator_loss=2.469686269760132, discriminator_loss=0.10574884712696075\n",
            "step 4490: generator_loss=2.483802318572998, discriminator_loss=0.10445569455623627\n",
            "step 4491: generator_loss=2.4803733825683594, discriminator_loss=0.10404372215270996\n",
            "step 4492: generator_loss=2.5002307891845703, discriminator_loss=0.1025388091802597\n",
            "step 4493: generator_loss=2.5133843421936035, discriminator_loss=0.1008385568857193\n",
            "step 4494: generator_loss=2.519040584564209, discriminator_loss=0.09965887665748596\n",
            "step 4495: generator_loss=2.513334274291992, discriminator_loss=0.09913793206214905\n",
            "step 4496: generator_loss=2.5027289390563965, discriminator_loss=0.09865669161081314\n",
            "step 4497: generator_loss=2.482395648956299, discriminator_loss=0.09866171330213547\n",
            "step 4498: generator_loss=2.4664463996887207, discriminator_loss=0.0985473245382309\n",
            "step 4499: generator_loss=2.4622907638549805, discriminator_loss=0.09772057831287384\n",
            "step 4500: generator_loss=2.4474265575408936, discriminator_loss=0.09772462397813797\n",
            "step 4501: generator_loss=2.425950527191162, discriminator_loss=0.098238505423069\n",
            "step 4502: generator_loss=2.4127326011657715, discriminator_loss=0.09826596081256866\n",
            "step 4503: generator_loss=2.398606538772583, discriminator_loss=0.09873349964618683\n",
            "step 4504: generator_loss=2.3769588470458984, discriminator_loss=0.1001211628317833\n",
            "step 4505: generator_loss=2.3300135135650635, discriminator_loss=0.10528381168842316\n",
            "step 4506: generator_loss=2.282689094543457, discriminator_loss=0.1123514473438263\n",
            "step 4507: generator_loss=2.2459354400634766, discriminator_loss=0.1243705227971077\n",
            "step 4508: generator_loss=2.1648707389831543, discriminator_loss=0.14413855969905853\n",
            "step 4509: generator_loss=2.2424564361572266, discriminator_loss=0.14509433507919312\n",
            "step 4510: generator_loss=2.2097206115722656, discriminator_loss=0.15687331557273865\n",
            "step 4511: generator_loss=2.1351823806762695, discriminator_loss=0.17439152300357819\n",
            "step 4512: generator_loss=2.180525302886963, discriminator_loss=0.178645059466362\n",
            "step 4513: generator_loss=2.257014274597168, discriminator_loss=0.17772352695465088\n",
            "step 4514: generator_loss=2.6843016147613525, discriminator_loss=0.1256309151649475\n",
            "step 4515: generator_loss=2.411386489868164, discriminator_loss=0.18804572522640228\n",
            "step 4516: generator_loss=2.7749269008636475, discriminator_loss=0.13321250677108765\n",
            "step 4517: generator_loss=2.8641982078552246, discriminator_loss=0.12211743742227554\n",
            "step 4518: generator_loss=2.900221347808838, discriminator_loss=0.12222635746002197\n",
            "step 4519: generator_loss=3.056434154510498, discriminator_loss=0.10259729623794556\n",
            "step 4520: generator_loss=2.864098072052002, discriminator_loss=0.11006414145231247\n",
            "step 4521: generator_loss=3.3929965496063232, discriminator_loss=0.08321511745452881\n",
            "step 4522: generator_loss=3.4551517963409424, discriminator_loss=0.08014288544654846\n",
            "step 4523: generator_loss=3.6728224754333496, discriminator_loss=0.07316307723522186\n",
            "step 4524: generator_loss=3.673600196838379, discriminator_loss=0.0738154724240303\n",
            "step 4525: generator_loss=3.9438934326171875, discriminator_loss=0.06815837323665619\n",
            "step 4526: generator_loss=3.9499337673187256, discriminator_loss=0.06757023930549622\n",
            "step 4527: generator_loss=3.905257225036621, discriminator_loss=0.06773513555526733\n",
            "step 4528: generator_loss=3.8633201122283936, discriminator_loss=0.06761960685253143\n",
            "step 4529: generator_loss=3.7161760330200195, discriminator_loss=0.0701918751001358\n",
            "step 4530: generator_loss=3.6053831577301025, discriminator_loss=0.0725148543715477\n",
            "step 4531: generator_loss=3.4647693634033203, discriminator_loss=0.07810671627521515\n",
            "step 4532: generator_loss=3.3476734161376953, discriminator_loss=0.07894421368837357\n",
            "step 4533: generator_loss=3.299471855163574, discriminator_loss=0.07997894287109375\n",
            "step 4534: generator_loss=3.2119381427764893, discriminator_loss=0.0798305794596672\n",
            "step 4535: generator_loss=2.992201805114746, discriminator_loss=0.08574624359607697\n",
            "step 4536: generator_loss=2.9860358238220215, discriminator_loss=0.08427092432975769\n",
            "step 4537: generator_loss=2.9320645332336426, discriminator_loss=0.08380253612995148\n",
            "step 4538: generator_loss=2.9361305236816406, discriminator_loss=0.0817260891199112\n",
            "step 4539: generator_loss=2.8974080085754395, discriminator_loss=0.08222740888595581\n",
            "step 4540: generator_loss=2.8968234062194824, discriminator_loss=0.0777004286646843\n",
            "step 4541: generator_loss=2.9235310554504395, discriminator_loss=0.07649916410446167\n",
            "step 4542: generator_loss=2.91066312789917, discriminator_loss=0.07704266160726547\n",
            "step 4543: generator_loss=2.784090518951416, discriminator_loss=0.08049634099006653\n",
            "step 4544: generator_loss=2.9312148094177246, discriminator_loss=0.07921063899993896\n",
            "step 4545: generator_loss=2.8498969078063965, discriminator_loss=0.0811431035399437\n",
            "step 4546: generator_loss=2.8532614707946777, discriminator_loss=0.08157780021429062\n",
            "step 4547: generator_loss=2.7636499404907227, discriminator_loss=0.084678053855896\n",
            "step 4548: generator_loss=2.701667070388794, discriminator_loss=0.0866994559764862\n",
            "step 4549: generator_loss=2.5108983516693115, discriminator_loss=0.08997073024511337\n",
            "step 4550: generator_loss=2.519839286804199, discriminator_loss=0.09114304929971695\n",
            "step 4551: generator_loss=2.4126524925231934, discriminator_loss=0.09381040185689926\n",
            "step 4552: generator_loss=2.4072232246398926, discriminator_loss=0.09561098366975784\n",
            "step 4553: generator_loss=2.381185531616211, discriminator_loss=0.09676216542720795\n",
            "step 4554: generator_loss=2.3969216346740723, discriminator_loss=0.09787966310977936\n",
            "step 4555: generator_loss=2.331007480621338, discriminator_loss=0.09914013743400574\n",
            "step 4556: generator_loss=2.2932565212249756, discriminator_loss=0.1018887534737587\n",
            "step 4557: generator_loss=2.2208540439605713, discriminator_loss=0.10516966879367828\n",
            "step 4558: generator_loss=2.231520175933838, discriminator_loss=0.10810123383998871\n",
            "step 4559: generator_loss=2.2329325675964355, discriminator_loss=0.10974182933568954\n",
            "step 4560: generator_loss=2.086148262023926, discriminator_loss=0.11575973033905029\n",
            "step 4561: generator_loss=2.06962251663208, discriminator_loss=0.11688977479934692\n",
            "step 4562: generator_loss=2.0577025413513184, discriminator_loss=0.11908436566591263\n",
            "step 4563: generator_loss=2.145782470703125, discriminator_loss=0.11769919097423553\n",
            "step 4564: generator_loss=2.1043012142181396, discriminator_loss=0.11818723380565643\n",
            "step 4565: generator_loss=2.114227294921875, discriminator_loss=0.11890007555484772\n",
            "step 4566: generator_loss=2.1985602378845215, discriminator_loss=0.11585689336061478\n",
            "step 4567: generator_loss=2.2828125953674316, discriminator_loss=0.11291462182998657\n",
            "step 4568: generator_loss=2.23427152633667, discriminator_loss=0.11365047097206116\n",
            "step 4569: generator_loss=2.266167640686035, discriminator_loss=0.1128067821264267\n",
            "step 4570: generator_loss=2.2971043586730957, discriminator_loss=0.11076265573501587\n",
            "step 4571: generator_loss=2.3406829833984375, discriminator_loss=0.10927101969718933\n",
            "step 4572: generator_loss=2.4165070056915283, discriminator_loss=0.10724680125713348\n",
            "step 4573: generator_loss=2.3593249320983887, discriminator_loss=0.10903133451938629\n",
            "step 4574: generator_loss=2.404597759246826, discriminator_loss=0.10683465749025345\n",
            "step 4575: generator_loss=2.4399781227111816, discriminator_loss=0.1053866371512413\n",
            "step 4576: generator_loss=2.4355087280273438, discriminator_loss=0.1056106686592102\n",
            "step 4577: generator_loss=2.4281506538391113, discriminator_loss=0.10596329718828201\n",
            "step 4578: generator_loss=2.4044833183288574, discriminator_loss=0.10689568519592285\n",
            "step 4579: generator_loss=2.4052252769470215, discriminator_loss=0.10670031607151031\n",
            "step 4580: generator_loss=2.4052953720092773, discriminator_loss=0.1062304824590683\n",
            "step 4581: generator_loss=2.3908159732818604, discriminator_loss=0.10650938749313354\n",
            "step 4582: generator_loss=2.360729694366455, discriminator_loss=0.10749013721942902\n",
            "step 4583: generator_loss=2.321929693222046, discriminator_loss=0.10901152342557907\n",
            "step 4584: generator_loss=2.276643753051758, discriminator_loss=0.11091120541095734\n",
            "step 4585: generator_loss=2.2206177711486816, discriminator_loss=0.11347578465938568\n",
            "step 4586: generator_loss=2.156132936477661, discriminator_loss=0.11721818149089813\n",
            "step 4587: generator_loss=2.102261543273926, discriminator_loss=0.12047506123781204\n",
            "step 4588: generator_loss=2.0591859817504883, discriminator_loss=0.12332058697938919\n",
            "step 4589: generator_loss=2.0384929180145264, discriminator_loss=0.12491666525602341\n",
            "step 4590: generator_loss=2.039442300796509, discriminator_loss=0.12485577166080475\n",
            "step 4591: generator_loss=2.075852155685425, discriminator_loss=0.12235172837972641\n",
            "step 4592: generator_loss=2.13132381439209, discriminator_loss=0.11853966116905212\n",
            "step 4593: generator_loss=2.1553995609283447, discriminator_loss=0.11700347065925598\n",
            "step 4594: generator_loss=2.15017032623291, discriminator_loss=0.11742866039276123\n",
            "step 4595: generator_loss=2.1244750022888184, discriminator_loss=0.11924822628498077\n",
            "step 4596: generator_loss=2.086069107055664, discriminator_loss=0.12198445200920105\n",
            "step 4597: generator_loss=2.0523900985717773, discriminator_loss=0.12476103007793427\n",
            "step 4598: generator_loss=2.0373618602752686, discriminator_loss=0.12614229321479797\n",
            "step 4599: generator_loss=2.029273748397827, discriminator_loss=0.1273512840270996\n",
            "step 4600: generator_loss=2.0051565170288086, discriminator_loss=0.12982338666915894\n",
            "step 4601: generator_loss=1.988671898841858, discriminator_loss=0.13178235292434692\n",
            "step 4602: generator_loss=1.9789150953292847, discriminator_loss=0.13361269235610962\n",
            "step 4603: generator_loss=1.9714524745941162, discriminator_loss=0.1353008896112442\n",
            "step 4604: generator_loss=1.9621070623397827, discriminator_loss=0.13701176643371582\n",
            "step 4605: generator_loss=1.9440288543701172, discriminator_loss=0.13962817192077637\n",
            "step 4606: generator_loss=1.9177573919296265, discriminator_loss=0.1429636925458908\n",
            "step 4607: generator_loss=1.8909722566604614, discriminator_loss=0.146466463804245\n",
            "step 4608: generator_loss=1.8717807531356812, discriminator_loss=0.1496766060590744\n",
            "step 4609: generator_loss=1.887793779373169, discriminator_loss=0.14952805638313293\n",
            "step 4610: generator_loss=1.931975245475769, discriminator_loss=0.14719292521476746\n",
            "step 4611: generator_loss=1.9911301136016846, discriminator_loss=0.14369337260723114\n",
            "step 4612: generator_loss=2.0559604167938232, discriminator_loss=0.1401044726371765\n",
            "step 4613: generator_loss=2.109922409057617, discriminator_loss=0.13728582859039307\n",
            "step 4614: generator_loss=2.147954225540161, discriminator_loss=0.13563993573188782\n",
            "step 4615: generator_loss=2.195676565170288, discriminator_loss=0.13313570618629456\n",
            "step 4616: generator_loss=2.254077672958374, discriminator_loss=0.12985333800315857\n",
            "step 4617: generator_loss=2.3053133487701416, discriminator_loss=0.12699870765209198\n",
            "step 4618: generator_loss=2.363826036453247, discriminator_loss=0.12358525395393372\n",
            "step 4619: generator_loss=2.39005970954895, discriminator_loss=0.12135544419288635\n",
            "step 4620: generator_loss=2.395397901535034, discriminator_loss=0.12009299546480179\n",
            "step 4621: generator_loss=2.359684944152832, discriminator_loss=0.12057190388441086\n",
            "step 4622: generator_loss=2.286932945251465, discriminator_loss=0.12316948175430298\n",
            "step 4623: generator_loss=2.2009530067443848, discriminator_loss=0.1265687793493271\n",
            "step 4624: generator_loss=2.1219663619995117, discriminator_loss=0.13043341040611267\n",
            "step 4625: generator_loss=2.0498692989349365, discriminator_loss=0.13430821895599365\n",
            "step 4626: generator_loss=2.010382890701294, discriminator_loss=0.13619443774223328\n",
            "step 4627: generator_loss=2.0047879219055176, discriminator_loss=0.13583239912986755\n",
            "step 4628: generator_loss=2.0069146156311035, discriminator_loss=0.13501903414726257\n",
            "step 4629: generator_loss=2.009228467941284, discriminator_loss=0.1344071626663208\n",
            "step 4630: generator_loss=2.015230894088745, discriminator_loss=0.13369855284690857\n",
            "step 4631: generator_loss=2.028318166732788, discriminator_loss=0.1323453187942505\n",
            "step 4632: generator_loss=2.037050724029541, discriminator_loss=0.1313280463218689\n",
            "step 4633: generator_loss=2.0460586547851562, discriminator_loss=0.13043195009231567\n",
            "step 4634: generator_loss=2.082336187362671, discriminator_loss=0.12760087847709656\n",
            "step 4635: generator_loss=2.1296846866607666, discriminator_loss=0.12436749041080475\n",
            "step 4636: generator_loss=2.2201004028320312, discriminator_loss=0.11851698905229568\n",
            "step 4637: generator_loss=2.3414602279663086, discriminator_loss=0.11168406903743744\n",
            "step 4638: generator_loss=2.456073760986328, discriminator_loss=0.10614101588726044\n",
            "step 4639: generator_loss=2.5496222972869873, discriminator_loss=0.10187945514917374\n",
            "step 4640: generator_loss=2.6306490898132324, discriminator_loss=0.09845450520515442\n",
            "step 4641: generator_loss=2.687030553817749, discriminator_loss=0.09599827229976654\n",
            "step 4642: generator_loss=2.727442741394043, discriminator_loss=0.09395956248044968\n",
            "step 4643: generator_loss=2.7482850551605225, discriminator_loss=0.09274928271770477\n",
            "step 4644: generator_loss=2.7489302158355713, discriminator_loss=0.0919201523065567\n",
            "step 4645: generator_loss=2.741748332977295, discriminator_loss=0.0912541076540947\n",
            "step 4646: generator_loss=2.7506091594696045, discriminator_loss=0.09004880487918854\n",
            "step 4647: generator_loss=2.746613025665283, discriminator_loss=0.0891941636800766\n",
            "step 4648: generator_loss=2.747204065322876, discriminator_loss=0.08803369849920273\n",
            "step 4649: generator_loss=2.7402515411376953, discriminator_loss=0.08721810579299927\n",
            "step 4650: generator_loss=2.7156054973602295, discriminator_loss=0.08688099682331085\n",
            "step 4651: generator_loss=2.6969289779663086, discriminator_loss=0.08645009994506836\n",
            "step 4652: generator_loss=2.672971725463867, discriminator_loss=0.08635014295578003\n",
            "step 4653: generator_loss=2.655046224594116, discriminator_loss=0.08599047362804413\n",
            "step 4654: generator_loss=2.653670310974121, discriminator_loss=0.08525930345058441\n",
            "step 4655: generator_loss=2.6586265563964844, discriminator_loss=0.08427805453538895\n",
            "step 4656: generator_loss=2.6584153175354004, discriminator_loss=0.08363203704357147\n",
            "step 4657: generator_loss=2.6587038040161133, discriminator_loss=0.0829673632979393\n",
            "step 4658: generator_loss=2.689152956008911, discriminator_loss=0.08125097304582596\n",
            "step 4659: generator_loss=2.721916913986206, discriminator_loss=0.07965035736560822\n",
            "step 4660: generator_loss=2.7436017990112305, discriminator_loss=0.07854387164115906\n",
            "step 4661: generator_loss=2.7615463733673096, discriminator_loss=0.07761141657829285\n",
            "step 4662: generator_loss=2.7839767932891846, discriminator_loss=0.07663965225219727\n",
            "step 4663: generator_loss=2.795257091522217, discriminator_loss=0.07601262629032135\n",
            "step 4664: generator_loss=2.796189308166504, discriminator_loss=0.07581203430891037\n",
            "step 4665: generator_loss=2.799680233001709, discriminator_loss=0.07547858357429504\n",
            "step 4666: generator_loss=2.796900987625122, discriminator_loss=0.07542312145233154\n",
            "step 4667: generator_loss=2.798909902572632, discriminator_loss=0.07515187561511993\n",
            "step 4668: generator_loss=2.796483278274536, discriminator_loss=0.074960857629776\n",
            "step 4669: generator_loss=2.776219367980957, discriminator_loss=0.07544493675231934\n",
            "step 4670: generator_loss=2.738070011138916, discriminator_loss=0.07661767303943634\n",
            "step 4671: generator_loss=2.697723865509033, discriminator_loss=0.0778651237487793\n",
            "step 4672: generator_loss=2.6742260456085205, discriminator_loss=0.07859007269144058\n",
            "step 4673: generator_loss=2.6391513347625732, discriminator_loss=0.07976247370243073\n",
            "step 4674: generator_loss=2.5992281436920166, discriminator_loss=0.08123551309108734\n",
            "step 4675: generator_loss=2.572476625442505, discriminator_loss=0.08225855231285095\n",
            "step 4676: generator_loss=2.5569732189178467, discriminator_loss=0.08284255862236023\n",
            "step 4677: generator_loss=2.539294481277466, discriminator_loss=0.0836232602596283\n",
            "step 4678: generator_loss=2.5359463691711426, discriminator_loss=0.0837041586637497\n",
            "step 4679: generator_loss=2.542410135269165, discriminator_loss=0.08343888819217682\n",
            "step 4680: generator_loss=2.5499794483184814, discriminator_loss=0.08319353312253952\n",
            "step 4681: generator_loss=2.5516934394836426, discriminator_loss=0.0832185298204422\n",
            "step 4682: generator_loss=2.582557201385498, discriminator_loss=0.08217974007129669\n",
            "step 4683: generator_loss=2.613403797149658, discriminator_loss=0.08097641170024872\n",
            "step 4684: generator_loss=2.6379666328430176, discriminator_loss=0.08022935688495636\n",
            "step 4685: generator_loss=2.6689014434814453, discriminator_loss=0.07915903627872467\n",
            "step 4686: generator_loss=2.6721644401550293, discriminator_loss=0.07912410795688629\n",
            "step 4687: generator_loss=2.653088092803955, discriminator_loss=0.07993528991937637\n",
            "step 4688: generator_loss=2.623810291290283, discriminator_loss=0.0811353474855423\n",
            "step 4689: generator_loss=2.5730433464050293, discriminator_loss=0.0832749605178833\n",
            "step 4690: generator_loss=2.534893274307251, discriminator_loss=0.08494573831558228\n",
            "step 4691: generator_loss=2.5078887939453125, discriminator_loss=0.08610976487398148\n",
            "step 4692: generator_loss=2.481370449066162, discriminator_loss=0.08743306249380112\n",
            "step 4693: generator_loss=2.4479832649230957, discriminator_loss=0.08896283805370331\n",
            "step 4694: generator_loss=2.437135696411133, discriminator_loss=0.08954966068267822\n",
            "step 4695: generator_loss=2.4249377250671387, discriminator_loss=0.09015995264053345\n",
            "step 4696: generator_loss=2.429116725921631, discriminator_loss=0.0901014506816864\n",
            "step 4697: generator_loss=2.442993640899658, discriminator_loss=0.0896926075220108\n",
            "step 4698: generator_loss=2.4629971981048584, discriminator_loss=0.08895586431026459\n",
            "step 4699: generator_loss=2.4727370738983154, discriminator_loss=0.08873537182807922\n",
            "step 4700: generator_loss=2.4957289695739746, discriminator_loss=0.0879206508398056\n",
            "step 4701: generator_loss=2.5222558975219727, discriminator_loss=0.08697313815355301\n",
            "step 4702: generator_loss=2.5394718647003174, discriminator_loss=0.08640697598457336\n",
            "step 4703: generator_loss=2.5415070056915283, discriminator_loss=0.0864226222038269\n",
            "step 4704: generator_loss=2.5417351722717285, discriminator_loss=0.08651408553123474\n",
            "step 4705: generator_loss=2.5343222618103027, discriminator_loss=0.08695260435342789\n",
            "step 4706: generator_loss=2.541407346725464, discriminator_loss=0.08663363754749298\n",
            "step 4707: generator_loss=2.5675110816955566, discriminator_loss=0.08562297374010086\n",
            "step 4708: generator_loss=2.610750675201416, discriminator_loss=0.08381021022796631\n",
            "step 4709: generator_loss=2.706190586090088, discriminator_loss=0.08021117746829987\n",
            "step 4710: generator_loss=2.818955898284912, discriminator_loss=0.07625582814216614\n",
            "step 4711: generator_loss=2.92525315284729, discriminator_loss=0.07293419539928436\n",
            "step 4712: generator_loss=2.9948651790618896, discriminator_loss=0.07066668570041656\n",
            "step 4713: generator_loss=3.049757719039917, discriminator_loss=0.06886343657970428\n",
            "step 4714: generator_loss=3.141857862472534, discriminator_loss=0.06621184200048447\n",
            "step 4715: generator_loss=3.1857810020446777, discriminator_loss=0.06473186612129211\n",
            "step 4716: generator_loss=3.1922543048858643, discriminator_loss=0.06405265629291534\n",
            "step 4717: generator_loss=3.1897497177124023, discriminator_loss=0.06354320049285889\n",
            "step 4718: generator_loss=3.1965529918670654, discriminator_loss=0.06285738945007324\n",
            "step 4719: generator_loss=3.1920793056488037, discriminator_loss=0.06245330721139908\n",
            "step 4720: generator_loss=3.171640396118164, discriminator_loss=0.06244022399187088\n",
            "step 4721: generator_loss=3.12318754196167, discriminator_loss=0.06298986077308655\n",
            "step 4722: generator_loss=3.0514838695526123, discriminator_loss=0.06436699628829956\n",
            "step 4723: generator_loss=2.972005844116211, discriminator_loss=0.06596685945987701\n",
            "step 4724: generator_loss=2.8840394020080566, discriminator_loss=0.06801113486289978\n",
            "step 4725: generator_loss=2.8037118911743164, discriminator_loss=0.070121631026268\n",
            "step 4726: generator_loss=2.7327866554260254, discriminator_loss=0.07219719886779785\n",
            "step 4727: generator_loss=2.665468215942383, discriminator_loss=0.07438139617443085\n",
            "step 4728: generator_loss=2.6055240631103516, discriminator_loss=0.07648853957653046\n",
            "step 4729: generator_loss=2.570608615875244, discriminator_loss=0.07779422402381897\n",
            "step 4730: generator_loss=2.595979690551758, discriminator_loss=0.07667945325374603\n",
            "step 4731: generator_loss=2.6590170860290527, discriminator_loss=0.074216827750206\n",
            "step 4732: generator_loss=2.714367151260376, discriminator_loss=0.07212816178798676\n",
            "step 4733: generator_loss=2.794564723968506, discriminator_loss=0.06943687796592712\n",
            "step 4734: generator_loss=2.9083521366119385, discriminator_loss=0.06585108488798141\n",
            "step 4735: generator_loss=3.0054287910461426, discriminator_loss=0.06320004165172577\n",
            "step 4736: generator_loss=3.077785015106201, discriminator_loss=0.061425402760505676\n",
            "step 4737: generator_loss=3.142915725708008, discriminator_loss=0.05979783833026886\n",
            "step 4738: generator_loss=3.2105560302734375, discriminator_loss=0.05824057757854462\n",
            "step 4739: generator_loss=3.2607712745666504, discriminator_loss=0.057091109454631805\n",
            "step 4740: generator_loss=3.2603402137756348, discriminator_loss=0.05687163397669792\n",
            "step 4741: generator_loss=3.2204127311706543, discriminator_loss=0.05747917294502258\n",
            "step 4742: generator_loss=3.160421848297119, discriminator_loss=0.05849672853946686\n",
            "step 4743: generator_loss=3.0942044258117676, discriminator_loss=0.05974766984581947\n",
            "step 4744: generator_loss=3.04807186126709, discriminator_loss=0.06065496802330017\n",
            "step 4745: generator_loss=2.9938669204711914, discriminator_loss=0.0618164986371994\n",
            "step 4746: generator_loss=2.9077820777893066, discriminator_loss=0.06395573914051056\n",
            "step 4747: generator_loss=2.816366195678711, discriminator_loss=0.06662680208683014\n",
            "step 4748: generator_loss=2.73726487159729, discriminator_loss=0.0690804272890091\n",
            "step 4749: generator_loss=2.6732707023620605, discriminator_loss=0.07130253314971924\n",
            "step 4750: generator_loss=2.627286672592163, discriminator_loss=0.07298019528388977\n",
            "step 4751: generator_loss=2.5995945930480957, discriminator_loss=0.07406497001647949\n",
            "step 4752: generator_loss=2.578477382659912, discriminator_loss=0.07494783401489258\n",
            "step 4753: generator_loss=2.582000494003296, discriminator_loss=0.07485558092594147\n",
            "step 4754: generator_loss=2.6025891304016113, discriminator_loss=0.07417847216129303\n",
            "step 4755: generator_loss=2.6209330558776855, discriminator_loss=0.07364660501480103\n",
            "step 4756: generator_loss=2.622541904449463, discriminator_loss=0.07371951639652252\n",
            "step 4757: generator_loss=2.6154286861419678, discriminator_loss=0.07418909668922424\n",
            "step 4758: generator_loss=2.59670352935791, discriminator_loss=0.07528659701347351\n",
            "step 4759: generator_loss=2.592761754989624, discriminator_loss=0.07577274739742279\n",
            "step 4760: generator_loss=2.5818872451782227, discriminator_loss=0.07659461349248886\n",
            "step 4761: generator_loss=2.577911853790283, discriminator_loss=0.07713457196950912\n",
            "step 4762: generator_loss=2.563643455505371, discriminator_loss=0.07820028811693192\n",
            "step 4763: generator_loss=2.5658164024353027, discriminator_loss=0.07859573513269424\n",
            "step 4764: generator_loss=2.5899524688720703, discriminator_loss=0.07799134403467178\n",
            "step 4765: generator_loss=2.6158699989318848, discriminator_loss=0.0773104876279831\n",
            "step 4766: generator_loss=2.6521992683410645, discriminator_loss=0.07626350224018097\n",
            "step 4767: generator_loss=2.7141470909118652, discriminator_loss=0.0742666944861412\n",
            "step 4768: generator_loss=2.7479255199432373, discriminator_loss=0.07326148450374603\n",
            "step 4769: generator_loss=2.739452362060547, discriminator_loss=0.07368087768554688\n",
            "step 4770: generator_loss=2.6932783126831055, discriminator_loss=0.07535979151725769\n",
            "step 4771: generator_loss=2.6575565338134766, discriminator_loss=0.0767570436000824\n",
            "step 4772: generator_loss=2.602445602416992, discriminator_loss=0.07890501618385315\n",
            "step 4773: generator_loss=2.5363030433654785, discriminator_loss=0.08166415989398956\n",
            "step 4774: generator_loss=2.4762914180755615, discriminator_loss=0.08428771048784256\n",
            "step 4775: generator_loss=2.438690185546875, discriminator_loss=0.08607123792171478\n",
            "step 4776: generator_loss=2.4447591304779053, discriminator_loss=0.08580382168292999\n",
            "step 4777: generator_loss=2.470869779586792, discriminator_loss=0.08472396433353424\n",
            "step 4778: generator_loss=2.4962894916534424, discriminator_loss=0.08372603356838226\n",
            "step 4779: generator_loss=2.5469274520874023, discriminator_loss=0.08185739815235138\n",
            "step 4780: generator_loss=2.589195728302002, discriminator_loss=0.08027532696723938\n",
            "step 4781: generator_loss=2.6412172317504883, discriminator_loss=0.07830338925123215\n",
            "step 4782: generator_loss=2.6654820442199707, discriminator_loss=0.07758134603500366\n",
            "step 4783: generator_loss=2.6756277084350586, discriminator_loss=0.07743868231773376\n",
            "step 4784: generator_loss=2.6964423656463623, discriminator_loss=0.07669630646705627\n",
            "step 4785: generator_loss=2.696849822998047, discriminator_loss=0.0769185796380043\n",
            "step 4786: generator_loss=2.7020511627197266, discriminator_loss=0.07678663730621338\n",
            "step 4787: generator_loss=2.719085216522217, discriminator_loss=0.07629866898059845\n",
            "step 4788: generator_loss=2.730837821960449, discriminator_loss=0.07596780359745026\n",
            "step 4789: generator_loss=2.764584541320801, discriminator_loss=0.07469727098941803\n",
            "step 4790: generator_loss=2.7853164672851562, discriminator_loss=0.07390618324279785\n",
            "step 4791: generator_loss=2.819592237472534, discriminator_loss=0.0726027712225914\n",
            "step 4792: generator_loss=2.8609063625335693, discriminator_loss=0.07092927396297455\n",
            "step 4793: generator_loss=2.881502866744995, discriminator_loss=0.070088692009449\n",
            "step 4794: generator_loss=2.8754444122314453, discriminator_loss=0.06974274665117264\n",
            "step 4795: generator_loss=2.8411834239959717, discriminator_loss=0.07041973620653152\n",
            "step 4796: generator_loss=2.78676700592041, discriminator_loss=0.07178466767072678\n",
            "step 4797: generator_loss=2.723930597305298, discriminator_loss=0.07362006604671478\n",
            "step 4798: generator_loss=2.676560640335083, discriminator_loss=0.07502518594264984\n",
            "step 4799: generator_loss=2.632028579711914, discriminator_loss=0.07639073580503464\n",
            "step 4800: generator_loss=2.608585834503174, discriminator_loss=0.07721184194087982\n",
            "step 4801: generator_loss=2.618018388748169, discriminator_loss=0.07667979598045349\n",
            "step 4802: generator_loss=2.6681549549102783, discriminator_loss=0.0746910497546196\n",
            "step 4803: generator_loss=2.712584972381592, discriminator_loss=0.07304634898900986\n",
            "step 4804: generator_loss=2.7633533477783203, discriminator_loss=0.07127469033002853\n",
            "step 4805: generator_loss=2.775176525115967, discriminator_loss=0.07120062410831451\n",
            "step 4806: generator_loss=2.8146181106567383, discriminator_loss=0.07022114098072052\n",
            "step 4807: generator_loss=2.7808480262756348, discriminator_loss=0.07283467799425125\n",
            "step 4808: generator_loss=2.8086884021759033, discriminator_loss=0.07319772243499756\n",
            "step 4809: generator_loss=2.802917957305908, discriminator_loss=0.07639291882514954\n",
            "step 4810: generator_loss=2.7964377403259277, discriminator_loss=0.07880279421806335\n",
            "step 4811: generator_loss=2.732668876647949, discriminator_loss=0.09129707515239716\n",
            "step 4812: generator_loss=2.8406238555908203, discriminator_loss=0.08528485894203186\n",
            "step 4813: generator_loss=2.674494504928589, discriminator_loss=0.11284497380256653\n",
            "step 4814: generator_loss=2.8035173416137695, discriminator_loss=0.10729114711284637\n",
            "step 4815: generator_loss=2.8018977642059326, discriminator_loss=0.1202358677983284\n",
            "step 4816: generator_loss=2.811720132827759, discriminator_loss=0.13202795386314392\n",
            "step 4817: generator_loss=2.754852294921875, discriminator_loss=0.14804613590240479\n",
            "step 4818: generator_loss=2.984666347503662, discriminator_loss=0.14863422513008118\n",
            "step 4819: generator_loss=3.1338157653808594, discriminator_loss=0.1388932168483734\n",
            "step 4820: generator_loss=3.1757001876831055, discriminator_loss=0.14583230018615723\n",
            "step 4821: generator_loss=3.4397125244140625, discriminator_loss=0.128767192363739\n",
            "step 4822: generator_loss=3.5679261684417725, discriminator_loss=0.12464926391839981\n",
            "step 4823: generator_loss=3.8883864879608154, discriminator_loss=0.09304507076740265\n",
            "step 4824: generator_loss=4.321341037750244, discriminator_loss=0.0753670334815979\n",
            "step 4825: generator_loss=4.481842041015625, discriminator_loss=0.0643310546875\n",
            "step 4826: generator_loss=4.5096635818481445, discriminator_loss=0.06212206557393074\n",
            "step 4827: generator_loss=4.796173095703125, discriminator_loss=0.056051623076200485\n",
            "step 4828: generator_loss=4.6004791259765625, discriminator_loss=0.057887002825737\n",
            "step 4829: generator_loss=4.717073440551758, discriminator_loss=0.056003957986831665\n",
            "step 4830: generator_loss=4.992791652679443, discriminator_loss=0.05407567322254181\n",
            "step 4831: generator_loss=4.804469108581543, discriminator_loss=0.05602865666151047\n",
            "step 4832: generator_loss=4.590009689331055, discriminator_loss=0.059241145849227905\n",
            "step 4833: generator_loss=4.708575248718262, discriminator_loss=0.05816378444433212\n",
            "step 4834: generator_loss=4.536535263061523, discriminator_loss=0.06078597530722618\n",
            "step 4835: generator_loss=4.490503311157227, discriminator_loss=0.06181573122739792\n",
            "step 4836: generator_loss=4.247899532318115, discriminator_loss=0.06347312033176422\n",
            "step 4837: generator_loss=4.012809753417969, discriminator_loss=0.06820860505104065\n",
            "step 4838: generator_loss=3.927215576171875, discriminator_loss=0.06598176807165146\n",
            "step 4839: generator_loss=3.7915499210357666, discriminator_loss=0.06712344288825989\n",
            "step 4840: generator_loss=3.452955961227417, discriminator_loss=0.07647547125816345\n",
            "step 4841: generator_loss=3.457958221435547, discriminator_loss=0.07510428875684738\n",
            "step 4842: generator_loss=3.636146068572998, discriminator_loss=0.06447599083185196\n",
            "step 4843: generator_loss=3.4044432640075684, discriminator_loss=0.06874485313892365\n",
            "step 4844: generator_loss=3.5242738723754883, discriminator_loss=0.06488581001758575\n",
            "step 4845: generator_loss=3.5140457153320312, discriminator_loss=0.05982740968465805\n",
            "step 4846: generator_loss=3.4820199012756348, discriminator_loss=0.06269418448209763\n",
            "step 4847: generator_loss=3.3033246994018555, discriminator_loss=0.06326981633901596\n",
            "step 4848: generator_loss=3.4901962280273438, discriminator_loss=0.06210801377892494\n",
            "step 4849: generator_loss=3.1510045528411865, discriminator_loss=0.06401920318603516\n",
            "step 4850: generator_loss=3.2971444129943848, discriminator_loss=0.062049299478530884\n",
            "step 4851: generator_loss=3.1872189044952393, discriminator_loss=0.06598159670829773\n",
            "step 4852: generator_loss=2.793374538421631, discriminator_loss=0.071708545088768\n",
            "step 4853: generator_loss=2.963085889816284, discriminator_loss=0.0705561637878418\n",
            "step 4854: generator_loss=2.8185572624206543, discriminator_loss=0.07329240441322327\n",
            "step 4855: generator_loss=2.8146047592163086, discriminator_loss=0.07457149028778076\n",
            "step 4856: generator_loss=2.7592477798461914, discriminator_loss=0.07673947513103485\n",
            "step 4857: generator_loss=2.557758331298828, discriminator_loss=0.08120011538267136\n",
            "step 4858: generator_loss=2.5607199668884277, discriminator_loss=0.08330366760492325\n",
            "step 4859: generator_loss=2.5846986770629883, discriminator_loss=0.08322636038064957\n",
            "step 4860: generator_loss=2.4111993312835693, discriminator_loss=0.08821268379688263\n",
            "step 4861: generator_loss=2.4197850227355957, discriminator_loss=0.08953025937080383\n",
            "step 4862: generator_loss=2.3897721767425537, discriminator_loss=0.09027890861034393\n",
            "step 4863: generator_loss=2.3881006240844727, discriminator_loss=0.0920381098985672\n",
            "step 4864: generator_loss=2.4575493335723877, discriminator_loss=0.09092573821544647\n",
            "step 4865: generator_loss=2.3576302528381348, discriminator_loss=0.09223593026399612\n",
            "step 4866: generator_loss=2.362039089202881, discriminator_loss=0.09186769276857376\n",
            "step 4867: generator_loss=2.4780983924865723, discriminator_loss=0.08936600387096405\n",
            "step 4868: generator_loss=2.45320987701416, discriminator_loss=0.08832484483718872\n",
            "step 4869: generator_loss=2.4912524223327637, discriminator_loss=0.0873817503452301\n",
            "step 4870: generator_loss=2.508284330368042, discriminator_loss=0.08687561750411987\n",
            "step 4871: generator_loss=2.4726810455322266, discriminator_loss=0.08757703006267548\n",
            "step 4872: generator_loss=2.487821102142334, discriminator_loss=0.08940549939870834\n",
            "step 4873: generator_loss=2.5021820068359375, discriminator_loss=0.09155100584030151\n",
            "step 4874: generator_loss=2.379876136779785, discriminator_loss=0.09824229776859283\n",
            "step 4875: generator_loss=2.343513011932373, discriminator_loss=0.10328827053308487\n",
            "step 4876: generator_loss=2.372854709625244, discriminator_loss=0.1062827929854393\n",
            "step 4877: generator_loss=2.231177806854248, discriminator_loss=0.12883713841438293\n",
            "step 4878: generator_loss=2.273529291152954, discriminator_loss=0.13796693086624146\n",
            "step 4879: generator_loss=2.4479575157165527, discriminator_loss=0.12336356937885284\n",
            "step 4880: generator_loss=2.2824316024780273, discriminator_loss=0.16157816350460052\n",
            "step 4881: generator_loss=2.3447327613830566, discriminator_loss=0.1671203225851059\n",
            "step 4882: generator_loss=2.4339680671691895, discriminator_loss=0.15910017490386963\n",
            "step 4883: generator_loss=2.4162449836730957, discriminator_loss=0.18342630565166473\n",
            "step 4884: generator_loss=2.5643959045410156, discriminator_loss=0.1586291790008545\n",
            "step 4885: generator_loss=2.7516961097717285, discriminator_loss=0.14440613985061646\n",
            "step 4886: generator_loss=2.992225170135498, discriminator_loss=0.13278891146183014\n",
            "step 4887: generator_loss=2.7643630504608154, discriminator_loss=0.14543378353118896\n",
            "step 4888: generator_loss=3.0463719367980957, discriminator_loss=0.11610971391201019\n",
            "step 4889: generator_loss=3.1716203689575195, discriminator_loss=0.10737249255180359\n",
            "step 4890: generator_loss=3.66155743598938, discriminator_loss=0.08311405777931213\n",
            "step 4891: generator_loss=3.6350064277648926, discriminator_loss=0.08245088160037994\n",
            "step 4892: generator_loss=3.8419036865234375, discriminator_loss=0.07619722932577133\n",
            "step 4893: generator_loss=3.857381820678711, discriminator_loss=0.07473431527614594\n",
            "step 4894: generator_loss=3.6650590896606445, discriminator_loss=0.07743621617555618\n",
            "step 4895: generator_loss=3.7779464721679688, discriminator_loss=0.0743178129196167\n",
            "step 4896: generator_loss=3.6771788597106934, discriminator_loss=0.07455739378929138\n",
            "step 4897: generator_loss=3.4435360431671143, discriminator_loss=0.07871781289577484\n",
            "step 4898: generator_loss=3.4762074947357178, discriminator_loss=0.07730495929718018\n",
            "step 4899: generator_loss=3.3133039474487305, discriminator_loss=0.07860231399536133\n",
            "step 4900: generator_loss=3.126809597015381, discriminator_loss=0.07919098436832428\n",
            "step 4901: generator_loss=3.1053500175476074, discriminator_loss=0.08046622574329376\n",
            "step 4902: generator_loss=2.903188705444336, discriminator_loss=0.08551890403032303\n",
            "step 4903: generator_loss=2.926142692565918, discriminator_loss=0.08547773212194443\n",
            "step 4904: generator_loss=2.812375545501709, discriminator_loss=0.08859986811876297\n",
            "step 4905: generator_loss=2.50752592086792, discriminator_loss=0.09913943707942963\n",
            "step 4906: generator_loss=2.451725959777832, discriminator_loss=0.10431823134422302\n",
            "step 4907: generator_loss=2.3729734420776367, discriminator_loss=0.10730509459972382\n",
            "step 4908: generator_loss=2.4101996421813965, discriminator_loss=0.10712830722332001\n",
            "step 4909: generator_loss=2.410200834274292, discriminator_loss=0.10738087445497513\n",
            "step 4910: generator_loss=2.1736812591552734, discriminator_loss=0.1391930878162384\n",
            "step 4911: generator_loss=2.39176082611084, discriminator_loss=0.11755235493183136\n",
            "step 4912: generator_loss=2.440861701965332, discriminator_loss=0.13484209775924683\n",
            "step 4913: generator_loss=2.3975563049316406, discriminator_loss=0.14419394731521606\n",
            "step 4914: generator_loss=2.3725852966308594, discriminator_loss=0.15862546861171722\n",
            "step 4915: generator_loss=2.2674262523651123, discriminator_loss=0.1809520125389099\n",
            "step 4916: generator_loss=2.7036709785461426, discriminator_loss=0.1175416111946106\n",
            "step 4917: generator_loss=2.498387575149536, discriminator_loss=0.18155497312545776\n",
            "step 4918: generator_loss=2.495244026184082, discriminator_loss=0.1929532289505005\n",
            "step 4919: generator_loss=2.708345890045166, discriminator_loss=0.16889643669128418\n",
            "step 4920: generator_loss=2.838888645172119, discriminator_loss=0.16187676787376404\n",
            "step 4921: generator_loss=2.765421152114868, discriminator_loss=0.17572197318077087\n",
            "step 4922: generator_loss=3.0596532821655273, discriminator_loss=0.1355091631412506\n",
            "step 4923: generator_loss=3.0820677280426025, discriminator_loss=0.12231017649173737\n",
            "step 4924: generator_loss=3.042160987854004, discriminator_loss=0.12419858574867249\n",
            "step 4925: generator_loss=3.2981958389282227, discriminator_loss=0.09912917017936707\n",
            "step 4926: generator_loss=3.472287654876709, discriminator_loss=0.08673121780157089\n",
            "step 4927: generator_loss=3.4846017360687256, discriminator_loss=0.08221492171287537\n",
            "step 4928: generator_loss=3.3661835193634033, discriminator_loss=0.08455678820610046\n",
            "step 4929: generator_loss=3.4787163734436035, discriminator_loss=0.07546494901180267\n",
            "step 4930: generator_loss=3.42651104927063, discriminator_loss=0.07490897178649902\n",
            "step 4931: generator_loss=3.462506055831909, discriminator_loss=0.072716124355793\n",
            "step 4932: generator_loss=3.3105270862579346, discriminator_loss=0.07483722269535065\n",
            "step 4933: generator_loss=3.1506476402282715, discriminator_loss=0.07875628769397736\n",
            "step 4934: generator_loss=3.041184186935425, discriminator_loss=0.08015282452106476\n",
            "step 4935: generator_loss=3.0253562927246094, discriminator_loss=0.0810316801071167\n",
            "step 4936: generator_loss=2.8598461151123047, discriminator_loss=0.08708713948726654\n",
            "step 4937: generator_loss=2.7396223545074463, discriminator_loss=0.09135004878044128\n",
            "step 4938: generator_loss=2.7360739707946777, discriminator_loss=0.09159614145755768\n",
            "step 4939: generator_loss=2.6141128540039062, discriminator_loss=0.09314046055078506\n",
            "step 4940: generator_loss=2.4228813648223877, discriminator_loss=0.10360915958881378\n",
            "step 4941: generator_loss=2.4408135414123535, discriminator_loss=0.10051939636468887\n",
            "step 4942: generator_loss=2.2853875160217285, discriminator_loss=0.10943131148815155\n",
            "step 4943: generator_loss=2.370582103729248, discriminator_loss=0.10367777943611145\n",
            "step 4944: generator_loss=2.2755377292633057, discriminator_loss=0.10605110228061676\n",
            "step 4945: generator_loss=2.358549118041992, discriminator_loss=0.10428053140640259\n",
            "step 4946: generator_loss=2.2712109088897705, discriminator_loss=0.10605418682098389\n",
            "step 4947: generator_loss=2.376542091369629, discriminator_loss=0.1027889996767044\n",
            "step 4948: generator_loss=2.4859378337860107, discriminator_loss=0.09997972846031189\n",
            "step 4949: generator_loss=2.516704559326172, discriminator_loss=0.0960346907377243\n",
            "step 4950: generator_loss=2.5298972129821777, discriminator_loss=0.09512907266616821\n",
            "step 4951: generator_loss=2.6469154357910156, discriminator_loss=0.09170229732990265\n",
            "step 4952: generator_loss=2.642550230026245, discriminator_loss=0.09400326013565063\n",
            "step 4953: generator_loss=2.54750919342041, discriminator_loss=0.09629277884960175\n",
            "step 4954: generator_loss=2.330451011657715, discriminator_loss=0.1009739562869072\n",
            "step 4955: generator_loss=2.487182378768921, discriminator_loss=0.10064753890037537\n",
            "step 4956: generator_loss=2.318159341812134, discriminator_loss=0.10749562084674835\n",
            "step 4957: generator_loss=2.394425392150879, discriminator_loss=0.10938439518213272\n",
            "step 4958: generator_loss=2.1394081115722656, discriminator_loss=0.11871494352817535\n",
            "step 4959: generator_loss=2.0816807746887207, discriminator_loss=0.12181073427200317\n",
            "step 4960: generator_loss=2.137725830078125, discriminator_loss=0.12421312928199768\n",
            "step 4961: generator_loss=2.0951948165893555, discriminator_loss=0.12470574676990509\n",
            "step 4962: generator_loss=1.9933868646621704, discriminator_loss=0.12824317812919617\n",
            "step 4963: generator_loss=2.1068577766418457, discriminator_loss=0.12583422660827637\n",
            "step 4964: generator_loss=1.9762659072875977, discriminator_loss=0.1318536102771759\n",
            "step 4965: generator_loss=2.0323486328125, discriminator_loss=0.13050314784049988\n",
            "step 4966: generator_loss=2.003925323486328, discriminator_loss=0.1332826018333435\n",
            "step 4967: generator_loss=1.9671289920806885, discriminator_loss=0.1356583684682846\n",
            "step 4968: generator_loss=1.986738681793213, discriminator_loss=0.13539758324623108\n",
            "step 4969: generator_loss=2.0061473846435547, discriminator_loss=0.1353641003370285\n",
            "step 4970: generator_loss=2.05466365814209, discriminator_loss=0.1345367729663849\n",
            "step 4971: generator_loss=2.0767064094543457, discriminator_loss=0.13355344533920288\n",
            "step 4972: generator_loss=2.1427979469299316, discriminator_loss=0.13158860802650452\n",
            "step 4973: generator_loss=2.1309690475463867, discriminator_loss=0.1321549415588379\n",
            "step 4974: generator_loss=2.154068946838379, discriminator_loss=0.13152524828910828\n",
            "step 4975: generator_loss=2.1745710372924805, discriminator_loss=0.13104216754436493\n",
            "step 4976: generator_loss=2.195410966873169, discriminator_loss=0.13120536506175995\n",
            "step 4977: generator_loss=2.1859281063079834, discriminator_loss=0.13201463222503662\n",
            "step 4978: generator_loss=2.1514170169830322, discriminator_loss=0.13398800790309906\n",
            "step 4979: generator_loss=2.143766403198242, discriminator_loss=0.13438645005226135\n",
            "step 4980: generator_loss=2.1653518676757812, discriminator_loss=0.13283342123031616\n",
            "step 4981: generator_loss=2.152465581893921, discriminator_loss=0.13336455821990967\n",
            "step 4982: generator_loss=2.1339168548583984, discriminator_loss=0.13399258255958557\n",
            "step 4983: generator_loss=2.1236720085144043, discriminator_loss=0.134246826171875\n",
            "step 4984: generator_loss=2.1023638248443604, discriminator_loss=0.13521072268486023\n",
            "step 4985: generator_loss=2.0906291007995605, discriminator_loss=0.13538174331188202\n",
            "step 4986: generator_loss=2.128920793533325, discriminator_loss=0.13215261697769165\n",
            "step 4987: generator_loss=2.218891143798828, discriminator_loss=0.12584087252616882\n",
            "step 4988: generator_loss=2.316953659057617, discriminator_loss=0.11928167939186096\n",
            "step 4989: generator_loss=2.3867735862731934, discriminator_loss=0.11482350528240204\n",
            "step 4990: generator_loss=2.4279699325561523, discriminator_loss=0.11175166815519333\n",
            "step 4991: generator_loss=2.4805688858032227, discriminator_loss=0.10830501466989517\n",
            "step 4992: generator_loss=2.509681224822998, discriminator_loss=0.10581642389297485\n",
            "step 4993: generator_loss=2.5182723999023438, discriminator_loss=0.1043505072593689\n",
            "step 4994: generator_loss=2.5292696952819824, discriminator_loss=0.1027773767709732\n",
            "step 4995: generator_loss=2.573120594024658, discriminator_loss=0.09999820590019226\n",
            "step 4996: generator_loss=2.6112890243530273, discriminator_loss=0.09735031425952911\n",
            "step 4997: generator_loss=2.621886968612671, discriminator_loss=0.09612363576889038\n",
            "step 4998: generator_loss=2.6170573234558105, discriminator_loss=0.09535644948482513\n",
            "step 4999: generator_loss=2.58255934715271, discriminator_loss=0.09579361230134964\n",
            "step 5000: generator_loss=2.5311715602874756, discriminator_loss=0.09715184569358826\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "n_steps = 5_000\n",
        "steps_per_save = 250\n",
        "seed = 0\n",
        "key = jax.random.key(seed)\n",
        "batch_size = 128\n",
        "latent_dim = 64\n",
        "loss_type = \"nonsaturating\"\n",
        "\n",
        "# Train GAN and save checkpoints\n",
        "generator_training_state, discriminator_training_state, key = train_gan(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    n_steps=n_steps,\n",
        "    steps_per_save=steps_per_save,\n",
        "    key=key,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "184e87a8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_real_vs_generated_histogram_from_last_checkpoint(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    y_max: float = 0.02,\n",
        "):\n",
        "    \"\"\"Plot one real histogram and one generated histogram from last checkpoint.\n",
        "\n",
        "    Restores the generator from the latest checkpoint for this constraints\n",
        "    experiment and plots a single real vs generated histogram.\n",
        "    \"\"\"\n",
        "    # Restore generator from the last checkpoint for this constraints experiment\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    steps = sorted(ckpt_manager.all_steps())\n",
        "    if not steps:\n",
        "        raise ValueError(\n",
        "            f\"No checkpoints found for experiment_name={experiment_name!r}.\"\n",
        "        )\n",
        "\n",
        "    # Template states for restore (we only actually use the generator here)\n",
        "    gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=jax.random.key(0),\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "    restored = ckpt_manager.restore(\n",
        "        steps[-1],\n",
        "        args=ocp.args.StandardRestore(\n",
        "            item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "        ),\n",
        "    )\n",
        "    generator_for_plot = restored[\"generator\"]\n",
        "\n",
        "    # Local PRNG keys for this plotting run\n",
        "    key = jax.random.key(0)\n",
        "    key, z_key, real_key = jax.random.split(key, 3)\n",
        "\n",
        "    # Sample a latent vector and generate one fake histogram\n",
        "    z = jax.random.normal(z_key, (1, latent_dim))\n",
        "    fake_hist = generator_for_plot.apply_fn(\n",
        "        {\"params\": generator_for_plot.params},\n",
        "        z,\n",
        "    )[0]\n",
        "\n",
        "    # Sample one real histogram from the training set\n",
        "    real_idx = jax.random.randint(\n",
        "        real_key, shape=(), minval=0, maxval=train_data.shape[0]\n",
        "    )\n",
        "    real_hist = train_data[real_idx]\n",
        "\n",
        "    # Ensure both are non-negative and normalized to sum to 1\n",
        "    fake_hist = jnp.maximum(fake_hist, 0.0)\n",
        "    fake_hist = fake_hist / fake_hist.sum()\n",
        "    real_hist = real_hist / real_hist.sum()\n",
        "\n",
        "    # Convert to NumPy for plotting\n",
        "    fake_hist_np = np.array(fake_hist)\n",
        "    real_hist_np = np.array(real_hist)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(real_hist_np, label=\"Real histogram\")\n",
        "    plt.plot(fake_hist_np, label=\"Generated histogram\")\n",
        "    plt.xlabel(\"Bin index\")\n",
        "    plt.ylabel(\"Probability (normalized)\")\n",
        "    plt.ylim(top=y_max)\n",
        "    plt.title(\"Real vs Generated Histogram\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot using the latest checkpoint available (e.g. after first 5000 steps)\n",
        "plot_real_vs_generated_histogram_from_last_checkpoint(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        "    y_max=0.02,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb76bbaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_experiment_for_loss_type(\n",
        "#     loss_type: str,\n",
        "#     train_data,\n",
        "#     learning_rate: float = 1e-3,\n",
        "#     batch_size: int = 128,\n",
        "#     latent_dim: int = 64,\n",
        "#     n_steps: int = 5_000,\n",
        "#     steps_per_save: int = 250,\n",
        "#     seed: int = 0,\n",
        "# ):\n",
        "#     \"\"\"Train GAN and save checkpoints for a given loss_type.\n",
        "\n",
        "#     This function returns training results; plotting is handled separately.\n",
        "\n",
        "#     loss_type: \"nonsaturating\" or \"saturating\".\n",
        "#     \"\"\"\n",
        "#     print(f\"=== Running experiment with loss_type={loss_type} ===\")\n",
        "\n",
        "#     # Optimizer and PRNG key\n",
        "#     optimizer = optax.adam(learning_rate)\n",
        "#     key = jax.random.key(seed)\n",
        "\n",
        "#     # Train GAN and save checkpoints\n",
        "#     generator_training_state, discriminator_training_state, key = train_gan(\n",
        "#         train_data=train_data,\n",
        "#         optimizer=optimizer,\n",
        "#         n_steps=n_steps,\n",
        "#         steps_per_save=steps_per_save,\n",
        "#         key=key,\n",
        "#         batch_size=batch_size,\n",
        "#         latent_dim=latent_dim,\n",
        "#         loss_type=loss_type,\n",
        "#     )\n",
        "\n",
        "#     # --- Loss curves over checkpoints for this loss_type ---\n",
        "#     gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "#         optimizer,\n",
        "#         key=jax.random.key(0),\n",
        "#         latent_dim=latent_dim,\n",
        "#     )\n",
        "#     experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "#     ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "#     steps = sorted(ckpt_manager.all_steps())\n",
        "\n",
        "#     g_losses, d_losses = [], []\n",
        "#     real_images = train_data[\"image\"]\n",
        "\n",
        "#     for step in steps:\n",
        "#         restored = ckpt_manager.restore(\n",
        "#             step,\n",
        "#             args=ocp.args.StandardRestore(\n",
        "#                 item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "#             ),\n",
        "#         )\n",
        "#         gen_state = restored[\"generator\"]\n",
        "#         disc_state = restored[\"discriminator\"]\n",
        "\n",
        "#         key = jax.random.key(0)\n",
        "#         key, z_key = jax.random.split(key)\n",
        "#         z_vectors = jax.random.normal(\n",
        "#             z_key,\n",
        "#             (real_images.shape[0], latent_dim),\n",
        "#         )\n",
        "\n",
        "#         g_loss = calculate_generator_loss(\n",
        "#             gen_state.params,\n",
        "#             disc_state.params,\n",
        "#             gen_state.apply_fn,\n",
        "#             disc_state.apply_fn,\n",
        "#             z_vectors,\n",
        "#             loss_type=loss_type,\n",
        "#         )\n",
        "#         d_loss = calculate_discriminator_loss(\n",
        "#             disc_state.params,\n",
        "#             gen_state.params,\n",
        "#             gen_state.apply_fn,\n",
        "#             disc_state.apply_fn,\n",
        "#             z_vectors,\n",
        "#             real_images,\n",
        "#         )\n",
        "\n",
        "#         g_losses.append(float(g_loss))\n",
        "#         d_losses.append(float(d_loss))\n",
        "\n",
        "#     # Collect results (plotting handled separately)\n",
        "#     return {\n",
        "#         \"generator_state\": generator_training_state,\n",
        "#         \"discriminator_state\": discriminator_training_state,\n",
        "#         \"steps\": steps,\n",
        "#         \"g_losses\": g_losses,\n",
        "#         \"d_losses\": d_losses,\n",
        "#         \"loss_type\": loss_type,\n",
        "#         \"latent_dim\": latent_dim,\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda4c1e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_experiment_for_loss_type(results, train_data):\n",
        "#     \"\"\"Plot sample images and loss dynamics for a single loss_type run.\n",
        "\n",
        "#     Expects the dict returned by `run_experiment_for_loss_type` and the\n",
        "#     corresponding `train_data` used for training.\n",
        "#     \"\"\"\n",
        "#     loss_type = results[\"loss_type\"]\n",
        "#     generator_training_state = results[\"generator_state\"]\n",
        "#     steps = results[\"steps\"]\n",
        "#     g_losses = results[\"g_losses\"]\n",
        "#     d_losses = results[\"d_losses\"]\n",
        "#     latent_dim = results.get(\"latent_dim\", 64)\n",
        "\n",
        "#     # --- Plot a sample real vs fake image ---\n",
        "#     key = jax.random.key(0)\n",
        "#     key, z_key, real_image_key = jax.random.split(key, 3)\n",
        "#     z_vector = jax.random.normal(z_key, (1, latent_dim))\n",
        "#     real_idx = jax.random.randint(\n",
        "#         real_image_key, shape=(), minval=0, maxval=train_data.shape[0]\n",
        "#     )\n",
        "\n",
        "#     fake_image_flat = generator_training_state.apply_fn(\n",
        "#         {\"params\": generator_training_state.params},\n",
        "#         z_vector,\n",
        "#     )\n",
        "#     fake_image = fake_image_flat[0].reshape(28, 28)\n",
        "\n",
        "#     real_image_flat = train_data[real_idx]\n",
        "#     real_image = real_image_flat.reshape(28, 28)\n",
        "\n",
        "#     fig, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
        "#     axes[0].imshow(real_image, cmap=\"gray\"); axes[0].set_title(f\"Real ({loss_type})\"); axes[0].axis(\"off\")\n",
        "#     axes[1].imshow(fake_image, cmap=\"gray\"); axes[1].set_title(f\"Fake ({loss_type})\"); axes[1].axis(\"off\")\n",
        "#     plt.tight_layout(); plt.show()\n",
        "\n",
        "#     # --- Plot loss curves over checkpoints for this loss_type ---\n",
        "#     plt.figure()\n",
        "#     plt.plot(steps, g_losses, label=f\"generator ({loss_type})\")\n",
        "#     plt.plot(steps, d_losses, label=f\"discriminator ({loss_type})\")\n",
        "#     plt.xlabel(\"training step\")\n",
        "#     plt.ylabel(\"loss\")\n",
        "#     plt.title(f\"GAN losses ({loss_type})\")\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n",
        "\n",
        "#     # --- Plot training dynamics in (g, d) loss space ---\n",
        "#     g = np.array(g_losses)\n",
        "#     d = np.array(d_losses)\n",
        "\n",
        "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
        "#     points = np.column_stack([g, d])\n",
        "#     segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "#     t = np.linspace(0.0, 1.0, len(points) - 1)\n",
        "#     colors = cm.coolwarm(t)\n",
        "\n",
        "#     lc = LineCollection(segments, colors=colors, linewidths=1.5)\n",
        "#     ax.add_collection(lc)\n",
        "\n",
        "#     ax.scatter(g, d, c=t.tolist() + [1.0], cmap=\"coolwarm\", s=8, alpha=0.7)\n",
        "#     ax.scatter(g[0], d[0], color=\"blue\", s=50, label=\"start\")\n",
        "#     ax.scatter(g[-1], d[-1], color=\"red\", s=50, label=\"end\")\n",
        "\n",
        "#     ax.set_xlabel(\"Generator loss\")\n",
        "#     ax.set_ylabel(\"Discriminator loss\")\n",
        "#     ax.set_title(f\"GAN training dynamics in loss space ({loss_type})\")\n",
        "#     ax.grid(True)\n",
        "#     ax.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6610b8cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Convenience: run the full experiment for both loss types\n",
        "\n",
        "# results_by_loss_type = {}\n",
        "# for lt in [\"saturating\", \"nonsaturating\"]:\n",
        "#     results_by_loss_type[lt] = run_experiment_for_loss_type(\n",
        "#         loss_type=lt,\n",
        "#         train_data=train_data,\n",
        "#         learning_rate=1e-3,\n",
        "#         batch_size=128,\n",
        "#         latent_dim=64,\n",
        "#         n_steps=5_000,\n",
        "#         steps_per_save=250,\n",
        "#         seed=0,\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a819429",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot_experiment_for_loss_type(results_by_loss_type[\"saturating\"], train_data)\n",
        "# plot_experiment_for_loss_type(results_by_loss_type[\"nonsaturating\"], train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c19808",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# out_dir = Path(\"../../lab-notes/2025-12-17_gan-nonsaturating_loss\")\n",
        "# out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# for i, num in enumerate(sorted(plt.get_fignums()), start=1):\n",
        "#     fig = plt.figure(num)\n",
        "#     fig.savefig(\n",
        "#         out_dir / f\"2025-12-17_gan-nonsaturating_loss_{i}.png\",\n",
        "#         dpi=200,\n",
        "#         bbox_inches=\"tight\",\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b932c49e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    batch_size: int,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "):\n",
        "    \"\"\"Reload checkpoints for this notebook's run and plot D vs G loss.\n",
        "\n",
        "    Uses the same naming convention as `train_gan` and evaluates losses\n",
        "    at each saved checkpoint on a fresh mini-batch from `train_data`.\n",
        "    \"\"\"\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    steps = sorted(ckpt_manager.all_steps())\n",
        "\n",
        "    if not steps:\n",
        "        print(f\"No checkpoints found for experiment_name={experiment_name!r}.\")\n",
        "        return\n",
        "\n",
        "    # Template states for restoring checkpoints\n",
        "    gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=jax.random.key(0),\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    for step in steps:\n",
        "        restored = ckpt_manager.restore(\n",
        "            step,\n",
        "            args=ocp.args.StandardRestore(\n",
        "                item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "            ),\n",
        "        )\n",
        "        gen_state = restored[\"generator\"]\n",
        "        disc_state = restored[\"discriminator\"]\n",
        "\n",
        "        # Use a fresh key for evaluation at each checkpoint\n",
        "        key_eval = jax.random.key(0)\n",
        "        key_eval, key_z, key_real = jax.random.split(key_eval, 3)\n",
        "\n",
        "        z_vectors = jax.random.normal(key_z, (batch_size, latent_dim))\n",
        "        real_images_batch = subsample_images_for_batch(key_real, train_data, batch_size)\n",
        "\n",
        "        g_loss = calculate_generator_loss(\n",
        "            gen_state.params,\n",
        "            disc_state.params,\n",
        "            gen_state.apply_fn,\n",
        "            disc_state.apply_fn,\n",
        "            z_vectors,\n",
        "            loss_type=loss_type,\n",
        "        )\n",
        "        d_loss = calculate_discriminator_loss(\n",
        "            disc_state.params,\n",
        "            gen_state.params,\n",
        "            gen_state.apply_fn,\n",
        "            disc_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "\n",
        "        g_losses.append(float(g_loss))\n",
        "        d_losses.append(float(d_loss))\n",
        "\n",
        "    # Convert to NumPy arrays for plotting\n",
        "    g = np.array(g_losses)\n",
        "    d = np.array(d_losses)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    points = np.column_stack([d, g])  # x: discriminator, y: generator\n",
        "\n",
        "    if len(points) > 1:\n",
        "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "        t = np.linspace(0.0, 1.0, len(points) - 1)\n",
        "        colors = cm.coolwarm(t)\n",
        "\n",
        "        lc = LineCollection(segments, colors=colors, linewidths=1.5)\n",
        "        ax.add_collection(lc)\n",
        "\n",
        "    # Scatter the points (including first/last)\n",
        "    ax.scatter(points[:, 0], points[:, 1], c=np.linspace(0.0, 1.0, len(points)), cmap=\"coolwarm\", s=12, alpha=0.8)\n",
        "\n",
        "    if len(points) > 0:\n",
        "        ax.scatter(points[0, 0], points[0, 1], color=\"blue\", s=60, label=\"start\")\n",
        "        ax.scatter(points[-1, 0], points[-1, 1], color=\"red\", s=60, label=\"end\")\n",
        "\n",
        "    ax.set_xlabel(\"Discriminator loss\")\n",
        "    ax.set_ylabel(\"Generator loss\")\n",
        "    ax.set_title(f\"GAN training dynamics in loss space ({loss_type})\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "e52d1762",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6bfb59f",
      "metadata": {},
      "source": [
        "Extended training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b792d56f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming training from global step 5000 (experiment_name='gan_constraints_GradientTransformationExtraArgs_nonsaturating')\n",
            "step 5001: generator_loss=2.4563093185424805, discriminator_loss=0.09967628121376038\n",
            "step 5002: generator_loss=2.379891872406006, discriminator_loss=0.10261476039886475\n",
            "step 5003: generator_loss=2.3234171867370605, discriminator_loss=0.10505685955286026\n",
            "step 5004: generator_loss=2.298403739929199, discriminator_loss=0.10610902309417725\n",
            "step 5005: generator_loss=2.301145553588867, discriminator_loss=0.10555579513311386\n",
            "step 5006: generator_loss=2.302332878112793, discriminator_loss=0.10532952845096588\n",
            "step 5007: generator_loss=2.297199249267578, discriminator_loss=0.10559152066707611\n",
            "step 5008: generator_loss=2.307370901107788, discriminator_loss=0.1050216406583786\n",
            "step 5009: generator_loss=2.3286795616149902, discriminator_loss=0.10418546199798584\n",
            "step 5010: generator_loss=2.3299927711486816, discriminator_loss=0.10436764359474182\n",
            "step 5011: generator_loss=2.347010612487793, discriminator_loss=0.10373232513666153\n",
            "step 5012: generator_loss=2.367783308029175, discriminator_loss=0.10294243693351746\n",
            "step 5013: generator_loss=2.4002573490142822, discriminator_loss=0.1015496551990509\n",
            "step 5014: generator_loss=2.4240212440490723, discriminator_loss=0.10079266130924225\n",
            "step 5015: generator_loss=2.4841411113739014, discriminator_loss=0.09817788004875183\n",
            "step 5016: generator_loss=2.525458335876465, discriminator_loss=0.09663469344377518\n",
            "step 5017: generator_loss=2.5365092754364014, discriminator_loss=0.09624636173248291\n",
            "step 5018: generator_loss=2.5269579887390137, discriminator_loss=0.09671398252248764\n",
            "step 5019: generator_loss=2.4870946407318115, discriminator_loss=0.09849503636360168\n",
            "step 5020: generator_loss=2.4434962272644043, discriminator_loss=0.10032420605421066\n",
            "step 5021: generator_loss=2.4209954738616943, discriminator_loss=0.10128937661647797\n",
            "step 5022: generator_loss=2.3872616291046143, discriminator_loss=0.10275501012802124\n",
            "step 5023: generator_loss=2.37229585647583, discriminator_loss=0.10341252386569977\n",
            "step 5024: generator_loss=2.406231641769409, discriminator_loss=0.10164976119995117\n",
            "step 5025: generator_loss=2.4468166828155518, discriminator_loss=0.09952595084905624\n",
            "step 5026: generator_loss=2.4696245193481445, discriminator_loss=0.09847407042980194\n",
            "step 5027: generator_loss=2.4781527519226074, discriminator_loss=0.09801141917705536\n",
            "step 5028: generator_loss=2.497587203979492, discriminator_loss=0.0971837043762207\n",
            "step 5029: generator_loss=2.5160136222839355, discriminator_loss=0.09650063514709473\n",
            "step 5030: generator_loss=2.5416529178619385, discriminator_loss=0.09536577761173248\n",
            "step 5031: generator_loss=2.579730987548828, discriminator_loss=0.09362620860338211\n",
            "step 5032: generator_loss=2.626033306121826, discriminator_loss=0.0917753055691719\n",
            "step 5033: generator_loss=2.6512067317962646, discriminator_loss=0.09062507748603821\n",
            "step 5034: generator_loss=2.6753029823303223, discriminator_loss=0.0893486887216568\n",
            "step 5035: generator_loss=2.715160608291626, discriminator_loss=0.08753132820129395\n",
            "step 5036: generator_loss=2.737748622894287, discriminator_loss=0.08625711500644684\n",
            "step 5037: generator_loss=2.7485392093658447, discriminator_loss=0.08527359366416931\n",
            "step 5038: generator_loss=2.7839643955230713, discriminator_loss=0.08369144052267075\n",
            "step 5039: generator_loss=2.7986953258514404, discriminator_loss=0.08263526111841202\n",
            "step 5040: generator_loss=2.8032350540161133, discriminator_loss=0.08194282650947571\n",
            "step 5041: generator_loss=2.8317534923553467, discriminator_loss=0.08042549341917038\n",
            "step 5042: generator_loss=2.8800253868103027, discriminator_loss=0.07844170928001404\n",
            "step 5043: generator_loss=2.9395577907562256, discriminator_loss=0.07611849159002304\n",
            "step 5044: generator_loss=2.9816272258758545, discriminator_loss=0.07440914958715439\n",
            "step 5045: generator_loss=2.9739298820495605, discriminator_loss=0.07389622926712036\n",
            "step 5046: generator_loss=2.9203708171844482, discriminator_loss=0.07469258457422256\n",
            "step 5047: generator_loss=2.856144905090332, discriminator_loss=0.07595810294151306\n",
            "step 5048: generator_loss=2.855891704559326, discriminator_loss=0.07541478425264359\n",
            "step 5049: generator_loss=2.879878282546997, discriminator_loss=0.074160635471344\n",
            "step 5050: generator_loss=2.888612747192383, discriminator_loss=0.0733727365732193\n",
            "step 5051: generator_loss=2.888582468032837, discriminator_loss=0.07297418266534805\n",
            "step 5052: generator_loss=2.875255823135376, discriminator_loss=0.07306331396102905\n",
            "step 5053: generator_loss=2.854116201400757, discriminator_loss=0.07340400665998459\n",
            "step 5054: generator_loss=2.816598415374756, discriminator_loss=0.07433902472257614\n",
            "step 5055: generator_loss=2.7668874263763428, discriminator_loss=0.07569995522499084\n",
            "step 5056: generator_loss=2.746628999710083, discriminator_loss=0.07627600431442261\n",
            "step 5057: generator_loss=2.7556610107421875, discriminator_loss=0.07574370503425598\n",
            "step 5058: generator_loss=2.7543857097625732, discriminator_loss=0.07569524645805359\n",
            "step 5059: generator_loss=2.7238121032714844, discriminator_loss=0.07663615047931671\n",
            "step 5060: generator_loss=2.687797784805298, discriminator_loss=0.07787837088108063\n",
            "step 5061: generator_loss=2.655115842819214, discriminator_loss=0.07908361405134201\n",
            "step 5062: generator_loss=2.6332786083221436, discriminator_loss=0.07988855987787247\n",
            "step 5063: generator_loss=2.6122958660125732, discriminator_loss=0.0807810053229332\n",
            "step 5064: generator_loss=2.5875439643859863, discriminator_loss=0.08183145523071289\n",
            "step 5065: generator_loss=2.5845837593078613, discriminator_loss=0.08203282952308655\n",
            "step 5066: generator_loss=2.5982017517089844, discriminator_loss=0.08150549232959747\n",
            "step 5067: generator_loss=2.6053993701934814, discriminator_loss=0.08128244429826736\n",
            "step 5068: generator_loss=2.624382972717285, discriminator_loss=0.08060432970523834\n",
            "step 5069: generator_loss=2.6420912742614746, discriminator_loss=0.07999292016029358\n",
            "step 5070: generator_loss=2.657357692718506, discriminator_loss=0.07945549488067627\n",
            "step 5071: generator_loss=2.648623466491699, discriminator_loss=0.07989820092916489\n",
            "step 5072: generator_loss=2.646547317504883, discriminator_loss=0.07997094839811325\n",
            "step 5073: generator_loss=2.6383557319641113, discriminator_loss=0.08028408139944077\n",
            "step 5074: generator_loss=2.6293861865997314, discriminator_loss=0.08063860237598419\n",
            "step 5075: generator_loss=2.618471145629883, discriminator_loss=0.08103541284799576\n",
            "step 5076: generator_loss=2.5948572158813477, discriminator_loss=0.0819811224937439\n",
            "step 5077: generator_loss=2.5684094429016113, discriminator_loss=0.0830124169588089\n",
            "step 5078: generator_loss=2.5363426208496094, discriminator_loss=0.08441346883773804\n",
            "step 5079: generator_loss=2.5161709785461426, discriminator_loss=0.08534617722034454\n",
            "step 5080: generator_loss=2.5068178176879883, discriminator_loss=0.08580181747674942\n",
            "step 5081: generator_loss=2.5044236183166504, discriminator_loss=0.08608728647232056\n",
            "step 5082: generator_loss=2.5349233150482178, discriminator_loss=0.08488581329584122\n",
            "step 5083: generator_loss=2.58459734916687, discriminator_loss=0.08285322785377502\n",
            "step 5084: generator_loss=2.6141629219055176, discriminator_loss=0.08190619200468063\n",
            "step 5085: generator_loss=2.6393327713012695, discriminator_loss=0.08111394941806793\n",
            "step 5086: generator_loss=2.646807909011841, discriminator_loss=0.08095443248748779\n",
            "step 5087: generator_loss=2.6428396701812744, discriminator_loss=0.08131518214941025\n",
            "step 5088: generator_loss=2.6161680221557617, discriminator_loss=0.08244740962982178\n",
            "step 5089: generator_loss=2.5666863918304443, discriminator_loss=0.08442133665084839\n",
            "step 5090: generator_loss=2.509690999984741, discriminator_loss=0.08696722239255905\n",
            "step 5091: generator_loss=2.4848244190216064, discriminator_loss=0.08808423578739166\n",
            "step 5092: generator_loss=2.4892022609710693, discriminator_loss=0.08777649700641632\n",
            "step 5093: generator_loss=2.509044647216797, discriminator_loss=0.08694218844175339\n",
            "step 5094: generator_loss=2.5321030616760254, discriminator_loss=0.08588899672031403\n",
            "step 5095: generator_loss=2.5687618255615234, discriminator_loss=0.08444270491600037\n",
            "step 5096: generator_loss=2.613309383392334, discriminator_loss=0.08268240094184875\n",
            "step 5097: generator_loss=2.6514718532562256, discriminator_loss=0.08126750588417053\n",
            "step 5098: generator_loss=2.6762900352478027, discriminator_loss=0.08050575852394104\n",
            "step 5099: generator_loss=2.6908092498779297, discriminator_loss=0.08005426824092865\n",
            "step 5100: generator_loss=2.7030653953552246, discriminator_loss=0.07975348830223083\n",
            "step 5101: generator_loss=2.732076406478882, discriminator_loss=0.07869590818881989\n",
            "step 5102: generator_loss=2.7589163780212402, discriminator_loss=0.07773743569850922\n",
            "step 5103: generator_loss=2.7737255096435547, discriminator_loss=0.07719232141971588\n",
            "step 5104: generator_loss=2.7746622562408447, discriminator_loss=0.07688339054584503\n",
            "step 5105: generator_loss=2.7516276836395264, discriminator_loss=0.07741716504096985\n",
            "step 5106: generator_loss=2.712686538696289, discriminator_loss=0.07838413119316101\n",
            "step 5107: generator_loss=2.6574673652648926, discriminator_loss=0.08018799126148224\n",
            "step 5108: generator_loss=2.5898923873901367, discriminator_loss=0.08243022859096527\n",
            "step 5109: generator_loss=2.540776491165161, discriminator_loss=0.08421306312084198\n",
            "step 5110: generator_loss=2.5136256217956543, discriminator_loss=0.08512256294488907\n",
            "step 5111: generator_loss=2.502953290939331, discriminator_loss=0.08542903512716293\n",
            "step 5112: generator_loss=2.534942626953125, discriminator_loss=0.08382219076156616\n",
            "step 5113: generator_loss=2.6211557388305664, discriminator_loss=0.0801086351275444\n",
            "step 5114: generator_loss=2.702120780944824, discriminator_loss=0.07693804800510406\n",
            "step 5115: generator_loss=2.769580125808716, discriminator_loss=0.07449881732463837\n",
            "step 5116: generator_loss=2.8504769802093506, discriminator_loss=0.07171594351530075\n",
            "step 5117: generator_loss=2.9320149421691895, discriminator_loss=0.06915399432182312\n",
            "step 5118: generator_loss=2.975250720977783, discriminator_loss=0.06772012263536453\n",
            "step 5119: generator_loss=3.0154895782470703, discriminator_loss=0.06638304889202118\n",
            "step 5120: generator_loss=3.0491795539855957, discriminator_loss=0.06521965563297272\n",
            "step 5121: generator_loss=3.0816292762756348, discriminator_loss=0.06404174119234085\n",
            "step 5122: generator_loss=3.108647346496582, discriminator_loss=0.06304778158664703\n",
            "step 5123: generator_loss=3.1059513092041016, discriminator_loss=0.06267838180065155\n",
            "step 5124: generator_loss=3.090756893157959, discriminator_loss=0.06261806190013885\n",
            "step 5125: generator_loss=3.0620603561401367, discriminator_loss=0.0629202276468277\n",
            "step 5126: generator_loss=3.03523588180542, discriminator_loss=0.06314957141876221\n",
            "step 5127: generator_loss=3.001511335372925, discriminator_loss=0.0635937750339508\n",
            "step 5128: generator_loss=2.9542417526245117, discriminator_loss=0.06445625424385071\n",
            "step 5129: generator_loss=2.884516716003418, discriminator_loss=0.06599331647157669\n",
            "step 5130: generator_loss=2.803898811340332, discriminator_loss=0.06824392080307007\n",
            "step 5131: generator_loss=2.7134313583374023, discriminator_loss=0.07094009965658188\n",
            "step 5132: generator_loss=2.6357836723327637, discriminator_loss=0.07361601293087006\n",
            "step 5133: generator_loss=2.5834689140319824, discriminator_loss=0.07551564276218414\n",
            "step 5134: generator_loss=2.569105625152588, discriminator_loss=0.07609695196151733\n",
            "step 5135: generator_loss=2.5763792991638184, discriminator_loss=0.07580316066741943\n",
            "step 5136: generator_loss=2.5960519313812256, discriminator_loss=0.07523380219936371\n",
            "step 5137: generator_loss=2.6279165744781494, discriminator_loss=0.0742487981915474\n",
            "step 5138: generator_loss=2.6936328411102295, discriminator_loss=0.07202926278114319\n",
            "step 5139: generator_loss=2.794875144958496, discriminator_loss=0.06871560215950012\n",
            "step 5140: generator_loss=2.888364315032959, discriminator_loss=0.06636247038841248\n",
            "step 5141: generator_loss=2.9141316413879395, discriminator_loss=0.06603093445301056\n",
            "step 5142: generator_loss=2.9468181133270264, discriminator_loss=0.06469090282917023\n",
            "step 5143: generator_loss=2.9052271842956543, discriminator_loss=0.06711605191230774\n",
            "step 5144: generator_loss=2.901247501373291, discriminator_loss=0.06752108037471771\n",
            "step 5145: generator_loss=2.8355093002319336, discriminator_loss=0.07259777188301086\n",
            "step 5146: generator_loss=2.8566768169403076, discriminator_loss=0.07495633512735367\n",
            "step 5147: generator_loss=2.8098440170288086, discriminator_loss=0.0854569524526596\n",
            "step 5148: generator_loss=2.9291343688964844, discriminator_loss=0.0844806358218193\n",
            "step 5149: generator_loss=2.895096778869629, discriminator_loss=0.09696829319000244\n",
            "step 5150: generator_loss=2.9128832817077637, discriminator_loss=0.10987027734518051\n",
            "step 5151: generator_loss=2.9411323070526123, discriminator_loss=0.1044289618730545\n",
            "step 5152: generator_loss=3.100755214691162, discriminator_loss=0.09730406105518341\n",
            "step 5153: generator_loss=3.1174628734588623, discriminator_loss=0.11376097798347473\n",
            "step 5154: generator_loss=3.0856897830963135, discriminator_loss=0.11461475491523743\n",
            "step 5155: generator_loss=3.196207046508789, discriminator_loss=0.11513771116733551\n",
            "step 5156: generator_loss=3.2352566719055176, discriminator_loss=0.11593478918075562\n",
            "step 5157: generator_loss=3.184957504272461, discriminator_loss=0.11598353832960129\n",
            "step 5158: generator_loss=3.476736545562744, discriminator_loss=0.09472928941249847\n",
            "step 5159: generator_loss=3.6487855911254883, discriminator_loss=0.08663704991340637\n",
            "step 5160: generator_loss=3.542863130569458, discriminator_loss=0.0876052975654602\n",
            "step 5161: generator_loss=3.484856367111206, discriminator_loss=0.09351903200149536\n",
            "step 5162: generator_loss=3.7329354286193848, discriminator_loss=0.07716155052185059\n",
            "step 5163: generator_loss=3.545708417892456, discriminator_loss=0.08441168069839478\n",
            "step 5164: generator_loss=4.088303089141846, discriminator_loss=0.06569654494524002\n",
            "step 5165: generator_loss=4.041738986968994, discriminator_loss=0.06843847036361694\n",
            "step 5166: generator_loss=4.03890323638916, discriminator_loss=0.06839456409215927\n",
            "step 5167: generator_loss=3.813889741897583, discriminator_loss=0.07202066481113434\n",
            "step 5168: generator_loss=4.233224868774414, discriminator_loss=0.06154263764619827\n",
            "step 5169: generator_loss=4.269454002380371, discriminator_loss=0.0601559616625309\n",
            "step 5170: generator_loss=4.239872932434082, discriminator_loss=0.05988595262169838\n",
            "step 5171: generator_loss=4.1367058753967285, discriminator_loss=0.06257512420415878\n",
            "step 5172: generator_loss=4.148894309997559, discriminator_loss=0.060840364545583725\n",
            "step 5173: generator_loss=3.91331148147583, discriminator_loss=0.06519554555416107\n",
            "step 5174: generator_loss=4.021572113037109, discriminator_loss=0.06036905199289322\n",
            "step 5175: generator_loss=4.128462791442871, discriminator_loss=0.058080993592739105\n",
            "step 5176: generator_loss=3.82832670211792, discriminator_loss=0.06077995151281357\n",
            "step 5177: generator_loss=3.729794979095459, discriminator_loss=0.06526626646518707\n",
            "step 5178: generator_loss=3.7095189094543457, discriminator_loss=0.058263517916202545\n",
            "step 5179: generator_loss=3.5441527366638184, discriminator_loss=0.06047354266047478\n",
            "step 5180: generator_loss=3.49220609664917, discriminator_loss=0.06077268719673157\n",
            "step 5181: generator_loss=3.3064687252044678, discriminator_loss=0.06374579668045044\n",
            "step 5182: generator_loss=3.1213860511779785, discriminator_loss=0.06727481633424759\n",
            "step 5183: generator_loss=3.0561301708221436, discriminator_loss=0.06690550595521927\n",
            "step 5184: generator_loss=3.029317855834961, discriminator_loss=0.06693409383296967\n",
            "step 5185: generator_loss=3.033702850341797, discriminator_loss=0.06678670644760132\n",
            "step 5186: generator_loss=2.904332160949707, discriminator_loss=0.06954269856214523\n",
            "step 5187: generator_loss=2.9635143280029297, discriminator_loss=0.0690421313047409\n",
            "step 5188: generator_loss=2.907569408416748, discriminator_loss=0.0689757838845253\n",
            "step 5189: generator_loss=2.7402851581573486, discriminator_loss=0.07444827258586884\n",
            "step 5190: generator_loss=2.791471004486084, discriminator_loss=0.0726727694272995\n",
            "step 5191: generator_loss=2.887748956680298, discriminator_loss=0.06935295462608337\n",
            "step 5192: generator_loss=2.84185528755188, discriminator_loss=0.06983475387096405\n",
            "step 5193: generator_loss=3.026202440261841, discriminator_loss=0.0667884424328804\n",
            "step 5194: generator_loss=3.0887680053710938, discriminator_loss=0.06273448467254639\n",
            "step 5195: generator_loss=2.9857442378997803, discriminator_loss=0.06250867247581482\n",
            "step 5196: generator_loss=3.0449466705322266, discriminator_loss=0.06175900995731354\n",
            "step 5197: generator_loss=2.991913080215454, discriminator_loss=0.061727650463581085\n",
            "step 5198: generator_loss=3.0016770362854004, discriminator_loss=0.06275945901870728\n",
            "step 5199: generator_loss=2.9200663566589355, discriminator_loss=0.06391865015029907\n",
            "step 5200: generator_loss=3.0168347358703613, discriminator_loss=0.06258919835090637\n",
            "step 5201: generator_loss=2.946629524230957, discriminator_loss=0.06375055015087128\n",
            "step 5202: generator_loss=2.92002534866333, discriminator_loss=0.06458614766597748\n",
            "step 5203: generator_loss=2.9229190349578857, discriminator_loss=0.06604858487844467\n",
            "step 5204: generator_loss=2.91856050491333, discriminator_loss=0.06596298515796661\n",
            "step 5205: generator_loss=2.911350965499878, discriminator_loss=0.06665213406085968\n",
            "step 5206: generator_loss=2.8793482780456543, discriminator_loss=0.0670769214630127\n",
            "step 5207: generator_loss=2.766436815261841, discriminator_loss=0.06884023547172546\n",
            "step 5208: generator_loss=2.7897324562072754, discriminator_loss=0.06931252032518387\n",
            "step 5209: generator_loss=2.763915538787842, discriminator_loss=0.07015635073184967\n",
            "step 5210: generator_loss=2.6791749000549316, discriminator_loss=0.07235177606344223\n",
            "step 5211: generator_loss=2.6936964988708496, discriminator_loss=0.07243519276380539\n",
            "step 5212: generator_loss=2.5854551792144775, discriminator_loss=0.07505639642477036\n",
            "step 5213: generator_loss=2.5575385093688965, discriminator_loss=0.07642355561256409\n",
            "step 5214: generator_loss=2.50197172164917, discriminator_loss=0.07769320905208588\n",
            "step 5215: generator_loss=2.528398275375366, discriminator_loss=0.07754378020763397\n",
            "step 5216: generator_loss=2.5075032711029053, discriminator_loss=0.0779523104429245\n",
            "step 5217: generator_loss=2.599148750305176, discriminator_loss=0.07667148858308792\n",
            "step 5218: generator_loss=2.4791338443756104, discriminator_loss=0.0794350877404213\n",
            "step 5219: generator_loss=2.505077362060547, discriminator_loss=0.079596608877182\n",
            "step 5220: generator_loss=2.481684684753418, discriminator_loss=0.08049458265304565\n",
            "step 5221: generator_loss=2.5003325939178467, discriminator_loss=0.08172260224819183\n",
            "step 5222: generator_loss=2.4318466186523438, discriminator_loss=0.08382149040699005\n",
            "step 5223: generator_loss=2.361760139465332, discriminator_loss=0.08717197179794312\n",
            "step 5224: generator_loss=2.307187080383301, discriminator_loss=0.09048289805650711\n",
            "step 5225: generator_loss=2.2504184246063232, discriminator_loss=0.09413757920265198\n",
            "step 5226: generator_loss=2.2826032638549805, discriminator_loss=0.0944385975599289\n",
            "step 5227: generator_loss=2.2076172828674316, discriminator_loss=0.09836986660957336\n",
            "step 5228: generator_loss=2.1726536750793457, discriminator_loss=0.10000629723072052\n",
            "step 5229: generator_loss=2.2050626277923584, discriminator_loss=0.09914980828762054\n",
            "step 5230: generator_loss=2.2049400806427, discriminator_loss=0.10015657544136047\n",
            "step 5231: generator_loss=2.2207753658294678, discriminator_loss=0.09998023509979248\n",
            "step 5232: generator_loss=2.2712454795837402, discriminator_loss=0.09898867458105087\n",
            "step 5233: generator_loss=2.239159345626831, discriminator_loss=0.10084106028079987\n",
            "step 5234: generator_loss=2.276315927505493, discriminator_loss=0.09988829493522644\n",
            "step 5235: generator_loss=2.36092472076416, discriminator_loss=0.09671159088611603\n",
            "step 5236: generator_loss=2.4386768341064453, discriminator_loss=0.09413830935955048\n",
            "step 5237: generator_loss=2.4974491596221924, discriminator_loss=0.09282086044549942\n",
            "step 5238: generator_loss=2.566035270690918, discriminator_loss=0.09100479632616043\n",
            "step 5239: generator_loss=2.6400082111358643, discriminator_loss=0.08913403004407883\n",
            "step 5240: generator_loss=2.6867406368255615, discriminator_loss=0.08831828832626343\n",
            "step 5241: generator_loss=2.6833455562591553, discriminator_loss=0.08909063786268234\n",
            "step 5242: generator_loss=2.6467578411102295, discriminator_loss=0.09079441428184509\n",
            "step 5243: generator_loss=2.6133625507354736, discriminator_loss=0.09215380996465683\n",
            "step 5244: generator_loss=2.5750696659088135, discriminator_loss=0.09352073073387146\n",
            "step 5245: generator_loss=2.5479376316070557, discriminator_loss=0.09443870931863785\n",
            "step 5246: generator_loss=2.5435166358947754, discriminator_loss=0.0942021906375885\n",
            "step 5247: generator_loss=2.5363330841064453, discriminator_loss=0.09405231475830078\n",
            "step 5248: generator_loss=2.533363103866577, discriminator_loss=0.09358783066272736\n",
            "step 5249: generator_loss=2.5580832958221436, discriminator_loss=0.09205901622772217\n",
            "step 5250: generator_loss=2.6098508834838867, discriminator_loss=0.08916803449392319\n",
            "step 5251: generator_loss=2.6431071758270264, discriminator_loss=0.0874428078532219\n",
            "step 5252: generator_loss=2.6379804611206055, discriminator_loss=0.08675529062747955\n",
            "step 5253: generator_loss=2.5995399951934814, discriminator_loss=0.08734606951475143\n",
            "step 5254: generator_loss=2.5865392684936523, discriminator_loss=0.0870184600353241\n",
            "step 5255: generator_loss=2.5559422969818115, discriminator_loss=0.0873403549194336\n",
            "step 5256: generator_loss=2.5433201789855957, discriminator_loss=0.08694639801979065\n",
            "step 5257: generator_loss=2.5693001747131348, discriminator_loss=0.08506500720977783\n",
            "step 5258: generator_loss=2.656608819961548, discriminator_loss=0.08082355558872223\n",
            "step 5259: generator_loss=2.732832431793213, discriminator_loss=0.07727749645709991\n",
            "step 5260: generator_loss=2.7590248584747314, discriminator_loss=0.07555858790874481\n",
            "step 5261: generator_loss=2.7505340576171875, discriminator_loss=0.07495124638080597\n",
            "step 5262: generator_loss=2.7155706882476807, discriminator_loss=0.07520666718482971\n",
            "step 5263: generator_loss=2.6733603477478027, discriminator_loss=0.07600612938404083\n",
            "step 5264: generator_loss=2.6368513107299805, discriminator_loss=0.07658243179321289\n",
            "step 5265: generator_loss=2.6155059337615967, discriminator_loss=0.07679684460163116\n",
            "step 5266: generator_loss=2.58813738822937, discriminator_loss=0.077303946018219\n",
            "step 5267: generator_loss=2.5660758018493652, discriminator_loss=0.07778088748455048\n",
            "step 5268: generator_loss=2.5609474182128906, discriminator_loss=0.0777474045753479\n",
            "step 5269: generator_loss=2.5648276805877686, discriminator_loss=0.07748617231845856\n",
            "step 5270: generator_loss=2.5557777881622314, discriminator_loss=0.07782728970050812\n",
            "step 5271: generator_loss=2.5145809650421143, discriminator_loss=0.0796026885509491\n",
            "step 5272: generator_loss=2.460446357727051, discriminator_loss=0.08219312876462936\n",
            "step 5273: generator_loss=2.4114179611206055, discriminator_loss=0.08468237519264221\n",
            "step 5274: generator_loss=2.3676416873931885, discriminator_loss=0.08708307147026062\n",
            "step 5275: generator_loss=2.336022138595581, discriminator_loss=0.08909666538238525\n",
            "step 5276: generator_loss=2.2998459339141846, discriminator_loss=0.09142029285430908\n",
            "step 5277: generator_loss=2.266256093978882, discriminator_loss=0.09386899322271347\n",
            "step 5278: generator_loss=2.263526439666748, discriminator_loss=0.09474073350429535\n",
            "step 5279: generator_loss=2.3047327995300293, discriminator_loss=0.09311874210834503\n",
            "step 5280: generator_loss=2.3591301441192627, discriminator_loss=0.09076829254627228\n",
            "step 5281: generator_loss=2.4049911499023438, discriminator_loss=0.08902540802955627\n",
            "step 5282: generator_loss=2.4716620445251465, discriminator_loss=0.0864340215921402\n",
            "step 5283: generator_loss=2.517961263656616, discriminator_loss=0.08492474257946014\n",
            "step 5284: generator_loss=2.569868564605713, discriminator_loss=0.08303312212228775\n",
            "step 5285: generator_loss=2.6114606857299805, discriminator_loss=0.08155988901853561\n",
            "step 5286: generator_loss=2.6381006240844727, discriminator_loss=0.08074875175952911\n",
            "step 5287: generator_loss=2.6613211631774902, discriminator_loss=0.07996512949466705\n",
            "step 5288: generator_loss=2.6636569499969482, discriminator_loss=0.07978753000497818\n",
            "step 5289: generator_loss=2.6438775062561035, discriminator_loss=0.08034403622150421\n",
            "step 5290: generator_loss=2.5934431552886963, discriminator_loss=0.08201644569635391\n",
            "step 5291: generator_loss=2.5144922733306885, discriminator_loss=0.08512582629919052\n",
            "step 5292: generator_loss=2.4566943645477295, discriminator_loss=0.0874309316277504\n",
            "step 5293: generator_loss=2.4417660236358643, discriminator_loss=0.0879429280757904\n",
            "step 5294: generator_loss=2.4241294860839844, discriminator_loss=0.08876268565654755\n",
            "step 5295: generator_loss=2.393947124481201, discriminator_loss=0.09011535346508026\n",
            "step 5296: generator_loss=2.350435972213745, discriminator_loss=0.09225496649742126\n",
            "step 5297: generator_loss=2.3035242557525635, discriminator_loss=0.09471835941076279\n",
            "step 5298: generator_loss=2.2634124755859375, discriminator_loss=0.09711606800556183\n",
            "step 5299: generator_loss=2.263888120651245, discriminator_loss=0.0972265750169754\n",
            "step 5300: generator_loss=2.281841516494751, discriminator_loss=0.096224844455719\n",
            "step 5301: generator_loss=2.2982006072998047, discriminator_loss=0.09557057917118073\n",
            "step 5302: generator_loss=2.3225631713867188, discriminator_loss=0.09451563656330109\n",
            "step 5303: generator_loss=2.357999324798584, discriminator_loss=0.09305523335933685\n",
            "step 5304: generator_loss=2.4152190685272217, discriminator_loss=0.09048721194267273\n",
            "step 5305: generator_loss=2.4845504760742188, discriminator_loss=0.08743973076343536\n",
            "step 5306: generator_loss=2.5293023586273193, discriminator_loss=0.08560679852962494\n",
            "step 5307: generator_loss=2.5532450675964355, discriminator_loss=0.08464200049638748\n",
            "step 5308: generator_loss=2.565479278564453, discriminator_loss=0.08422675728797913\n",
            "step 5309: generator_loss=2.602888345718384, discriminator_loss=0.08268552273511887\n",
            "step 5310: generator_loss=2.625359058380127, discriminator_loss=0.08183477073907852\n",
            "step 5311: generator_loss=2.6236941814422607, discriminator_loss=0.08165903389453888\n",
            "step 5312: generator_loss=2.619860887527466, discriminator_loss=0.08161672949790955\n",
            "step 5313: generator_loss=2.6352710723876953, discriminator_loss=0.08081066608428955\n",
            "step 5314: generator_loss=2.642143726348877, discriminator_loss=0.08030125498771667\n",
            "step 5315: generator_loss=2.6296634674072266, discriminator_loss=0.0803874135017395\n",
            "step 5316: generator_loss=2.6718199253082275, discriminator_loss=0.07841891795396805\n",
            "step 5317: generator_loss=2.718550205230713, discriminator_loss=0.0763079896569252\n",
            "step 5318: generator_loss=2.7459795475006104, discriminator_loss=0.07486514747142792\n",
            "step 5319: generator_loss=2.769030809402466, discriminator_loss=0.07366693019866943\n",
            "step 5320: generator_loss=2.8185689449310303, discriminator_loss=0.07169684767723083\n",
            "step 5321: generator_loss=2.8477578163146973, discriminator_loss=0.07038956880569458\n",
            "step 5322: generator_loss=2.8803019523620605, discriminator_loss=0.0688689574599266\n",
            "step 5323: generator_loss=2.879763126373291, discriminator_loss=0.06841788440942764\n",
            "step 5324: generator_loss=2.8945627212524414, discriminator_loss=0.06767375022172928\n",
            "step 5325: generator_loss=2.928769588470459, discriminator_loss=0.06627577543258667\n",
            "step 5326: generator_loss=2.955415725708008, discriminator_loss=0.06494203954935074\n",
            "step 5327: generator_loss=2.964780807495117, discriminator_loss=0.06415771692991257\n",
            "step 5328: generator_loss=2.9617340564727783, discriminator_loss=0.06369858980178833\n",
            "step 5329: generator_loss=2.9541428089141846, discriminator_loss=0.06328900158405304\n",
            "step 5330: generator_loss=2.9614648818969727, discriminator_loss=0.06252098083496094\n",
            "step 5331: generator_loss=2.957646608352661, discriminator_loss=0.06216743588447571\n",
            "step 5332: generator_loss=2.9302868843078613, discriminator_loss=0.062383025884628296\n",
            "step 5333: generator_loss=2.879646062850952, discriminator_loss=0.06348346173763275\n",
            "step 5334: generator_loss=2.8170642852783203, discriminator_loss=0.06507953256368637\n",
            "step 5335: generator_loss=2.753892183303833, discriminator_loss=0.06678987294435501\n",
            "step 5336: generator_loss=2.7315313816070557, discriminator_loss=0.06725570559501648\n",
            "step 5337: generator_loss=2.7390005588531494, discriminator_loss=0.06681070476770401\n",
            "step 5338: generator_loss=2.744828701019287, discriminator_loss=0.06641140580177307\n",
            "step 5339: generator_loss=2.779299259185791, discriminator_loss=0.06519895792007446\n",
            "step 5340: generator_loss=2.8233954906463623, discriminator_loss=0.06369834393262863\n",
            "step 5341: generator_loss=2.873344898223877, discriminator_loss=0.06218961626291275\n",
            "step 5342: generator_loss=2.9138083457946777, discriminator_loss=0.06103408336639404\n",
            "step 5343: generator_loss=2.9532017707824707, discriminator_loss=0.06000133231282234\n",
            "step 5344: generator_loss=2.989623546600342, discriminator_loss=0.05904017388820648\n",
            "step 5345: generator_loss=3.010226011276245, discriminator_loss=0.05853607878088951\n",
            "step 5346: generator_loss=3.03645920753479, discriminator_loss=0.05782592296600342\n",
            "step 5347: generator_loss=3.05159592628479, discriminator_loss=0.05744048207998276\n",
            "step 5348: generator_loss=3.0599069595336914, discriminator_loss=0.057181138545274734\n",
            "step 5349: generator_loss=3.074932098388672, discriminator_loss=0.05672961100935936\n",
            "step 5350: generator_loss=3.072718620300293, discriminator_loss=0.056628547608852386\n",
            "step 5351: generator_loss=3.0562691688537598, discriminator_loss=0.05679171532392502\n",
            "step 5352: generator_loss=3.02884578704834, discriminator_loss=0.05737098678946495\n",
            "step 5353: generator_loss=2.989776611328125, discriminator_loss=0.058209050446748734\n",
            "step 5354: generator_loss=2.9388716220855713, discriminator_loss=0.05940897390246391\n",
            "step 5355: generator_loss=2.8826582431793213, discriminator_loss=0.06086414307355881\n",
            "step 5356: generator_loss=2.815312623977661, discriminator_loss=0.06286165863275528\n",
            "step 5357: generator_loss=2.778386116027832, discriminator_loss=0.06398352235555649\n",
            "step 5358: generator_loss=2.7698795795440674, discriminator_loss=0.0642085000872612\n",
            "step 5359: generator_loss=2.7618205547332764, discriminator_loss=0.0644746720790863\n",
            "step 5360: generator_loss=2.773944854736328, discriminator_loss=0.06405198574066162\n",
            "step 5361: generator_loss=2.8089442253112793, discriminator_loss=0.06293545663356781\n",
            "step 5362: generator_loss=2.8328349590301514, discriminator_loss=0.06225692480802536\n",
            "step 5363: generator_loss=2.853405237197876, discriminator_loss=0.061658911406993866\n",
            "step 5364: generator_loss=2.8488194942474365, discriminator_loss=0.06187472492456436\n",
            "step 5365: generator_loss=2.832094669342041, discriminator_loss=0.0625123381614685\n",
            "step 5366: generator_loss=2.8213090896606445, discriminator_loss=0.06291432678699493\n",
            "step 5367: generator_loss=2.8179588317871094, discriminator_loss=0.0632445365190506\n",
            "step 5368: generator_loss=2.81760311126709, discriminator_loss=0.06348675489425659\n",
            "step 5369: generator_loss=2.8279809951782227, discriminator_loss=0.06328393518924713\n",
            "step 5370: generator_loss=2.8365256786346436, discriminator_loss=0.06319355964660645\n",
            "step 5371: generator_loss=2.8424413204193115, discriminator_loss=0.06315173208713531\n",
            "step 5372: generator_loss=2.8549129962921143, discriminator_loss=0.06296443939208984\n",
            "step 5373: generator_loss=2.846414566040039, discriminator_loss=0.06324182450771332\n",
            "step 5374: generator_loss=2.8164103031158447, discriminator_loss=0.06427513808012009\n",
            "step 5375: generator_loss=2.7780275344848633, discriminator_loss=0.06568056344985962\n",
            "step 5376: generator_loss=2.7292327880859375, discriminator_loss=0.06736554950475693\n",
            "step 5377: generator_loss=2.677058219909668, discriminator_loss=0.0692431628704071\n",
            "step 5378: generator_loss=2.6331827640533447, discriminator_loss=0.07091231644153595\n",
            "step 5379: generator_loss=2.610332489013672, discriminator_loss=0.07197210192680359\n",
            "step 5380: generator_loss=2.609330654144287, discriminator_loss=0.07225082814693451\n",
            "step 5381: generator_loss=2.6472349166870117, discriminator_loss=0.07099173963069916\n",
            "step 5382: generator_loss=2.7015655040740967, discriminator_loss=0.06923383474349976\n",
            "step 5383: generator_loss=2.761735439300537, discriminator_loss=0.06739969551563263\n",
            "step 5384: generator_loss=2.8267321586608887, discriminator_loss=0.06548874080181122\n",
            "step 5385: generator_loss=2.9251983165740967, discriminator_loss=0.06279612332582474\n",
            "step 5386: generator_loss=2.9955241680145264, discriminator_loss=0.06096801906824112\n",
            "step 5387: generator_loss=3.0226657390594482, discriminator_loss=0.060389094054698944\n",
            "step 5388: generator_loss=3.024937868118286, discriminator_loss=0.06024725362658501\n",
            "step 5389: generator_loss=3.0317115783691406, discriminator_loss=0.060055285692214966\n",
            "step 5390: generator_loss=3.0165693759918213, discriminator_loss=0.06039520353078842\n",
            "step 5391: generator_loss=2.9742069244384766, discriminator_loss=0.06131800264120102\n",
            "step 5392: generator_loss=2.9181435108184814, discriminator_loss=0.06268350780010223\n",
            "step 5393: generator_loss=2.861274480819702, discriminator_loss=0.06427834928035736\n",
            "step 5394: generator_loss=2.8041582107543945, discriminator_loss=0.06590580195188522\n",
            "step 5395: generator_loss=2.7615323066711426, discriminator_loss=0.06725198030471802\n",
            "step 5396: generator_loss=2.743988275527954, discriminator_loss=0.06772782653570175\n",
            "step 5397: generator_loss=2.7394142150878906, discriminator_loss=0.06786313652992249\n",
            "step 5398: generator_loss=2.742950439453125, discriminator_loss=0.06766453385353088\n",
            "step 5399: generator_loss=2.7545413970947266, discriminator_loss=0.06727661192417145\n",
            "step 5400: generator_loss=2.763261318206787, discriminator_loss=0.06704123318195343\n",
            "step 5401: generator_loss=2.7541568279266357, discriminator_loss=0.06739556789398193\n",
            "step 5402: generator_loss=2.781742811203003, discriminator_loss=0.06653078645467758\n",
            "step 5403: generator_loss=2.812298059463501, discriminator_loss=0.06566862016916275\n",
            "step 5404: generator_loss=2.826277494430542, discriminator_loss=0.06541816890239716\n",
            "step 5405: generator_loss=2.85518741607666, discriminator_loss=0.06466083973646164\n",
            "step 5406: generator_loss=2.8855581283569336, discriminator_loss=0.06389380991458893\n",
            "step 5407: generator_loss=2.9070723056793213, discriminator_loss=0.06337864696979523\n",
            "step 5408: generator_loss=2.928450107574463, discriminator_loss=0.06278620660305023\n",
            "step 5409: generator_loss=2.936976671218872, discriminator_loss=0.06255009770393372\n",
            "step 5410: generator_loss=2.9396188259124756, discriminator_loss=0.06249745935201645\n",
            "step 5411: generator_loss=2.941107749938965, discriminator_loss=0.06239040568470955\n",
            "step 5412: generator_loss=2.943301200866699, discriminator_loss=0.062326669692993164\n",
            "step 5413: generator_loss=2.9806113243103027, discriminator_loss=0.06128824129700661\n",
            "step 5414: generator_loss=3.01153826713562, discriminator_loss=0.06041722744703293\n",
            "step 5415: generator_loss=3.041870594024658, discriminator_loss=0.05960344150662422\n",
            "step 5416: generator_loss=3.078547716140747, discriminator_loss=0.0585809051990509\n",
            "step 5417: generator_loss=3.104025363922119, discriminator_loss=0.05773717164993286\n",
            "step 5418: generator_loss=3.1228389739990234, discriminator_loss=0.057091012597084045\n",
            "step 5419: generator_loss=3.149477481842041, discriminator_loss=0.056195564568042755\n",
            "step 5420: generator_loss=3.1812021732330322, discriminator_loss=0.05522863194346428\n",
            "step 5421: generator_loss=3.19600772857666, discriminator_loss=0.05452297627925873\n",
            "step 5422: generator_loss=3.1780776977539062, discriminator_loss=0.05457767844200134\n",
            "step 5423: generator_loss=3.1318209171295166, discriminator_loss=0.05521928146481514\n",
            "step 5424: generator_loss=3.069648265838623, discriminator_loss=0.05621778964996338\n",
            "step 5425: generator_loss=2.9870221614837646, discriminator_loss=0.05794313922524452\n",
            "step 5426: generator_loss=2.9068775177001953, discriminator_loss=0.05983319878578186\n",
            "step 5427: generator_loss=2.8670711517333984, discriminator_loss=0.06063232943415642\n",
            "step 5428: generator_loss=2.8419532775878906, discriminator_loss=0.061237432062625885\n",
            "step 5429: generator_loss=2.852116107940674, discriminator_loss=0.060759615153074265\n",
            "step 5430: generator_loss=2.8587887287139893, discriminator_loss=0.06042153388261795\n",
            "step 5431: generator_loss=2.8535478115081787, discriminator_loss=0.060659009963274\n",
            "step 5432: generator_loss=2.8393633365631104, discriminator_loss=0.06118470057845116\n",
            "step 5433: generator_loss=2.846381664276123, discriminator_loss=0.06108616665005684\n",
            "step 5434: generator_loss=2.8978238105773926, discriminator_loss=0.05970313027501106\n",
            "step 5435: generator_loss=2.9400792121887207, discriminator_loss=0.058701734989881516\n",
            "step 5436: generator_loss=3.0040931701660156, discriminator_loss=0.057083748281002045\n",
            "step 5437: generator_loss=3.054360866546631, discriminator_loss=0.05591851845383644\n",
            "step 5438: generator_loss=3.0959575176239014, discriminator_loss=0.0549725666642189\n",
            "step 5439: generator_loss=3.1486706733703613, discriminator_loss=0.05376376956701279\n",
            "step 5440: generator_loss=3.1947340965270996, discriminator_loss=0.052813753485679626\n",
            "step 5441: generator_loss=3.217630386352539, discriminator_loss=0.052194271236658096\n",
            "step 5442: generator_loss=3.2188644409179688, discriminator_loss=0.05211615189909935\n",
            "step 5443: generator_loss=3.180248737335205, discriminator_loss=0.05279327183961868\n",
            "step 5444: generator_loss=3.165377616882324, discriminator_loss=0.0529058612883091\n",
            "step 5445: generator_loss=3.1776251792907715, discriminator_loss=0.052443571388721466\n",
            "step 5446: generator_loss=3.1677956581115723, discriminator_loss=0.05244101583957672\n",
            "step 5447: generator_loss=3.146183967590332, discriminator_loss=0.052644506096839905\n",
            "step 5448: generator_loss=3.127737045288086, discriminator_loss=0.05276867374777794\n",
            "step 5449: generator_loss=3.144232988357544, discriminator_loss=0.052103377878665924\n",
            "step 5450: generator_loss=3.158432960510254, discriminator_loss=0.05152986943721771\n",
            "step 5451: generator_loss=3.1575112342834473, discriminator_loss=0.051221176981925964\n",
            "step 5452: generator_loss=3.1424942016601562, discriminator_loss=0.051279857754707336\n",
            "step 5453: generator_loss=3.131333351135254, discriminator_loss=0.05132008343935013\n",
            "step 5454: generator_loss=3.127824306488037, discriminator_loss=0.05112114176154137\n",
            "step 5455: generator_loss=3.141526699066162, discriminator_loss=0.05061697959899902\n",
            "step 5456: generator_loss=3.1292858123779297, discriminator_loss=0.05082719773054123\n",
            "step 5457: generator_loss=3.1058220863342285, discriminator_loss=0.05129784345626831\n",
            "step 5458: generator_loss=3.053610324859619, discriminator_loss=0.05272909626364708\n",
            "step 5459: generator_loss=3.0240073204040527, discriminator_loss=0.05357544124126434\n",
            "step 5460: generator_loss=2.999614953994751, discriminator_loss=0.054368793964385986\n",
            "step 5461: generator_loss=2.9060287475585938, discriminator_loss=0.06016605719923973\n",
            "step 5462: generator_loss=2.945023536682129, discriminator_loss=0.05983400717377663\n",
            "step 5463: generator_loss=2.9820475578308105, discriminator_loss=0.06200499087572098\n",
            "step 5464: generator_loss=2.8170318603515625, discriminator_loss=0.0800747275352478\n",
            "step 5465: generator_loss=2.862372875213623, discriminator_loss=0.0881088525056839\n",
            "step 5466: generator_loss=3.028555393218994, discriminator_loss=0.07874052226543427\n",
            "step 5467: generator_loss=3.050806760787964, discriminator_loss=0.08978486061096191\n",
            "step 5468: generator_loss=3.0183768272399902, discriminator_loss=0.09656024724245071\n",
            "step 5469: generator_loss=3.3504433631896973, discriminator_loss=0.07793805003166199\n",
            "step 5470: generator_loss=3.332305431365967, discriminator_loss=0.08543837070465088\n",
            "step 5471: generator_loss=3.6467273235321045, discriminator_loss=0.07116342335939407\n",
            "step 5472: generator_loss=3.8194196224212646, discriminator_loss=0.06828191876411438\n",
            "step 5473: generator_loss=3.9182004928588867, discriminator_loss=0.06374027580022812\n",
            "step 5474: generator_loss=4.115989685058594, discriminator_loss=0.05782945826649666\n",
            "step 5475: generator_loss=4.26207160949707, discriminator_loss=0.06083286181092262\n",
            "step 5476: generator_loss=4.472739219665527, discriminator_loss=0.05682172626256943\n",
            "step 5477: generator_loss=4.828303337097168, discriminator_loss=0.04972357302904129\n",
            "step 5478: generator_loss=4.660388469696045, discriminator_loss=0.053583405911922455\n",
            "step 5479: generator_loss=4.883279800415039, discriminator_loss=0.04882427304983139\n",
            "step 5480: generator_loss=4.7035956382751465, discriminator_loss=0.05004654452204704\n",
            "step 5481: generator_loss=4.818475246429443, discriminator_loss=0.046413809061050415\n",
            "step 5482: generator_loss=4.923457145690918, discriminator_loss=0.04930335283279419\n",
            "step 5483: generator_loss=4.756958961486816, discriminator_loss=0.051424212753772736\n",
            "step 5484: generator_loss=4.898373603820801, discriminator_loss=0.0489848367869854\n",
            "step 5485: generator_loss=4.658010482788086, discriminator_loss=0.05185697227716446\n",
            "step 5486: generator_loss=4.863515377044678, discriminator_loss=0.04388095438480377\n",
            "step 5487: generator_loss=4.394264221191406, discriminator_loss=0.05165120214223862\n",
            "step 5488: generator_loss=4.153294086456299, discriminator_loss=0.05248091369867325\n",
            "step 5489: generator_loss=4.319091796875, discriminator_loss=0.046720679849386215\n",
            "step 5490: generator_loss=4.350106716156006, discriminator_loss=0.043462611734867096\n",
            "step 5491: generator_loss=4.007387161254883, discriminator_loss=0.0465162917971611\n",
            "step 5492: generator_loss=3.874964475631714, discriminator_loss=0.047626130282878876\n",
            "step 5493: generator_loss=3.854048728942871, discriminator_loss=0.04752611368894577\n",
            "step 5494: generator_loss=3.5907886028289795, discriminator_loss=0.05271178483963013\n",
            "step 5495: generator_loss=3.50947642326355, discriminator_loss=0.05134712904691696\n",
            "step 5496: generator_loss=3.3996949195861816, discriminator_loss=0.053241509944200516\n",
            "step 5497: generator_loss=3.374041795730591, discriminator_loss=0.05360497534275055\n",
            "step 5498: generator_loss=3.2688729763031006, discriminator_loss=0.054187748581171036\n",
            "step 5499: generator_loss=3.0362789630889893, discriminator_loss=0.05860409885644913\n",
            "step 5500: generator_loss=2.9396748542785645, discriminator_loss=0.060875728726387024\n",
            "step 5501: generator_loss=3.212097644805908, discriminator_loss=0.058999188244342804\n",
            "step 5502: generator_loss=3.0527899265289307, discriminator_loss=0.06312814354896545\n",
            "step 5503: generator_loss=2.811835527420044, discriminator_loss=0.06497330963611603\n",
            "step 5504: generator_loss=2.8628201484680176, discriminator_loss=0.0689319372177124\n",
            "step 5505: generator_loss=2.925236940383911, discriminator_loss=0.06618238985538483\n",
            "step 5506: generator_loss=2.9281063079833984, discriminator_loss=0.06846660375595093\n",
            "step 5507: generator_loss=2.9486277103424072, discriminator_loss=0.06710910052061081\n",
            "step 5508: generator_loss=2.8517775535583496, discriminator_loss=0.06586910784244537\n",
            "step 5509: generator_loss=3.0621018409729004, discriminator_loss=0.06540007889270782\n",
            "step 5510: generator_loss=3.0899922847747803, discriminator_loss=0.06367534399032593\n",
            "step 5511: generator_loss=2.902754306793213, discriminator_loss=0.06464677304029465\n",
            "step 5512: generator_loss=2.9730963706970215, discriminator_loss=0.06338442862033844\n",
            "step 5513: generator_loss=3.0951309204101562, discriminator_loss=0.060671593993902206\n",
            "step 5514: generator_loss=3.0804476737976074, discriminator_loss=0.06232947111129761\n",
            "step 5515: generator_loss=2.9643115997314453, discriminator_loss=0.06350915133953094\n",
            "step 5516: generator_loss=2.93243408203125, discriminator_loss=0.06389501690864563\n",
            "step 5517: generator_loss=2.841198444366455, discriminator_loss=0.06584828346967697\n",
            "step 5518: generator_loss=2.8832883834838867, discriminator_loss=0.06585986167192459\n",
            "step 5519: generator_loss=2.9335782527923584, discriminator_loss=0.06497716158628464\n",
            "step 5520: generator_loss=2.856231689453125, discriminator_loss=0.0671224445104599\n",
            "step 5521: generator_loss=2.893353223800659, discriminator_loss=0.06794168800115585\n",
            "step 5522: generator_loss=2.7700741291046143, discriminator_loss=0.06952191889286041\n",
            "step 5523: generator_loss=2.805501937866211, discriminator_loss=0.06959941238164902\n",
            "step 5524: generator_loss=2.717590808868408, discriminator_loss=0.0722619816660881\n",
            "step 5525: generator_loss=2.6757278442382812, discriminator_loss=0.07351432740688324\n",
            "step 5526: generator_loss=2.630342960357666, discriminator_loss=0.07477438449859619\n",
            "step 5527: generator_loss=2.6202125549316406, discriminator_loss=0.07588746398687363\n",
            "step 5528: generator_loss=2.708247184753418, discriminator_loss=0.07548640668392181\n",
            "step 5529: generator_loss=2.6337389945983887, discriminator_loss=0.07588060200214386\n",
            "step 5530: generator_loss=2.7313053607940674, discriminator_loss=0.07422032952308655\n",
            "step 5531: generator_loss=2.7012271881103516, discriminator_loss=0.0743880495429039\n",
            "step 5532: generator_loss=2.673304796218872, discriminator_loss=0.07471906393766403\n",
            "step 5533: generator_loss=2.699429750442505, discriminator_loss=0.07387913763523102\n",
            "step 5534: generator_loss=2.79506254196167, discriminator_loss=0.07161390036344528\n",
            "step 5535: generator_loss=2.781611680984497, discriminator_loss=0.07183566689491272\n",
            "step 5536: generator_loss=2.7720541954040527, discriminator_loss=0.07169605046510696\n",
            "step 5537: generator_loss=2.8103065490722656, discriminator_loss=0.07020256668329239\n",
            "step 5538: generator_loss=2.888648748397827, discriminator_loss=0.06863593310117722\n",
            "step 5539: generator_loss=2.8472814559936523, discriminator_loss=0.069190114736557\n",
            "step 5540: generator_loss=2.8058083057403564, discriminator_loss=0.07015826553106308\n",
            "step 5541: generator_loss=2.7970173358917236, discriminator_loss=0.06986275315284729\n",
            "step 5542: generator_loss=2.780519485473633, discriminator_loss=0.07008355855941772\n",
            "step 5543: generator_loss=2.7682113647460938, discriminator_loss=0.07030823826789856\n",
            "step 5544: generator_loss=2.704318046569824, discriminator_loss=0.07200807332992554\n",
            "step 5545: generator_loss=2.6546945571899414, discriminator_loss=0.0734122097492218\n",
            "step 5546: generator_loss=2.6027028560638428, discriminator_loss=0.07502785325050354\n",
            "step 5547: generator_loss=2.5547380447387695, discriminator_loss=0.07675975561141968\n",
            "step 5548: generator_loss=2.5183053016662598, discriminator_loss=0.07806377112865448\n",
            "step 5549: generator_loss=2.504188060760498, discriminator_loss=0.07856913655996323\n",
            "step 5550: generator_loss=2.4943599700927734, discriminator_loss=0.07893101125955582\n",
            "step 5551: generator_loss=2.5044517517089844, discriminator_loss=0.0784914493560791\n",
            "step 5552: generator_loss=2.5700738430023193, discriminator_loss=0.07591576874256134\n",
            "step 5553: generator_loss=2.6361687183380127, discriminator_loss=0.07348991930484772\n",
            "step 5554: generator_loss=2.6612765789031982, discriminator_loss=0.07290580868721008\n",
            "step 5555: generator_loss=2.6471383571624756, discriminator_loss=0.07357272505760193\n",
            "step 5556: generator_loss=2.637648582458496, discriminator_loss=0.07421201467514038\n",
            "step 5557: generator_loss=2.6378836631774902, discriminator_loss=0.07442515343427658\n",
            "step 5558: generator_loss=2.6462244987487793, discriminator_loss=0.07431799173355103\n",
            "step 5559: generator_loss=2.660879373550415, discriminator_loss=0.0740431547164917\n",
            "step 5560: generator_loss=2.6845643520355225, discriminator_loss=0.07326480746269226\n",
            "step 5561: generator_loss=2.6854896545410156, discriminator_loss=0.07345368713140488\n",
            "step 5562: generator_loss=2.6647684574127197, discriminator_loss=0.07418234646320343\n",
            "step 5563: generator_loss=2.6174559593200684, discriminator_loss=0.0760701447725296\n",
            "step 5564: generator_loss=2.5504472255706787, discriminator_loss=0.07889876514673233\n",
            "step 5565: generator_loss=2.487098217010498, discriminator_loss=0.08158019930124283\n",
            "step 5566: generator_loss=2.43173885345459, discriminator_loss=0.08427131175994873\n",
            "step 5567: generator_loss=2.435103416442871, discriminator_loss=0.0841905027627945\n",
            "step 5568: generator_loss=2.4577746391296387, discriminator_loss=0.08314575254917145\n",
            "step 5569: generator_loss=2.4730284214019775, discriminator_loss=0.08272400498390198\n",
            "step 5570: generator_loss=2.482518434524536, discriminator_loss=0.08241565525531769\n",
            "step 5571: generator_loss=2.493251323699951, discriminator_loss=0.0820852741599083\n",
            "step 5572: generator_loss=2.509617805480957, discriminator_loss=0.0816786140203476\n",
            "step 5573: generator_loss=2.5362157821655273, discriminator_loss=0.08063025027513504\n",
            "step 5574: generator_loss=2.555112600326538, discriminator_loss=0.07986323535442352\n",
            "step 5575: generator_loss=2.5734307765960693, discriminator_loss=0.07936210185289383\n",
            "step 5576: generator_loss=2.603264570236206, discriminator_loss=0.07815675437450409\n",
            "step 5577: generator_loss=2.611811399459839, discriminator_loss=0.07782144844532013\n",
            "step 5578: generator_loss=2.592729091644287, discriminator_loss=0.07866375148296356\n",
            "step 5579: generator_loss=2.561734437942505, discriminator_loss=0.07985701411962509\n",
            "step 5580: generator_loss=2.5199570655822754, discriminator_loss=0.08181682229042053\n",
            "step 5581: generator_loss=2.509042739868164, discriminator_loss=0.08224696666002274\n",
            "step 5582: generator_loss=2.494684934616089, discriminator_loss=0.0829169750213623\n",
            "step 5583: generator_loss=2.4903740882873535, discriminator_loss=0.0832027792930603\n",
            "step 5584: generator_loss=2.522559642791748, discriminator_loss=0.08185454457998276\n",
            "step 5585: generator_loss=2.5792396068573, discriminator_loss=0.07964025437831879\n",
            "step 5586: generator_loss=2.636930465698242, discriminator_loss=0.07727916538715363\n",
            "step 5587: generator_loss=2.700516939163208, discriminator_loss=0.07494349777698517\n",
            "step 5588: generator_loss=2.755380868911743, discriminator_loss=0.0731271505355835\n",
            "step 5589: generator_loss=2.784763813018799, discriminator_loss=0.07197407633066177\n",
            "step 5590: generator_loss=2.8184421062469482, discriminator_loss=0.0707521066069603\n",
            "step 5591: generator_loss=2.8280527591705322, discriminator_loss=0.07008136808872223\n",
            "step 5592: generator_loss=2.827530860900879, discriminator_loss=0.0698503628373146\n",
            "step 5593: generator_loss=2.813762664794922, discriminator_loss=0.06996694952249527\n",
            "step 5594: generator_loss=2.783172845840454, discriminator_loss=0.07062307000160217\n",
            "step 5595: generator_loss=2.729658603668213, discriminator_loss=0.07206930965185165\n",
            "step 5596: generator_loss=2.660583734512329, discriminator_loss=0.0742412582039833\n",
            "step 5597: generator_loss=2.5834240913391113, discriminator_loss=0.07688049972057343\n",
            "step 5598: generator_loss=2.532822608947754, discriminator_loss=0.07878829538822174\n",
            "step 5599: generator_loss=2.509937286376953, discriminator_loss=0.07960422337055206\n",
            "step 5600: generator_loss=2.4852135181427, discriminator_loss=0.08063404262065887\n",
            "step 5601: generator_loss=2.451373815536499, discriminator_loss=0.08209210634231567\n",
            "step 5602: generator_loss=2.408460855484009, discriminator_loss=0.08422492444515228\n",
            "step 5603: generator_loss=2.368427038192749, discriminator_loss=0.08640510588884354\n",
            "step 5604: generator_loss=2.361410140991211, discriminator_loss=0.087122842669487\n",
            "step 5605: generator_loss=2.3762381076812744, discriminator_loss=0.08658075332641602\n",
            "step 5606: generator_loss=2.4161300659179688, discriminator_loss=0.08511379361152649\n",
            "step 5607: generator_loss=2.505568027496338, discriminator_loss=0.08136216551065445\n",
            "step 5608: generator_loss=2.5938520431518555, discriminator_loss=0.0781957134604454\n",
            "step 5609: generator_loss=2.6710152626037598, discriminator_loss=0.07569416612386703\n",
            "step 5610: generator_loss=2.7527196407318115, discriminator_loss=0.07324464619159698\n",
            "step 5611: generator_loss=2.8268346786499023, discriminator_loss=0.07125968486070633\n",
            "step 5612: generator_loss=2.8719654083251953, discriminator_loss=0.07006903737783432\n",
            "step 5613: generator_loss=2.8689723014831543, discriminator_loss=0.07023866474628448\n",
            "step 5614: generator_loss=2.8478455543518066, discriminator_loss=0.07091394066810608\n",
            "step 5615: generator_loss=2.7987523078918457, discriminator_loss=0.07227857410907745\n",
            "step 5616: generator_loss=2.7087042331695557, discriminator_loss=0.07515954971313477\n",
            "step 5617: generator_loss=2.640566349029541, discriminator_loss=0.07744134962558746\n",
            "step 5618: generator_loss=2.615621566772461, discriminator_loss=0.07818929851055145\n",
            "step 5619: generator_loss=2.6322364807128906, discriminator_loss=0.07728687673807144\n",
            "step 5620: generator_loss=2.6786692142486572, discriminator_loss=0.07532582432031631\n",
            "step 5621: generator_loss=2.7050743103027344, discriminator_loss=0.07421842962503433\n",
            "step 5622: generator_loss=2.710747480392456, discriminator_loss=0.07382619380950928\n",
            "step 5623: generator_loss=2.699674606323242, discriminator_loss=0.07391941547393799\n",
            "step 5624: generator_loss=2.672605037689209, discriminator_loss=0.07464607059955597\n",
            "step 5625: generator_loss=2.635806083679199, discriminator_loss=0.0757816880941391\n",
            "step 5626: generator_loss=2.5911970138549805, discriminator_loss=0.07737405598163605\n",
            "step 5627: generator_loss=2.561345100402832, discriminator_loss=0.07834559679031372\n",
            "step 5628: generator_loss=2.5730671882629395, discriminator_loss=0.07774341106414795\n",
            "step 5629: generator_loss=2.6080827713012695, discriminator_loss=0.07637321203947067\n",
            "step 5630: generator_loss=2.6530961990356445, discriminator_loss=0.07451869547367096\n",
            "step 5631: generator_loss=2.6858644485473633, discriminator_loss=0.07341565936803818\n",
            "step 5632: generator_loss=2.7108263969421387, discriminator_loss=0.07247981429100037\n",
            "step 5633: generator_loss=2.715461254119873, discriminator_loss=0.0723353698849678\n",
            "step 5634: generator_loss=2.6891732215881348, discriminator_loss=0.07321561872959137\n",
            "step 5635: generator_loss=2.6667492389678955, discriminator_loss=0.07400104403495789\n",
            "step 5636: generator_loss=2.680763006210327, discriminator_loss=0.07345537841320038\n",
            "step 5637: generator_loss=2.6895382404327393, discriminator_loss=0.07304520905017853\n",
            "step 5638: generator_loss=2.6787755489349365, discriminator_loss=0.07337655127048492\n",
            "step 5639: generator_loss=2.659210443496704, discriminator_loss=0.07408493757247925\n",
            "step 5640: generator_loss=2.6496775150299072, discriminator_loss=0.07445499300956726\n",
            "step 5641: generator_loss=2.653285026550293, discriminator_loss=0.07441972196102142\n",
            "step 5642: generator_loss=2.6742184162139893, discriminator_loss=0.07361051440238953\n",
            "step 5643: generator_loss=2.697007656097412, discriminator_loss=0.07292357087135315\n",
            "step 5644: generator_loss=2.7074055671691895, discriminator_loss=0.07269002497196198\n",
            "step 5645: generator_loss=2.6944448947906494, discriminator_loss=0.07331593334674835\n",
            "step 5646: generator_loss=2.6670899391174316, discriminator_loss=0.07461348921060562\n",
            "step 5647: generator_loss=2.648862838745117, discriminator_loss=0.07541222870349884\n",
            "step 5648: generator_loss=2.648998737335205, discriminator_loss=0.07559381425380707\n",
            "step 5649: generator_loss=2.6909844875335693, discriminator_loss=0.07409584522247314\n",
            "step 5650: generator_loss=2.8119924068450928, discriminator_loss=0.07011260092258453\n",
            "step 5651: generator_loss=2.9436817169189453, discriminator_loss=0.06625838577747345\n",
            "step 5652: generator_loss=3.0513229370117188, discriminator_loss=0.06338028609752655\n",
            "step 5653: generator_loss=3.1147122383117676, discriminator_loss=0.06157980114221573\n",
            "step 5654: generator_loss=3.154934883117676, discriminator_loss=0.06049463525414467\n",
            "step 5655: generator_loss=3.16013765335083, discriminator_loss=0.06000211462378502\n",
            "step 5656: generator_loss=3.1263458728790283, discriminator_loss=0.06022964417934418\n",
            "step 5657: generator_loss=3.0596108436584473, discriminator_loss=0.06134109944105148\n",
            "step 5658: generator_loss=3.0041403770446777, discriminator_loss=0.06221519410610199\n",
            "step 5659: generator_loss=2.9393296241760254, discriminator_loss=0.0634797066450119\n",
            "step 5660: generator_loss=2.8815548419952393, discriminator_loss=0.06453855335712433\n",
            "step 5661: generator_loss=2.8410627841949463, discriminator_loss=0.06524613499641418\n",
            "step 5662: generator_loss=2.807737350463867, discriminator_loss=0.06586425006389618\n",
            "step 5663: generator_loss=2.798625946044922, discriminator_loss=0.06582619249820709\n",
            "step 5664: generator_loss=2.8148419857025146, discriminator_loss=0.06506077200174332\n",
            "step 5665: generator_loss=2.8495864868164062, discriminator_loss=0.06366272270679474\n",
            "step 5666: generator_loss=2.90608286857605, discriminator_loss=0.06186439096927643\n",
            "step 5667: generator_loss=2.9508814811706543, discriminator_loss=0.06046448275446892\n",
            "step 5668: generator_loss=3.040595531463623, discriminator_loss=0.05800304561853409\n",
            "step 5669: generator_loss=3.12375545501709, discriminator_loss=0.05586126446723938\n",
            "step 5670: generator_loss=3.1685891151428223, discriminator_loss=0.05461721122264862\n",
            "step 5671: generator_loss=3.1791043281555176, discriminator_loss=0.0542626678943634\n",
            "step 5672: generator_loss=3.169215679168701, discriminator_loss=0.05424903333187103\n",
            "step 5673: generator_loss=3.1425516605377197, discriminator_loss=0.054569460451602936\n",
            "step 5674: generator_loss=3.1294524669647217, discriminator_loss=0.05458230897784233\n",
            "step 5675: generator_loss=3.097787618637085, discriminator_loss=0.055079177021980286\n",
            "step 5676: generator_loss=3.045867681503296, discriminator_loss=0.05601336807012558\n",
            "step 5677: generator_loss=2.969757556915283, discriminator_loss=0.057772569358348846\n",
            "step 5678: generator_loss=2.8772943019866943, discriminator_loss=0.060202524065971375\n",
            "step 5679: generator_loss=2.817075490951538, discriminator_loss=0.06183268874883652\n",
            "step 5680: generator_loss=2.7817842960357666, discriminator_loss=0.0629630982875824\n",
            "step 5681: generator_loss=2.8015167713165283, discriminator_loss=0.062309518456459045\n",
            "step 5682: generator_loss=2.852769136428833, discriminator_loss=0.06070020794868469\n",
            "step 5683: generator_loss=2.922636032104492, discriminator_loss=0.05873706936836243\n",
            "step 5684: generator_loss=2.977503776550293, discriminator_loss=0.05734269320964813\n",
            "step 5685: generator_loss=3.025045394897461, discriminator_loss=0.05620790272951126\n",
            "step 5686: generator_loss=3.060818910598755, discriminator_loss=0.055352821946144104\n",
            "step 5687: generator_loss=3.092912197113037, discriminator_loss=0.05466293543577194\n",
            "step 5688: generator_loss=3.1499786376953125, discriminator_loss=0.05337879806756973\n",
            "step 5689: generator_loss=3.1921069622039795, discriminator_loss=0.05247080698609352\n",
            "step 5690: generator_loss=3.197148561477661, discriminator_loss=0.052247386425733566\n",
            "step 5691: generator_loss=3.1731600761413574, discriminator_loss=0.052643079310655594\n",
            "step 5692: generator_loss=3.114797830581665, discriminator_loss=0.053719356656074524\n",
            "step 5693: generator_loss=3.046126127243042, discriminator_loss=0.05515524744987488\n",
            "step 5694: generator_loss=2.9784371852874756, discriminator_loss=0.056751661002635956\n",
            "step 5695: generator_loss=2.9099960327148438, discriminator_loss=0.05849163234233856\n",
            "step 5696: generator_loss=2.8495144844055176, discriminator_loss=0.06008607894182205\n",
            "step 5697: generator_loss=2.7966554164886475, discriminator_loss=0.06172718107700348\n",
            "step 5698: generator_loss=2.7499406337738037, discriminator_loss=0.06326907873153687\n",
            "step 5699: generator_loss=2.738799810409546, discriminator_loss=0.06377027928829193\n",
            "step 5700: generator_loss=2.7625772953033447, discriminator_loss=0.06308344006538391\n",
            "step 5701: generator_loss=2.8105428218841553, discriminator_loss=0.06169313192367554\n",
            "step 5702: generator_loss=2.8505890369415283, discriminator_loss=0.06058965623378754\n",
            "step 5703: generator_loss=2.889937162399292, discriminator_loss=0.05960288643836975\n",
            "step 5704: generator_loss=2.9131646156311035, discriminator_loss=0.05905863270163536\n",
            "step 5705: generator_loss=2.9071743488311768, discriminator_loss=0.05942973494529724\n",
            "step 5706: generator_loss=2.890934944152832, discriminator_loss=0.06008287891745567\n",
            "step 5707: generator_loss=2.8866019248962402, discriminator_loss=0.06032398343086243\n",
            "step 5708: generator_loss=2.9618711471557617, discriminator_loss=0.05830897390842438\n",
            "step 5709: generator_loss=3.0181362628936768, discriminator_loss=0.056838929653167725\n",
            "step 5710: generator_loss=3.092515707015991, discriminator_loss=0.05500175803899765\n",
            "step 5711: generator_loss=3.1319103240966797, discriminator_loss=0.054001230746507645\n",
            "step 5712: generator_loss=3.1600441932678223, discriminator_loss=0.053293224424123764\n",
            "step 5713: generator_loss=3.1569557189941406, discriminator_loss=0.05329460650682449\n",
            "step 5714: generator_loss=3.1376216411590576, discriminator_loss=0.053382508456707\n",
            "step 5715: generator_loss=3.119966983795166, discriminator_loss=0.05348530039191246\n",
            "step 5716: generator_loss=3.084435224533081, discriminator_loss=0.054029226303100586\n",
            "step 5717: generator_loss=3.0367789268493652, discriminator_loss=0.05492424592375755\n",
            "step 5718: generator_loss=3.0054714679718018, discriminator_loss=0.05535394325852394\n",
            "step 5719: generator_loss=3.0123465061187744, discriminator_loss=0.054941967129707336\n",
            "step 5720: generator_loss=3.052818536758423, discriminator_loss=0.053628481924533844\n",
            "step 5721: generator_loss=3.1148438453674316, discriminator_loss=0.05193738639354706\n",
            "step 5722: generator_loss=3.182474374771118, discriminator_loss=0.05012640729546547\n",
            "step 5723: generator_loss=3.2508397102355957, discriminator_loss=0.04842988774180412\n",
            "step 5724: generator_loss=3.320692539215088, discriminator_loss=0.04677274078130722\n",
            "step 5725: generator_loss=3.398329257965088, discriminator_loss=0.04508061334490776\n",
            "step 5726: generator_loss=3.4404473304748535, discriminator_loss=0.04401876777410507\n",
            "step 5727: generator_loss=3.4594109058380127, discriminator_loss=0.04338719695806503\n",
            "step 5728: generator_loss=3.4513349533081055, discriminator_loss=0.043184760957956314\n",
            "step 5729: generator_loss=3.4119601249694824, discriminator_loss=0.043491147458553314\n",
            "step 5730: generator_loss=3.3678174018859863, discriminator_loss=0.04394502192735672\n",
            "step 5731: generator_loss=3.322904348373413, discriminator_loss=0.04446950554847717\n",
            "step 5732: generator_loss=3.2712643146514893, discriminator_loss=0.04517127573490143\n",
            "step 5733: generator_loss=3.239485740661621, discriminator_loss=0.04552970826625824\n",
            "step 5734: generator_loss=3.2638096809387207, discriminator_loss=0.044830773025751114\n",
            "step 5735: generator_loss=3.3050708770751953, discriminator_loss=0.043860238045454025\n",
            "step 5736: generator_loss=3.3298215866088867, discriminator_loss=0.043226029723882675\n",
            "step 5737: generator_loss=3.3510053157806396, discriminator_loss=0.04265274852514267\n",
            "step 5738: generator_loss=3.3463101387023926, discriminator_loss=0.042547039687633514\n",
            "step 5739: generator_loss=3.3079304695129395, discriminator_loss=0.04318733513355255\n",
            "step 5740: generator_loss=3.256840229034424, discriminator_loss=0.044023171067237854\n",
            "step 5741: generator_loss=3.223482608795166, discriminator_loss=0.04461349546909332\n",
            "step 5742: generator_loss=3.182786703109741, discriminator_loss=0.045395173132419586\n",
            "step 5743: generator_loss=3.1564011573791504, discriminator_loss=0.04589720070362091\n",
            "step 5744: generator_loss=3.1311492919921875, discriminator_loss=0.04640382528305054\n",
            "step 5745: generator_loss=3.1077263355255127, discriminator_loss=0.0468762181699276\n",
            "step 5746: generator_loss=3.097322940826416, discriminator_loss=0.047088444232940674\n",
            "step 5747: generator_loss=3.0869557857513428, discriminator_loss=0.04732675105333328\n",
            "step 5748: generator_loss=3.0752296447753906, discriminator_loss=0.047583311796188354\n",
            "step 5749: generator_loss=3.078423023223877, discriminator_loss=0.047533076256513596\n",
            "step 5750: generator_loss=3.0746896266937256, discriminator_loss=0.047697048634290695\n",
            "step 5751: generator_loss=3.0414388179779053, discriminator_loss=0.0485793799161911\n",
            "step 5752: generator_loss=3.012122631072998, discriminator_loss=0.04940953105688095\n",
            "step 5753: generator_loss=3.0151138305664062, discriminator_loss=0.0493621751666069\n",
            "step 5754: generator_loss=3.036104202270508, discriminator_loss=0.048925697803497314\n",
            "step 5755: generator_loss=3.0498151779174805, discriminator_loss=0.04866687208414078\n",
            "step 5756: generator_loss=3.0587594509124756, discriminator_loss=0.04859239235520363\n",
            "step 5757: generator_loss=3.042940139770508, discriminator_loss=0.04905901476740837\n",
            "step 5758: generator_loss=2.9989867210388184, discriminator_loss=0.05023069679737091\n",
            "step 5759: generator_loss=2.939117431640625, discriminator_loss=0.05199340730905533\n",
            "step 5760: generator_loss=2.895526170730591, discriminator_loss=0.053321246057748795\n",
            "step 5761: generator_loss=2.885713577270508, discriminator_loss=0.05371354520320892\n",
            "step 5762: generator_loss=2.898179292678833, discriminator_loss=0.05351351946592331\n",
            "step 5763: generator_loss=2.919581651687622, discriminator_loss=0.05298034846782684\n",
            "step 5764: generator_loss=2.9445130825042725, discriminator_loss=0.05247991904616356\n",
            "step 5765: generator_loss=2.98714542388916, discriminator_loss=0.05144582688808441\n",
            "step 5766: generator_loss=3.0237319469451904, discriminator_loss=0.050602056086063385\n",
            "step 5767: generator_loss=3.0460011959075928, discriminator_loss=0.05011032149195671\n",
            "step 5768: generator_loss=3.047212600708008, discriminator_loss=0.05012543499469757\n",
            "step 5769: generator_loss=3.036761522293091, discriminator_loss=0.050320036709308624\n",
            "step 5770: generator_loss=3.001610279083252, discriminator_loss=0.051221542060375214\n",
            "step 5771: generator_loss=2.947392463684082, discriminator_loss=0.0526374951004982\n",
            "step 5772: generator_loss=2.9174129962921143, discriminator_loss=0.05349447578191757\n",
            "step 5773: generator_loss=2.8948073387145996, discriminator_loss=0.05414389818906784\n",
            "step 5774: generator_loss=2.864785671234131, discriminator_loss=0.055017031729221344\n",
            "step 5775: generator_loss=2.8633480072021484, discriminator_loss=0.0551297552883625\n",
            "step 5776: generator_loss=2.878734827041626, discriminator_loss=0.054646387696266174\n",
            "step 5777: generator_loss=2.8927102088928223, discriminator_loss=0.05438182130455971\n",
            "step 5778: generator_loss=2.907299041748047, discriminator_loss=0.05404592305421829\n",
            "step 5779: generator_loss=2.9310452938079834, discriminator_loss=0.05338326841592789\n",
            "step 5780: generator_loss=2.9390153884887695, discriminator_loss=0.05332045257091522\n",
            "step 5781: generator_loss=2.9146997928619385, discriminator_loss=0.05408378690481186\n",
            "step 5782: generator_loss=2.8803884983062744, discriminator_loss=0.05519593134522438\n",
            "step 5783: generator_loss=2.859973669052124, discriminator_loss=0.05591072887182236\n",
            "step 5784: generator_loss=2.8460254669189453, discriminator_loss=0.056446678936481476\n",
            "step 5785: generator_loss=2.83864426612854, discriminator_loss=0.05675278976559639\n",
            "step 5786: generator_loss=2.868682622909546, discriminator_loss=0.05592968687415123\n",
            "step 5787: generator_loss=2.904329299926758, discriminator_loss=0.05492628365755081\n",
            "step 5788: generator_loss=2.9696197509765625, discriminator_loss=0.05317772552371025\n",
            "step 5789: generator_loss=3.052114248275757, discriminator_loss=0.051108963787555695\n",
            "step 5790: generator_loss=3.1018853187561035, discriminator_loss=0.05000462755560875\n",
            "step 5791: generator_loss=3.1037979125976562, discriminator_loss=0.049992017447948456\n",
            "step 5792: generator_loss=3.086480140686035, discriminator_loss=0.05041457712650299\n",
            "step 5793: generator_loss=3.064244031906128, discriminator_loss=0.05100696533918381\n",
            "step 5794: generator_loss=3.0122792720794678, discriminator_loss=0.052263446152210236\n",
            "step 5795: generator_loss=2.9430861473083496, discriminator_loss=0.05423884466290474\n",
            "step 5796: generator_loss=2.8904731273651123, discriminator_loss=0.05577719956636429\n",
            "step 5797: generator_loss=2.8642661571502686, discriminator_loss=0.05645328387618065\n",
            "step 5798: generator_loss=2.829821825027466, discriminator_loss=0.05759918689727783\n",
            "step 5799: generator_loss=2.8497402667999268, discriminator_loss=0.05717340111732483\n",
            "step 5800: generator_loss=2.867074966430664, discriminator_loss=0.05679621174931526\n",
            "step 5801: generator_loss=2.919616937637329, discriminator_loss=0.055448371917009354\n",
            "step 5802: generator_loss=2.963715076446533, discriminator_loss=0.05440463125705719\n",
            "step 5803: generator_loss=3.0024077892303467, discriminator_loss=0.05354766175150871\n",
            "step 5804: generator_loss=3.03175950050354, discriminator_loss=0.052946824580430984\n",
            "step 5805: generator_loss=3.093435525894165, discriminator_loss=0.05149642378091812\n",
            "step 5806: generator_loss=3.1807680130004883, discriminator_loss=0.04943332448601723\n",
            "step 5807: generator_loss=3.27673077583313, discriminator_loss=0.04748174548149109\n",
            "step 5808: generator_loss=3.36332368850708, discriminator_loss=0.045786984264850616\n",
            "step 5809: generator_loss=3.4290809631347656, discriminator_loss=0.044385310262441635\n",
            "step 5810: generator_loss=3.437617301940918, discriminator_loss=0.04406886547803879\n",
            "step 5811: generator_loss=3.406404495239258, discriminator_loss=0.04427023231983185\n",
            "step 5812: generator_loss=3.3968372344970703, discriminator_loss=0.04445323348045349\n",
            "step 5813: generator_loss=3.311734437942505, discriminator_loss=0.04537104070186615\n",
            "step 5814: generator_loss=3.2575531005859375, discriminator_loss=0.046166203916072845\n",
            "step 5815: generator_loss=3.183305501937866, discriminator_loss=0.04736911877989769\n",
            "step 5816: generator_loss=3.1367645263671875, discriminator_loss=0.04812478646636009\n",
            "step 5817: generator_loss=3.0902276039123535, discriminator_loss=0.0489523820579052\n",
            "step 5818: generator_loss=3.0419201850891113, discriminator_loss=0.04998115822672844\n",
            "step 5819: generator_loss=3.0259134769439697, discriminator_loss=0.050190433859825134\n",
            "step 5820: generator_loss=3.0311098098754883, discriminator_loss=0.04989761859178543\n",
            "step 5821: generator_loss=3.0486812591552734, discriminator_loss=0.04932175576686859\n",
            "step 5822: generator_loss=3.0799598693847656, discriminator_loss=0.04852443188428879\n",
            "step 5823: generator_loss=3.0736935138702393, discriminator_loss=0.04866998642683029\n",
            "step 5824: generator_loss=3.0433757305145264, discriminator_loss=0.04942988604307175\n",
            "step 5825: generator_loss=3.0090882778167725, discriminator_loss=0.0503791868686676\n",
            "step 5826: generator_loss=2.9928672313690186, discriminator_loss=0.0509200245141983\n",
            "step 5827: generator_loss=2.9818966388702393, discriminator_loss=0.05141296237707138\n",
            "step 5828: generator_loss=2.996425151824951, discriminator_loss=0.051253706216812134\n",
            "step 5829: generator_loss=3.0050971508026123, discriminator_loss=0.05113614350557327\n",
            "step 5830: generator_loss=3.003556728363037, discriminator_loss=0.051404595375061035\n",
            "step 5831: generator_loss=2.9658265113830566, discriminator_loss=0.05262302607297897\n",
            "step 5832: generator_loss=2.9130961894989014, discriminator_loss=0.054285164922475815\n",
            "step 5833: generator_loss=2.841738224029541, discriminator_loss=0.05655208230018616\n",
            "step 5834: generator_loss=2.783824920654297, discriminator_loss=0.05874358117580414\n",
            "step 5835: generator_loss=2.7169079780578613, discriminator_loss=0.061205752193927765\n",
            "step 5836: generator_loss=2.6706113815307617, discriminator_loss=0.06321537494659424\n",
            "step 5837: generator_loss=2.6328299045562744, discriminator_loss=0.064986452460289\n",
            "step 5838: generator_loss=2.588996410369873, discriminator_loss=0.06716562062501907\n",
            "step 5839: generator_loss=2.5557825565338135, discriminator_loss=0.0690610408782959\n",
            "step 5840: generator_loss=2.5531673431396484, discriminator_loss=0.06972908973693848\n",
            "step 5841: generator_loss=2.563411235809326, discriminator_loss=0.07004030048847198\n",
            "step 5842: generator_loss=2.588179111480713, discriminator_loss=0.06979452073574066\n",
            "step 5843: generator_loss=2.639753580093384, discriminator_loss=0.06857012957334518\n",
            "step 5844: generator_loss=2.6825881004333496, discriminator_loss=0.0676933079957962\n",
            "step 5845: generator_loss=2.721006393432617, discriminator_loss=0.06710655987262726\n",
            "step 5846: generator_loss=2.7394609451293945, discriminator_loss=0.06712308526039124\n",
            "step 5847: generator_loss=2.738617420196533, discriminator_loss=0.06786973774433136\n",
            "step 5848: generator_loss=2.7324228286743164, discriminator_loss=0.06875113397836685\n",
            "step 5849: generator_loss=2.7279164791107178, discriminator_loss=0.06932692229747772\n",
            "step 5850: generator_loss=2.7000513076782227, discriminator_loss=0.07076366245746613\n",
            "step 5851: generator_loss=2.689249277114868, discriminator_loss=0.07145734131336212\n",
            "step 5852: generator_loss=2.721987247467041, discriminator_loss=0.07051722705364227\n",
            "step 5853: generator_loss=2.7371482849121094, discriminator_loss=0.07032738626003265\n",
            "step 5854: generator_loss=2.713318347930908, discriminator_loss=0.07112958282232285\n",
            "step 5855: generator_loss=2.657268524169922, discriminator_loss=0.07317228615283966\n",
            "step 5856: generator_loss=2.5765342712402344, discriminator_loss=0.0762534812092781\n",
            "step 5857: generator_loss=2.5293054580688477, discriminator_loss=0.07812786102294922\n",
            "step 5858: generator_loss=2.4962077140808105, discriminator_loss=0.07968840003013611\n",
            "step 5859: generator_loss=2.4723312854766846, discriminator_loss=0.08070634305477142\n",
            "step 5860: generator_loss=2.4536869525909424, discriminator_loss=0.08167364448308945\n",
            "step 5861: generator_loss=2.441269636154175, discriminator_loss=0.08236093819141388\n",
            "step 5862: generator_loss=2.4843459129333496, discriminator_loss=0.08067817986011505\n",
            "step 5863: generator_loss=2.5874409675598145, discriminator_loss=0.07665368914604187\n",
            "step 5864: generator_loss=2.7096595764160156, discriminator_loss=0.07234500348567963\n",
            "step 5865: generator_loss=2.8380613327026367, discriminator_loss=0.0686078816652298\n",
            "step 5866: generator_loss=2.970705986022949, discriminator_loss=0.06505833566188812\n",
            "step 5867: generator_loss=3.0474040508270264, discriminator_loss=0.06320615112781525\n",
            "step 5868: generator_loss=3.0576186180114746, discriminator_loss=0.06290073692798615\n",
            "step 5869: generator_loss=3.0676918029785156, discriminator_loss=0.06240268424153328\n",
            "step 5870: generator_loss=3.0819091796875, discriminator_loss=0.06188206374645233\n",
            "step 5871: generator_loss=3.0937588214874268, discriminator_loss=0.06109218671917915\n",
            "step 5872: generator_loss=3.12888503074646, discriminator_loss=0.059586405754089355\n",
            "step 5873: generator_loss=3.178596019744873, discriminator_loss=0.05774584412574768\n",
            "step 5874: generator_loss=3.210475444793701, discriminator_loss=0.05619334429502487\n",
            "step 5875: generator_loss=3.1995320320129395, discriminator_loss=0.05549837648868561\n",
            "step 5876: generator_loss=3.1600112915039062, discriminator_loss=0.05523824691772461\n",
            "step 5877: generator_loss=3.1406242847442627, discriminator_loss=0.05471761152148247\n",
            "step 5878: generator_loss=3.128296136856079, discriminator_loss=0.053909096866846085\n",
            "step 5879: generator_loss=3.147189140319824, discriminator_loss=0.0523202084004879\n",
            "step 5880: generator_loss=3.1569199562072754, discriminator_loss=0.05105993524193764\n",
            "step 5881: generator_loss=3.1910364627838135, discriminator_loss=0.04938722029328346\n",
            "step 5882: generator_loss=3.2429885864257812, discriminator_loss=0.0475674569606781\n",
            "step 5883: generator_loss=3.2755937576293945, discriminator_loss=0.04628010466694832\n",
            "step 5884: generator_loss=3.2625365257263184, discriminator_loss=0.0459461435675621\n",
            "step 5885: generator_loss=3.2472195625305176, discriminator_loss=0.0457184836268425\n",
            "step 5886: generator_loss=3.233367919921875, discriminator_loss=0.04542384296655655\n",
            "step 5887: generator_loss=3.220158100128174, discriminator_loss=0.04525862634181976\n",
            "step 5888: generator_loss=3.1844797134399414, discriminator_loss=0.04568959027528763\n",
            "step 5889: generator_loss=3.1576614379882812, discriminator_loss=0.045961979776620865\n",
            "step 5890: generator_loss=3.139958143234253, discriminator_loss=0.04606768488883972\n",
            "step 5891: generator_loss=3.115993022918701, discriminator_loss=0.04647248238325119\n",
            "step 5892: generator_loss=3.106020927429199, discriminator_loss=0.04649827629327774\n",
            "step 5893: generator_loss=3.0703134536743164, discriminator_loss=0.047232747077941895\n",
            "step 5894: generator_loss=3.0245766639709473, discriminator_loss=0.04831628128886223\n",
            "step 5895: generator_loss=2.986333131790161, discriminator_loss=0.049372196197509766\n",
            "step 5896: generator_loss=2.92462158203125, discriminator_loss=0.051293764263391495\n",
            "step 5897: generator_loss=2.8421778678894043, discriminator_loss=0.05475880205631256\n",
            "step 5898: generator_loss=2.8802525997161865, discriminator_loss=0.05371536687016487\n",
            "step 5899: generator_loss=2.8250744342803955, discriminator_loss=0.057433634996414185\n",
            "step 5900: generator_loss=2.711437702178955, discriminator_loss=0.06799232959747314\n",
            "step 5901: generator_loss=2.679591417312622, discriminator_loss=0.0843612551689148\n",
            "step 5902: generator_loss=2.71738862991333, discriminator_loss=0.09858863055706024\n",
            "step 5903: generator_loss=2.582256317138672, discriminator_loss=0.14088016748428345\n",
            "step 5904: generator_loss=2.5554141998291016, discriminator_loss=0.1675940752029419\n",
            "step 5905: generator_loss=2.6424078941345215, discriminator_loss=0.21049244701862335\n",
            "step 5906: generator_loss=2.9729461669921875, discriminator_loss=0.17273768782615662\n",
            "step 5907: generator_loss=2.954458236694336, discriminator_loss=0.20308586955070496\n",
            "step 5908: generator_loss=3.176241159439087, discriminator_loss=0.16936936974525452\n",
            "step 5909: generator_loss=3.6953041553497314, discriminator_loss=0.10996947437524796\n",
            "step 5910: generator_loss=3.73335599899292, discriminator_loss=0.10193416476249695\n",
            "step 5911: generator_loss=3.924730062484741, discriminator_loss=0.08324495702981949\n",
            "step 5912: generator_loss=4.066605567932129, discriminator_loss=0.07180771976709366\n",
            "step 5913: generator_loss=4.6258625984191895, discriminator_loss=0.05927333980798721\n",
            "step 5914: generator_loss=4.708883285522461, discriminator_loss=0.056062325835227966\n",
            "step 5915: generator_loss=4.865837097167969, discriminator_loss=0.05263800919055939\n",
            "step 5916: generator_loss=5.05727481842041, discriminator_loss=0.047785766422748566\n",
            "step 5917: generator_loss=5.110832691192627, discriminator_loss=0.04588383436203003\n",
            "step 5918: generator_loss=5.251380920410156, discriminator_loss=0.045174721628427505\n",
            "step 5919: generator_loss=5.221246719360352, discriminator_loss=0.04549380764365196\n",
            "step 5920: generator_loss=5.240455150604248, discriminator_loss=0.044843196868896484\n",
            "step 5921: generator_loss=5.1517791748046875, discriminator_loss=0.04490073025226593\n",
            "step 5922: generator_loss=4.872347354888916, discriminator_loss=0.04658639803528786\n",
            "step 5923: generator_loss=4.847128868103027, discriminator_loss=0.047471094876527786\n",
            "step 5924: generator_loss=4.675619602203369, discriminator_loss=0.049338240176439285\n",
            "step 5925: generator_loss=4.297102928161621, discriminator_loss=0.05551321804523468\n",
            "step 5926: generator_loss=4.352850437164307, discriminator_loss=0.053383953869342804\n",
            "step 5927: generator_loss=4.171428680419922, discriminator_loss=0.05571823567152023\n",
            "step 5928: generator_loss=3.750263214111328, discriminator_loss=0.06254924088716507\n",
            "step 5929: generator_loss=3.8256101608276367, discriminator_loss=0.05980692058801651\n",
            "step 5930: generator_loss=3.457357883453369, discriminator_loss=0.0613204650580883\n",
            "step 5931: generator_loss=3.480811357498169, discriminator_loss=0.06167677044868469\n",
            "step 5932: generator_loss=3.542715311050415, discriminator_loss=0.05989697203040123\n",
            "step 5933: generator_loss=3.3836522102355957, discriminator_loss=0.055058322846889496\n",
            "step 5934: generator_loss=3.137840509414673, discriminator_loss=0.06575335562229156\n",
            "step 5935: generator_loss=3.2809367179870605, discriminator_loss=0.05712955445051193\n",
            "step 5936: generator_loss=3.3969271183013916, discriminator_loss=0.05767768621444702\n",
            "step 5937: generator_loss=3.276512622833252, discriminator_loss=0.05584632232785225\n",
            "step 5938: generator_loss=3.3256747722625732, discriminator_loss=0.053793765604496\n",
            "step 5939: generator_loss=3.3431591987609863, discriminator_loss=0.05458299443125725\n",
            "step 5940: generator_loss=3.28599214553833, discriminator_loss=0.05328618735074997\n",
            "step 5941: generator_loss=3.1667914390563965, discriminator_loss=0.053794968873262405\n",
            "step 5942: generator_loss=3.0415191650390625, discriminator_loss=0.05803622305393219\n",
            "step 5943: generator_loss=2.974841356277466, discriminator_loss=0.059026945382356644\n",
            "step 5944: generator_loss=2.9433364868164062, discriminator_loss=0.05911549553275108\n",
            "step 5945: generator_loss=2.9152369499206543, discriminator_loss=0.059293437749147415\n",
            "step 5946: generator_loss=2.861233949661255, discriminator_loss=0.06315165013074875\n",
            "step 5947: generator_loss=2.8195109367370605, discriminator_loss=0.06353961676359177\n",
            "step 5948: generator_loss=2.8533153533935547, discriminator_loss=0.06289516389369965\n",
            "step 5949: generator_loss=2.8097453117370605, discriminator_loss=0.06374450027942657\n",
            "step 5950: generator_loss=2.734698534011841, discriminator_loss=0.06710008531808853\n",
            "step 5951: generator_loss=2.555356025695801, discriminator_loss=0.07024441659450531\n",
            "step 5952: generator_loss=2.6556949615478516, discriminator_loss=0.0698777437210083\n",
            "step 5953: generator_loss=2.7535698413848877, discriminator_loss=0.06793491542339325\n",
            "step 5954: generator_loss=2.540851593017578, discriminator_loss=0.07250151038169861\n",
            "step 5955: generator_loss=2.7010035514831543, discriminator_loss=0.07186681032180786\n",
            "step 5956: generator_loss=2.606076717376709, discriminator_loss=0.0748039036989212\n",
            "step 5957: generator_loss=2.5852043628692627, discriminator_loss=0.07630425691604614\n",
            "step 5958: generator_loss=2.5107040405273438, discriminator_loss=0.07789371907711029\n",
            "step 5959: generator_loss=2.438603401184082, discriminator_loss=0.07901487499475479\n",
            "step 5960: generator_loss=2.5794103145599365, discriminator_loss=0.07809511572122574\n",
            "step 5961: generator_loss=2.519848108291626, discriminator_loss=0.07994851469993591\n",
            "step 5962: generator_loss=2.4884328842163086, discriminator_loss=0.07946471869945526\n",
            "step 5963: generator_loss=2.5759708881378174, discriminator_loss=0.08065272122621536\n",
            "step 5964: generator_loss=2.4623827934265137, discriminator_loss=0.0824948400259018\n",
            "step 5965: generator_loss=2.441849708557129, discriminator_loss=0.08402466773986816\n",
            "step 5966: generator_loss=2.530734062194824, discriminator_loss=0.08393104374408722\n",
            "step 5967: generator_loss=2.4528961181640625, discriminator_loss=0.08558733761310577\n",
            "step 5968: generator_loss=2.3991827964782715, discriminator_loss=0.08707299828529358\n",
            "step 5969: generator_loss=2.441080093383789, discriminator_loss=0.08663290739059448\n",
            "step 5970: generator_loss=2.444420337677002, discriminator_loss=0.08736695349216461\n",
            "step 5971: generator_loss=2.450256824493408, discriminator_loss=0.08698674291372299\n",
            "step 5972: generator_loss=2.453885316848755, discriminator_loss=0.0872783362865448\n",
            "step 5973: generator_loss=2.48972225189209, discriminator_loss=0.08641236275434494\n",
            "step 5974: generator_loss=2.4978041648864746, discriminator_loss=0.08686455339193344\n",
            "step 5975: generator_loss=2.4823131561279297, discriminator_loss=0.08828863501548767\n",
            "step 5976: generator_loss=2.497640609741211, discriminator_loss=0.088192880153656\n",
            "step 5977: generator_loss=2.547734260559082, discriminator_loss=0.08676838874816895\n",
            "step 5978: generator_loss=2.5825414657592773, discriminator_loss=0.08609551936388016\n",
            "step 5979: generator_loss=2.598036050796509, discriminator_loss=0.08610332012176514\n",
            "step 5980: generator_loss=2.605333089828491, discriminator_loss=0.08626952767372131\n",
            "step 5981: generator_loss=2.617479085922241, discriminator_loss=0.08624053001403809\n",
            "step 5982: generator_loss=2.59342098236084, discriminator_loss=0.08752478659152985\n",
            "step 5983: generator_loss=2.5804901123046875, discriminator_loss=0.08841560781002045\n",
            "step 5984: generator_loss=2.6359150409698486, discriminator_loss=0.08643339574337006\n",
            "step 5985: generator_loss=2.768190860748291, discriminator_loss=0.08183834701776505\n",
            "step 5986: generator_loss=2.9011998176574707, discriminator_loss=0.077479287981987\n",
            "step 5987: generator_loss=3.015505313873291, discriminator_loss=0.07376043498516083\n",
            "step 5988: generator_loss=3.1083579063415527, discriminator_loss=0.07079073786735535\n",
            "step 5989: generator_loss=3.17539119720459, discriminator_loss=0.06830523908138275\n",
            "step 5990: generator_loss=3.2186732292175293, discriminator_loss=0.06609794497489929\n",
            "step 5991: generator_loss=3.1989145278930664, discriminator_loss=0.06536394357681274\n",
            "step 5992: generator_loss=3.1459875106811523, discriminator_loss=0.06489163637161255\n",
            "step 5993: generator_loss=3.101841926574707, discriminator_loss=0.06434953212738037\n",
            "step 5994: generator_loss=3.041693687438965, discriminator_loss=0.06417738646268845\n",
            "step 5995: generator_loss=2.995626211166382, discriminator_loss=0.06373666971921921\n",
            "step 5996: generator_loss=3.0278661251068115, discriminator_loss=0.06150038540363312\n",
            "step 5997: generator_loss=3.072194814682007, discriminator_loss=0.05906746909022331\n",
            "step 5998: generator_loss=3.1029534339904785, discriminator_loss=0.057125478982925415\n",
            "step 5999: generator_loss=3.0845389366149902, discriminator_loss=0.05642049014568329\n",
            "step 6000: generator_loss=3.0373737812042236, discriminator_loss=0.056556299328804016\n",
            "step 6001: generator_loss=2.9925377368927, discriminator_loss=0.056902676820755005\n",
            "step 6002: generator_loss=2.9647345542907715, discriminator_loss=0.05701621621847153\n",
            "step 6003: generator_loss=2.9111080169677734, discriminator_loss=0.05797189474105835\n",
            "step 6004: generator_loss=2.829974889755249, discriminator_loss=0.06003601849079132\n",
            "step 6005: generator_loss=2.751265048980713, discriminator_loss=0.062263812869787216\n",
            "step 6006: generator_loss=2.6784889698028564, discriminator_loss=0.06467205286026001\n",
            "step 6007: generator_loss=2.6444125175476074, discriminator_loss=0.0659039244055748\n",
            "step 6008: generator_loss=2.6350860595703125, discriminator_loss=0.06638456881046295\n",
            "step 6009: generator_loss=2.6697347164154053, discriminator_loss=0.0653616264462471\n",
            "step 6010: generator_loss=2.721256732940674, discriminator_loss=0.06378214061260223\n",
            "step 6011: generator_loss=2.770364999771118, discriminator_loss=0.06249651312828064\n",
            "step 6012: generator_loss=2.821561336517334, discriminator_loss=0.06125931441783905\n",
            "step 6013: generator_loss=2.8536434173583984, discriminator_loss=0.06061442941427231\n",
            "step 6014: generator_loss=2.865452289581299, discriminator_loss=0.06060659512877464\n",
            "step 6015: generator_loss=2.8573431968688965, discriminator_loss=0.06121060624718666\n",
            "step 6016: generator_loss=2.8307647705078125, discriminator_loss=0.06238049268722534\n",
            "step 6017: generator_loss=2.7890748977661133, discriminator_loss=0.06413093954324722\n",
            "step 6018: generator_loss=2.742924213409424, discriminator_loss=0.06594609469175339\n",
            "step 6019: generator_loss=2.717104196548462, discriminator_loss=0.06723632663488388\n",
            "step 6020: generator_loss=2.6886179447174072, discriminator_loss=0.06853510439395905\n",
            "step 6021: generator_loss=2.663877248764038, discriminator_loss=0.06986482441425323\n",
            "step 6022: generator_loss=2.6342873573303223, discriminator_loss=0.07121992111206055\n",
            "step 6023: generator_loss=2.5957953929901123, discriminator_loss=0.07297930121421814\n",
            "step 6024: generator_loss=2.55419659614563, discriminator_loss=0.07494231313467026\n",
            "step 6025: generator_loss=2.509533166885376, discriminator_loss=0.07711292803287506\n",
            "step 6026: generator_loss=2.4635915756225586, discriminator_loss=0.07957928627729416\n",
            "step 6027: generator_loss=2.427095890045166, discriminator_loss=0.08172839879989624\n",
            "step 6028: generator_loss=2.4245481491088867, discriminator_loss=0.08230778574943542\n",
            "step 6029: generator_loss=2.438502788543701, discriminator_loss=0.08203425258398056\n",
            "step 6030: generator_loss=2.451277256011963, discriminator_loss=0.08197401463985443\n",
            "step 6031: generator_loss=2.4920296669006348, discriminator_loss=0.0805865153670311\n",
            "step 6032: generator_loss=2.5226223468780518, discriminator_loss=0.07972732931375504\n",
            "step 6033: generator_loss=2.5435216426849365, discriminator_loss=0.07922728359699249\n",
            "step 6034: generator_loss=2.5931553840637207, discriminator_loss=0.0776195228099823\n",
            "step 6035: generator_loss=2.654362201690674, discriminator_loss=0.07563488930463791\n",
            "step 6036: generator_loss=2.7018284797668457, discriminator_loss=0.07393960654735565\n",
            "step 6037: generator_loss=2.770970582962036, discriminator_loss=0.07173264026641846\n",
            "step 6038: generator_loss=2.815580368041992, discriminator_loss=0.07030253112316132\n",
            "step 6039: generator_loss=2.8695127964019775, discriminator_loss=0.06855978816747665\n",
            "step 6040: generator_loss=2.9507298469543457, discriminator_loss=0.06600920110940933\n",
            "step 6041: generator_loss=3.017245054244995, discriminator_loss=0.06399296969175339\n",
            "step 6042: generator_loss=3.046963930130005, discriminator_loss=0.06285778433084488\n",
            "step 6043: generator_loss=3.041566848754883, discriminator_loss=0.062496814876794815\n",
            "step 6044: generator_loss=2.99777889251709, discriminator_loss=0.0631786584854126\n",
            "step 6045: generator_loss=2.916368246078491, discriminator_loss=0.06474803388118744\n",
            "step 6046: generator_loss=2.8362879753112793, discriminator_loss=0.06650903820991516\n",
            "step 6047: generator_loss=2.7978734970092773, discriminator_loss=0.06701433658599854\n",
            "step 6048: generator_loss=2.751586437225342, discriminator_loss=0.0679624006152153\n",
            "step 6049: generator_loss=2.6970858573913574, discriminator_loss=0.06925841420888901\n",
            "step 6050: generator_loss=2.644620656967163, discriminator_loss=0.07085062563419342\n",
            "step 6051: generator_loss=2.576798677444458, discriminator_loss=0.07324228435754776\n",
            "step 6052: generator_loss=2.507936477661133, discriminator_loss=0.07618449628353119\n",
            "step 6053: generator_loss=2.452519416809082, discriminator_loss=0.07891222834587097\n",
            "step 6054: generator_loss=2.4486069679260254, discriminator_loss=0.08002122491598129\n",
            "step 6055: generator_loss=2.4260902404785156, discriminator_loss=0.08113402128219604\n",
            "step 6056: generator_loss=2.56864857673645, discriminator_loss=0.07501339912414551\n",
            "step 6057: generator_loss=2.660515069961548, discriminator_loss=0.07336001098155975\n",
            "step 6058: generator_loss=2.6937689781188965, discriminator_loss=0.07502207159996033\n",
            "step 6059: generator_loss=2.8178467750549316, discriminator_loss=0.07171487808227539\n",
            "step 6060: generator_loss=2.8645613193511963, discriminator_loss=0.07215180993080139\n",
            "step 6061: generator_loss=3.011396646499634, discriminator_loss=0.06846464425325394\n",
            "step 6062: generator_loss=3.0616371631622314, discriminator_loss=0.06964154541492462\n",
            "step 6063: generator_loss=3.1298675537109375, discriminator_loss=0.06953655183315277\n",
            "step 6064: generator_loss=3.2010912895202637, discriminator_loss=0.06874115020036697\n",
            "step 6065: generator_loss=3.14579439163208, discriminator_loss=0.07439756393432617\n",
            "step 6066: generator_loss=3.251668691635132, discriminator_loss=0.07302987575531006\n",
            "step 6067: generator_loss=3.105863094329834, discriminator_loss=0.08016017824411392\n",
            "step 6068: generator_loss=3.243969202041626, discriminator_loss=0.08188962191343307\n",
            "step 6069: generator_loss=2.9776620864868164, discriminator_loss=0.0970340222120285\n",
            "step 6070: generator_loss=3.1218032836914062, discriminator_loss=0.09273576736450195\n",
            "step 6071: generator_loss=3.1831979751586914, discriminator_loss=0.09454450756311417\n",
            "step 6072: generator_loss=3.0737509727478027, discriminator_loss=0.10399696230888367\n",
            "step 6073: generator_loss=3.3002305030822754, discriminator_loss=0.0917380303144455\n",
            "step 6074: generator_loss=3.212770700454712, discriminator_loss=0.09074071794748306\n",
            "step 6075: generator_loss=3.4744057655334473, discriminator_loss=0.07394599914550781\n",
            "step 6076: generator_loss=3.7927021980285645, discriminator_loss=0.06157299503684044\n",
            "step 6077: generator_loss=4.3864216804504395, discriminator_loss=0.05079463869333267\n",
            "step 6078: generator_loss=4.088765621185303, discriminator_loss=0.055280882865190506\n",
            "step 6079: generator_loss=4.287660598754883, discriminator_loss=0.05046447739005089\n",
            "step 6080: generator_loss=4.316265106201172, discriminator_loss=0.05085758864879608\n",
            "step 6081: generator_loss=4.437380790710449, discriminator_loss=0.047435034066438675\n",
            "step 6082: generator_loss=4.161215305328369, discriminator_loss=0.051829010248184204\n",
            "step 6083: generator_loss=4.205419540405273, discriminator_loss=0.04992089420557022\n",
            "step 6084: generator_loss=4.170087814331055, discriminator_loss=0.05021846294403076\n",
            "step 6085: generator_loss=3.879657030105591, discriminator_loss=0.054668474942445755\n",
            "step 6086: generator_loss=3.7952680587768555, discriminator_loss=0.05502687394618988\n",
            "step 6087: generator_loss=3.9687516689300537, discriminator_loss=0.05170001462101936\n",
            "step 6088: generator_loss=3.724271774291992, discriminator_loss=0.055694207549095154\n",
            "step 6089: generator_loss=3.76603102684021, discriminator_loss=0.051942043006420135\n",
            "step 6090: generator_loss=3.5994038581848145, discriminator_loss=0.05623912438750267\n",
            "step 6091: generator_loss=3.569901704788208, discriminator_loss=0.05468647927045822\n",
            "step 6092: generator_loss=3.403306007385254, discriminator_loss=0.05827225744724274\n",
            "step 6093: generator_loss=3.270170211791992, discriminator_loss=0.059196390211582184\n",
            "step 6094: generator_loss=3.2763051986694336, discriminator_loss=0.05672003701329231\n",
            "step 6095: generator_loss=3.263078212738037, discriminator_loss=0.05745568126440048\n",
            "step 6096: generator_loss=3.2160472869873047, discriminator_loss=0.05657372623682022\n",
            "step 6097: generator_loss=3.1335859298706055, discriminator_loss=0.056759461760520935\n",
            "step 6098: generator_loss=3.1536335945129395, discriminator_loss=0.05578404664993286\n",
            "step 6099: generator_loss=3.0961380004882812, discriminator_loss=0.057478949427604675\n",
            "step 6100: generator_loss=3.1455278396606445, discriminator_loss=0.05533188581466675\n",
            "step 6101: generator_loss=3.097177505493164, discriminator_loss=0.05579189211130142\n",
            "step 6102: generator_loss=2.926926612854004, discriminator_loss=0.059386178851127625\n",
            "step 6103: generator_loss=2.8894338607788086, discriminator_loss=0.06097356230020523\n",
            "step 6104: generator_loss=2.8622007369995117, discriminator_loss=0.06197076290845871\n",
            "step 6105: generator_loss=2.785005569458008, discriminator_loss=0.06387706845998764\n",
            "step 6106: generator_loss=2.704796314239502, discriminator_loss=0.06593088805675507\n",
            "step 6107: generator_loss=2.706885814666748, discriminator_loss=0.06581763923168182\n",
            "step 6108: generator_loss=2.6862006187438965, discriminator_loss=0.06585712730884552\n",
            "step 6109: generator_loss=2.7674508094787598, discriminator_loss=0.06293490529060364\n",
            "step 6110: generator_loss=2.8184266090393066, discriminator_loss=0.061359092593193054\n",
            "step 6111: generator_loss=2.864837646484375, discriminator_loss=0.06014230102300644\n",
            "step 6112: generator_loss=2.9401087760925293, discriminator_loss=0.05855061858892441\n",
            "step 6113: generator_loss=3.025623083114624, discriminator_loss=0.057150259613990784\n",
            "step 6114: generator_loss=3.0575623512268066, discriminator_loss=0.056595854461193085\n",
            "step 6115: generator_loss=3.1313445568084717, discriminator_loss=0.05617975443601608\n",
            "step 6116: generator_loss=3.107903480529785, discriminator_loss=0.058294154703617096\n",
            "step 6117: generator_loss=3.0007236003875732, discriminator_loss=0.061382099986076355\n",
            "step 6118: generator_loss=2.8210015296936035, discriminator_loss=0.06593827903270721\n",
            "step 6119: generator_loss=2.799258232116699, discriminator_loss=0.06782585382461548\n",
            "step 6120: generator_loss=2.6833367347717285, discriminator_loss=0.07149092853069305\n",
            "step 6121: generator_loss=2.5679047107696533, discriminator_loss=0.07620476186275482\n",
            "step 6122: generator_loss=2.415469169616699, discriminator_loss=0.08202708512544632\n",
            "step 6123: generator_loss=2.3440864086151123, discriminator_loss=0.08659382164478302\n",
            "step 6124: generator_loss=2.324357032775879, discriminator_loss=0.0880955383181572\n",
            "step 6125: generator_loss=2.3829078674316406, discriminator_loss=0.0856793224811554\n",
            "step 6126: generator_loss=2.3962769508361816, discriminator_loss=0.08621884882450104\n",
            "step 6127: generator_loss=2.4573287963867188, discriminator_loss=0.08484478294849396\n",
            "step 6128: generator_loss=2.5964365005493164, discriminator_loss=0.08096995949745178\n",
            "step 6129: generator_loss=2.5814661979675293, discriminator_loss=0.08151677995920181\n",
            "step 6130: generator_loss=2.570211887359619, discriminator_loss=0.08275657147169113\n",
            "step 6131: generator_loss=2.5237154960632324, discriminator_loss=0.08568812906742096\n",
            "step 6132: generator_loss=2.435072898864746, discriminator_loss=0.09060090780258179\n",
            "step 6133: generator_loss=2.381709575653076, discriminator_loss=0.09428627789020538\n",
            "step 6134: generator_loss=2.3074846267700195, discriminator_loss=0.09896529465913773\n",
            "step 6135: generator_loss=2.326049327850342, discriminator_loss=0.09896264970302582\n",
            "step 6136: generator_loss=2.4219613075256348, discriminator_loss=0.09518396109342575\n",
            "step 6137: generator_loss=2.555335283279419, discriminator_loss=0.09036305546760559\n",
            "step 6138: generator_loss=2.73928165435791, discriminator_loss=0.08404582738876343\n",
            "step 6139: generator_loss=2.894162654876709, discriminator_loss=0.07966524362564087\n",
            "step 6140: generator_loss=2.9898033142089844, discriminator_loss=0.07694423198699951\n",
            "step 6141: generator_loss=3.056147575378418, discriminator_loss=0.07467614859342575\n",
            "step 6142: generator_loss=3.059131145477295, discriminator_loss=0.0735960528254509\n",
            "step 6143: generator_loss=3.0591347217559814, discriminator_loss=0.07227872312068939\n",
            "step 6144: generator_loss=3.036816120147705, discriminator_loss=0.07120133936405182\n",
            "step 6145: generator_loss=2.982062339782715, discriminator_loss=0.07081478834152222\n",
            "step 6146: generator_loss=2.967284679412842, discriminator_loss=0.06949271261692047\n",
            "step 6147: generator_loss=2.977705478668213, discriminator_loss=0.06741498410701752\n",
            "step 6148: generator_loss=3.0096733570098877, discriminator_loss=0.06474276632070541\n",
            "step 6149: generator_loss=3.0117297172546387, discriminator_loss=0.06324174255132675\n",
            "step 6150: generator_loss=3.011744976043701, discriminator_loss=0.06186138838529587\n",
            "step 6151: generator_loss=2.9368677139282227, discriminator_loss=0.062354929745197296\n",
            "step 6152: generator_loss=2.8772263526916504, discriminator_loss=0.06288900971412659\n",
            "step 6153: generator_loss=2.885396957397461, discriminator_loss=0.061640653759241104\n",
            "step 6154: generator_loss=2.940403699874878, discriminator_loss=0.059207089245319366\n",
            "step 6155: generator_loss=2.988997459411621, discriminator_loss=0.05709465593099594\n",
            "step 6156: generator_loss=3.016186475753784, discriminator_loss=0.05579930543899536\n",
            "step 6157: generator_loss=3.0261383056640625, discriminator_loss=0.055041104555130005\n",
            "step 6158: generator_loss=3.007941246032715, discriminator_loss=0.05509589612483978\n",
            "step 6159: generator_loss=2.987318992614746, discriminator_loss=0.055351242423057556\n",
            "step 6160: generator_loss=2.9578821659088135, discriminator_loss=0.055843137204647064\n",
            "step 6161: generator_loss=2.909980058670044, discriminator_loss=0.057070158421993256\n",
            "step 6162: generator_loss=2.8510940074920654, discriminator_loss=0.05868905782699585\n",
            "step 6163: generator_loss=2.804691791534424, discriminator_loss=0.06007762253284454\n",
            "step 6164: generator_loss=2.76871657371521, discriminator_loss=0.061245039105415344\n",
            "step 6165: generator_loss=2.7422308921813965, discriminator_loss=0.06228369474411011\n",
            "step 6166: generator_loss=2.7170801162719727, discriminator_loss=0.06329907476902008\n",
            "step 6167: generator_loss=2.713125467300415, discriminator_loss=0.06360343098640442\n",
            "step 6168: generator_loss=2.714606285095215, discriminator_loss=0.06378260254859924\n",
            "step 6169: generator_loss=2.6998038291931152, discriminator_loss=0.06458688527345657\n",
            "step 6170: generator_loss=2.695375680923462, discriminator_loss=0.06511066854000092\n",
            "step 6171: generator_loss=2.6877617835998535, discriminator_loss=0.06580998003482819\n",
            "step 6172: generator_loss=2.69944429397583, discriminator_loss=0.06577267497777939\n",
            "step 6173: generator_loss=2.7166390419006348, discriminator_loss=0.06571453809738159\n",
            "step 6174: generator_loss=2.7195661067962646, discriminator_loss=0.0661407858133316\n",
            "step 6175: generator_loss=2.7109286785125732, discriminator_loss=0.06704294681549072\n",
            "step 6176: generator_loss=2.698605537414551, discriminator_loss=0.06807857006788254\n",
            "step 6177: generator_loss=2.704566717147827, discriminator_loss=0.06840208172798157\n",
            "step 6178: generator_loss=2.706787109375, discriminator_loss=0.068805992603302\n",
            "step 6179: generator_loss=2.7318737506866455, discriminator_loss=0.06834331154823303\n",
            "step 6180: generator_loss=2.752143383026123, discriminator_loss=0.06798896193504333\n",
            "step 6181: generator_loss=2.760819673538208, discriminator_loss=0.06792623549699783\n",
            "step 6182: generator_loss=2.7544655799865723, discriminator_loss=0.06829697638750076\n",
            "step 6183: generator_loss=2.7388389110565186, discriminator_loss=0.06888815015554428\n",
            "step 6184: generator_loss=2.7343599796295166, discriminator_loss=0.06909428536891937\n",
            "step 6185: generator_loss=2.768467664718628, discriminator_loss=0.06785032153129578\n",
            "step 6186: generator_loss=2.806396722793579, discriminator_loss=0.06650495529174805\n",
            "step 6187: generator_loss=2.8301000595092773, discriminator_loss=0.06549888849258423\n",
            "step 6188: generator_loss=2.8362393379211426, discriminator_loss=0.06491632759571075\n",
            "step 6189: generator_loss=2.8106493949890137, discriminator_loss=0.06540204584598541\n",
            "step 6190: generator_loss=2.7724480628967285, discriminator_loss=0.06633010506629944\n",
            "step 6191: generator_loss=2.7373135089874268, discriminator_loss=0.0671466663479805\n",
            "step 6192: generator_loss=2.7044787406921387, discriminator_loss=0.06806086003780365\n",
            "step 6193: generator_loss=2.689502716064453, discriminator_loss=0.06823240220546722\n",
            "step 6194: generator_loss=2.667397975921631, discriminator_loss=0.06880520284175873\n",
            "step 6195: generator_loss=2.6414642333984375, discriminator_loss=0.06952138990163803\n",
            "step 6196: generator_loss=2.6244983673095703, discriminator_loss=0.07023655623197556\n",
            "step 6197: generator_loss=2.604889392852783, discriminator_loss=0.07095003128051758\n",
            "step 6198: generator_loss=2.590773582458496, discriminator_loss=0.07164332270622253\n",
            "step 6199: generator_loss=2.5836172103881836, discriminator_loss=0.07203240692615509\n",
            "step 6200: generator_loss=2.5927846431732178, discriminator_loss=0.07177498936653137\n",
            "step 6201: generator_loss=2.612150192260742, discriminator_loss=0.071233831346035\n",
            "step 6202: generator_loss=2.666888952255249, discriminator_loss=0.0692724734544754\n",
            "step 6203: generator_loss=2.7017836570739746, discriminator_loss=0.06817051768302917\n",
            "step 6204: generator_loss=2.7262589931488037, discriminator_loss=0.06747183203697205\n",
            "step 6205: generator_loss=2.7468156814575195, discriminator_loss=0.06681030988693237\n",
            "step 6206: generator_loss=2.7896900177001953, discriminator_loss=0.06544754654169083\n",
            "step 6207: generator_loss=2.8117778301239014, discriminator_loss=0.06483446061611176\n",
            "step 6208: generator_loss=2.8271045684814453, discriminator_loss=0.0643698126077652\n",
            "step 6209: generator_loss=2.827526330947876, discriminator_loss=0.06420154124498367\n",
            "step 6210: generator_loss=2.8337512016296387, discriminator_loss=0.06403293460607529\n",
            "step 6211: generator_loss=2.8204784393310547, discriminator_loss=0.06425975263118744\n",
            "step 6212: generator_loss=2.823178291320801, discriminator_loss=0.06415083259344101\n",
            "step 6213: generator_loss=2.8341493606567383, discriminator_loss=0.06374607980251312\n",
            "step 6214: generator_loss=2.8436732292175293, discriminator_loss=0.0632905662059784\n",
            "step 6215: generator_loss=2.879457950592041, discriminator_loss=0.06208442896604538\n",
            "step 6216: generator_loss=2.957183361053467, discriminator_loss=0.0597367100417614\n",
            "step 6217: generator_loss=3.039264678955078, discriminator_loss=0.05748419463634491\n",
            "step 6218: generator_loss=3.0968501567840576, discriminator_loss=0.05580548197031021\n",
            "step 6219: generator_loss=3.1262054443359375, discriminator_loss=0.054822005331516266\n",
            "step 6220: generator_loss=3.147005081176758, discriminator_loss=0.05413484573364258\n",
            "step 6221: generator_loss=3.19242525100708, discriminator_loss=0.05277080461382866\n",
            "step 6222: generator_loss=3.22141695022583, discriminator_loss=0.05165848881006241\n",
            "step 6223: generator_loss=3.2405500411987305, discriminator_loss=0.05092602223157883\n",
            "step 6224: generator_loss=3.2197628021240234, discriminator_loss=0.05081315338611603\n",
            "step 6225: generator_loss=3.1646711826324463, discriminator_loss=0.05152296647429466\n",
            "step 6226: generator_loss=3.1075856685638428, discriminator_loss=0.05232284963130951\n",
            "step 6227: generator_loss=3.0175228118896484, discriminator_loss=0.05404284596443176\n",
            "step 6228: generator_loss=2.940147638320923, discriminator_loss=0.055764999240636826\n",
            "step 6229: generator_loss=2.9019203186035156, discriminator_loss=0.05648903548717499\n",
            "step 6230: generator_loss=2.8666229248046875, discriminator_loss=0.05720597878098488\n",
            "step 6231: generator_loss=2.848242998123169, discriminator_loss=0.057551052421331406\n",
            "step 6232: generator_loss=2.8971383571624756, discriminator_loss=0.05589932203292847\n",
            "step 6233: generator_loss=2.9787378311157227, discriminator_loss=0.0535186342895031\n",
            "step 6234: generator_loss=3.048534393310547, discriminator_loss=0.05163354426622391\n",
            "step 6235: generator_loss=3.099494457244873, discriminator_loss=0.05041758343577385\n",
            "step 6236: generator_loss=3.13692569732666, discriminator_loss=0.04952572286128998\n",
            "step 6237: generator_loss=3.155855655670166, discriminator_loss=0.04916946962475777\n",
            "step 6238: generator_loss=3.17175030708313, discriminator_loss=0.04889906197786331\n",
            "step 6239: generator_loss=3.1999716758728027, discriminator_loss=0.0483967587351799\n",
            "step 6240: generator_loss=3.2090492248535156, discriminator_loss=0.04813116043806076\n",
            "step 6241: generator_loss=3.2116057872772217, discriminator_loss=0.04805950075387955\n",
            "step 6242: generator_loss=3.1848435401916504, discriminator_loss=0.0485515296459198\n",
            "step 6243: generator_loss=3.1436305046081543, discriminator_loss=0.0493687205016613\n",
            "step 6244: generator_loss=3.114170789718628, discriminator_loss=0.04997856914997101\n",
            "step 6245: generator_loss=3.0870113372802734, discriminator_loss=0.050540775060653687\n",
            "step 6246: generator_loss=3.0622196197509766, discriminator_loss=0.05110005661845207\n",
            "step 6247: generator_loss=3.0392770767211914, discriminator_loss=0.051632191985845566\n",
            "step 6248: generator_loss=2.999971628189087, discriminator_loss=0.05256395787000656\n",
            "step 6249: generator_loss=2.9766459465026855, discriminator_loss=0.05320272594690323\n",
            "step 6250: generator_loss=3.0080058574676514, discriminator_loss=0.0524304136633873\n",
            "step 6251: generator_loss=3.067481517791748, discriminator_loss=0.05081314221024513\n",
            "step 6252: generator_loss=3.094905138015747, discriminator_loss=0.050150737166404724\n",
            "step 6253: generator_loss=3.114767074584961, discriminator_loss=0.0496751070022583\n",
            "step 6254: generator_loss=3.1381940841674805, discriminator_loss=0.049075596034526825\n",
            "step 6255: generator_loss=3.160623550415039, discriminator_loss=0.0485437735915184\n",
            "step 6256: generator_loss=3.179450035095215, discriminator_loss=0.048076748847961426\n",
            "step 6257: generator_loss=3.186150312423706, discriminator_loss=0.047847434878349304\n",
            "step 6258: generator_loss=3.173959255218506, discriminator_loss=0.04807685688138008\n",
            "step 6259: generator_loss=3.1537158489227295, discriminator_loss=0.04836387559771538\n",
            "step 6260: generator_loss=3.128876209259033, discriminator_loss=0.048876792192459106\n",
            "step 6261: generator_loss=3.1026628017425537, discriminator_loss=0.049319006502628326\n",
            "step 6262: generator_loss=3.0611867904663086, discriminator_loss=0.05025741457939148\n",
            "step 6263: generator_loss=3.0120773315429688, discriminator_loss=0.051417477428913116\n",
            "step 6264: generator_loss=2.968562126159668, discriminator_loss=0.052492283284664154\n",
            "step 6265: generator_loss=2.9574341773986816, discriminator_loss=0.052688293159008026\n",
            "step 6266: generator_loss=2.9525954723358154, discriminator_loss=0.052798181772232056\n",
            "step 6267: generator_loss=2.9860990047454834, discriminator_loss=0.051832713186740875\n",
            "step 6268: generator_loss=3.0428647994995117, discriminator_loss=0.05035964399576187\n",
            "step 6269: generator_loss=3.1307034492492676, discriminator_loss=0.048321403563022614\n",
            "step 6270: generator_loss=3.2249648571014404, discriminator_loss=0.046243637800216675\n",
            "step 6271: generator_loss=3.293304204940796, discriminator_loss=0.04490663856267929\n",
            "step 6272: generator_loss=3.343402147293091, discriminator_loss=0.04397488385438919\n",
            "step 6273: generator_loss=3.3470869064331055, discriminator_loss=0.04381433501839638\n",
            "step 6274: generator_loss=3.3127052783966064, discriminator_loss=0.04435465484857559\n",
            "step 6275: generator_loss=3.2460131645202637, discriminator_loss=0.045546017587184906\n",
            "step 6276: generator_loss=3.1706044673919678, discriminator_loss=0.04702076315879822\n",
            "step 6277: generator_loss=3.1137232780456543, discriminator_loss=0.04819905012845993\n",
            "step 6278: generator_loss=3.0898170471191406, discriminator_loss=0.04871004819869995\n",
            "step 6279: generator_loss=3.0890204906463623, discriminator_loss=0.04865102097392082\n",
            "step 6280: generator_loss=3.1186158657073975, discriminator_loss=0.047937724739313126\n",
            "step 6281: generator_loss=3.1446428298950195, discriminator_loss=0.04725498706102371\n",
            "step 6282: generator_loss=3.155806541442871, discriminator_loss=0.0469292476773262\n",
            "step 6283: generator_loss=3.1747822761535645, discriminator_loss=0.04645410180091858\n",
            "step 6284: generator_loss=3.170531749725342, discriminator_loss=0.046527717262506485\n",
            "step 6285: generator_loss=3.1440155506134033, discriminator_loss=0.04704665392637253\n",
            "step 6286: generator_loss=3.1185452938079834, discriminator_loss=0.04755458980798721\n",
            "step 6287: generator_loss=3.0864100456237793, discriminator_loss=0.04824741929769516\n",
            "step 6288: generator_loss=3.062175989151001, discriminator_loss=0.04886389151215553\n",
            "step 6289: generator_loss=3.075110912322998, discriminator_loss=0.04855300113558769\n",
            "step 6290: generator_loss=3.09796142578125, discriminator_loss=0.04799536243081093\n",
            "step 6291: generator_loss=3.1295881271362305, discriminator_loss=0.04732006788253784\n",
            "step 6292: generator_loss=3.1417171955108643, discriminator_loss=0.047046199440956116\n",
            "step 6293: generator_loss=3.1758487224578857, discriminator_loss=0.046290285885334015\n",
            "step 6294: generator_loss=3.2310009002685547, discriminator_loss=0.045169323682785034\n",
            "step 6295: generator_loss=3.2764313220977783, discriminator_loss=0.04420686513185501\n",
            "step 6296: generator_loss=3.290097951889038, discriminator_loss=0.04388152062892914\n",
            "step 6297: generator_loss=3.269979953765869, discriminator_loss=0.0442686527967453\n",
            "step 6298: generator_loss=3.2649824619293213, discriminator_loss=0.04427270591259003\n",
            "step 6299: generator_loss=3.2389607429504395, discriminator_loss=0.044653505086898804\n",
            "step 6300: generator_loss=3.2223408222198486, discriminator_loss=0.044950250536203384\n",
            "step 6301: generator_loss=3.2045578956604004, discriminator_loss=0.04519336670637131\n",
            "step 6302: generator_loss=3.186060667037964, discriminator_loss=0.04553428292274475\n",
            "step 6303: generator_loss=3.1605477333068848, discriminator_loss=0.04608016833662987\n",
            "step 6304: generator_loss=3.1372742652893066, discriminator_loss=0.04649621248245239\n",
            "step 6305: generator_loss=3.1126370429992676, discriminator_loss=0.04706651344895363\n",
            "step 6306: generator_loss=3.091352701187134, discriminator_loss=0.04753740131855011\n",
            "step 6307: generator_loss=3.089205265045166, discriminator_loss=0.0475173220038414\n",
            "step 6308: generator_loss=3.120239734649658, discriminator_loss=0.04675711318850517\n",
            "step 6309: generator_loss=3.138550281524658, discriminator_loss=0.04635758325457573\n",
            "step 6310: generator_loss=3.1228256225585938, discriminator_loss=0.046657662838697433\n",
            "step 6311: generator_loss=3.0764920711517334, discriminator_loss=0.04778016358613968\n",
            "step 6312: generator_loss=3.0256705284118652, discriminator_loss=0.04905334860086441\n",
            "step 6313: generator_loss=2.993457794189453, discriminator_loss=0.049959536641836166\n",
            "step 6314: generator_loss=2.9850213527679443, discriminator_loss=0.050249263644218445\n",
            "step 6315: generator_loss=3.032459020614624, discriminator_loss=0.04915338009595871\n",
            "step 6316: generator_loss=3.1046674251556396, discriminator_loss=0.04752510040998459\n",
            "step 6317: generator_loss=3.1625757217407227, discriminator_loss=0.0463307648897171\n",
            "step 6318: generator_loss=3.2132840156555176, discriminator_loss=0.04534091055393219\n",
            "step 6319: generator_loss=3.2633068561553955, discriminator_loss=0.04433710128068924\n",
            "step 6320: generator_loss=3.30129337310791, discriminator_loss=0.043592825531959534\n",
            "step 6321: generator_loss=3.3237409591674805, discriminator_loss=0.04317847266793251\n",
            "step 6322: generator_loss=3.3371894359588623, discriminator_loss=0.04286215826869011\n",
            "step 6323: generator_loss=3.3606510162353516, discriminator_loss=0.0423603355884552\n",
            "step 6324: generator_loss=3.3695881366729736, discriminator_loss=0.04211553931236267\n",
            "step 6325: generator_loss=3.337209463119507, discriminator_loss=0.04259957745671272\n",
            "step 6326: generator_loss=3.3293280601501465, discriminator_loss=0.04264169558882713\n",
            "step 6327: generator_loss=3.323535919189453, discriminator_loss=0.04263019561767578\n",
            "step 6328: generator_loss=3.323988437652588, discriminator_loss=0.042476482689380646\n",
            "step 6329: generator_loss=3.328049898147583, discriminator_loss=0.04234762489795685\n",
            "step 6330: generator_loss=3.315809726715088, discriminator_loss=0.042354099452495575\n",
            "step 6331: generator_loss=3.292907238006592, discriminator_loss=0.042741499841213226\n",
            "step 6332: generator_loss=3.3255770206451416, discriminator_loss=0.04198185354471207\n",
            "step 6333: generator_loss=3.3841240406036377, discriminator_loss=0.040832407772541046\n",
            "step 6334: generator_loss=3.4213082790374756, discriminator_loss=0.04006664454936981\n",
            "step 6335: generator_loss=3.42353892326355, discriminator_loss=0.03992478549480438\n",
            "step 6336: generator_loss=3.427602767944336, discriminator_loss=0.03974105417728424\n",
            "step 6337: generator_loss=3.3825559616088867, discriminator_loss=0.0403972864151001\n",
            "step 6338: generator_loss=3.353205919265747, discriminator_loss=0.04080002009868622\n",
            "step 6339: generator_loss=3.3400766849517822, discriminator_loss=0.04101138561964035\n",
            "step 6340: generator_loss=3.3179078102111816, discriminator_loss=0.041282691061496735\n",
            "step 6341: generator_loss=3.2756664752960205, discriminator_loss=0.04196404293179512\n",
            "step 6342: generator_loss=3.202514410018921, discriminator_loss=0.04338456317782402\n",
            "step 6343: generator_loss=3.1440699100494385, discriminator_loss=0.04458145052194595\n",
            "step 6344: generator_loss=3.1123602390289307, discriminator_loss=0.045264117419719696\n",
            "step 6345: generator_loss=3.076188325881958, discriminator_loss=0.04613765701651573\n",
            "step 6346: generator_loss=3.05971097946167, discriminator_loss=0.04654400050640106\n",
            "step 6347: generator_loss=3.052656412124634, discriminator_loss=0.046794138848781586\n",
            "step 6348: generator_loss=3.0781476497650146, discriminator_loss=0.046276628971099854\n",
            "step 6349: generator_loss=3.0933022499084473, discriminator_loss=0.04604370519518852\n",
            "step 6350: generator_loss=3.1016743183135986, discriminator_loss=0.04609496146440506\n",
            "step 6351: generator_loss=3.107963800430298, discriminator_loss=0.04610953852534294\n",
            "step 6352: generator_loss=3.151268482208252, discriminator_loss=0.045309387147426605\n",
            "step 6353: generator_loss=3.1814239025115967, discriminator_loss=0.04484635964035988\n",
            "step 6354: generator_loss=3.176137924194336, discriminator_loss=0.045131463557481766\n",
            "step 6355: generator_loss=3.1486623287200928, discriminator_loss=0.045830339193344116\n",
            "step 6356: generator_loss=3.1213090419769287, discriminator_loss=0.04653673619031906\n",
            "step 6357: generator_loss=3.0804555416107178, discriminator_loss=0.04766325280070305\n",
            "step 6358: generator_loss=3.0342013835906982, discriminator_loss=0.04883185029029846\n",
            "step 6359: generator_loss=2.9603922367095947, discriminator_loss=0.05090637505054474\n",
            "step 6360: generator_loss=2.8846590518951416, discriminator_loss=0.053091563284397125\n",
            "step 6361: generator_loss=2.84970760345459, discriminator_loss=0.054303355515003204\n",
            "step 6362: generator_loss=2.8581628799438477, discriminator_loss=0.05413760617375374\n",
            "step 6363: generator_loss=2.9103965759277344, discriminator_loss=0.05282231420278549\n",
            "step 6364: generator_loss=2.991011619567871, discriminator_loss=0.05080171674489975\n",
            "step 6365: generator_loss=3.0820398330688477, discriminator_loss=0.04861724376678467\n",
            "step 6366: generator_loss=3.146087646484375, discriminator_loss=0.04727058857679367\n",
            "step 6367: generator_loss=3.1697781085968018, discriminator_loss=0.0467415526509285\n",
            "step 6368: generator_loss=3.152886390686035, discriminator_loss=0.04716435447335243\n",
            "step 6369: generator_loss=3.110109567642212, discriminator_loss=0.04815034940838814\n",
            "step 6370: generator_loss=3.0420725345611572, discriminator_loss=0.04980523884296417\n",
            "step 6371: generator_loss=2.9508724212646484, discriminator_loss=0.052157819271087646\n",
            "step 6372: generator_loss=2.843156099319458, discriminator_loss=0.05531754344701767\n",
            "step 6373: generator_loss=2.7955727577209473, discriminator_loss=0.05695848912000656\n",
            "step 6374: generator_loss=2.8232951164245605, discriminator_loss=0.05613593012094498\n",
            "step 6375: generator_loss=2.9396860599517822, discriminator_loss=0.052826203405857086\n",
            "step 6376: generator_loss=3.051622152328491, discriminator_loss=0.04999289661645889\n",
            "step 6377: generator_loss=3.1360907554626465, discriminator_loss=0.04824324697256088\n",
            "step 6378: generator_loss=3.2102787494659424, discriminator_loss=0.046741992235183716\n",
            "step 6379: generator_loss=3.238130569458008, discriminator_loss=0.04618335887789726\n",
            "step 6380: generator_loss=3.2146549224853516, discriminator_loss=0.04682859033346176\n",
            "step 6381: generator_loss=3.2077996730804443, discriminator_loss=0.04694876819849014\n",
            "step 6382: generator_loss=3.208932638168335, discriminator_loss=0.046834684908390045\n",
            "step 6383: generator_loss=3.2024574279785156, discriminator_loss=0.04691344499588013\n",
            "step 6384: generator_loss=3.2139463424682617, discriminator_loss=0.04658719524741173\n",
            "step 6385: generator_loss=3.197218418121338, discriminator_loss=0.04679485037922859\n",
            "step 6386: generator_loss=3.233081340789795, discriminator_loss=0.04594224691390991\n",
            "step 6387: generator_loss=3.2392749786376953, discriminator_loss=0.04552228003740311\n",
            "step 6388: generator_loss=3.2194061279296875, discriminator_loss=0.04583416506648064\n",
            "step 6389: generator_loss=3.199106216430664, discriminator_loss=0.04597035422921181\n",
            "step 6390: generator_loss=3.187049388885498, discriminator_loss=0.046037707477808\n",
            "step 6391: generator_loss=3.1612491607666016, discriminator_loss=0.04637761414051056\n",
            "step 6392: generator_loss=3.1408650875091553, discriminator_loss=0.04656539112329483\n",
            "step 6393: generator_loss=3.0935497283935547, discriminator_loss=0.04736218601465225\n",
            "step 6394: generator_loss=3.0560226440429688, discriminator_loss=0.04804304987192154\n",
            "step 6395: generator_loss=3.020611047744751, discriminator_loss=0.04871848225593567\n",
            "step 6396: generator_loss=2.990841865539551, discriminator_loss=0.04935218393802643\n",
            "step 6397: generator_loss=2.9702517986297607, discriminator_loss=0.04980005323886871\n",
            "step 6398: generator_loss=2.963845729827881, discriminator_loss=0.04988115280866623\n",
            "step 6399: generator_loss=2.981503486633301, discriminator_loss=0.04932725802063942\n",
            "step 6400: generator_loss=3.0030839443206787, discriminator_loss=0.048742737621068954\n",
            "step 6401: generator_loss=3.033802032470703, discriminator_loss=0.047960370779037476\n",
            "step 6402: generator_loss=3.0580954551696777, discriminator_loss=0.04735420644283295\n",
            "step 6403: generator_loss=3.0525317192077637, discriminator_loss=0.04750461131334305\n",
            "step 6404: generator_loss=3.021599531173706, discriminator_loss=0.04821236431598663\n",
            "step 6405: generator_loss=2.975389242172241, discriminator_loss=0.04945521056652069\n",
            "step 6406: generator_loss=2.9285857677459717, discriminator_loss=0.05077692121267319\n",
            "step 6407: generator_loss=2.918131113052368, discriminator_loss=0.051135189831256866\n",
            "step 6408: generator_loss=2.9499285221099854, discriminator_loss=0.05027318745851517\n",
            "step 6409: generator_loss=2.982328414916992, discriminator_loss=0.049531489610672\n",
            "step 6410: generator_loss=3.0147829055786133, discriminator_loss=0.04873838275671005\n",
            "step 6411: generator_loss=3.0810256004333496, discriminator_loss=0.047217581421136856\n",
            "step 6412: generator_loss=3.1387791633605957, discriminator_loss=0.04597211629152298\n",
            "step 6413: generator_loss=3.172833204269409, discriminator_loss=0.045239053666591644\n",
            "step 6414: generator_loss=3.183337450027466, discriminator_loss=0.045049116015434265\n",
            "step 6415: generator_loss=3.1983096599578857, discriminator_loss=0.04474951699376106\n",
            "step 6416: generator_loss=3.195352077484131, discriminator_loss=0.04481559991836548\n",
            "step 6417: generator_loss=3.1755692958831787, discriminator_loss=0.04523363709449768\n",
            "step 6418: generator_loss=3.1398766040802, discriminator_loss=0.04601678252220154\n",
            "step 6419: generator_loss=3.104274272918701, discriminator_loss=0.04672647640109062\n",
            "step 6420: generator_loss=3.118781328201294, discriminator_loss=0.046413905918598175\n",
            "step 6421: generator_loss=3.138073682785034, discriminator_loss=0.045966632664203644\n",
            "step 6422: generator_loss=3.1808109283447266, discriminator_loss=0.04495900869369507\n",
            "step 6423: generator_loss=3.19985294342041, discriminator_loss=0.04453887790441513\n",
            "step 6424: generator_loss=3.249655246734619, discriminator_loss=0.043449461460113525\n",
            "step 6425: generator_loss=3.3899898529052734, discriminator_loss=0.040781181305646896\n",
            "step 6426: generator_loss=3.580009698867798, discriminator_loss=0.03759881481528282\n",
            "step 6427: generator_loss=3.7625045776367188, discriminator_loss=0.03497987985610962\n",
            "step 6428: generator_loss=3.882157802581787, discriminator_loss=0.03325960040092468\n",
            "step 6429: generator_loss=3.952275276184082, discriminator_loss=0.03218016028404236\n",
            "step 6430: generator_loss=3.9658985137939453, discriminator_loss=0.03168802708387375\n",
            "step 6431: generator_loss=3.9281961917877197, discriminator_loss=0.03162850812077522\n",
            "step 6432: generator_loss=3.841543197631836, discriminator_loss=0.03200202062726021\n",
            "step 6433: generator_loss=3.735748291015625, discriminator_loss=0.03275216743350029\n",
            "step 6434: generator_loss=3.6142516136169434, discriminator_loss=0.03387469798326492\n",
            "step 6435: generator_loss=3.4703500270843506, discriminator_loss=0.03561878204345703\n",
            "step 6436: generator_loss=3.347153425216675, discriminator_loss=0.03737340867519379\n",
            "step 6437: generator_loss=3.259303331375122, discriminator_loss=0.038751110434532166\n",
            "step 6438: generator_loss=3.1939661502838135, discriminator_loss=0.03984285145998001\n",
            "step 6439: generator_loss=3.1422266960144043, discriminator_loss=0.04078877717256546\n",
            "step 6440: generator_loss=3.0868661403656006, discriminator_loss=0.04194962978363037\n",
            "step 6441: generator_loss=3.052008867263794, discriminator_loss=0.042792558670043945\n",
            "step 6442: generator_loss=3.0248868465423584, discriminator_loss=0.04341179504990578\n",
            "step 6443: generator_loss=3.0604450702667236, discriminator_loss=0.042542651295661926\n",
            "step 6444: generator_loss=3.090141534805298, discriminator_loss=0.04190819710493088\n",
            "step 6445: generator_loss=3.121873617172241, discriminator_loss=0.04127209633588791\n",
            "step 6446: generator_loss=3.1535427570343018, discriminator_loss=0.0406494066119194\n",
            "step 6447: generator_loss=3.1855578422546387, discriminator_loss=0.04005169868469238\n",
            "step 6448: generator_loss=3.216834306716919, discriminator_loss=0.039556220173835754\n",
            "step 6449: generator_loss=3.246473550796509, discriminator_loss=0.03906814754009247\n",
            "step 6450: generator_loss=3.2690677642822266, discriminator_loss=0.03876979276537895\n",
            "step 6451: generator_loss=3.2967369556427, discriminator_loss=0.0383911207318306\n",
            "step 6452: generator_loss=3.297255039215088, discriminator_loss=0.03849031776189804\n",
            "step 6453: generator_loss=3.275564670562744, discriminator_loss=0.03908590227365494\n",
            "step 6454: generator_loss=3.2548744678497314, discriminator_loss=0.0396571010351181\n",
            "step 6455: generator_loss=3.223823308944702, discriminator_loss=0.04043036699295044\n",
            "step 6456: generator_loss=3.1997175216674805, discriminator_loss=0.04108577221632004\n",
            "step 6457: generator_loss=3.178826332092285, discriminator_loss=0.04166081175208092\n",
            "step 6458: generator_loss=3.1538400650024414, discriminator_loss=0.04237271845340729\n",
            "step 6459: generator_loss=3.1336309909820557, discriminator_loss=0.042897142469882965\n",
            "step 6460: generator_loss=3.111936330795288, discriminator_loss=0.04351963847875595\n",
            "step 6461: generator_loss=3.089675188064575, discriminator_loss=0.0441473051905632\n",
            "step 6462: generator_loss=3.061858654022217, discriminator_loss=0.04494377225637436\n",
            "step 6463: generator_loss=3.053943634033203, discriminator_loss=0.04527314007282257\n",
            "step 6464: generator_loss=3.0663630962371826, discriminator_loss=0.045075200498104095\n",
            "step 6465: generator_loss=3.095768690109253, discriminator_loss=0.04454275965690613\n",
            "step 6466: generator_loss=3.108694076538086, discriminator_loss=0.04439016431570053\n",
            "step 6467: generator_loss=3.155048370361328, discriminator_loss=0.043501123785972595\n",
            "step 6468: generator_loss=3.214421272277832, discriminator_loss=0.04235843941569328\n",
            "step 6469: generator_loss=3.248321294784546, discriminator_loss=0.04184635728597641\n",
            "step 6470: generator_loss=3.2886970043182373, discriminator_loss=0.041115716099739075\n",
            "step 6471: generator_loss=3.3375375270843506, discriminator_loss=0.04032426327466965\n",
            "step 6472: generator_loss=3.4121201038360596, discriminator_loss=0.038994017988443375\n",
            "step 6473: generator_loss=3.4696671962738037, discriminator_loss=0.0380781814455986\n",
            "step 6474: generator_loss=3.493837356567383, discriminator_loss=0.037674639374017715\n",
            "step 6475: generator_loss=3.48457932472229, discriminator_loss=0.037695590406656265\n",
            "step 6476: generator_loss=3.454254627227783, discriminator_loss=0.03810177743434906\n",
            "step 6477: generator_loss=3.4091126918792725, discriminator_loss=0.038701701909303665\n",
            "step 6478: generator_loss=3.369100570678711, discriminator_loss=0.03923347592353821\n",
            "step 6479: generator_loss=3.3272037506103516, discriminator_loss=0.03983471542596817\n",
            "step 6480: generator_loss=3.2590572834014893, discriminator_loss=0.04095235466957092\n",
            "step 6481: generator_loss=3.1722683906555176, discriminator_loss=0.04261510819196701\n",
            "step 6482: generator_loss=3.151451826095581, discriminator_loss=0.04295436292886734\n",
            "step 6483: generator_loss=3.1353073120117188, discriminator_loss=0.04312381148338318\n",
            "step 6484: generator_loss=3.1332366466522217, discriminator_loss=0.04314398020505905\n",
            "step 6485: generator_loss=3.127563953399658, discriminator_loss=0.04323867708444595\n",
            "step 6486: generator_loss=3.1424551010131836, discriminator_loss=0.042924441397190094\n",
            "step 6487: generator_loss=3.166109085083008, discriminator_loss=0.0424041673541069\n",
            "step 6488: generator_loss=3.205997943878174, discriminator_loss=0.04156963527202606\n",
            "step 6489: generator_loss=3.251042127609253, discriminator_loss=0.040757473558187485\n",
            "step 6490: generator_loss=3.291329860687256, discriminator_loss=0.04006347060203552\n",
            "step 6491: generator_loss=3.3408684730529785, discriminator_loss=0.03921977058053017\n",
            "step 6492: generator_loss=3.369396448135376, discriminator_loss=0.03882509842514992\n",
            "step 6493: generator_loss=3.3905251026153564, discriminator_loss=0.03858380392193794\n",
            "step 6494: generator_loss=3.463869094848633, discriminator_loss=0.037423714995384216\n",
            "step 6495: generator_loss=3.530473470687866, discriminator_loss=0.03647802770137787\n",
            "step 6496: generator_loss=3.574054002761841, discriminator_loss=0.03583332151174545\n",
            "step 6497: generator_loss=3.6374473571777344, discriminator_loss=0.03494570404291153\n",
            "step 6498: generator_loss=3.6727638244628906, discriminator_loss=0.034386005252599716\n",
            "step 6499: generator_loss=3.6903493404388428, discriminator_loss=0.03406834974884987\n",
            "step 6500: generator_loss=3.6789190769195557, discriminator_loss=0.03410995379090309\n",
            "step 6501: generator_loss=3.6786203384399414, discriminator_loss=0.0338861346244812\n",
            "step 6502: generator_loss=3.66876220703125, discriminator_loss=0.03386111557483673\n",
            "step 6503: generator_loss=3.6480214595794678, discriminator_loss=0.03397008776664734\n",
            "step 6504: generator_loss=3.6102755069732666, discriminator_loss=0.03430435433983803\n",
            "step 6505: generator_loss=3.5584096908569336, discriminator_loss=0.03485260531306267\n",
            "step 6506: generator_loss=3.5138370990753174, discriminator_loss=0.0353441946208477\n",
            "step 6507: generator_loss=3.4701855182647705, discriminator_loss=0.03585493192076683\n",
            "step 6508: generator_loss=3.4242732524871826, discriminator_loss=0.036492131650447845\n",
            "step 6509: generator_loss=3.407697916030884, discriminator_loss=0.036648791283369064\n",
            "step 6510: generator_loss=3.3945937156677246, discriminator_loss=0.03675435483455658\n",
            "step 6511: generator_loss=3.358288526535034, discriminator_loss=0.03734375163912773\n",
            "step 6512: generator_loss=3.317621946334839, discriminator_loss=0.038007643073797226\n",
            "step 6513: generator_loss=3.2804179191589355, discriminator_loss=0.03871502727270126\n",
            "step 6514: generator_loss=3.2394847869873047, discriminator_loss=0.03954685479402542\n",
            "step 6515: generator_loss=3.2033421993255615, discriminator_loss=0.0403318889439106\n",
            "step 6516: generator_loss=3.1989645957946777, discriminator_loss=0.04049501568078995\n",
            "step 6517: generator_loss=3.234398603439331, discriminator_loss=0.03984491899609566\n",
            "step 6518: generator_loss=3.2926247119903564, discriminator_loss=0.038777969777584076\n",
            "step 6519: generator_loss=3.3717987537384033, discriminator_loss=0.03743329644203186\n",
            "step 6520: generator_loss=3.425520181655884, discriminator_loss=0.036664705723524094\n",
            "step 6521: generator_loss=3.4400227069854736, discriminator_loss=0.036534570157527924\n",
            "step 6522: generator_loss=3.424884080886841, discriminator_loss=0.03690217435359955\n",
            "step 6523: generator_loss=3.390195369720459, discriminator_loss=0.037624359130859375\n",
            "step 6524: generator_loss=3.3621768951416016, discriminator_loss=0.03825107961893082\n",
            "step 6525: generator_loss=3.3288097381591797, discriminator_loss=0.038943249732255936\n",
            "step 6526: generator_loss=3.3020803928375244, discriminator_loss=0.03956208378076553\n",
            "step 6527: generator_loss=3.2757439613342285, discriminator_loss=0.04020194709300995\n",
            "step 6528: generator_loss=3.255565881729126, discriminator_loss=0.040669701993465424\n",
            "step 6529: generator_loss=3.2304835319519043, discriminator_loss=0.04129631072282791\n",
            "step 6530: generator_loss=3.221557140350342, discriminator_loss=0.04162365570664406\n",
            "step 6531: generator_loss=3.199744462966919, discriminator_loss=0.04214458912611008\n",
            "step 6532: generator_loss=3.1521286964416504, discriminator_loss=0.04330424964427948\n",
            "step 6533: generator_loss=3.106532573699951, discriminator_loss=0.044368863105773926\n",
            "step 6534: generator_loss=3.0661792755126953, discriminator_loss=0.04544362425804138\n",
            "step 6535: generator_loss=3.034721612930298, discriminator_loss=0.04630811884999275\n",
            "step 6536: generator_loss=3.038069486618042, discriminator_loss=0.04638390243053436\n",
            "step 6537: generator_loss=3.0513901710510254, discriminator_loss=0.04619631543755531\n",
            "step 6538: generator_loss=3.0761964321136475, discriminator_loss=0.04569987580180168\n",
            "step 6539: generator_loss=3.0983526706695557, discriminator_loss=0.04529500752687454\n",
            "step 6540: generator_loss=3.128021240234375, discriminator_loss=0.04472677782177925\n",
            "step 6541: generator_loss=3.1495540142059326, discriminator_loss=0.044388458132743835\n",
            "step 6542: generator_loss=3.1978559494018555, discriminator_loss=0.043455250561237335\n",
            "step 6543: generator_loss=3.220069646835327, discriminator_loss=0.04301958903670311\n",
            "step 6544: generator_loss=3.2331252098083496, discriminator_loss=0.04286225512623787\n",
            "step 6545: generator_loss=3.2477152347564697, discriminator_loss=0.04262445494532585\n",
            "step 6546: generator_loss=3.2579095363616943, discriminator_loss=0.04238073527812958\n",
            "step 6547: generator_loss=3.23337984085083, discriminator_loss=0.042925745248794556\n",
            "step 6548: generator_loss=3.193319797515869, discriminator_loss=0.04373590648174286\n",
            "step 6549: generator_loss=3.1645283699035645, discriminator_loss=0.04437165707349777\n",
            "step 6550: generator_loss=3.1701953411102295, discriminator_loss=0.044228337705135345\n",
            "step 6551: generator_loss=3.221095323562622, discriminator_loss=0.04311131685972214\n",
            "step 6552: generator_loss=3.247375011444092, discriminator_loss=0.04256204888224602\n",
            "step 6553: generator_loss=3.236178398132324, discriminator_loss=0.04272693395614624\n",
            "step 6554: generator_loss=3.1990530490875244, discriminator_loss=0.043430183082818985\n",
            "step 6555: generator_loss=3.142573833465576, discriminator_loss=0.04457087442278862\n",
            "step 6556: generator_loss=3.1122419834136963, discriminator_loss=0.04517010971903801\n",
            "step 6557: generator_loss=3.111215114593506, discriminator_loss=0.045100778341293335\n",
            "step 6558: generator_loss=3.100961208343506, discriminator_loss=0.04531970992684364\n",
            "step 6559: generator_loss=3.1138432025909424, discriminator_loss=0.04494664818048477\n",
            "step 6560: generator_loss=3.1119918823242188, discriminator_loss=0.04488065838813782\n",
            "step 6561: generator_loss=3.088787078857422, discriminator_loss=0.04544655606150627\n",
            "step 6562: generator_loss=3.0726404190063477, discriminator_loss=0.04583505541086197\n",
            "step 6563: generator_loss=3.071816921234131, discriminator_loss=0.045942045748233795\n",
            "step 6564: generator_loss=3.0560200214385986, discriminator_loss=0.046500321477651596\n",
            "step 6565: generator_loss=3.085944890975952, discriminator_loss=0.04607417806982994\n",
            "step 6566: generator_loss=3.1477928161621094, discriminator_loss=0.04488971084356308\n",
            "step 6567: generator_loss=3.2605459690093994, discriminator_loss=0.04284704104065895\n",
            "step 6568: generator_loss=3.363326072692871, discriminator_loss=0.04118237644433975\n",
            "step 6569: generator_loss=3.4222447872161865, discriminator_loss=0.04033835977315903\n",
            "step 6570: generator_loss=3.433894395828247, discriminator_loss=0.04036172851920128\n",
            "step 6571: generator_loss=3.3990511894226074, discriminator_loss=0.041082963347435\n",
            "step 6572: generator_loss=3.3359463214874268, discriminator_loss=0.042226001620292664\n",
            "step 6573: generator_loss=3.2846922874450684, discriminator_loss=0.043137140572071075\n",
            "step 6574: generator_loss=3.248828411102295, discriminator_loss=0.043854568153619766\n",
            "step 6575: generator_loss=3.2689199447631836, discriminator_loss=0.043338436633348465\n",
            "step 6576: generator_loss=3.301025390625, discriminator_loss=0.04261361062526703\n",
            "step 6577: generator_loss=3.3135313987731934, discriminator_loss=0.04224175959825516\n",
            "step 6578: generator_loss=3.2749714851379395, discriminator_loss=0.04285735636949539\n",
            "step 6579: generator_loss=3.198086738586426, discriminator_loss=0.04429147392511368\n",
            "step 6580: generator_loss=3.134983539581299, discriminator_loss=0.04544122889637947\n",
            "step 6581: generator_loss=3.0777173042297363, discriminator_loss=0.046645309776067734\n",
            "step 6582: generator_loss=3.0572831630706787, discriminator_loss=0.04705433547496796\n",
            "step 6583: generator_loss=3.055905818939209, discriminator_loss=0.04697614908218384\n",
            "step 6584: generator_loss=3.048311710357666, discriminator_loss=0.04716676473617554\n",
            "step 6585: generator_loss=3.0496296882629395, discriminator_loss=0.047148946672677994\n",
            "step 6586: generator_loss=3.065016984939575, discriminator_loss=0.04682746157050133\n",
            "step 6587: generator_loss=3.123375177383423, discriminator_loss=0.045358896255493164\n",
            "step 6588: generator_loss=3.15887188911438, discriminator_loss=0.04455821216106415\n",
            "step 6589: generator_loss=3.1571133136749268, discriminator_loss=0.044609129428863525\n",
            "step 6590: generator_loss=3.1422390937805176, discriminator_loss=0.04497960954904556\n",
            "step 6591: generator_loss=3.13739275932312, discriminator_loss=0.044982828199863434\n",
            "step 6592: generator_loss=3.132092237472534, discriminator_loss=0.045125171542167664\n",
            "step 6593: generator_loss=3.112571954727173, discriminator_loss=0.045550763607025146\n",
            "step 6594: generator_loss=3.0808017253875732, discriminator_loss=0.04628799855709076\n",
            "step 6595: generator_loss=3.0431411266326904, discriminator_loss=0.047173455357551575\n",
            "step 6596: generator_loss=3.0232696533203125, discriminator_loss=0.047646939754486084\n",
            "step 6597: generator_loss=2.9954609870910645, discriminator_loss=0.048434238880872726\n",
            "step 6598: generator_loss=2.988032341003418, discriminator_loss=0.048652052879333496\n",
            "step 6599: generator_loss=2.9766879081726074, discriminator_loss=0.048975251615047455\n",
            "step 6600: generator_loss=2.9616308212280273, discriminator_loss=0.0494610071182251\n",
            "step 6601: generator_loss=2.9384944438934326, discriminator_loss=0.05019282549619675\n",
            "step 6602: generator_loss=2.9152841567993164, discriminator_loss=0.05097947269678116\n",
            "step 6603: generator_loss=2.8863706588745117, discriminator_loss=0.051965393126010895\n",
            "step 6604: generator_loss=2.843038320541382, discriminator_loss=0.053493548184633255\n",
            "step 6605: generator_loss=2.8092236518859863, discriminator_loss=0.05475849285721779\n",
            "step 6606: generator_loss=2.800140619277954, discriminator_loss=0.05539852753281593\n",
            "step 6607: generator_loss=2.832105875015259, discriminator_loss=0.054673001170158386\n",
            "step 6608: generator_loss=2.9099602699279785, discriminator_loss=0.05258415639400482\n",
            "step 6609: generator_loss=2.983077049255371, discriminator_loss=0.051013197749853134\n",
            "step 6610: generator_loss=3.018697500228882, discriminator_loss=0.05039890483021736\n",
            "step 6611: generator_loss=3.025113582611084, discriminator_loss=0.050661660730838776\n",
            "step 6612: generator_loss=3.0011396408081055, discriminator_loss=0.0516580194234848\n",
            "step 6613: generator_loss=2.976707935333252, discriminator_loss=0.05251602083444595\n",
            "step 6614: generator_loss=2.9323813915252686, discriminator_loss=0.054079361259937286\n",
            "step 6615: generator_loss=2.8785295486450195, discriminator_loss=0.055964235216379166\n",
            "step 6616: generator_loss=2.877643346786499, discriminator_loss=0.05625218525528908\n",
            "step 6617: generator_loss=2.8813912868499756, discriminator_loss=0.05643484368920326\n",
            "step 6618: generator_loss=2.8850560188293457, discriminator_loss=0.05657225102186203\n",
            "step 6619: generator_loss=2.9116368293762207, discriminator_loss=0.05614669248461723\n",
            "step 6620: generator_loss=2.931835412979126, discriminator_loss=0.055866681039333344\n",
            "step 6621: generator_loss=2.9378786087036133, discriminator_loss=0.05592436343431473\n",
            "step 6622: generator_loss=3.028628349304199, discriminator_loss=0.05375584959983826\n",
            "step 6623: generator_loss=3.1069061756134033, discriminator_loss=0.051943615078926086\n",
            "step 6624: generator_loss=3.167360305786133, discriminator_loss=0.05069679021835327\n",
            "step 6625: generator_loss=3.2081305980682373, discriminator_loss=0.04972369968891144\n",
            "step 6626: generator_loss=3.2263574600219727, discriminator_loss=0.049272067844867706\n",
            "step 6627: generator_loss=3.2170050144195557, discriminator_loss=0.049383409321308136\n",
            "step 6628: generator_loss=3.2055747509002686, discriminator_loss=0.04926355928182602\n",
            "step 6629: generator_loss=3.1685452461242676, discriminator_loss=0.049810171127319336\n",
            "step 6630: generator_loss=3.1136600971221924, discriminator_loss=0.050607047975063324\n",
            "step 6631: generator_loss=3.0972142219543457, discriminator_loss=0.050618976354599\n",
            "step 6632: generator_loss=3.09188175201416, discriminator_loss=0.0504167266190052\n",
            "step 6633: generator_loss=3.071475028991699, discriminator_loss=0.050427116453647614\n",
            "step 6634: generator_loss=3.062864065170288, discriminator_loss=0.05025579780340195\n",
            "step 6635: generator_loss=3.0415658950805664, discriminator_loss=0.05032932385802269\n",
            "step 6636: generator_loss=3.0103960037231445, discriminator_loss=0.0507209487259388\n",
            "step 6637: generator_loss=2.975639581680298, discriminator_loss=0.051251575350761414\n",
            "step 6638: generator_loss=2.9562020301818848, discriminator_loss=0.05147847160696983\n",
            "step 6639: generator_loss=3.001701831817627, discriminator_loss=0.050137363374233246\n",
            "step 6640: generator_loss=3.0329794883728027, discriminator_loss=0.049060821533203125\n",
            "step 6641: generator_loss=3.064152717590332, discriminator_loss=0.048181988298892975\n",
            "step 6642: generator_loss=3.098853826522827, discriminator_loss=0.04726698249578476\n",
            "step 6643: generator_loss=3.1837058067321777, discriminator_loss=0.04527101665735245\n",
            "step 6644: generator_loss=3.290090322494507, discriminator_loss=0.0429864227771759\n",
            "step 6645: generator_loss=3.4356493949890137, discriminator_loss=0.040269508957862854\n",
            "step 6646: generator_loss=3.5208234786987305, discriminator_loss=0.03865977004170418\n",
            "step 6647: generator_loss=3.524394989013672, discriminator_loss=0.038454800844192505\n",
            "step 6648: generator_loss=3.457590103149414, discriminator_loss=0.039249055087566376\n",
            "step 6649: generator_loss=3.3873395919799805, discriminator_loss=0.040126144886016846\n",
            "step 6650: generator_loss=3.3092994689941406, discriminator_loss=0.04123612120747566\n",
            "step 6651: generator_loss=3.2173800468444824, discriminator_loss=0.042789291590452194\n",
            "step 6652: generator_loss=3.109351634979248, discriminator_loss=0.044924233108758926\n",
            "step 6653: generator_loss=2.9877521991729736, discriminator_loss=0.04772487282752991\n",
            "step 6654: generator_loss=2.8760223388671875, discriminator_loss=0.0508112870156765\n",
            "step 6655: generator_loss=2.8789515495300293, discriminator_loss=0.050598181784152985\n",
            "step 6656: generator_loss=2.9022939205169678, discriminator_loss=0.04992055147886276\n",
            "step 6657: generator_loss=2.931230068206787, discriminator_loss=0.04919030889868736\n",
            "step 6658: generator_loss=2.964435577392578, discriminator_loss=0.04838991165161133\n",
            "step 6659: generator_loss=2.9836525917053223, discriminator_loss=0.04801175743341446\n",
            "step 6660: generator_loss=2.9868552684783936, discriminator_loss=0.048180337995290756\n",
            "step 6661: generator_loss=3.004323959350586, discriminator_loss=0.0479389950633049\n",
            "step 6662: generator_loss=3.0257375240325928, discriminator_loss=0.04764566197991371\n",
            "step 6663: generator_loss=3.0322492122650146, discriminator_loss=0.047724559903144836\n",
            "step 6664: generator_loss=3.039670705795288, discriminator_loss=0.04779505357146263\n",
            "step 6665: generator_loss=3.063671827316284, discriminator_loss=0.04745166748762131\n",
            "step 6666: generator_loss=3.070277214050293, discriminator_loss=0.04745284467935562\n",
            "step 6667: generator_loss=3.0880682468414307, discriminator_loss=0.04722229391336441\n",
            "step 6668: generator_loss=3.0942211151123047, discriminator_loss=0.04718080163002014\n",
            "step 6669: generator_loss=3.098324775695801, discriminator_loss=0.04717428609728813\n",
            "step 6670: generator_loss=3.128175973892212, discriminator_loss=0.046565018594264984\n",
            "step 6671: generator_loss=3.1611099243164062, discriminator_loss=0.045896682888269424\n",
            "step 6672: generator_loss=3.1800551414489746, discriminator_loss=0.04548836499452591\n",
            "step 6673: generator_loss=3.1956679821014404, discriminator_loss=0.04499180614948273\n",
            "step 6674: generator_loss=3.1917402744293213, discriminator_loss=0.04499456286430359\n",
            "step 6675: generator_loss=3.169182538986206, discriminator_loss=0.04536973312497139\n",
            "step 6676: generator_loss=3.140195846557617, discriminator_loss=0.0458756722509861\n",
            "step 6677: generator_loss=3.1106560230255127, discriminator_loss=0.04640652984380722\n",
            "step 6678: generator_loss=3.098081588745117, discriminator_loss=0.046564191579818726\n",
            "step 6679: generator_loss=3.0927748680114746, discriminator_loss=0.04655618220567703\n",
            "step 6680: generator_loss=3.0871782302856445, discriminator_loss=0.04663246124982834\n",
            "step 6681: generator_loss=3.0916130542755127, discriminator_loss=0.04651154577732086\n",
            "step 6682: generator_loss=3.1064984798431396, discriminator_loss=0.04622162878513336\n",
            "step 6683: generator_loss=3.1269285678863525, discriminator_loss=0.04571092128753662\n",
            "step 6684: generator_loss=3.124763011932373, discriminator_loss=0.04575611650943756\n",
            "step 6685: generator_loss=3.098123073577881, discriminator_loss=0.04637271910905838\n",
            "step 6686: generator_loss=3.0775535106658936, discriminator_loss=0.04682314395904541\n",
            "step 6687: generator_loss=3.0466346740722656, discriminator_loss=0.047604650259017944\n",
            "step 6688: generator_loss=3.0210790634155273, discriminator_loss=0.048280615359544754\n",
            "step 6689: generator_loss=3.009370803833008, discriminator_loss=0.04857613146305084\n",
            "step 6690: generator_loss=3.0554065704345703, discriminator_loss=0.04749452322721481\n",
            "step 6691: generator_loss=3.125113010406494, discriminator_loss=0.04598720371723175\n",
            "step 6692: generator_loss=3.1730167865753174, discriminator_loss=0.04503179341554642\n",
            "step 6693: generator_loss=3.254307508468628, discriminator_loss=0.04344024509191513\n",
            "step 6694: generator_loss=3.3488714694976807, discriminator_loss=0.04174673929810524\n",
            "step 6695: generator_loss=3.4047155380249023, discriminator_loss=0.04077370837330818\n",
            "step 6696: generator_loss=3.4352192878723145, discriminator_loss=0.0402350090444088\n",
            "step 6697: generator_loss=3.4581565856933594, discriminator_loss=0.03981398418545723\n",
            "step 6698: generator_loss=3.5043487548828125, discriminator_loss=0.03898586332798004\n",
            "step 6699: generator_loss=3.527355670928955, discriminator_loss=0.03845647722482681\n",
            "step 6700: generator_loss=3.50966215133667, discriminator_loss=0.03857078030705452\n",
            "step 6701: generator_loss=3.4589571952819824, discriminator_loss=0.03918563202023506\n",
            "step 6702: generator_loss=3.3812103271484375, discriminator_loss=0.04023619741201401\n",
            "step 6703: generator_loss=3.2936129570007324, discriminator_loss=0.041632477194070816\n",
            "step 6704: generator_loss=3.226961135864258, discriminator_loss=0.04276750236749649\n",
            "step 6705: generator_loss=3.1830437183380127, discriminator_loss=0.04348091036081314\n",
            "step 6706: generator_loss=3.1520562171936035, discriminator_loss=0.04397664591670036\n",
            "step 6707: generator_loss=3.1383490562438965, discriminator_loss=0.04417144134640694\n",
            "step 6708: generator_loss=3.13301420211792, discriminator_loss=0.04416372627019882\n",
            "step 6709: generator_loss=3.1065874099731445, discriminator_loss=0.04473719000816345\n",
            "step 6710: generator_loss=3.054992437362671, discriminator_loss=0.0459262989461422\n",
            "step 6711: generator_loss=2.998579978942871, discriminator_loss=0.04738437756896019\n",
            "step 6712: generator_loss=2.959132671356201, discriminator_loss=0.048429690301418304\n",
            "step 6713: generator_loss=2.934786796569824, discriminator_loss=0.04923024773597717\n",
            "step 6714: generator_loss=2.93046498298645, discriminator_loss=0.04951045662164688\n",
            "step 6715: generator_loss=2.9414429664611816, discriminator_loss=0.04944123700261116\n",
            "step 6716: generator_loss=2.9428958892822266, discriminator_loss=0.049603838473558426\n",
            "step 6717: generator_loss=2.984830141067505, discriminator_loss=0.04878305271267891\n",
            "step 6718: generator_loss=3.0733487606048584, discriminator_loss=0.04687872901558876\n",
            "step 6719: generator_loss=3.1789302825927734, discriminator_loss=0.0447244718670845\n",
            "step 6720: generator_loss=3.268362522125244, discriminator_loss=0.04314927011728287\n",
            "step 6721: generator_loss=3.40842342376709, discriminator_loss=0.04082748293876648\n",
            "step 6722: generator_loss=3.5470921993255615, discriminator_loss=0.0387607142329216\n",
            "step 6723: generator_loss=3.629423141479492, discriminator_loss=0.03763524070382118\n",
            "step 6724: generator_loss=3.669158697128296, discriminator_loss=0.0370984822511673\n",
            "step 6725: generator_loss=3.6705965995788574, discriminator_loss=0.0368943028151989\n",
            "step 6726: generator_loss=3.6367533206939697, discriminator_loss=0.03703436255455017\n",
            "step 6727: generator_loss=3.5547285079956055, discriminator_loss=0.037899669259786606\n",
            "step 6728: generator_loss=3.4945390224456787, discriminator_loss=0.03846306353807449\n",
            "step 6729: generator_loss=3.440857172012329, discriminator_loss=0.03891729936003685\n",
            "step 6730: generator_loss=3.375176191329956, discriminator_loss=0.039616912603378296\n",
            "step 6731: generator_loss=3.310281753540039, discriminator_loss=0.040457405149936676\n",
            "step 6732: generator_loss=3.2678797245025635, discriminator_loss=0.04084493964910507\n",
            "step 6733: generator_loss=3.237363338470459, discriminator_loss=0.041149694472551346\n",
            "step 6734: generator_loss=3.202208995819092, discriminator_loss=0.04157911241054535\n",
            "step 6735: generator_loss=3.1638119220733643, discriminator_loss=0.042199186980724335\n",
            "step 6736: generator_loss=3.107607126235962, discriminator_loss=0.04323413968086243\n",
            "step 6737: generator_loss=3.0190372467041016, discriminator_loss=0.04532080888748169\n",
            "step 6738: generator_loss=2.976705312728882, discriminator_loss=0.04633765667676926\n",
            "step 6739: generator_loss=2.955456256866455, discriminator_loss=0.046990323811769485\n",
            "step 6740: generator_loss=2.934926986694336, discriminator_loss=0.04761993885040283\n",
            "step 6741: generator_loss=2.914788007736206, discriminator_loss=0.04837717488408089\n",
            "step 6742: generator_loss=2.9039254188537598, discriminator_loss=0.04892931133508682\n",
            "step 6743: generator_loss=2.922759532928467, discriminator_loss=0.04866958409547806\n",
            "step 6744: generator_loss=2.9571399688720703, discriminator_loss=0.048080191016197205\n",
            "step 6745: generator_loss=3.017285108566284, discriminator_loss=0.046897996217012405\n",
            "step 6746: generator_loss=3.1129133701324463, discriminator_loss=0.044914647936820984\n",
            "step 6747: generator_loss=3.186922311782837, discriminator_loss=0.043651923537254333\n",
            "step 6748: generator_loss=3.210470676422119, discriminator_loss=0.0434734933078289\n",
            "step 6749: generator_loss=3.1988162994384766, discriminator_loss=0.044083572924137115\n",
            "step 6750: generator_loss=3.173186779022217, discriminator_loss=0.0448165200650692\n",
            "step 6751: generator_loss=3.1541390419006348, discriminator_loss=0.045464009046554565\n",
            "step 6752: generator_loss=3.1086883544921875, discriminator_loss=0.046630121767520905\n",
            "step 6753: generator_loss=3.0554327964782715, discriminator_loss=0.04812629148364067\n",
            "step 6754: generator_loss=3.0606472492218018, discriminator_loss=0.04811975359916687\n",
            "step 6755: generator_loss=3.08203125, discriminator_loss=0.04771912842988968\n",
            "step 6756: generator_loss=3.0899033546447754, discriminator_loss=0.0475417822599411\n",
            "step 6757: generator_loss=3.102137565612793, discriminator_loss=0.04730873554944992\n",
            "step 6758: generator_loss=3.099006175994873, discriminator_loss=0.04729678854346275\n",
            "step 6759: generator_loss=3.0950474739074707, discriminator_loss=0.04735387861728668\n",
            "step 6760: generator_loss=3.096392869949341, discriminator_loss=0.047250404953956604\n",
            "step 6761: generator_loss=3.089721202850342, discriminator_loss=0.047418877482414246\n",
            "step 6762: generator_loss=3.094620943069458, discriminator_loss=0.04724861681461334\n",
            "step 6763: generator_loss=3.078871488571167, discriminator_loss=0.04754126816987991\n",
            "step 6764: generator_loss=3.063988208770752, discriminator_loss=0.047904834151268005\n",
            "step 6765: generator_loss=3.0335822105407715, discriminator_loss=0.048607997596263885\n",
            "step 6766: generator_loss=3.0008299350738525, discriminator_loss=0.04948808252811432\n",
            "step 6767: generator_loss=2.9671008586883545, discriminator_loss=0.05040194094181061\n",
            "step 6768: generator_loss=2.9790761470794678, discriminator_loss=0.05006047710776329\n",
            "step 6769: generator_loss=3.0515449047088623, discriminator_loss=0.04834635555744171\n",
            "step 6770: generator_loss=3.1584296226501465, discriminator_loss=0.04596319794654846\n",
            "step 6771: generator_loss=3.242281436920166, discriminator_loss=0.04428718984127045\n",
            "step 6772: generator_loss=3.296518325805664, discriminator_loss=0.04327298328280449\n",
            "step 6773: generator_loss=3.2993524074554443, discriminator_loss=0.043272797018289566\n",
            "step 6774: generator_loss=3.264648199081421, discriminator_loss=0.04397737607359886\n",
            "step 6775: generator_loss=3.2088418006896973, discriminator_loss=0.04519842565059662\n",
            "step 6776: generator_loss=3.1780946254730225, discriminator_loss=0.045969657599925995\n",
            "step 6777: generator_loss=3.193181276321411, discriminator_loss=0.045666251331567764\n",
            "step 6778: generator_loss=3.1941490173339844, discriminator_loss=0.045602671802043915\n",
            "step 6779: generator_loss=3.186359167098999, discriminator_loss=0.04565957188606262\n",
            "step 6780: generator_loss=3.1612534523010254, discriminator_loss=0.046097464859485626\n",
            "step 6781: generator_loss=3.1047539710998535, discriminator_loss=0.04732086509466171\n",
            "step 6782: generator_loss=3.017728328704834, discriminator_loss=0.049311984330415726\n",
            "step 6783: generator_loss=2.9273290634155273, discriminator_loss=0.0516694001853466\n",
            "step 6784: generator_loss=2.9008781909942627, discriminator_loss=0.05240911245346069\n",
            "step 6785: generator_loss=2.9418210983276367, discriminator_loss=0.0513266995549202\n",
            "step 6786: generator_loss=2.9917845726013184, discriminator_loss=0.05013694614171982\n",
            "step 6787: generator_loss=3.0292186737060547, discriminator_loss=0.04908714443445206\n",
            "step 6788: generator_loss=3.085655450820923, discriminator_loss=0.047788914293050766\n",
            "step 6789: generator_loss=3.158979892730713, discriminator_loss=0.04618459194898605\n",
            "step 6790: generator_loss=3.202761650085449, discriminator_loss=0.045371346175670624\n",
            "step 6791: generator_loss=3.217090606689453, discriminator_loss=0.045189790427684784\n",
            "step 6792: generator_loss=3.2087161540985107, discriminator_loss=0.04537561535835266\n",
            "step 6793: generator_loss=3.1741766929626465, discriminator_loss=0.04616571217775345\n",
            "step 6794: generator_loss=3.1991450786590576, discriminator_loss=0.04555920511484146\n",
            "step 6795: generator_loss=3.2288994789123535, discriminator_loss=0.04489761218428612\n",
            "step 6796: generator_loss=3.2567243576049805, discriminator_loss=0.04432075470685959\n",
            "step 6797: generator_loss=3.2550008296966553, discriminator_loss=0.044194143265485764\n",
            "step 6798: generator_loss=3.232907772064209, discriminator_loss=0.04452398791909218\n",
            "step 6799: generator_loss=3.216322660446167, discriminator_loss=0.04484066739678383\n",
            "step 6800: generator_loss=3.2210545539855957, discriminator_loss=0.044494420289993286\n",
            "step 6801: generator_loss=3.2654025554656982, discriminator_loss=0.04346824437379837\n",
            "step 6802: generator_loss=3.333650588989258, discriminator_loss=0.042053721845149994\n",
            "step 6803: generator_loss=3.3711130619049072, discriminator_loss=0.041149966418743134\n",
            "step 6804: generator_loss=3.4066402912139893, discriminator_loss=0.040293559432029724\n",
            "step 6805: generator_loss=3.4191620349884033, discriminator_loss=0.039756499230861664\n",
            "step 6806: generator_loss=3.388803482055664, discriminator_loss=0.03989758342504501\n",
            "step 6807: generator_loss=3.3098983764648438, discriminator_loss=0.04099707305431366\n",
            "step 6808: generator_loss=3.209840774536133, discriminator_loss=0.04264132305979729\n",
            "step 6809: generator_loss=3.112150192260742, discriminator_loss=0.04449724778532982\n",
            "step 6810: generator_loss=3.0514111518859863, discriminator_loss=0.04567231982946396\n",
            "step 6811: generator_loss=3.0037622451782227, discriminator_loss=0.04659070074558258\n",
            "step 6812: generator_loss=2.9973137378692627, discriminator_loss=0.046657003462314606\n",
            "step 6813: generator_loss=3.021120309829712, discriminator_loss=0.04588029161095619\n",
            "step 6814: generator_loss=3.0414791107177734, discriminator_loss=0.04532238468527794\n",
            "step 6815: generator_loss=3.062661647796631, discriminator_loss=0.04471069574356079\n",
            "step 6816: generator_loss=3.083491563796997, discriminator_loss=0.044276878237724304\n",
            "step 6817: generator_loss=3.117072105407715, discriminator_loss=0.04353701323270798\n",
            "step 6818: generator_loss=3.1392993927001953, discriminator_loss=0.04312938079237938\n",
            "step 6819: generator_loss=3.145949363708496, discriminator_loss=0.04309995472431183\n",
            "step 6820: generator_loss=3.1389236450195312, discriminator_loss=0.043333739042282104\n",
            "step 6821: generator_loss=3.158189058303833, discriminator_loss=0.04303310438990593\n",
            "step 6822: generator_loss=3.195911407470703, discriminator_loss=0.042314473539590836\n",
            "step 6823: generator_loss=3.23374080657959, discriminator_loss=0.04168868064880371\n",
            "step 6824: generator_loss=3.2489495277404785, discriminator_loss=0.041412726044654846\n",
            "step 6825: generator_loss=3.2326574325561523, discriminator_loss=0.04183278977870941\n",
            "step 6826: generator_loss=3.19111967086792, discriminator_loss=0.04281223565340042\n",
            "step 6827: generator_loss=3.1279256343841553, discriminator_loss=0.044212549924850464\n",
            "step 6828: generator_loss=3.0430147647857666, discriminator_loss=0.04630791395902634\n",
            "step 6829: generator_loss=2.970365285873413, discriminator_loss=0.04827126860618591\n",
            "step 6830: generator_loss=2.954066753387451, discriminator_loss=0.04881243780255318\n",
            "step 6831: generator_loss=2.9639244079589844, discriminator_loss=0.04856356233358383\n",
            "step 6832: generator_loss=2.971127510070801, discriminator_loss=0.04848720133304596\n",
            "step 6833: generator_loss=2.973111867904663, discriminator_loss=0.048479724675416946\n",
            "step 6834: generator_loss=2.954132080078125, discriminator_loss=0.04911559075117111\n",
            "step 6835: generator_loss=2.95981502532959, discriminator_loss=0.0490943118929863\n",
            "step 6836: generator_loss=2.9715044498443604, discriminator_loss=0.04895057529211044\n",
            "step 6837: generator_loss=2.9900033473968506, discriminator_loss=0.048631396144628525\n",
            "step 6838: generator_loss=3.0173628330230713, discriminator_loss=0.048079751431941986\n",
            "step 6839: generator_loss=3.027003288269043, discriminator_loss=0.0481252484023571\n",
            "step 6840: generator_loss=3.033723831176758, discriminator_loss=0.048189520835876465\n",
            "step 6841: generator_loss=3.022463083267212, discriminator_loss=0.048678863793611526\n",
            "step 6842: generator_loss=3.005751609802246, discriminator_loss=0.04929320514202118\n",
            "step 6843: generator_loss=2.991511821746826, discriminator_loss=0.05018392950296402\n",
            "step 6844: generator_loss=2.967771530151367, discriminator_loss=0.05096162110567093\n",
            "step 6845: generator_loss=2.9750404357910156, discriminator_loss=0.05103008449077606\n",
            "step 6846: generator_loss=3.013205051422119, discriminator_loss=0.050293371081352234\n",
            "step 6847: generator_loss=3.0346596240997314, discriminator_loss=0.04996763914823532\n",
            "step 6848: generator_loss=3.032707691192627, discriminator_loss=0.05004662647843361\n",
            "step 6849: generator_loss=3.035104751586914, discriminator_loss=0.05023379623889923\n",
            "step 6850: generator_loss=3.030057430267334, discriminator_loss=0.050658077001571655\n",
            "step 6851: generator_loss=2.9976534843444824, discriminator_loss=0.05136030539870262\n",
            "step 6852: generator_loss=2.9769558906555176, discriminator_loss=0.05200831964612007\n",
            "step 6853: generator_loss=3.003567695617676, discriminator_loss=0.05134865269064903\n",
            "step 6854: generator_loss=3.0333828926086426, discriminator_loss=0.0504741445183754\n",
            "step 6855: generator_loss=3.067355155944824, discriminator_loss=0.04959254711866379\n",
            "step 6856: generator_loss=3.091386556625366, discriminator_loss=0.04898257926106453\n",
            "step 6857: generator_loss=3.1422173976898193, discriminator_loss=0.04766669124364853\n",
            "step 6858: generator_loss=3.1661953926086426, discriminator_loss=0.04701028764247894\n",
            "step 6859: generator_loss=3.1986043453216553, discriminator_loss=0.04614592343568802\n",
            "step 6860: generator_loss=3.2900309562683105, discriminator_loss=0.04411295801401138\n",
            "step 6861: generator_loss=3.3864169120788574, discriminator_loss=0.04209044948220253\n",
            "step 6862: generator_loss=3.4353857040405273, discriminator_loss=0.041096486151218414\n",
            "step 6863: generator_loss=3.4454426765441895, discriminator_loss=0.04109936207532883\n",
            "step 6864: generator_loss=3.4340410232543945, discriminator_loss=0.04252813011407852\n",
            "step 6865: generator_loss=3.4525058269500732, discriminator_loss=0.04179820418357849\n",
            "step 6866: generator_loss=3.4005446434020996, discriminator_loss=0.047134071588516235\n",
            "step 6867: generator_loss=3.179468870162964, discriminator_loss=0.07043005526065826\n",
            "step 6868: generator_loss=3.327263593673706, discriminator_loss=0.0605497881770134\n",
            "step 6869: generator_loss=3.2367358207702637, discriminator_loss=0.08365419507026672\n",
            "step 6870: generator_loss=3.3968799114227295, discriminator_loss=0.09429466724395752\n",
            "step 6871: generator_loss=3.5369362831115723, discriminator_loss=0.08707554638385773\n",
            "step 6872: generator_loss=3.4879350662231445, discriminator_loss=0.11609718203544617\n",
            "step 6873: generator_loss=3.7160587310791016, discriminator_loss=0.10657413303852081\n",
            "step 6874: generator_loss=4.3843512535095215, discriminator_loss=0.06833508610725403\n",
            "step 6875: generator_loss=4.630378723144531, discriminator_loss=0.05569376051425934\n",
            "step 6876: generator_loss=5.051732540130615, discriminator_loss=0.04823675751686096\n",
            "step 6877: generator_loss=5.460504531860352, discriminator_loss=0.0442032516002655\n",
            "step 6878: generator_loss=5.327371597290039, discriminator_loss=0.044044531881809235\n",
            "step 6879: generator_loss=5.937796115875244, discriminator_loss=0.036558475345373154\n",
            "step 6880: generator_loss=5.934145450592041, discriminator_loss=0.034669533371925354\n",
            "step 6881: generator_loss=6.290358066558838, discriminator_loss=0.032937824726104736\n",
            "step 6882: generator_loss=6.053266525268555, discriminator_loss=0.0337408185005188\n",
            "step 6883: generator_loss=6.838494300842285, discriminator_loss=0.02877035178244114\n",
            "step 6884: generator_loss=6.5408759117126465, discriminator_loss=0.03134651482105255\n",
            "step 6885: generator_loss=6.228268623352051, discriminator_loss=0.033770859241485596\n",
            "step 6886: generator_loss=5.891687393188477, discriminator_loss=0.03637497499585152\n",
            "step 6887: generator_loss=5.880256175994873, discriminator_loss=0.03547809645533562\n",
            "step 6888: generator_loss=5.659205913543701, discriminator_loss=0.04023461788892746\n",
            "step 6889: generator_loss=6.125431060791016, discriminator_loss=0.03616711497306824\n",
            "step 6890: generator_loss=5.597405910491943, discriminator_loss=0.04059842973947525\n",
            "step 6891: generator_loss=5.7458953857421875, discriminator_loss=0.03402157872915268\n",
            "step 6892: generator_loss=5.857605934143066, discriminator_loss=0.03149944171309471\n",
            "step 6893: generator_loss=6.028946876525879, discriminator_loss=0.028838902711868286\n",
            "step 6894: generator_loss=5.720541954040527, discriminator_loss=0.028809580951929092\n",
            "step 6895: generator_loss=5.478128433227539, discriminator_loss=0.030515547841787338\n",
            "step 6896: generator_loss=5.442288398742676, discriminator_loss=0.029937706887722015\n",
            "step 6897: generator_loss=5.486855983734131, discriminator_loss=0.03064507618546486\n",
            "step 6898: generator_loss=5.2075042724609375, discriminator_loss=0.0310961976647377\n",
            "step 6899: generator_loss=5.150069236755371, discriminator_loss=0.030099034309387207\n",
            "step 6900: generator_loss=5.090737342834473, discriminator_loss=0.029654040932655334\n",
            "step 6901: generator_loss=5.0916852951049805, discriminator_loss=0.03001197800040245\n",
            "step 6902: generator_loss=5.044148921966553, discriminator_loss=0.028618810698390007\n",
            "step 6903: generator_loss=4.89479923248291, discriminator_loss=0.030296113342046738\n",
            "step 6904: generator_loss=4.960352897644043, discriminator_loss=0.027902500703930855\n",
            "step 6905: generator_loss=4.5864362716674805, discriminator_loss=0.03010934591293335\n",
            "step 6906: generator_loss=4.697084426879883, discriminator_loss=0.030079614371061325\n",
            "step 6907: generator_loss=4.479559421539307, discriminator_loss=0.030868493020534515\n",
            "step 6908: generator_loss=4.437938690185547, discriminator_loss=0.030343299731612206\n",
            "step 6909: generator_loss=4.478365898132324, discriminator_loss=0.03057676926255226\n",
            "step 6910: generator_loss=4.679701805114746, discriminator_loss=0.026541199535131454\n",
            "step 6911: generator_loss=4.585628032684326, discriminator_loss=0.026325281709432602\n",
            "step 6912: generator_loss=4.482114315032959, discriminator_loss=0.02654699981212616\n",
            "step 6913: generator_loss=4.417280197143555, discriminator_loss=0.02783988043665886\n",
            "step 6914: generator_loss=4.252359390258789, discriminator_loss=0.02750278264284134\n",
            "step 6915: generator_loss=4.177074432373047, discriminator_loss=0.02817855030298233\n",
            "step 6916: generator_loss=4.079768180847168, discriminator_loss=0.028198016807436943\n",
            "step 6917: generator_loss=3.8584113121032715, discriminator_loss=0.030934181064367294\n",
            "step 6918: generator_loss=4.157474040985107, discriminator_loss=0.028431277722120285\n",
            "step 6919: generator_loss=3.81120228767395, discriminator_loss=0.030978044494986534\n",
            "step 6920: generator_loss=3.6807682514190674, discriminator_loss=0.03294197842478752\n",
            "step 6921: generator_loss=3.782477617263794, discriminator_loss=0.03152592480182648\n",
            "step 6922: generator_loss=3.661278247833252, discriminator_loss=0.03330732509493828\n",
            "step 6923: generator_loss=3.516584873199463, discriminator_loss=0.03380952775478363\n",
            "step 6924: generator_loss=3.6077330112457275, discriminator_loss=0.03295152634382248\n",
            "step 6925: generator_loss=3.6197266578674316, discriminator_loss=0.0325545072555542\n",
            "step 6926: generator_loss=3.624819755554199, discriminator_loss=0.034227728843688965\n",
            "step 6927: generator_loss=3.468859910964966, discriminator_loss=0.035020604729652405\n",
            "step 6928: generator_loss=3.5249032974243164, discriminator_loss=0.03500829264521599\n",
            "step 6929: generator_loss=3.3756632804870605, discriminator_loss=0.03698220103979111\n",
            "step 6930: generator_loss=3.3720881938934326, discriminator_loss=0.03593851253390312\n",
            "step 6931: generator_loss=3.244011878967285, discriminator_loss=0.037634219974279404\n",
            "step 6932: generator_loss=3.4385571479797363, discriminator_loss=0.03645247220993042\n",
            "step 6933: generator_loss=3.2642900943756104, discriminator_loss=0.037719935178756714\n",
            "step 6934: generator_loss=3.301172971725464, discriminator_loss=0.03745056316256523\n",
            "step 6935: generator_loss=3.3853440284729004, discriminator_loss=0.03683050349354744\n",
            "step 6936: generator_loss=3.3181166648864746, discriminator_loss=0.03751561790704727\n",
            "step 6937: generator_loss=3.310267448425293, discriminator_loss=0.03722692281007767\n",
            "step 6938: generator_loss=3.341092109680176, discriminator_loss=0.03706281632184982\n",
            "step 6939: generator_loss=3.2509372234344482, discriminator_loss=0.0382193885743618\n",
            "step 6940: generator_loss=3.221709966659546, discriminator_loss=0.038813069462776184\n",
            "step 6941: generator_loss=3.2464523315429688, discriminator_loss=0.0385231077671051\n",
            "step 6942: generator_loss=3.200118064880371, discriminator_loss=0.03946412727236748\n",
            "step 6943: generator_loss=3.2120447158813477, discriminator_loss=0.03954543173313141\n",
            "step 6944: generator_loss=3.192472457885742, discriminator_loss=0.0400693416595459\n",
            "step 6945: generator_loss=3.1941001415252686, discriminator_loss=0.04023367911577225\n",
            "step 6946: generator_loss=3.1916069984436035, discriminator_loss=0.04021985083818436\n",
            "step 6947: generator_loss=3.243114471435547, discriminator_loss=0.03935014829039574\n",
            "step 6948: generator_loss=3.2590272426605225, discriminator_loss=0.03915217146277428\n",
            "step 6949: generator_loss=3.2774016857147217, discriminator_loss=0.038981739431619644\n",
            "step 6950: generator_loss=3.2690768241882324, discriminator_loss=0.03931829705834389\n",
            "step 6951: generator_loss=3.265623092651367, discriminator_loss=0.039585113525390625\n",
            "step 6952: generator_loss=3.238931179046631, discriminator_loss=0.04021866247057915\n",
            "step 6953: generator_loss=3.2246289253234863, discriminator_loss=0.04070957005023956\n",
            "step 6954: generator_loss=3.199831962585449, discriminator_loss=0.04143255576491356\n",
            "step 6955: generator_loss=3.150935649871826, discriminator_loss=0.042618364095687866\n",
            "step 6956: generator_loss=3.1277358531951904, discriminator_loss=0.043426547199487686\n",
            "step 6957: generator_loss=3.0688064098358154, discriminator_loss=0.04498158022761345\n",
            "step 6958: generator_loss=3.021935224533081, discriminator_loss=0.046225354075431824\n",
            "step 6959: generator_loss=3.0434303283691406, discriminator_loss=0.046048350632190704\n",
            "step 6960: generator_loss=3.1542770862579346, discriminator_loss=0.0437219999730587\n",
            "step 6961: generator_loss=3.27786922454834, discriminator_loss=0.04134520888328552\n",
            "step 6962: generator_loss=3.3966803550720215, discriminator_loss=0.039436254650354385\n",
            "step 6963: generator_loss=3.5387864112854004, discriminator_loss=0.03725199028849602\n",
            "step 6964: generator_loss=3.6297523975372314, discriminator_loss=0.036043670028448105\n",
            "step 6965: generator_loss=3.7020578384399414, discriminator_loss=0.035114310681819916\n",
            "step 6966: generator_loss=3.728200912475586, discriminator_loss=0.03464939072728157\n",
            "step 6967: generator_loss=3.735475778579712, discriminator_loss=0.03436589241027832\n",
            "step 6968: generator_loss=3.76263427734375, discriminator_loss=0.03370906785130501\n",
            "step 6969: generator_loss=3.7529125213623047, discriminator_loss=0.03335972875356674\n",
            "step 6970: generator_loss=3.719906806945801, discriminator_loss=0.033211514353752136\n",
            "step 6971: generator_loss=3.671980619430542, discriminator_loss=0.03326098248362541\n",
            "step 6972: generator_loss=3.586540937423706, discriminator_loss=0.033820878714323044\n",
            "step 6973: generator_loss=3.47042179107666, discriminator_loss=0.035013746470212936\n",
            "step 6974: generator_loss=3.349392890930176, discriminator_loss=0.03656504303216934\n",
            "step 6975: generator_loss=3.257012128829956, discriminator_loss=0.037793729454278946\n",
            "step 6976: generator_loss=3.1984634399414062, discriminator_loss=0.0386241190135479\n",
            "step 6977: generator_loss=3.156759023666382, discriminator_loss=0.03919667750597\n",
            "step 6978: generator_loss=3.131377935409546, discriminator_loss=0.039604976773262024\n",
            "step 6979: generator_loss=3.152636766433716, discriminator_loss=0.039006516337394714\n",
            "step 6980: generator_loss=3.200397253036499, discriminator_loss=0.03784983232617378\n",
            "step 6981: generator_loss=3.255734920501709, discriminator_loss=0.036690887063741684\n",
            "step 6982: generator_loss=3.2796452045440674, discriminator_loss=0.03628265857696533\n",
            "step 6983: generator_loss=3.311553955078125, discriminator_loss=0.03571245074272156\n",
            "step 6984: generator_loss=3.327017068862915, discriminator_loss=0.03551654890179634\n",
            "step 6985: generator_loss=3.3254599571228027, discriminator_loss=0.03562891110777855\n",
            "step 6986: generator_loss=3.337792158126831, discriminator_loss=0.03556779399514198\n",
            "step 6987: generator_loss=3.375861167907715, discriminator_loss=0.035084374248981476\n",
            "step 6988: generator_loss=3.428101062774658, discriminator_loss=0.034289512783288956\n",
            "step 6989: generator_loss=3.463330030441284, discriminator_loss=0.033844348043203354\n",
            "step 6990: generator_loss=3.4713821411132812, discriminator_loss=0.03381737321615219\n",
            "step 6991: generator_loss=3.431248903274536, discriminator_loss=0.034516625106334686\n",
            "step 6992: generator_loss=3.346233367919922, discriminator_loss=0.036102525889873505\n",
            "step 6993: generator_loss=3.284809112548828, discriminator_loss=0.03729737550020218\n",
            "step 6994: generator_loss=3.2314517498016357, discriminator_loss=0.038454942405223846\n",
            "step 6995: generator_loss=3.1819851398468018, discriminator_loss=0.03962001949548721\n",
            "step 6996: generator_loss=3.1557185649871826, discriminator_loss=0.04029812291264534\n",
            "step 6997: generator_loss=3.138137102127075, discriminator_loss=0.04085409641265869\n",
            "step 6998: generator_loss=3.1373214721679688, discriminator_loss=0.04101942852139473\n",
            "step 6999: generator_loss=3.127910852432251, discriminator_loss=0.04143451154232025\n",
            "step 7000: generator_loss=3.1108407974243164, discriminator_loss=0.04210580140352249\n",
            "step 7001: generator_loss=3.10654354095459, discriminator_loss=0.04249543324112892\n",
            "step 7002: generator_loss=3.1027913093566895, discriminator_loss=0.04287972301244736\n",
            "step 7003: generator_loss=3.106257915496826, discriminator_loss=0.043103963136672974\n",
            "step 7004: generator_loss=3.1114704608917236, discriminator_loss=0.04338381811976433\n",
            "step 7005: generator_loss=3.126415729522705, discriminator_loss=0.043448664247989655\n",
            "step 7006: generator_loss=3.14199161529541, discriminator_loss=0.043386060744524\n",
            "step 7007: generator_loss=3.1442956924438477, discriminator_loss=0.04365000128746033\n",
            "step 7008: generator_loss=3.1520347595214844, discriminator_loss=0.04377996176481247\n",
            "step 7009: generator_loss=3.1575467586517334, discriminator_loss=0.04395170137286186\n",
            "step 7010: generator_loss=3.1730735301971436, discriminator_loss=0.043836191296577454\n",
            "step 7011: generator_loss=3.209561824798584, discriminator_loss=0.043335676193237305\n",
            "step 7012: generator_loss=3.257910966873169, discriminator_loss=0.042473405599594116\n",
            "step 7013: generator_loss=3.3092267513275146, discriminator_loss=0.04155801236629486\n",
            "step 7014: generator_loss=3.342634439468384, discriminator_loss=0.04103603586554527\n",
            "step 7015: generator_loss=3.3430144786834717, discriminator_loss=0.04104568064212799\n",
            "step 7016: generator_loss=3.2879650592803955, discriminator_loss=0.04195614531636238\n",
            "step 7017: generator_loss=3.2162258625030518, discriminator_loss=0.04327448084950447\n",
            "step 7018: generator_loss=3.135345220565796, discriminator_loss=0.044954631477594376\n",
            "step 7019: generator_loss=3.06868577003479, discriminator_loss=0.046471673995256424\n",
            "step 7020: generator_loss=3.0389270782470703, discriminator_loss=0.04711710289120674\n",
            "step 7021: generator_loss=3.022420883178711, discriminator_loss=0.047502484172582626\n",
            "step 7022: generator_loss=2.99916672706604, discriminator_loss=0.04806309938430786\n",
            "step 7023: generator_loss=2.9876022338867188, discriminator_loss=0.0484427735209465\n",
            "step 7024: generator_loss=3.0591635704040527, discriminator_loss=0.046599313616752625\n",
            "step 7025: generator_loss=3.159719705581665, discriminator_loss=0.04438278451561928\n",
            "step 7026: generator_loss=3.251006841659546, discriminator_loss=0.04256777465343475\n",
            "step 7027: generator_loss=3.345395088195801, discriminator_loss=0.04079730808734894\n",
            "step 7028: generator_loss=3.4130656719207764, discriminator_loss=0.039735764265060425\n",
            "step 7029: generator_loss=3.4493653774261475, discriminator_loss=0.03913571685552597\n",
            "step 7030: generator_loss=3.49509859085083, discriminator_loss=0.03844456374645233\n",
            "step 7031: generator_loss=3.5142369270324707, discriminator_loss=0.038099031895399094\n",
            "step 7032: generator_loss=3.4938929080963135, discriminator_loss=0.03836642950773239\n",
            "step 7033: generator_loss=3.4779765605926514, discriminator_loss=0.03846149519085884\n",
            "step 7034: generator_loss=3.4698901176452637, discriminator_loss=0.03847965598106384\n",
            "step 7035: generator_loss=3.4477176666259766, discriminator_loss=0.03864388167858124\n",
            "step 7036: generator_loss=3.437354564666748, discriminator_loss=0.03855953365564346\n",
            "step 7037: generator_loss=3.4272029399871826, discriminator_loss=0.03847169876098633\n",
            "step 7038: generator_loss=3.4240381717681885, discriminator_loss=0.038260817527770996\n",
            "step 7039: generator_loss=3.4813730716705322, discriminator_loss=0.0370805524289608\n",
            "step 7040: generator_loss=3.5327281951904297, discriminator_loss=0.03596131131052971\n",
            "step 7041: generator_loss=3.6106557846069336, discriminator_loss=0.0344666913151741\n",
            "step 7042: generator_loss=3.6699297428131104, discriminator_loss=0.03332666680216789\n",
            "step 7043: generator_loss=3.7380409240722656, discriminator_loss=0.03217056021094322\n",
            "step 7044: generator_loss=3.7787866592407227, discriminator_loss=0.03134128078818321\n",
            "step 7045: generator_loss=3.794806957244873, discriminator_loss=0.030810251832008362\n",
            "step 7046: generator_loss=3.82080078125, discriminator_loss=0.03026905655860901\n",
            "step 7047: generator_loss=3.8239448070526123, discriminator_loss=0.029891304671764374\n",
            "step 7048: generator_loss=3.79803466796875, discriminator_loss=0.029902935028076172\n",
            "step 7049: generator_loss=3.742288112640381, discriminator_loss=0.030316539108753204\n",
            "step 7050: generator_loss=3.698115348815918, discriminator_loss=0.030593255534768105\n",
            "step 7051: generator_loss=3.644228458404541, discriminator_loss=0.031063973903656006\n",
            "step 7052: generator_loss=3.589592456817627, discriminator_loss=0.03162171691656113\n",
            "step 7053: generator_loss=3.5232901573181152, discriminator_loss=0.0323343425989151\n",
            "step 7054: generator_loss=3.451626777648926, discriminator_loss=0.03325720876455307\n",
            "step 7055: generator_loss=3.3931169509887695, discriminator_loss=0.034121885895729065\n",
            "step 7056: generator_loss=3.3905043601989746, discriminator_loss=0.03402644395828247\n",
            "step 7057: generator_loss=3.440429449081421, discriminator_loss=0.033137477934360504\n",
            "step 7058: generator_loss=3.474489450454712, discriminator_loss=0.03252990543842316\n",
            "step 7059: generator_loss=3.5037872791290283, discriminator_loss=0.032070983201265335\n",
            "step 7060: generator_loss=3.5091440677642822, discriminator_loss=0.03197520226240158\n",
            "step 7061: generator_loss=3.4861679077148438, discriminator_loss=0.032398905605077744\n",
            "step 7062: generator_loss=3.465066909790039, discriminator_loss=0.03274817764759064\n",
            "step 7063: generator_loss=3.439467430114746, discriminator_loss=0.03322923183441162\n",
            "step 7064: generator_loss=3.441540479660034, discriminator_loss=0.033203743398189545\n",
            "step 7065: generator_loss=3.4941539764404297, discriminator_loss=0.03245214745402336\n",
            "step 7066: generator_loss=3.531001567840576, discriminator_loss=0.031967226415872574\n",
            "step 7067: generator_loss=3.5594635009765625, discriminator_loss=0.03157462179660797\n",
            "step 7068: generator_loss=3.5744211673736572, discriminator_loss=0.03141902759671211\n",
            "step 7069: generator_loss=3.565829277038574, discriminator_loss=0.031602248549461365\n",
            "step 7070: generator_loss=3.546668291091919, discriminator_loss=0.03189628943800926\n",
            "step 7071: generator_loss=3.52158522605896, discriminator_loss=0.0323047935962677\n",
            "step 7072: generator_loss=3.4827637672424316, discriminator_loss=0.03288845717906952\n",
            "step 7073: generator_loss=3.438882827758789, discriminator_loss=0.033595554530620575\n",
            "step 7074: generator_loss=3.4189493656158447, discriminator_loss=0.033968500792980194\n",
            "step 7075: generator_loss=3.396207094192505, discriminator_loss=0.03434273600578308\n",
            "step 7076: generator_loss=3.360940456390381, discriminator_loss=0.03499449044466019\n",
            "step 7077: generator_loss=3.328035831451416, discriminator_loss=0.03554537147283554\n",
            "step 7078: generator_loss=3.293344020843506, discriminator_loss=0.036270029842853546\n",
            "step 7079: generator_loss=3.273430585861206, discriminator_loss=0.03672262281179428\n",
            "step 7080: generator_loss=3.272815704345703, discriminator_loss=0.03680647909641266\n",
            "step 7081: generator_loss=3.2546637058258057, discriminator_loss=0.03726853057742119\n",
            "step 7082: generator_loss=3.238464117050171, discriminator_loss=0.03764469176530838\n",
            "step 7083: generator_loss=3.2504470348358154, discriminator_loss=0.0375010147690773\n",
            "step 7084: generator_loss=3.2452430725097656, discriminator_loss=0.03778313100337982\n",
            "step 7085: generator_loss=3.2295963764190674, discriminator_loss=0.03818979114294052\n",
            "step 7086: generator_loss=3.2189102172851562, discriminator_loss=0.038602929562330246\n",
            "step 7087: generator_loss=3.2134885787963867, discriminator_loss=0.03886614739894867\n",
            "step 7088: generator_loss=3.2279274463653564, discriminator_loss=0.038716111332178116\n",
            "step 7089: generator_loss=3.2267959117889404, discriminator_loss=0.03898369520902634\n",
            "step 7090: generator_loss=3.218295097351074, discriminator_loss=0.0393625870347023\n",
            "step 7091: generator_loss=3.213287591934204, discriminator_loss=0.039679259061813354\n",
            "step 7092: generator_loss=3.1904296875, discriminator_loss=0.040306396782398224\n",
            "step 7093: generator_loss=3.189413547515869, discriminator_loss=0.040512677282094955\n",
            "step 7094: generator_loss=3.1850132942199707, discriminator_loss=0.04078676924109459\n",
            "step 7095: generator_loss=3.166746139526367, discriminator_loss=0.04132731258869171\n",
            "step 7096: generator_loss=3.1245694160461426, discriminator_loss=0.042496878653764725\n",
            "step 7097: generator_loss=3.0901639461517334, discriminator_loss=0.04344679415225983\n",
            "step 7098: generator_loss=3.0718300342559814, discriminator_loss=0.044125791639089584\n",
            "step 7099: generator_loss=3.0610432624816895, discriminator_loss=0.0446757972240448\n",
            "step 7100: generator_loss=3.0275049209594727, discriminator_loss=0.04567692428827286\n",
            "step 7101: generator_loss=3.037132740020752, discriminator_loss=0.045663125813007355\n",
            "step 7102: generator_loss=3.0613269805908203, discriminator_loss=0.045278556644916534\n",
            "step 7103: generator_loss=3.0888888835906982, discriminator_loss=0.04520515352487564\n",
            "step 7104: generator_loss=3.1222763061523438, discriminator_loss=0.04461076855659485\n",
            "step 7105: generator_loss=3.148141860961914, discriminator_loss=0.04464167356491089\n",
            "step 7106: generator_loss=3.244692325592041, discriminator_loss=0.04364398494362831\n",
            "step 7107: generator_loss=3.211918354034424, discriminator_loss=0.04381321370601654\n",
            "step 7108: generator_loss=3.179353713989258, discriminator_loss=0.04500815272331238\n",
            "step 7109: generator_loss=3.185905933380127, discriminator_loss=0.04547814279794693\n",
            "step 7110: generator_loss=3.128969192504883, discriminator_loss=0.0470893532037735\n",
            "step 7111: generator_loss=3.0279853343963623, discriminator_loss=0.049462854862213135\n",
            "step 7112: generator_loss=3.043302536010742, discriminator_loss=0.049540068954229355\n",
            "step 7113: generator_loss=2.961167097091675, discriminator_loss=0.05173555761575699\n",
            "step 7114: generator_loss=2.978024482727051, discriminator_loss=0.051927629858255386\n",
            "step 7115: generator_loss=2.957319498062134, discriminator_loss=0.05265592783689499\n",
            "step 7116: generator_loss=2.946173906326294, discriminator_loss=0.052832357585430145\n",
            "step 7117: generator_loss=2.9526138305664062, discriminator_loss=0.053052645176649094\n",
            "step 7118: generator_loss=3.0900566577911377, discriminator_loss=0.05032601207494736\n",
            "step 7119: generator_loss=3.1589999198913574, discriminator_loss=0.048732444643974304\n",
            "step 7120: generator_loss=3.2432796955108643, discriminator_loss=0.04732483625411987\n",
            "step 7121: generator_loss=3.2662529945373535, discriminator_loss=0.046741485595703125\n",
            "step 7122: generator_loss=3.324098587036133, discriminator_loss=0.04584284499287605\n",
            "step 7123: generator_loss=3.3348822593688965, discriminator_loss=0.04564489424228668\n",
            "step 7124: generator_loss=3.339247226715088, discriminator_loss=0.04548157751560211\n",
            "step 7125: generator_loss=3.260740280151367, discriminator_loss=0.04649568349123001\n",
            "step 7126: generator_loss=3.206418991088867, discriminator_loss=0.04724496603012085\n",
            "step 7127: generator_loss=3.2009809017181396, discriminator_loss=0.047530174255371094\n",
            "step 7128: generator_loss=3.1802070140838623, discriminator_loss=0.04768916592001915\n",
            "step 7129: generator_loss=3.163292407989502, discriminator_loss=0.047396641224622726\n",
            "step 7130: generator_loss=3.2758190631866455, discriminator_loss=0.04451775550842285\n",
            "step 7131: generator_loss=3.4020538330078125, discriminator_loss=0.041861921548843384\n",
            "step 7132: generator_loss=3.513608694076538, discriminator_loss=0.039610106498003006\n",
            "step 7133: generator_loss=3.6009373664855957, discriminator_loss=0.03798476234078407\n",
            "step 7134: generator_loss=3.6669933795928955, discriminator_loss=0.03658216446638107\n",
            "step 7135: generator_loss=3.772143840789795, discriminator_loss=0.034813713282346725\n",
            "step 7136: generator_loss=3.8533883094787598, discriminator_loss=0.0332622192800045\n",
            "step 7137: generator_loss=3.9116625785827637, discriminator_loss=0.03206692636013031\n",
            "step 7138: generator_loss=3.9552266597747803, discriminator_loss=0.031060978770256042\n",
            "step 7139: generator_loss=3.985954761505127, discriminator_loss=0.030181884765625\n",
            "step 7140: generator_loss=3.9719882011413574, discriminator_loss=0.029784463346004486\n",
            "step 7141: generator_loss=3.9360790252685547, discriminator_loss=0.029598066583275795\n",
            "step 7142: generator_loss=3.8668816089630127, discriminator_loss=0.029838230460882187\n",
            "step 7143: generator_loss=3.7670347690582275, discriminator_loss=0.030494431033730507\n",
            "step 7144: generator_loss=3.6983492374420166, discriminator_loss=0.030935965478420258\n",
            "step 7145: generator_loss=3.641869068145752, discriminator_loss=0.03130731359124184\n",
            "step 7146: generator_loss=3.5940277576446533, discriminator_loss=0.03167812526226044\n",
            "step 7147: generator_loss=3.5397918224334717, discriminator_loss=0.03223562613129616\n",
            "step 7148: generator_loss=3.480485439300537, discriminator_loss=0.03295605629682541\n",
            "step 7149: generator_loss=3.4089126586914062, discriminator_loss=0.03400561586022377\n",
            "step 7150: generator_loss=3.345606565475464, discriminator_loss=0.03503397852182388\n",
            "step 7151: generator_loss=3.3063573837280273, discriminator_loss=0.035718291997909546\n",
            "step 7152: generator_loss=3.301331043243408, discriminator_loss=0.03583358973264694\n",
            "step 7153: generator_loss=3.3233449459075928, discriminator_loss=0.035499390214681625\n",
            "step 7154: generator_loss=3.3695476055145264, discriminator_loss=0.034799255430698395\n",
            "step 7155: generator_loss=3.407095432281494, discriminator_loss=0.03432847186923027\n",
            "step 7156: generator_loss=3.4221081733703613, discriminator_loss=0.03433123975992203\n",
            "step 7157: generator_loss=3.4076383113861084, discriminator_loss=0.03480761498212814\n",
            "step 7158: generator_loss=3.3853554725646973, discriminator_loss=0.0354161411523819\n",
            "step 7159: generator_loss=3.3680694103240967, discriminator_loss=0.03602634742856026\n",
            "step 7160: generator_loss=3.3242781162261963, discriminator_loss=0.037054188549518585\n",
            "step 7161: generator_loss=3.2562034130096436, discriminator_loss=0.03861008584499359\n",
            "step 7162: generator_loss=3.1673219203948975, discriminator_loss=0.040721651166677475\n",
            "step 7163: generator_loss=3.119302988052368, discriminator_loss=0.04205602407455444\n",
            "step 7164: generator_loss=3.11238694190979, discriminator_loss=0.042443498969078064\n",
            "step 7165: generator_loss=3.1228833198547363, discriminator_loss=0.04252641648054123\n",
            "step 7166: generator_loss=3.1385228633880615, discriminator_loss=0.042434900999069214\n",
            "step 7167: generator_loss=3.1506969928741455, discriminator_loss=0.042495228350162506\n",
            "step 7168: generator_loss=3.1536498069763184, discriminator_loss=0.042629025876522064\n",
            "step 7169: generator_loss=3.128504753112793, discriminator_loss=0.043575454503297806\n",
            "step 7170: generator_loss=3.088357448577881, discriminator_loss=0.04475671052932739\n",
            "step 7171: generator_loss=3.0487866401672363, discriminator_loss=0.04611921310424805\n",
            "step 7172: generator_loss=2.9992427825927734, discriminator_loss=0.04772225767374039\n",
            "step 7173: generator_loss=2.9853732585906982, discriminator_loss=0.048422373831272125\n",
            "step 7174: generator_loss=2.996215343475342, discriminator_loss=0.04851889610290527\n",
            "step 7175: generator_loss=3.048009157180786, discriminator_loss=0.04768974706530571\n",
            "step 7176: generator_loss=3.119455337524414, discriminator_loss=0.04637352377176285\n",
            "step 7177: generator_loss=3.16890287399292, discriminator_loss=0.04601317644119263\n",
            "step 7178: generator_loss=3.2046990394592285, discriminator_loss=0.046673186123371124\n",
            "step 7179: generator_loss=3.2458336353302, discriminator_loss=0.046925436705350876\n",
            "step 7180: generator_loss=3.2489778995513916, discriminator_loss=0.05048718303442001\n",
            "step 7181: generator_loss=3.1641252040863037, discriminator_loss=0.058153823018074036\n",
            "step 7182: generator_loss=3.0273146629333496, discriminator_loss=0.08294133096933365\n",
            "step 7183: generator_loss=3.2195487022399902, discriminator_loss=0.07568157464265823\n",
            "step 7184: generator_loss=3.1925129890441895, discriminator_loss=0.10285921394824982\n",
            "step 7185: generator_loss=3.3107407093048096, discriminator_loss=0.10200828313827515\n",
            "step 7186: generator_loss=3.5784592628479004, discriminator_loss=0.1096380203962326\n",
            "step 7187: generator_loss=3.6198229789733887, discriminator_loss=0.11821006238460541\n",
            "step 7188: generator_loss=4.1162285804748535, discriminator_loss=0.09054487943649292\n",
            "step 7189: generator_loss=4.275984764099121, discriminator_loss=0.08315132558345795\n",
            "step 7190: generator_loss=4.790773391723633, discriminator_loss=0.06531272828578949\n",
            "step 7191: generator_loss=5.193973541259766, discriminator_loss=0.05470329895615578\n",
            "step 7192: generator_loss=5.482057571411133, discriminator_loss=0.051755428314208984\n",
            "step 7193: generator_loss=5.826730728149414, discriminator_loss=0.04134073853492737\n",
            "step 7194: generator_loss=6.070886611938477, discriminator_loss=0.03530678153038025\n",
            "step 7195: generator_loss=6.30192756652832, discriminator_loss=0.032000213861465454\n",
            "step 7196: generator_loss=6.045994281768799, discriminator_loss=0.03299327567219734\n",
            "step 7197: generator_loss=6.002236843109131, discriminator_loss=0.03288102522492409\n",
            "step 7198: generator_loss=5.699721336364746, discriminator_loss=0.03295981511473656\n",
            "step 7199: generator_loss=6.035770416259766, discriminator_loss=0.02919697016477585\n",
            "step 7200: generator_loss=5.775442123413086, discriminator_loss=0.031344927847385406\n",
            "step 7201: generator_loss=5.522885322570801, discriminator_loss=0.030521200969815254\n",
            "step 7202: generator_loss=5.093610763549805, discriminator_loss=0.034908074885606766\n",
            "step 7203: generator_loss=5.138613700866699, discriminator_loss=0.032331306487321854\n",
            "step 7204: generator_loss=4.692964553833008, discriminator_loss=0.03398614376783371\n",
            "step 7205: generator_loss=4.452696800231934, discriminator_loss=0.03882354870438576\n",
            "step 7206: generator_loss=4.374147415161133, discriminator_loss=0.03416923061013222\n",
            "step 7207: generator_loss=4.233105659484863, discriminator_loss=0.03428136929869652\n",
            "step 7208: generator_loss=3.897735595703125, discriminator_loss=0.03853687644004822\n",
            "step 7209: generator_loss=3.9700963497161865, discriminator_loss=0.03314332664012909\n",
            "step 7210: generator_loss=3.6616554260253906, discriminator_loss=0.037151824682950974\n",
            "step 7211: generator_loss=3.5883727073669434, discriminator_loss=0.037464700639247894\n",
            "step 7212: generator_loss=3.614631175994873, discriminator_loss=0.037950944155454636\n",
            "step 7213: generator_loss=3.5828423500061035, discriminator_loss=0.03739843890070915\n",
            "step 7214: generator_loss=3.563689708709717, discriminator_loss=0.0371740460395813\n",
            "step 7215: generator_loss=3.780730724334717, discriminator_loss=0.03314851224422455\n",
            "step 7216: generator_loss=3.80485200881958, discriminator_loss=0.031345319002866745\n",
            "step 7217: generator_loss=3.861891746520996, discriminator_loss=0.030718274414539337\n",
            "step 7218: generator_loss=3.9632129669189453, discriminator_loss=0.02946140617132187\n",
            "step 7219: generator_loss=4.077287197113037, discriminator_loss=0.027959616854786873\n",
            "step 7220: generator_loss=4.108569145202637, discriminator_loss=0.02725239470601082\n",
            "step 7221: generator_loss=4.211126327514648, discriminator_loss=0.02607094496488571\n",
            "step 7222: generator_loss=4.128963947296143, discriminator_loss=0.026917094364762306\n",
            "step 7223: generator_loss=4.2840495109558105, discriminator_loss=0.025494247674942017\n",
            "step 7224: generator_loss=4.118995666503906, discriminator_loss=0.026737729087471962\n",
            "step 7225: generator_loss=4.1449198722839355, discriminator_loss=0.026405945420265198\n",
            "step 7226: generator_loss=4.110531806945801, discriminator_loss=0.02694900892674923\n",
            "step 7227: generator_loss=4.082625389099121, discriminator_loss=0.027790531516075134\n",
            "step 7228: generator_loss=3.957139253616333, discriminator_loss=0.028893711045384407\n",
            "step 7229: generator_loss=3.7886390686035156, discriminator_loss=0.030755871906876564\n",
            "step 7230: generator_loss=3.7201032638549805, discriminator_loss=0.03123895823955536\n",
            "step 7231: generator_loss=3.666982650756836, discriminator_loss=0.03236506134271622\n",
            "step 7232: generator_loss=3.5467607975006104, discriminator_loss=0.03373664990067482\n",
            "step 7233: generator_loss=3.5785093307495117, discriminator_loss=0.0327095165848732\n",
            "step 7234: generator_loss=3.515705108642578, discriminator_loss=0.03328080102801323\n",
            "step 7235: generator_loss=3.5859570503234863, discriminator_loss=0.03155640512704849\n",
            "step 7236: generator_loss=3.4896676540374756, discriminator_loss=0.03252223879098892\n",
            "step 7237: generator_loss=3.424971103668213, discriminator_loss=0.03351379185914993\n",
            "step 7238: generator_loss=3.471822738647461, discriminator_loss=0.03308137506246567\n",
            "step 7239: generator_loss=3.470630168914795, discriminator_loss=0.0331178642809391\n",
            "step 7240: generator_loss=3.4707653522491455, discriminator_loss=0.033140793442726135\n",
            "step 7241: generator_loss=3.4290876388549805, discriminator_loss=0.033284686505794525\n",
            "step 7242: generator_loss=3.3588240146636963, discriminator_loss=0.034323789179325104\n",
            "step 7243: generator_loss=3.3304214477539062, discriminator_loss=0.03485668823122978\n",
            "step 7244: generator_loss=3.411484479904175, discriminator_loss=0.03418847918510437\n",
            "step 7245: generator_loss=3.2782058715820312, discriminator_loss=0.0358843058347702\n",
            "step 7246: generator_loss=3.193657159805298, discriminator_loss=0.037972379475831985\n",
            "step 7247: generator_loss=3.203126907348633, discriminator_loss=0.03828136622905731\n",
            "step 7248: generator_loss=3.0831429958343506, discriminator_loss=0.040094323456287384\n",
            "step 7249: generator_loss=3.049095630645752, discriminator_loss=0.04175630956888199\n",
            "step 7250: generator_loss=2.99676513671875, discriminator_loss=0.043630555272102356\n",
            "step 7251: generator_loss=2.9914331436157227, discriminator_loss=0.044603265821933746\n",
            "step 7252: generator_loss=3.0488781929016113, discriminator_loss=0.044817689806222916\n",
            "step 7253: generator_loss=2.8482847213745117, discriminator_loss=0.04795342683792114\n",
            "step 7254: generator_loss=2.913303852081299, discriminator_loss=0.04721041023731232\n",
            "step 7255: generator_loss=2.8811521530151367, discriminator_loss=0.04825659096240997\n",
            "step 7256: generator_loss=3.1764962673187256, discriminator_loss=0.04460690915584564\n",
            "step 7257: generator_loss=3.108034372329712, discriminator_loss=0.044543664902448654\n",
            "step 7258: generator_loss=3.091566562652588, discriminator_loss=0.04470962658524513\n",
            "step 7259: generator_loss=3.137390613555908, discriminator_loss=0.04436427354812622\n",
            "step 7260: generator_loss=3.114680290222168, discriminator_loss=0.04434885084629059\n",
            "step 7261: generator_loss=3.255645751953125, discriminator_loss=0.0425340011715889\n",
            "step 7262: generator_loss=3.28548526763916, discriminator_loss=0.04228624701499939\n",
            "step 7263: generator_loss=3.303860664367676, discriminator_loss=0.04246269538998604\n",
            "step 7264: generator_loss=3.25590181350708, discriminator_loss=0.04311545193195343\n",
            "step 7265: generator_loss=3.2604141235351562, discriminator_loss=0.04351302981376648\n",
            "step 7266: generator_loss=3.357835054397583, discriminator_loss=0.04258546978235245\n",
            "step 7267: generator_loss=3.2903709411621094, discriminator_loss=0.043207064270973206\n",
            "step 7268: generator_loss=3.1525187492370605, discriminator_loss=0.04522589594125748\n",
            "step 7269: generator_loss=3.1929128170013428, discriminator_loss=0.04538583382964134\n",
            "step 7270: generator_loss=3.1136229038238525, discriminator_loss=0.04667539522051811\n",
            "step 7271: generator_loss=3.0699563026428223, discriminator_loss=0.0473579503595829\n",
            "step 7272: generator_loss=3.0578341484069824, discriminator_loss=0.04775337129831314\n",
            "step 7273: generator_loss=3.0042686462402344, discriminator_loss=0.048749443143606186\n",
            "step 7274: generator_loss=2.967916965484619, discriminator_loss=0.04956294968724251\n",
            "step 7275: generator_loss=2.944486141204834, discriminator_loss=0.05030123144388199\n",
            "step 7276: generator_loss=2.9395108222961426, discriminator_loss=0.05038346350193024\n",
            "step 7277: generator_loss=2.957670211791992, discriminator_loss=0.050112999975681305\n",
            "step 7278: generator_loss=2.9569454193115234, discriminator_loss=0.05010185018181801\n",
            "step 7279: generator_loss=2.977755069732666, discriminator_loss=0.04969927668571472\n",
            "step 7280: generator_loss=3.0546934604644775, discriminator_loss=0.04810664430260658\n",
            "step 7281: generator_loss=3.1386468410491943, discriminator_loss=0.046242907643318176\n",
            "step 7282: generator_loss=3.204590320587158, discriminator_loss=0.04510945826768875\n",
            "step 7283: generator_loss=3.240450620651245, discriminator_loss=0.04457256197929382\n",
            "step 7284: generator_loss=3.248861789703369, discriminator_loss=0.044465094804763794\n",
            "step 7285: generator_loss=3.276731014251709, discriminator_loss=0.04399300739169121\n",
            "step 7286: generator_loss=3.2866878509521484, discriminator_loss=0.04383257403969765\n",
            "step 7287: generator_loss=3.304882526397705, discriminator_loss=0.04355347156524658\n",
            "step 7288: generator_loss=3.3412883281707764, discriminator_loss=0.04279707372188568\n",
            "step 7289: generator_loss=3.3398053646087646, discriminator_loss=0.042924150824546814\n",
            "step 7290: generator_loss=3.32686710357666, discriminator_loss=0.04326357692480087\n",
            "step 7291: generator_loss=3.217644691467285, discriminator_loss=0.04649997875094414\n",
            "step 7292: generator_loss=3.1762211322784424, discriminator_loss=0.04736785963177681\n",
            "step 7293: generator_loss=3.050926923751831, discriminator_loss=0.05290297418832779\n",
            "step 7294: generator_loss=2.769735813140869, discriminator_loss=0.07255157828330994\n",
            "step 7295: generator_loss=2.685884952545166, discriminator_loss=0.09104366600513458\n",
            "step 7296: generator_loss=2.6649723052978516, discriminator_loss=0.10170299559831619\n",
            "step 7297: generator_loss=2.5265395641326904, discriminator_loss=0.14864382147789001\n",
            "step 7298: generator_loss=2.871318817138672, discriminator_loss=0.136152446269989\n",
            "step 7299: generator_loss=3.1242690086364746, discriminator_loss=0.1424139440059662\n",
            "step 7300: generator_loss=3.087867021560669, discriminator_loss=0.16021424531936646\n",
            "step 7301: generator_loss=3.5800929069519043, discriminator_loss=0.14633193612098694\n",
            "step 7302: generator_loss=3.9274826049804688, discriminator_loss=0.12080465257167816\n",
            "step 7303: generator_loss=4.278079986572266, discriminator_loss=0.1136389970779419\n",
            "step 7304: generator_loss=4.703054428100586, discriminator_loss=0.0815184935927391\n",
            "step 7305: generator_loss=4.926743507385254, discriminator_loss=0.05944013223052025\n",
            "step 7306: generator_loss=5.593592166900635, discriminator_loss=0.0446094386279583\n",
            "step 7307: generator_loss=6.125648498535156, discriminator_loss=0.041574589908123016\n",
            "step 7308: generator_loss=6.030500411987305, discriminator_loss=0.040687404572963715\n",
            "step 7309: generator_loss=6.2630791664123535, discriminator_loss=0.03976770490407944\n",
            "step 7310: generator_loss=6.257591247558594, discriminator_loss=0.039219703525304794\n",
            "step 7311: generator_loss=5.954269886016846, discriminator_loss=0.03943169489502907\n",
            "step 7312: generator_loss=5.822218418121338, discriminator_loss=0.03948061168193817\n",
            "step 7313: generator_loss=5.7103376388549805, discriminator_loss=0.039017464965581894\n",
            "step 7314: generator_loss=5.41988468170166, discriminator_loss=0.03902767226099968\n",
            "step 7315: generator_loss=5.049386024475098, discriminator_loss=0.041954509913921356\n",
            "step 7316: generator_loss=4.494967937469482, discriminator_loss=0.04816770926117897\n",
            "step 7317: generator_loss=4.37298583984375, discriminator_loss=0.04774948209524155\n",
            "step 7318: generator_loss=4.087502479553223, discriminator_loss=0.04855853691697121\n",
            "step 7319: generator_loss=3.9159505367279053, discriminator_loss=0.04892747849225998\n",
            "step 7320: generator_loss=3.5884251594543457, discriminator_loss=0.05862748622894287\n",
            "step 7321: generator_loss=3.4770712852478027, discriminator_loss=0.058748818933963776\n",
            "step 7322: generator_loss=3.1583316326141357, discriminator_loss=0.061363186687231064\n",
            "step 7323: generator_loss=3.2677838802337646, discriminator_loss=0.05764913186430931\n",
            "step 7324: generator_loss=2.941981315612793, discriminator_loss=0.06752761453390121\n",
            "step 7325: generator_loss=3.2455296516418457, discriminator_loss=0.05877813324332237\n",
            "step 7326: generator_loss=3.163550853729248, discriminator_loss=0.05803052335977554\n",
            "step 7327: generator_loss=3.255666494369507, discriminator_loss=0.05525488406419754\n",
            "step 7328: generator_loss=3.254530668258667, discriminator_loss=0.05444301664829254\n",
            "step 7329: generator_loss=3.53183650970459, discriminator_loss=0.0496789887547493\n",
            "step 7330: generator_loss=3.324385643005371, discriminator_loss=0.05028306692838669\n",
            "step 7331: generator_loss=3.4056153297424316, discriminator_loss=0.049425158649683\n",
            "step 7332: generator_loss=3.4389872550964355, discriminator_loss=0.04873649403452873\n",
            "step 7333: generator_loss=3.432321548461914, discriminator_loss=0.047857776284217834\n",
            "step 7334: generator_loss=3.520986557006836, discriminator_loss=0.04874164238572121\n",
            "step 7335: generator_loss=3.312039852142334, discriminator_loss=0.04789695143699646\n",
            "step 7336: generator_loss=3.491112232208252, discriminator_loss=0.04753710329532623\n",
            "step 7337: generator_loss=3.4300243854522705, discriminator_loss=0.04767414927482605\n",
            "step 7338: generator_loss=3.325472116470337, discriminator_loss=0.04855663329362869\n",
            "step 7339: generator_loss=3.3307347297668457, discriminator_loss=0.04777640849351883\n",
            "step 7340: generator_loss=3.2569587230682373, discriminator_loss=0.04898685961961746\n",
            "step 7341: generator_loss=3.1157326698303223, discriminator_loss=0.04983293265104294\n",
            "step 7342: generator_loss=3.1772491931915283, discriminator_loss=0.04845573380589485\n",
            "step 7343: generator_loss=3.03437876701355, discriminator_loss=0.05068469047546387\n",
            "step 7344: generator_loss=3.0232808589935303, discriminator_loss=0.051104046404361725\n",
            "step 7345: generator_loss=3.024837017059326, discriminator_loss=0.05110258609056473\n",
            "step 7346: generator_loss=2.962066650390625, discriminator_loss=0.05308971554040909\n",
            "step 7347: generator_loss=2.922719955444336, discriminator_loss=0.05381738394498825\n",
            "step 7348: generator_loss=2.85806941986084, discriminator_loss=0.05572338402271271\n",
            "step 7349: generator_loss=2.8436050415039062, discriminator_loss=0.05618647485971451\n",
            "step 7350: generator_loss=2.815639019012451, discriminator_loss=0.056609995663166046\n",
            "step 7351: generator_loss=2.8552541732788086, discriminator_loss=0.05584162101149559\n",
            "step 7352: generator_loss=2.9061689376831055, discriminator_loss=0.054863281548023224\n",
            "step 7353: generator_loss=2.9574403762817383, discriminator_loss=0.054536208510398865\n",
            "step 7354: generator_loss=2.922793388366699, discriminator_loss=0.05505788326263428\n",
            "step 7355: generator_loss=2.896961212158203, discriminator_loss=0.056005340069532394\n",
            "step 7356: generator_loss=2.887260913848877, discriminator_loss=0.05679985135793686\n",
            "step 7357: generator_loss=2.8221426010131836, discriminator_loss=0.05871755629777908\n",
            "step 7358: generator_loss=2.8001856803894043, discriminator_loss=0.059804920107126236\n",
            "step 7359: generator_loss=2.793837070465088, discriminator_loss=0.06066158786416054\n",
            "step 7360: generator_loss=2.763082504272461, discriminator_loss=0.06170765683054924\n",
            "step 7361: generator_loss=2.7574076652526855, discriminator_loss=0.06244390457868576\n",
            "step 7362: generator_loss=2.7537894248962402, discriminator_loss=0.0630563348531723\n",
            "step 7363: generator_loss=2.729433536529541, discriminator_loss=0.06442604959011078\n",
            "step 7364: generator_loss=2.69775390625, discriminator_loss=0.06627924740314484\n",
            "step 7365: generator_loss=2.6651997566223145, discriminator_loss=0.06796631217002869\n",
            "step 7366: generator_loss=2.639077663421631, discriminator_loss=0.06981398910284042\n",
            "step 7367: generator_loss=2.5963354110717773, discriminator_loss=0.07197785377502441\n",
            "step 7368: generator_loss=2.5595040321350098, discriminator_loss=0.07433032989501953\n",
            "step 7369: generator_loss=2.522144317626953, discriminator_loss=0.07666735351085663\n",
            "step 7370: generator_loss=2.500917911529541, discriminator_loss=0.0786205381155014\n",
            "step 7371: generator_loss=2.521430015563965, discriminator_loss=0.07851830869913101\n",
            "step 7372: generator_loss=2.595278263092041, discriminator_loss=0.07654775679111481\n",
            "step 7373: generator_loss=2.6754255294799805, discriminator_loss=0.07435077428817749\n",
            "step 7374: generator_loss=2.793721914291382, discriminator_loss=0.0709948018193245\n",
            "step 7375: generator_loss=2.9009342193603516, discriminator_loss=0.0682438537478447\n",
            "step 7376: generator_loss=2.9600725173950195, discriminator_loss=0.06668603420257568\n",
            "step 7377: generator_loss=2.9859323501586914, discriminator_loss=0.06592363864183426\n",
            "step 7378: generator_loss=2.9763906002044678, discriminator_loss=0.06570648401975632\n",
            "step 7379: generator_loss=2.9416723251342773, discriminator_loss=0.0662882924079895\n",
            "step 7380: generator_loss=2.88016939163208, discriminator_loss=0.06725245714187622\n",
            "step 7381: generator_loss=2.8514206409454346, discriminator_loss=0.06723629683256149\n",
            "step 7382: generator_loss=2.8847553730010986, discriminator_loss=0.06527760624885559\n",
            "step 7383: generator_loss=2.9459569454193115, discriminator_loss=0.06257909536361694\n",
            "step 7384: generator_loss=3.079357624053955, discriminator_loss=0.05807219445705414\n",
            "step 7385: generator_loss=3.242485284805298, discriminator_loss=0.053216852247714996\n",
            "step 7386: generator_loss=3.4098434448242188, discriminator_loss=0.049167245626449585\n",
            "step 7387: generator_loss=3.5957233905792236, discriminator_loss=0.04505607485771179\n",
            "step 7388: generator_loss=3.744887590408325, discriminator_loss=0.04186316207051277\n",
            "step 7389: generator_loss=3.861949920654297, discriminator_loss=0.039238594472408295\n",
            "step 7390: generator_loss=3.928098678588867, discriminator_loss=0.037451546639204025\n",
            "step 7391: generator_loss=3.9490461349487305, discriminator_loss=0.0360441654920578\n",
            "step 7392: generator_loss=3.947770833969116, discriminator_loss=0.03489181026816368\n",
            "step 7393: generator_loss=3.9157955646514893, discriminator_loss=0.03418480232357979\n",
            "step 7394: generator_loss=3.8939263820648193, discriminator_loss=0.0333947129547596\n",
            "step 7395: generator_loss=3.8478429317474365, discriminator_loss=0.03313921391963959\n",
            "step 7396: generator_loss=3.7850632667541504, discriminator_loss=0.0331411138176918\n",
            "step 7397: generator_loss=3.7615513801574707, discriminator_loss=0.03297291323542595\n",
            "step 7398: generator_loss=3.6592671871185303, discriminator_loss=0.03385505452752113\n",
            "step 7399: generator_loss=3.559713363647461, discriminator_loss=0.034932829439640045\n",
            "step 7400: generator_loss=3.499347686767578, discriminator_loss=0.035662513226270676\n",
            "step 7401: generator_loss=3.4742653369903564, discriminator_loss=0.03584226220846176\n",
            "step 7402: generator_loss=3.4542412757873535, discriminator_loss=0.03600730746984482\n",
            "step 7403: generator_loss=3.459618091583252, discriminator_loss=0.035788267850875854\n",
            "step 7404: generator_loss=3.4454259872436523, discriminator_loss=0.036012496799230576\n",
            "step 7405: generator_loss=3.4403023719787598, discriminator_loss=0.03609676659107208\n",
            "step 7406: generator_loss=3.460089683532715, discriminator_loss=0.03573373705148697\n",
            "step 7407: generator_loss=3.494051218032837, discriminator_loss=0.03520387038588524\n",
            "step 7408: generator_loss=3.521235942840576, discriminator_loss=0.03488333895802498\n",
            "step 7409: generator_loss=3.574019193649292, discriminator_loss=0.034136537462472916\n",
            "step 7410: generator_loss=3.603339672088623, discriminator_loss=0.033765483647584915\n",
            "step 7411: generator_loss=3.6242666244506836, discriminator_loss=0.033528778702020645\n",
            "step 7412: generator_loss=3.632619857788086, discriminator_loss=0.03350830078125\n",
            "step 7413: generator_loss=3.611992120742798, discriminator_loss=0.03374410420656204\n",
            "step 7414: generator_loss=3.5628268718719482, discriminator_loss=0.0345124751329422\n",
            "step 7415: generator_loss=3.4826462268829346, discriminator_loss=0.035773925483226776\n",
            "step 7416: generator_loss=3.4319260120391846, discriminator_loss=0.036673061549663544\n",
            "step 7417: generator_loss=3.4570865631103516, discriminator_loss=0.03626209869980812\n",
            "step 7418: generator_loss=3.4773778915405273, discriminator_loss=0.03599610552191734\n",
            "step 7419: generator_loss=3.477236032485962, discriminator_loss=0.035991981625556946\n",
            "step 7420: generator_loss=3.447748899459839, discriminator_loss=0.036507148295640945\n",
            "step 7421: generator_loss=3.3820786476135254, discriminator_loss=0.03756258636713028\n",
            "step 7422: generator_loss=3.3022470474243164, discriminator_loss=0.03906955569982529\n",
            "step 7423: generator_loss=3.263485908508301, discriminator_loss=0.03977537155151367\n",
            "step 7424: generator_loss=3.252643346786499, discriminator_loss=0.0400054007768631\n",
            "step 7425: generator_loss=3.256014585494995, discriminator_loss=0.0399889200925827\n",
            "step 7426: generator_loss=3.2676875591278076, discriminator_loss=0.03978331387042999\n",
            "step 7427: generator_loss=3.339620590209961, discriminator_loss=0.039175424724817276\n",
            "step 7428: generator_loss=3.2966349124908447, discriminator_loss=0.039307352155447006\n",
            "step 7429: generator_loss=3.3127925395965576, discriminator_loss=0.03904617577791214\n",
            "step 7430: generator_loss=3.3411238193511963, discriminator_loss=0.038596220314502716\n",
            "step 7431: generator_loss=3.4685516357421875, discriminator_loss=0.036414287984371185\n",
            "step 7432: generator_loss=3.5630040168762207, discriminator_loss=0.035087987780570984\n",
            "step 7433: generator_loss=3.61224627494812, discriminator_loss=0.03437744081020355\n",
            "step 7434: generator_loss=3.612126350402832, discriminator_loss=0.03443093225359917\n",
            "step 7435: generator_loss=3.5664031505584717, discriminator_loss=0.03508128225803375\n",
            "step 7436: generator_loss=3.479595899581909, discriminator_loss=0.03636452928185463\n",
            "step 7437: generator_loss=3.424981117248535, discriminator_loss=0.037276722490787506\n",
            "step 7438: generator_loss=3.3828303813934326, discriminator_loss=0.03798222914338112\n",
            "step 7439: generator_loss=3.3461954593658447, discriminator_loss=0.038664139807224274\n",
            "step 7440: generator_loss=3.3426923751831055, discriminator_loss=0.038634996861219406\n",
            "step 7441: generator_loss=3.368999481201172, discriminator_loss=0.03825521841645241\n",
            "step 7442: generator_loss=3.4592456817626953, discriminator_loss=0.036711692810058594\n",
            "step 7443: generator_loss=3.5082216262817383, discriminator_loss=0.03575617074966431\n",
            "step 7444: generator_loss=3.592130422592163, discriminator_loss=0.03445950150489807\n",
            "step 7445: generator_loss=3.7053298950195312, discriminator_loss=0.032803863286972046\n",
            "step 7446: generator_loss=3.805837631225586, discriminator_loss=0.03147083893418312\n",
            "step 7447: generator_loss=3.856449842453003, discriminator_loss=0.03067830763757229\n",
            "step 7448: generator_loss=3.8788602352142334, discriminator_loss=0.030303670093417168\n",
            "step 7449: generator_loss=3.8454227447509766, discriminator_loss=0.03041898086667061\n",
            "step 7450: generator_loss=3.785292863845825, discriminator_loss=0.03084636852145195\n",
            "step 7451: generator_loss=3.734300374984741, discriminator_loss=0.03127160668373108\n",
            "step 7452: generator_loss=3.676325798034668, discriminator_loss=0.03179854527115822\n",
            "step 7453: generator_loss=3.608962059020996, discriminator_loss=0.032456833869218826\n",
            "step 7454: generator_loss=3.5337977409362793, discriminator_loss=0.033410754054784775\n",
            "step 7455: generator_loss=3.460935592651367, discriminator_loss=0.034358520060777664\n",
            "step 7456: generator_loss=3.455368995666504, discriminator_loss=0.03429225832223892\n",
            "step 7457: generator_loss=3.4426536560058594, discriminator_loss=0.03437507897615433\n",
            "step 7458: generator_loss=3.4348409175872803, discriminator_loss=0.034361474215984344\n",
            "step 7459: generator_loss=3.429014205932617, discriminator_loss=0.03437734395265579\n",
            "step 7460: generator_loss=3.419841766357422, discriminator_loss=0.03443581610918045\n",
            "step 7461: generator_loss=3.3984127044677734, discriminator_loss=0.03468985855579376\n",
            "step 7462: generator_loss=3.390429973602295, discriminator_loss=0.03472336381673813\n",
            "step 7463: generator_loss=3.349954843521118, discriminator_loss=0.03546189144253731\n",
            "step 7464: generator_loss=3.3149547576904297, discriminator_loss=0.036010801792144775\n",
            "step 7465: generator_loss=3.281508207321167, discriminator_loss=0.036611735820770264\n",
            "step 7466: generator_loss=3.270124912261963, discriminator_loss=0.0369676798582077\n",
            "step 7467: generator_loss=3.3212203979492188, discriminator_loss=0.03599831461906433\n",
            "step 7468: generator_loss=3.384579658508301, discriminator_loss=0.03484766185283661\n",
            "step 7469: generator_loss=3.455348253250122, discriminator_loss=0.03370634838938713\n",
            "step 7470: generator_loss=3.5013818740844727, discriminator_loss=0.03315885737538338\n",
            "step 7471: generator_loss=3.5185437202453613, discriminator_loss=0.03314044699072838\n",
            "step 7472: generator_loss=3.4941225051879883, discriminator_loss=0.03409780561923981\n",
            "step 7473: generator_loss=3.5048487186431885, discriminator_loss=0.03366293013095856\n",
            "step 7474: generator_loss=3.4820261001586914, discriminator_loss=0.03431878238916397\n",
            "step 7475: generator_loss=3.3382091522216797, discriminator_loss=0.03907783329486847\n",
            "step 7476: generator_loss=3.3559064865112305, discriminator_loss=0.03738337382674217\n",
            "step 7477: generator_loss=3.19374942779541, discriminator_loss=0.04401438310742378\n",
            "step 7478: generator_loss=3.1991567611694336, discriminator_loss=0.0440654493868351\n",
            "step 7479: generator_loss=3.142087459564209, discriminator_loss=0.05118007957935333\n",
            "step 7480: generator_loss=3.1535513401031494, discriminator_loss=0.05421752855181694\n",
            "step 7481: generator_loss=3.121401071548462, discriminator_loss=0.06679613143205643\n",
            "step 7482: generator_loss=3.152315378189087, discriminator_loss=0.07272763550281525\n",
            "step 7483: generator_loss=3.2909889221191406, discriminator_loss=0.07420257478952408\n",
            "step 7484: generator_loss=3.260850429534912, discriminator_loss=0.09152372181415558\n",
            "step 7485: generator_loss=3.6545186042785645, discriminator_loss=0.07252032309770584\n",
            "step 7486: generator_loss=3.676229953765869, discriminator_loss=0.08454255014657974\n",
            "step 7487: generator_loss=4.162083625793457, discriminator_loss=0.06197986751794815\n",
            "step 7488: generator_loss=4.344888210296631, discriminator_loss=0.054213520139455795\n",
            "step 7489: generator_loss=4.806219577789307, discriminator_loss=0.041526176035404205\n",
            "step 7490: generator_loss=4.859822750091553, discriminator_loss=0.03979681804776192\n",
            "step 7491: generator_loss=5.472686290740967, discriminator_loss=0.034229379147291183\n",
            "step 7492: generator_loss=5.967636585235596, discriminator_loss=0.028965242207050323\n",
            "step 7493: generator_loss=6.050333023071289, discriminator_loss=0.028495831415057182\n",
            "step 7494: generator_loss=5.83068323135376, discriminator_loss=0.030903808772563934\n",
            "step 7495: generator_loss=6.109428882598877, discriminator_loss=0.02737407013773918\n",
            "step 7496: generator_loss=5.938057899475098, discriminator_loss=0.027862288057804108\n",
            "step 7497: generator_loss=5.920465469360352, discriminator_loss=0.02729179710149765\n",
            "step 7498: generator_loss=5.908622741699219, discriminator_loss=0.026370041072368622\n",
            "step 7499: generator_loss=5.331963539123535, discriminator_loss=0.029736291617155075\n",
            "step 7500: generator_loss=5.527673721313477, discriminator_loss=0.028626345098018646\n",
            "step 7501: generator_loss=5.218390464782715, discriminator_loss=0.031717631965875626\n",
            "step 7502: generator_loss=4.998590469360352, discriminator_loss=0.034336864948272705\n",
            "step 7503: generator_loss=4.961185932159424, discriminator_loss=0.03395979106426239\n",
            "step 7504: generator_loss=5.024324893951416, discriminator_loss=0.031787317246198654\n",
            "step 7505: generator_loss=4.828974723815918, discriminator_loss=0.03281282261013985\n",
            "step 7506: generator_loss=4.495691776275635, discriminator_loss=0.03448554128408432\n",
            "step 7507: generator_loss=4.469400405883789, discriminator_loss=0.03366599977016449\n",
            "step 7508: generator_loss=4.64298677444458, discriminator_loss=0.02972772903740406\n",
            "step 7509: generator_loss=4.639758110046387, discriminator_loss=0.02739306166768074\n",
            "step 7510: generator_loss=4.289342880249023, discriminator_loss=0.029528765007853508\n",
            "step 7511: generator_loss=4.491949558258057, discriminator_loss=0.02579157054424286\n",
            "step 7512: generator_loss=4.5407538414001465, discriminator_loss=0.024628935381770134\n",
            "step 7513: generator_loss=4.718398094177246, discriminator_loss=0.023751266300678253\n",
            "step 7514: generator_loss=4.361086845397949, discriminator_loss=0.024962004274129868\n",
            "step 7515: generator_loss=4.395226001739502, discriminator_loss=0.024602174758911133\n",
            "step 7516: generator_loss=4.223862648010254, discriminator_loss=0.026119600981473923\n",
            "step 7517: generator_loss=4.155665397644043, discriminator_loss=0.02603326179087162\n",
            "step 7518: generator_loss=3.8656020164489746, discriminator_loss=0.030180340632796288\n",
            "step 7519: generator_loss=3.7725863456726074, discriminator_loss=0.030631430447101593\n",
            "step 7520: generator_loss=3.7202820777893066, discriminator_loss=0.03283149376511574\n",
            "step 7521: generator_loss=3.496154546737671, discriminator_loss=0.036209769546985626\n",
            "step 7522: generator_loss=3.533194065093994, discriminator_loss=0.033417362719774246\n",
            "step 7523: generator_loss=3.447640895843506, discriminator_loss=0.03788744658231735\n",
            "step 7524: generator_loss=3.4697160720825195, discriminator_loss=0.03544500470161438\n",
            "step 7525: generator_loss=3.223512649536133, discriminator_loss=0.0387960821390152\n",
            "step 7526: generator_loss=3.3870294094085693, discriminator_loss=0.03637459874153137\n",
            "step 7527: generator_loss=3.3145532608032227, discriminator_loss=0.03649284690618515\n",
            "step 7528: generator_loss=3.56815767288208, discriminator_loss=0.03424379602074623\n",
            "step 7529: generator_loss=3.4360620975494385, discriminator_loss=0.03492105007171631\n",
            "step 7530: generator_loss=3.424363136291504, discriminator_loss=0.034473590552806854\n",
            "step 7531: generator_loss=3.4975626468658447, discriminator_loss=0.03382943570613861\n",
            "step 7532: generator_loss=3.4265193939208984, discriminator_loss=0.03472035750746727\n",
            "step 7533: generator_loss=3.4074134826660156, discriminator_loss=0.035332418978214264\n",
            "step 7534: generator_loss=3.3414323329925537, discriminator_loss=0.03669501468539238\n",
            "step 7535: generator_loss=3.377511978149414, discriminator_loss=0.03694978356361389\n",
            "step 7536: generator_loss=3.3923754692077637, discriminator_loss=0.038043949753046036\n",
            "step 7537: generator_loss=3.2147741317749023, discriminator_loss=0.03995146229863167\n",
            "step 7538: generator_loss=3.2748632431030273, discriminator_loss=0.04041995108127594\n",
            "step 7539: generator_loss=3.2871789932250977, discriminator_loss=0.041281186044216156\n",
            "step 7540: generator_loss=3.116891622543335, discriminator_loss=0.0428982675075531\n",
            "step 7541: generator_loss=3.116880416870117, discriminator_loss=0.04369942843914032\n",
            "step 7542: generator_loss=3.087397336959839, discriminator_loss=0.04405025392770767\n",
            "step 7543: generator_loss=3.12839412689209, discriminator_loss=0.04342307895421982\n",
            "step 7544: generator_loss=3.1005873680114746, discriminator_loss=0.04399467259645462\n",
            "step 7545: generator_loss=3.1713521480560303, discriminator_loss=0.043616242706775665\n",
            "step 7546: generator_loss=3.069489002227783, discriminator_loss=0.0456189326941967\n",
            "step 7547: generator_loss=3.094956159591675, discriminator_loss=0.04565669596195221\n",
            "step 7548: generator_loss=3.1139800548553467, discriminator_loss=0.045874930918216705\n",
            "step 7549: generator_loss=3.1182126998901367, discriminator_loss=0.04646383970975876\n",
            "step 7550: generator_loss=3.0793657302856445, discriminator_loss=0.04787357896566391\n",
            "step 7551: generator_loss=3.0687947273254395, discriminator_loss=0.048835985362529755\n",
            "step 7552: generator_loss=3.0413713455200195, discriminator_loss=0.04984607547521591\n",
            "step 7553: generator_loss=3.077463388442993, discriminator_loss=0.04950173199176788\n",
            "step 7554: generator_loss=3.0878617763519287, discriminator_loss=0.04971201717853546\n",
            "step 7555: generator_loss=3.0650370121002197, discriminator_loss=0.05057822912931442\n",
            "step 7556: generator_loss=3.0351858139038086, discriminator_loss=0.05169113725423813\n",
            "step 7557: generator_loss=3.01336407661438, discriminator_loss=0.052352990955114365\n",
            "step 7558: generator_loss=3.0391862392425537, discriminator_loss=0.05174431949853897\n",
            "step 7559: generator_loss=3.0948879718780518, discriminator_loss=0.05030040442943573\n",
            "step 7560: generator_loss=3.1725993156433105, discriminator_loss=0.04832504689693451\n",
            "step 7561: generator_loss=3.264878749847412, discriminator_loss=0.04610127955675125\n",
            "step 7562: generator_loss=3.3843207359313965, discriminator_loss=0.0434742271900177\n",
            "step 7563: generator_loss=3.452874183654785, discriminator_loss=0.04170073941349983\n",
            "step 7564: generator_loss=3.492771863937378, discriminator_loss=0.0405154824256897\n",
            "step 7565: generator_loss=3.4966883659362793, discriminator_loss=0.03990103676915169\n",
            "step 7566: generator_loss=3.50068998336792, discriminator_loss=0.039107806980609894\n",
            "step 7567: generator_loss=3.492161750793457, discriminator_loss=0.03865046054124832\n",
            "step 7568: generator_loss=3.4459404945373535, discriminator_loss=0.038723547011613846\n",
            "step 7569: generator_loss=3.3794195652008057, discriminator_loss=0.03916264325380325\n",
            "step 7570: generator_loss=3.3277993202209473, discriminator_loss=0.03946956619620323\n",
            "step 7571: generator_loss=3.310167074203491, discriminator_loss=0.039252910763025284\n",
            "step 7572: generator_loss=3.418855905532837, discriminator_loss=0.0367407500743866\n",
            "step 7573: generator_loss=3.5480282306671143, discriminator_loss=0.034212592989206314\n",
            "step 7574: generator_loss=3.63887882232666, discriminator_loss=0.03246106207370758\n",
            "step 7575: generator_loss=3.6864356994628906, discriminator_loss=0.0313897430896759\n",
            "step 7576: generator_loss=3.691314697265625, discriminator_loss=0.03086831048130989\n",
            "step 7577: generator_loss=3.6608614921569824, discriminator_loss=0.03083179146051407\n",
            "step 7578: generator_loss=3.5991694927215576, discriminator_loss=0.03130602836608887\n",
            "step 7579: generator_loss=3.515265703201294, discriminator_loss=0.03223920613527298\n",
            "step 7580: generator_loss=3.5209622383117676, discriminator_loss=0.031787123531103134\n",
            "step 7581: generator_loss=3.5198516845703125, discriminator_loss=0.031595051288604736\n",
            "step 7582: generator_loss=3.5019776821136475, discriminator_loss=0.03170750290155411\n",
            "step 7583: generator_loss=3.4663989543914795, discriminator_loss=0.032103411853313446\n",
            "step 7584: generator_loss=3.4105396270751953, discriminator_loss=0.03286480903625488\n",
            "step 7585: generator_loss=3.3262572288513184, discriminator_loss=0.03430381417274475\n",
            "step 7586: generator_loss=3.2183947563171387, discriminator_loss=0.036404505372047424\n",
            "step 7587: generator_loss=3.1495800018310547, discriminator_loss=0.03781690448522568\n",
            "step 7588: generator_loss=3.1389026641845703, discriminator_loss=0.03816705197095871\n",
            "step 7589: generator_loss=3.1940555572509766, discriminator_loss=0.03704939782619476\n",
            "step 7590: generator_loss=3.2982940673828125, discriminator_loss=0.035105206072330475\n",
            "step 7591: generator_loss=3.3943538665771484, discriminator_loss=0.033577099442481995\n",
            "step 7592: generator_loss=3.4508373737335205, discriminator_loss=0.032848209142684937\n",
            "step 7593: generator_loss=3.4673125743865967, discriminator_loss=0.03282981365919113\n",
            "step 7594: generator_loss=3.4812161922454834, discriminator_loss=0.032807864248752594\n",
            "step 7595: generator_loss=3.511583089828491, discriminator_loss=0.03249482437968254\n",
            "step 7596: generator_loss=3.564389228820801, discriminator_loss=0.03189674764871597\n",
            "step 7597: generator_loss=3.574068784713745, discriminator_loss=0.03186914324760437\n",
            "step 7598: generator_loss=3.5424060821533203, discriminator_loss=0.03247830271720886\n",
            "step 7599: generator_loss=3.5129787921905518, discriminator_loss=0.03296360373497009\n",
            "step 7600: generator_loss=3.5004634857177734, discriminator_loss=0.0331822894513607\n",
            "step 7601: generator_loss=3.4925618171691895, discriminator_loss=0.033286578953266144\n",
            "step 7602: generator_loss=3.5167360305786133, discriminator_loss=0.03290285915136337\n",
            "step 7603: generator_loss=3.5088205337524414, discriminator_loss=0.032969750463962555\n",
            "step 7604: generator_loss=3.4901254177093506, discriminator_loss=0.0331694595515728\n",
            "step 7605: generator_loss=3.4595208168029785, discriminator_loss=0.033535659313201904\n",
            "step 7606: generator_loss=3.435074806213379, discriminator_loss=0.03383594751358032\n",
            "step 7607: generator_loss=3.4287285804748535, discriminator_loss=0.03383854776620865\n",
            "step 7608: generator_loss=3.403373956680298, discriminator_loss=0.034101828932762146\n",
            "step 7609: generator_loss=3.357332944869995, discriminator_loss=0.03481149673461914\n",
            "step 7610: generator_loss=3.306065320968628, discriminator_loss=0.035728272050619125\n",
            "step 7611: generator_loss=3.2465834617614746, discriminator_loss=0.036780621856451035\n",
            "step 7612: generator_loss=3.178142547607422, discriminator_loss=0.0381479412317276\n",
            "step 7613: generator_loss=3.137887477874756, discriminator_loss=0.039041563868522644\n",
            "step 7614: generator_loss=3.1293647289276123, discriminator_loss=0.03935406357049942\n",
            "step 7615: generator_loss=3.1481943130493164, discriminator_loss=0.03898990899324417\n",
            "step 7616: generator_loss=3.1848044395446777, discriminator_loss=0.03834211081266403\n",
            "step 7617: generator_loss=3.2297492027282715, discriminator_loss=0.037568747997283936\n",
            "step 7618: generator_loss=3.273894786834717, discriminator_loss=0.03685108572244644\n",
            "step 7619: generator_loss=3.3230268955230713, discriminator_loss=0.036064326763153076\n",
            "step 7620: generator_loss=3.3512628078460693, discriminator_loss=0.03576681762933731\n",
            "step 7621: generator_loss=3.3725898265838623, discriminator_loss=0.03560256212949753\n",
            "step 7622: generator_loss=3.3670003414154053, discriminator_loss=0.03582727909088135\n",
            "step 7623: generator_loss=3.3475329875946045, discriminator_loss=0.03632534295320511\n",
            "step 7624: generator_loss=3.321427345275879, discriminator_loss=0.036896638572216034\n",
            "step 7625: generator_loss=3.3024954795837402, discriminator_loss=0.03740234673023224\n",
            "step 7626: generator_loss=3.2866523265838623, discriminator_loss=0.03791071102023125\n",
            "step 7627: generator_loss=3.2862980365753174, discriminator_loss=0.03801346570253372\n",
            "step 7628: generator_loss=3.280834436416626, discriminator_loss=0.03819769620895386\n",
            "step 7629: generator_loss=3.2910141944885254, discriminator_loss=0.03812377527356148\n",
            "step 7630: generator_loss=3.3086957931518555, discriminator_loss=0.037787798792123795\n",
            "step 7631: generator_loss=3.32668399810791, discriminator_loss=0.037549734115600586\n",
            "step 7632: generator_loss=3.361953020095825, discriminator_loss=0.03685513883829117\n",
            "step 7633: generator_loss=3.3736636638641357, discriminator_loss=0.03662882745265961\n",
            "step 7634: generator_loss=3.3529019355773926, discriminator_loss=0.036858346313238144\n",
            "step 7635: generator_loss=3.311945676803589, discriminator_loss=0.0376223586499691\n",
            "step 7636: generator_loss=3.2743051052093506, discriminator_loss=0.03821129351854324\n",
            "step 7637: generator_loss=3.25449538230896, discriminator_loss=0.03857692331075668\n",
            "step 7638: generator_loss=3.256850242614746, discriminator_loss=0.03847818821668625\n",
            "step 7639: generator_loss=3.285465717315674, discriminator_loss=0.037903860211372375\n",
            "step 7640: generator_loss=3.3054423332214355, discriminator_loss=0.03752174228429794\n",
            "step 7641: generator_loss=3.365241527557373, discriminator_loss=0.036425117403268814\n",
            "step 7642: generator_loss=3.4161393642425537, discriminator_loss=0.03561538830399513\n",
            "step 7643: generator_loss=3.4258604049682617, discriminator_loss=0.035478994250297546\n",
            "step 7644: generator_loss=3.407822370529175, discriminator_loss=0.03577445074915886\n",
            "step 7645: generator_loss=3.375666618347168, discriminator_loss=0.036228910088539124\n",
            "step 7646: generator_loss=3.320063352584839, discriminator_loss=0.0372115895152092\n",
            "step 7647: generator_loss=3.2944154739379883, discriminator_loss=0.03760321065783501\n",
            "step 7648: generator_loss=3.245563507080078, discriminator_loss=0.038483552634716034\n",
            "step 7649: generator_loss=3.206916332244873, discriminator_loss=0.039251554757356644\n",
            "step 7650: generator_loss=3.1751046180725098, discriminator_loss=0.039929959923028946\n",
            "step 7651: generator_loss=3.185826539993286, discriminator_loss=0.039617329835891724\n",
            "step 7652: generator_loss=3.2060985565185547, discriminator_loss=0.03916848450899124\n",
            "step 7653: generator_loss=3.2465200424194336, discriminator_loss=0.03840517997741699\n",
            "step 7654: generator_loss=3.3431649208068848, discriminator_loss=0.036532290279865265\n",
            "step 7655: generator_loss=3.4357151985168457, discriminator_loss=0.03497200459241867\n",
            "step 7656: generator_loss=3.5081474781036377, discriminator_loss=0.033738598227500916\n",
            "step 7657: generator_loss=3.5785276889801025, discriminator_loss=0.0327075757086277\n",
            "step 7658: generator_loss=3.6381142139434814, discriminator_loss=0.03170380741357803\n",
            "step 7659: generator_loss=3.699589490890503, discriminator_loss=0.030845854431390762\n",
            "step 7660: generator_loss=3.739967107772827, discriminator_loss=0.030180077999830246\n",
            "step 7661: generator_loss=3.7454702854156494, discriminator_loss=0.029949413612484932\n",
            "step 7662: generator_loss=3.730206251144409, discriminator_loss=0.02988927997648716\n",
            "step 7663: generator_loss=3.7001829147338867, discriminator_loss=0.030020054429769516\n",
            "step 7664: generator_loss=3.624659538269043, discriminator_loss=0.030722767114639282\n",
            "step 7665: generator_loss=3.5912137031555176, discriminator_loss=0.030993390828371048\n",
            "step 7666: generator_loss=3.5415244102478027, discriminator_loss=0.031455472111701965\n",
            "step 7667: generator_loss=3.5133326053619385, discriminator_loss=0.03157918155193329\n",
            "step 7668: generator_loss=3.4914162158966064, discriminator_loss=0.03173559159040451\n",
            "step 7669: generator_loss=3.4692046642303467, discriminator_loss=0.03181725740432739\n",
            "step 7670: generator_loss=3.4532463550567627, discriminator_loss=0.03193476423621178\n",
            "step 7671: generator_loss=3.44326114654541, discriminator_loss=0.03194013983011246\n",
            "step 7672: generator_loss=3.4549717903137207, discriminator_loss=0.03164442256093025\n",
            "step 7673: generator_loss=3.4699490070343018, discriminator_loss=0.031288765370845795\n",
            "step 7674: generator_loss=3.4995205402374268, discriminator_loss=0.030750280246138573\n",
            "step 7675: generator_loss=3.5552263259887695, discriminator_loss=0.029837291687726974\n",
            "step 7676: generator_loss=3.5979537963867188, discriminator_loss=0.02918219193816185\n",
            "step 7677: generator_loss=3.6299691200256348, discriminator_loss=0.028726104646921158\n",
            "step 7678: generator_loss=3.6462416648864746, discriminator_loss=0.028491701930761337\n",
            "step 7679: generator_loss=3.6399643421173096, discriminator_loss=0.02852727100253105\n",
            "step 7680: generator_loss=3.6055891513824463, discriminator_loss=0.029001586139202118\n",
            "step 7681: generator_loss=3.5439326763153076, discriminator_loss=0.029887355864048004\n",
            "step 7682: generator_loss=3.492424488067627, discriminator_loss=0.03074868582189083\n",
            "step 7683: generator_loss=3.479017496109009, discriminator_loss=0.030966125428676605\n",
            "step 7684: generator_loss=3.484947681427002, discriminator_loss=0.0309382863342762\n",
            "step 7685: generator_loss=3.532834053039551, discriminator_loss=0.030289074406027794\n",
            "step 7686: generator_loss=3.59342622756958, discriminator_loss=0.029413994401693344\n",
            "step 7687: generator_loss=3.670436382293701, discriminator_loss=0.028449390083551407\n",
            "step 7688: generator_loss=3.7223310470581055, discriminator_loss=0.027780137956142426\n",
            "step 7689: generator_loss=3.7303969860076904, discriminator_loss=0.02772611752152443\n",
            "step 7690: generator_loss=3.7687015533447266, discriminator_loss=0.027228061109781265\n",
            "step 7691: generator_loss=3.765596389770508, discriminator_loss=0.02721770480275154\n",
            "step 7692: generator_loss=3.746743679046631, discriminator_loss=0.027411574497818947\n",
            "step 7693: generator_loss=3.7225873470306396, discriminator_loss=0.027609672397375107\n",
            "step 7694: generator_loss=3.6696600914001465, discriminator_loss=0.028190527111291885\n",
            "step 7695: generator_loss=3.6139166355133057, discriminator_loss=0.028857199475169182\n",
            "step 7696: generator_loss=3.561366081237793, discriminator_loss=0.02953055500984192\n",
            "step 7697: generator_loss=3.5382802486419678, discriminator_loss=0.029770847409963608\n",
            "step 7698: generator_loss=3.5278091430664062, discriminator_loss=0.02984743006527424\n",
            "step 7699: generator_loss=3.518934726715088, discriminator_loss=0.02992890030145645\n",
            "step 7700: generator_loss=3.541398525238037, discriminator_loss=0.02953707054257393\n",
            "step 7701: generator_loss=3.6270439624786377, discriminator_loss=0.02832658588886261\n",
            "step 7702: generator_loss=3.703723430633545, discriminator_loss=0.027302492409944534\n",
            "step 7703: generator_loss=3.77323579788208, discriminator_loss=0.0264456607401371\n",
            "step 7704: generator_loss=3.846914052963257, discriminator_loss=0.025624612346291542\n",
            "step 7705: generator_loss=3.8640520572662354, discriminator_loss=0.02539980784058571\n",
            "step 7706: generator_loss=3.8311047554016113, discriminator_loss=0.02573450282216072\n",
            "step 7707: generator_loss=3.74200177192688, discriminator_loss=0.026752177625894547\n",
            "step 7708: generator_loss=3.706411361694336, discriminator_loss=0.02716437540948391\n",
            "step 7709: generator_loss=3.741157293319702, discriminator_loss=0.02666047029197216\n",
            "step 7710: generator_loss=3.759328842163086, discriminator_loss=0.026436109095811844\n",
            "step 7711: generator_loss=3.767465114593506, discriminator_loss=0.02624071016907692\n",
            "step 7712: generator_loss=3.7727816104888916, discriminator_loss=0.026151886209845543\n",
            "step 7713: generator_loss=3.8164327144622803, discriminator_loss=0.02557622641324997\n",
            "step 7714: generator_loss=3.835599899291992, discriminator_loss=0.025236178189516068\n",
            "step 7715: generator_loss=3.8326303958892822, discriminator_loss=0.025198306888341904\n",
            "step 7716: generator_loss=3.8562564849853516, discriminator_loss=0.024815350770950317\n",
            "step 7717: generator_loss=3.875208616256714, discriminator_loss=0.024517618119716644\n",
            "step 7718: generator_loss=3.888655662536621, discriminator_loss=0.02426776848733425\n",
            "step 7719: generator_loss=3.8860220909118652, discriminator_loss=0.024181514978408813\n",
            "step 7720: generator_loss=3.87261700630188, discriminator_loss=0.02420154958963394\n",
            "step 7721: generator_loss=3.8587541580200195, discriminator_loss=0.024241384118795395\n",
            "step 7722: generator_loss=3.8493714332580566, discriminator_loss=0.024215998128056526\n",
            "step 7723: generator_loss=3.806995391845703, discriminator_loss=0.02458915486931801\n",
            "step 7724: generator_loss=3.7546772956848145, discriminator_loss=0.02510472945868969\n",
            "step 7725: generator_loss=3.6840896606445312, discriminator_loss=0.025890439748764038\n",
            "step 7726: generator_loss=3.641392230987549, discriminator_loss=0.026369426399469376\n",
            "step 7727: generator_loss=3.607163190841675, discriminator_loss=0.02680383436381817\n",
            "step 7728: generator_loss=3.604919910430908, discriminator_loss=0.02676727995276451\n",
            "step 7729: generator_loss=3.634946346282959, discriminator_loss=0.026405751705169678\n",
            "step 7730: generator_loss=3.6898841857910156, discriminator_loss=0.02568432316184044\n",
            "step 7731: generator_loss=3.75449538230896, discriminator_loss=0.024903271347284317\n",
            "step 7732: generator_loss=3.8228917121887207, discriminator_loss=0.024133555591106415\n",
            "step 7733: generator_loss=3.9528281688690186, discriminator_loss=0.02281787246465683\n",
            "step 7734: generator_loss=4.071577072143555, discriminator_loss=0.021716618910431862\n",
            "step 7735: generator_loss=4.187166690826416, discriminator_loss=0.020763011649250984\n",
            "step 7736: generator_loss=4.296200275421143, discriminator_loss=0.01991729810833931\n",
            "step 7737: generator_loss=4.345473766326904, discriminator_loss=0.019544638693332672\n",
            "step 7738: generator_loss=4.328073978424072, discriminator_loss=0.01951609179377556\n",
            "step 7739: generator_loss=4.2389750480651855, discriminator_loss=0.020054448395967484\n",
            "step 7740: generator_loss=4.107150554656982, discriminator_loss=0.020916182547807693\n",
            "step 7741: generator_loss=3.948429822921753, discriminator_loss=0.022248800843954086\n",
            "step 7742: generator_loss=3.799046277999878, discriminator_loss=0.023680347949266434\n",
            "step 7743: generator_loss=3.6753008365631104, discriminator_loss=0.025072988122701645\n",
            "step 7744: generator_loss=3.6462464332580566, discriminator_loss=0.02539806440472603\n",
            "step 7745: generator_loss=3.6406753063201904, discriminator_loss=0.025378655642271042\n",
            "step 7746: generator_loss=3.615628242492676, discriminator_loss=0.025698699057102203\n",
            "step 7747: generator_loss=3.6000618934631348, discriminator_loss=0.025905055925250053\n",
            "step 7748: generator_loss=3.5976691246032715, discriminator_loss=0.025956589728593826\n",
            "step 7749: generator_loss=3.6052865982055664, discriminator_loss=0.02589164301753044\n",
            "step 7750: generator_loss=3.6757869720458984, discriminator_loss=0.024990204721689224\n",
            "step 7751: generator_loss=3.7415974140167236, discriminator_loss=0.024257611483335495\n",
            "step 7752: generator_loss=3.7939207553863525, discriminator_loss=0.023730047047138214\n",
            "step 7753: generator_loss=3.8468282222747803, discriminator_loss=0.023236937820911407\n",
            "step 7754: generator_loss=3.9123001098632812, discriminator_loss=0.022668901830911636\n",
            "step 7755: generator_loss=3.996018409729004, discriminator_loss=0.02194269374012947\n",
            "step 7756: generator_loss=4.0802459716796875, discriminator_loss=0.02130756340920925\n",
            "step 7757: generator_loss=4.125435829162598, discriminator_loss=0.02096177637577057\n",
            "step 7758: generator_loss=4.11952018737793, discriminator_loss=0.021039722487330437\n",
            "step 7759: generator_loss=4.1243977546691895, discriminator_loss=0.020982008427381516\n",
            "step 7760: generator_loss=4.1078033447265625, discriminator_loss=0.02115292102098465\n",
            "step 7761: generator_loss=4.111731052398682, discriminator_loss=0.021056078374385834\n",
            "step 7762: generator_loss=4.127732276916504, discriminator_loss=0.020859643816947937\n",
            "step 7763: generator_loss=4.12348747253418, discriminator_loss=0.02087480016052723\n",
            "step 7764: generator_loss=4.088008403778076, discriminator_loss=0.021062232553958893\n",
            "step 7765: generator_loss=4.023200035095215, discriminator_loss=0.02154865860939026\n",
            "step 7766: generator_loss=3.987334966659546, discriminator_loss=0.02181641384959221\n",
            "step 7767: generator_loss=3.928499698638916, discriminator_loss=0.022271348163485527\n",
            "step 7768: generator_loss=3.9009063243865967, discriminator_loss=0.022455517202615738\n",
            "step 7769: generator_loss=3.8943049907684326, discriminator_loss=0.022468624636530876\n",
            "step 7770: generator_loss=3.876706600189209, discriminator_loss=0.022562485188245773\n",
            "step 7771: generator_loss=3.8520753383636475, discriminator_loss=0.022743914276361465\n",
            "step 7772: generator_loss=3.824693441390991, discriminator_loss=0.0229567252099514\n",
            "step 7773: generator_loss=3.7785964012145996, discriminator_loss=0.023404113948345184\n",
            "step 7774: generator_loss=3.782308340072632, discriminator_loss=0.02333245985209942\n",
            "step 7775: generator_loss=3.777496337890625, discriminator_loss=0.023354582488536835\n",
            "step 7776: generator_loss=3.758988618850708, discriminator_loss=0.02356739155948162\n",
            "step 7777: generator_loss=3.7347023487091064, discriminator_loss=0.023818381130695343\n",
            "step 7778: generator_loss=3.6916823387145996, discriminator_loss=0.024384167045354843\n",
            "step 7779: generator_loss=3.701103687286377, discriminator_loss=0.024299830198287964\n",
            "step 7780: generator_loss=3.7154250144958496, discriminator_loss=0.024151276797056198\n",
            "step 7781: generator_loss=3.7275474071502686, discriminator_loss=0.02405691146850586\n",
            "step 7782: generator_loss=3.7447659969329834, discriminator_loss=0.02391921356320381\n",
            "step 7783: generator_loss=3.755162239074707, discriminator_loss=0.023876668885350227\n",
            "step 7784: generator_loss=3.7777459621429443, discriminator_loss=0.0236629880964756\n",
            "step 7785: generator_loss=3.807222366333008, discriminator_loss=0.023434925824403763\n",
            "step 7786: generator_loss=3.815213441848755, discriminator_loss=0.023480119183659554\n",
            "step 7787: generator_loss=3.8078994750976562, discriminator_loss=0.023653171956539154\n",
            "step 7788: generator_loss=3.8045921325683594, discriminator_loss=0.023818178102374077\n",
            "step 7789: generator_loss=3.7949981689453125, discriminator_loss=0.02403055503964424\n",
            "step 7790: generator_loss=3.7629339694976807, discriminator_loss=0.02450587786734104\n",
            "step 7791: generator_loss=3.7602083683013916, discriminator_loss=0.024700133129954338\n",
            "step 7792: generator_loss=3.7843167781829834, discriminator_loss=0.024503588676452637\n",
            "step 7793: generator_loss=3.7631077766418457, discriminator_loss=0.024827823042869568\n",
            "step 7794: generator_loss=3.692720890045166, discriminator_loss=0.02580074779689312\n",
            "step 7795: generator_loss=3.60390567779541, discriminator_loss=0.027043823152780533\n",
            "step 7796: generator_loss=3.5340542793273926, discriminator_loss=0.028149591758847237\n",
            "step 7797: generator_loss=3.488039016723633, discriminator_loss=0.028942778706550598\n",
            "step 7798: generator_loss=3.435101270675659, discriminator_loss=0.029869187623262405\n",
            "step 7799: generator_loss=3.3756752014160156, discriminator_loss=0.031011100858449936\n",
            "step 7800: generator_loss=3.3183834552764893, discriminator_loss=0.03224638104438782\n",
            "step 7801: generator_loss=3.277249336242676, discriminator_loss=0.03320286422967911\n",
            "step 7802: generator_loss=3.245898485183716, discriminator_loss=0.03402946889400482\n",
            "step 7803: generator_loss=3.2191648483276367, discriminator_loss=0.034830622375011444\n",
            "step 7804: generator_loss=3.2171144485473633, discriminator_loss=0.035209521651268005\n",
            "step 7805: generator_loss=3.2722489833831787, discriminator_loss=0.03437625616788864\n",
            "step 7806: generator_loss=3.354083776473999, discriminator_loss=0.03322025388479233\n",
            "step 7807: generator_loss=3.5323357582092285, discriminator_loss=0.03058100864291191\n",
            "step 7808: generator_loss=3.677182674407959, discriminator_loss=0.028888672590255737\n",
            "step 7809: generator_loss=3.764300584793091, discriminator_loss=0.02800634875893593\n",
            "step 7810: generator_loss=3.8037848472595215, discriminator_loss=0.0277554914355278\n",
            "step 7811: generator_loss=3.8037302494049072, discriminator_loss=0.027907665818929672\n",
            "step 7812: generator_loss=3.769740104675293, discriminator_loss=0.028346391394734383\n",
            "step 7813: generator_loss=3.7083077430725098, discriminator_loss=0.029069364070892334\n",
            "step 7814: generator_loss=3.631784439086914, discriminator_loss=0.030041035264730453\n",
            "step 7815: generator_loss=3.548579216003418, discriminator_loss=0.031163349747657776\n",
            "step 7816: generator_loss=3.4222776889801025, discriminator_loss=0.03303183242678642\n",
            "step 7817: generator_loss=3.295778274536133, discriminator_loss=0.03530115634202957\n",
            "step 7818: generator_loss=3.1943671703338623, discriminator_loss=0.03733091801404953\n",
            "step 7819: generator_loss=3.1842901706695557, discriminator_loss=0.03760892525315285\n",
            "step 7820: generator_loss=3.369724988937378, discriminator_loss=0.034123338758945465\n",
            "step 7821: generator_loss=3.5656960010528564, discriminator_loss=0.031122606247663498\n",
            "step 7822: generator_loss=3.725757360458374, discriminator_loss=0.029132666066288948\n",
            "step 7823: generator_loss=3.8281490802764893, discriminator_loss=0.027997013181447983\n",
            "step 7824: generator_loss=3.906289577484131, discriminator_loss=0.027261393144726753\n",
            "step 7825: generator_loss=3.985764265060425, discriminator_loss=0.026477806270122528\n",
            "step 7826: generator_loss=4.007405757904053, discriminator_loss=0.02620575949549675\n",
            "step 7827: generator_loss=3.9566054344177246, discriminator_loss=0.02655465342104435\n",
            "step 7828: generator_loss=3.8806772232055664, discriminator_loss=0.027369733899831772\n",
            "step 7829: generator_loss=3.8025026321411133, discriminator_loss=0.027723584324121475\n",
            "step 7830: generator_loss=3.795546531677246, discriminator_loss=0.027530070394277573\n",
            "step 7831: generator_loss=3.751986503601074, discriminator_loss=0.027766983956098557\n",
            "step 7832: generator_loss=3.704075336456299, discriminator_loss=0.028056444600224495\n",
            "step 7833: generator_loss=3.681351900100708, discriminator_loss=0.028036803007125854\n",
            "step 7834: generator_loss=3.6154775619506836, discriminator_loss=0.028619423508644104\n",
            "step 7835: generator_loss=3.553447961807251, discriminator_loss=0.0292043499648571\n",
            "step 7836: generator_loss=3.4975461959838867, discriminator_loss=0.02981145679950714\n",
            "step 7837: generator_loss=3.4512343406677246, discriminator_loss=0.030353030189871788\n",
            "step 7838: generator_loss=3.449969530105591, discriminator_loss=0.030228666961193085\n",
            "step 7839: generator_loss=3.4681169986724854, discriminator_loss=0.029808631166815758\n",
            "step 7840: generator_loss=3.5104830265045166, discriminator_loss=0.02908271923661232\n",
            "step 7841: generator_loss=3.5278444290161133, discriminator_loss=0.028811898082494736\n",
            "step 7842: generator_loss=3.569227695465088, discriminator_loss=0.02825324609875679\n",
            "step 7843: generator_loss=3.6175334453582764, discriminator_loss=0.027589254081249237\n",
            "step 7844: generator_loss=3.6515817642211914, discriminator_loss=0.027170665562152863\n",
            "step 7845: generator_loss=3.671114683151245, discriminator_loss=0.027003314346075058\n",
            "step 7846: generator_loss=3.6769115924835205, discriminator_loss=0.02698611468076706\n",
            "step 7847: generator_loss=3.668015241622925, discriminator_loss=0.027174167335033417\n",
            "step 7848: generator_loss=3.619279384613037, discriminator_loss=0.02784423530101776\n",
            "step 7849: generator_loss=3.5246737003326416, discriminator_loss=0.029240258038043976\n",
            "step 7850: generator_loss=3.4543378353118896, discriminator_loss=0.03043174371123314\n",
            "step 7851: generator_loss=3.4607009887695312, discriminator_loss=0.030362067744135857\n",
            "step 7852: generator_loss=3.4959795475006104, discriminator_loss=0.02987479418516159\n",
            "step 7853: generator_loss=3.505927085876465, discriminator_loss=0.029710516333580017\n",
            "step 7854: generator_loss=3.4881255626678467, discriminator_loss=0.030076950788497925\n",
            "step 7855: generator_loss=3.4285831451416016, discriminator_loss=0.03108031302690506\n",
            "step 7856: generator_loss=3.3511533737182617, discriminator_loss=0.03250272199511528\n",
            "step 7857: generator_loss=3.310959815979004, discriminator_loss=0.03329271823167801\n",
            "step 7858: generator_loss=3.3266422748565674, discriminator_loss=0.0330854132771492\n",
            "step 7859: generator_loss=3.3533661365509033, discriminator_loss=0.03269064426422119\n",
            "step 7860: generator_loss=3.3887813091278076, discriminator_loss=0.03215362876653671\n",
            "step 7861: generator_loss=3.401841640472412, discriminator_loss=0.03200872614979744\n",
            "step 7862: generator_loss=3.391603469848633, discriminator_loss=0.03224140405654907\n",
            "step 7863: generator_loss=3.361360549926758, discriminator_loss=0.03279236704111099\n",
            "step 7864: generator_loss=3.3183929920196533, discriminator_loss=0.033694274723529816\n",
            "step 7865: generator_loss=3.2830350399017334, discriminator_loss=0.03447892516851425\n",
            "step 7866: generator_loss=3.260504722595215, discriminator_loss=0.0349731519818306\n",
            "step 7867: generator_loss=3.235567331314087, discriminator_loss=0.03563116118311882\n",
            "step 7868: generator_loss=3.2245028018951416, discriminator_loss=0.03606753796339035\n",
            "step 7869: generator_loss=3.206839084625244, discriminator_loss=0.036579012870788574\n",
            "step 7870: generator_loss=3.185403823852539, discriminator_loss=0.03728306666016579\n",
            "step 7871: generator_loss=3.191993236541748, discriminator_loss=0.03739580512046814\n",
            "step 7872: generator_loss=3.185067653656006, discriminator_loss=0.0377921462059021\n",
            "step 7873: generator_loss=3.1864213943481445, discriminator_loss=0.03812417387962341\n",
            "step 7874: generator_loss=3.183378219604492, discriminator_loss=0.03854122385382652\n",
            "step 7875: generator_loss=3.172888994216919, discriminator_loss=0.039094194769859314\n",
            "step 7876: generator_loss=3.1659207344055176, discriminator_loss=0.03973215073347092\n",
            "step 7877: generator_loss=3.1650781631469727, discriminator_loss=0.04019516333937645\n",
            "step 7878: generator_loss=3.1754002571105957, discriminator_loss=0.04031752794981003\n",
            "step 7879: generator_loss=3.2099270820617676, discriminator_loss=0.0400402694940567\n",
            "step 7880: generator_loss=3.2730324268341064, discriminator_loss=0.03916015475988388\n",
            "step 7881: generator_loss=3.343872308731079, discriminator_loss=0.0381111279129982\n",
            "step 7882: generator_loss=3.3954548835754395, discriminator_loss=0.037496041506528854\n",
            "step 7883: generator_loss=3.4280831813812256, discriminator_loss=0.037147924304008484\n",
            "step 7884: generator_loss=3.4247593879699707, discriminator_loss=0.03741990402340889\n",
            "step 7885: generator_loss=3.403977155685425, discriminator_loss=0.038030460476875305\n",
            "step 7886: generator_loss=3.3294270038604736, discriminator_loss=0.03908919543027878\n",
            "step 7887: generator_loss=3.3762118816375732, discriminator_loss=0.0382423996925354\n",
            "step 7888: generator_loss=3.4073214530944824, discriminator_loss=0.03759053722023964\n",
            "step 7889: generator_loss=3.414733409881592, discriminator_loss=0.037387266755104065\n",
            "step 7890: generator_loss=3.445293664932251, discriminator_loss=0.036709435284137726\n",
            "step 7891: generator_loss=3.4454517364501953, discriminator_loss=0.036565832793712616\n",
            "step 7892: generator_loss=3.4166650772094727, discriminator_loss=0.036763258278369904\n",
            "step 7893: generator_loss=3.4007110595703125, discriminator_loss=0.036758650094270706\n",
            "step 7894: generator_loss=3.396355152130127, discriminator_loss=0.03668496757745743\n",
            "step 7895: generator_loss=3.395247220993042, discriminator_loss=0.03652413934469223\n",
            "step 7896: generator_loss=3.42441987991333, discriminator_loss=0.0356937050819397\n",
            "step 7897: generator_loss=3.4354071617126465, discriminator_loss=0.03530601039528847\n",
            "step 7898: generator_loss=3.422096014022827, discriminator_loss=0.035245638340711594\n",
            "step 7899: generator_loss=3.410762310028076, discriminator_loss=0.03523269668221474\n",
            "step 7900: generator_loss=3.3879358768463135, discriminator_loss=0.03536197543144226\n",
            "step 7901: generator_loss=3.3485379219055176, discriminator_loss=0.03587835654616356\n",
            "step 7902: generator_loss=3.308171272277832, discriminator_loss=0.036513425409793854\n",
            "step 7903: generator_loss=3.32088041305542, discriminator_loss=0.0361812487244606\n",
            "step 7904: generator_loss=3.367734670639038, discriminator_loss=0.03533123433589935\n",
            "step 7905: generator_loss=3.428281545639038, discriminator_loss=0.03432247042655945\n",
            "step 7906: generator_loss=3.4621994495391846, discriminator_loss=0.033785201609134674\n",
            "step 7907: generator_loss=3.5031256675720215, discriminator_loss=0.0331927165389061\n",
            "step 7908: generator_loss=3.510525703430176, discriminator_loss=0.03312106058001518\n",
            "step 7909: generator_loss=3.574348211288452, discriminator_loss=0.03218025714159012\n",
            "step 7910: generator_loss=3.628204345703125, discriminator_loss=0.03148050233721733\n",
            "step 7911: generator_loss=3.649613618850708, discriminator_loss=0.03106488287448883\n",
            "step 7912: generator_loss=3.6663057804107666, discriminator_loss=0.030755676329135895\n",
            "step 7913: generator_loss=3.7008938789367676, discriminator_loss=0.030218780040740967\n",
            "step 7914: generator_loss=3.733471632003784, discriminator_loss=0.029648754745721817\n",
            "step 7915: generator_loss=3.72466778755188, discriminator_loss=0.02956424467265606\n",
            "step 7916: generator_loss=3.696988582611084, discriminator_loss=0.029630271717905998\n",
            "step 7917: generator_loss=3.6231985092163086, discriminator_loss=0.030299918726086617\n",
            "step 7918: generator_loss=3.5423574447631836, discriminator_loss=0.031139707192778587\n",
            "step 7919: generator_loss=3.4430994987487793, discriminator_loss=0.03249548375606537\n",
            "step 7920: generator_loss=3.3626580238342285, discriminator_loss=0.033601149916648865\n",
            "step 7921: generator_loss=3.2974448204040527, discriminator_loss=0.0346265584230423\n",
            "step 7922: generator_loss=3.251429557800293, discriminator_loss=0.03539778292179108\n",
            "step 7923: generator_loss=3.245990514755249, discriminator_loss=0.035434894263744354\n",
            "step 7924: generator_loss=3.25327467918396, discriminator_loss=0.035281624644994736\n",
            "step 7925: generator_loss=3.270160675048828, discriminator_loss=0.034999698400497437\n",
            "step 7926: generator_loss=3.3157830238342285, discriminator_loss=0.034218184649944305\n",
            "step 7927: generator_loss=3.3571836948394775, discriminator_loss=0.033641982823610306\n",
            "step 7928: generator_loss=3.3778834342956543, discriminator_loss=0.03334493562579155\n",
            "step 7929: generator_loss=3.3962531089782715, discriminator_loss=0.03319430351257324\n",
            "step 7930: generator_loss=3.4080209732055664, discriminator_loss=0.033100470900535583\n",
            "step 7931: generator_loss=3.4204859733581543, discriminator_loss=0.03306663781404495\n",
            "step 7932: generator_loss=3.404982089996338, discriminator_loss=0.03350747376680374\n",
            "step 7933: generator_loss=3.4134206771850586, discriminator_loss=0.033530183136463165\n",
            "step 7934: generator_loss=3.459115982055664, discriminator_loss=0.03291758894920349\n",
            "step 7935: generator_loss=3.509556293487549, discriminator_loss=0.03219541534781456\n",
            "step 7936: generator_loss=3.5303056240081787, discriminator_loss=0.032036490738391876\n",
            "step 7937: generator_loss=3.5240371227264404, discriminator_loss=0.03218218684196472\n",
            "step 7938: generator_loss=3.520017147064209, discriminator_loss=0.03241545706987381\n",
            "step 7939: generator_loss=3.4592177867889404, discriminator_loss=0.03332354500889778\n",
            "step 7940: generator_loss=3.4275474548339844, discriminator_loss=0.03387649357318878\n",
            "step 7941: generator_loss=3.4161946773529053, discriminator_loss=0.03413526341319084\n",
            "step 7942: generator_loss=3.4184494018554688, discriminator_loss=0.03410175442695618\n",
            "step 7943: generator_loss=3.4470579624176025, discriminator_loss=0.03367125615477562\n",
            "step 7944: generator_loss=3.5378947257995605, discriminator_loss=0.032266922295093536\n",
            "step 7945: generator_loss=3.6154356002807617, discriminator_loss=0.0311374943703413\n",
            "step 7946: generator_loss=3.6815967559814453, discriminator_loss=0.030288876965641975\n",
            "step 7947: generator_loss=3.734271287918091, discriminator_loss=0.029608171433210373\n",
            "step 7948: generator_loss=3.7592415809631348, discriminator_loss=0.029272831976413727\n",
            "step 7949: generator_loss=3.7680721282958984, discriminator_loss=0.029099002480506897\n",
            "step 7950: generator_loss=3.7466623783111572, discriminator_loss=0.029253365471959114\n",
            "step 7951: generator_loss=3.7271406650543213, discriminator_loss=0.02936618961393833\n",
            "step 7952: generator_loss=3.6923694610595703, discriminator_loss=0.0296335332095623\n",
            "step 7953: generator_loss=3.663181781768799, discriminator_loss=0.029909774661064148\n",
            "step 7954: generator_loss=3.6654250621795654, discriminator_loss=0.029706832021474838\n",
            "step 7955: generator_loss=3.6547446250915527, discriminator_loss=0.02968345209956169\n",
            "step 7956: generator_loss=3.631798028945923, discriminator_loss=0.029829666018486023\n",
            "step 7957: generator_loss=3.704955577850342, discriminator_loss=0.028701364994049072\n",
            "step 7958: generator_loss=3.80320143699646, discriminator_loss=0.027314821258187294\n",
            "step 7959: generator_loss=3.906703472137451, discriminator_loss=0.026027370244264603\n",
            "step 7960: generator_loss=3.978394031524658, discriminator_loss=0.02507663145661354\n",
            "step 7961: generator_loss=4.043532371520996, discriminator_loss=0.024288631975650787\n",
            "step 7962: generator_loss=4.099246501922607, discriminator_loss=0.02359766885638237\n",
            "step 7963: generator_loss=4.112418174743652, discriminator_loss=0.023296896368265152\n",
            "step 7964: generator_loss=4.136012554168701, discriminator_loss=0.02286273054778576\n",
            "step 7965: generator_loss=4.15028190612793, discriminator_loss=0.022562190890312195\n",
            "step 7966: generator_loss=4.138031482696533, discriminator_loss=0.022501572966575623\n",
            "step 7967: generator_loss=4.084624767303467, discriminator_loss=0.0227665938436985\n",
            "step 7968: generator_loss=4.031145095825195, discriminator_loss=0.023035136982798576\n",
            "step 7969: generator_loss=4.012444496154785, discriminator_loss=0.023067394271492958\n",
            "step 7970: generator_loss=4.0119428634643555, discriminator_loss=0.022884748876094818\n",
            "step 7971: generator_loss=4.039636135101318, discriminator_loss=0.022473931312561035\n",
            "step 7972: generator_loss=4.041335582733154, discriminator_loss=0.022276591509580612\n",
            "step 7973: generator_loss=4.010532379150391, discriminator_loss=0.022452514618635178\n",
            "step 7974: generator_loss=3.9688937664031982, discriminator_loss=0.022644255310297012\n",
            "step 7975: generator_loss=3.901869773864746, discriminator_loss=0.023202694952487946\n",
            "step 7976: generator_loss=3.860112190246582, discriminator_loss=0.02350560948252678\n",
            "step 7977: generator_loss=3.817281484603882, discriminator_loss=0.023878049105405807\n",
            "step 7978: generator_loss=3.8158628940582275, discriminator_loss=0.023779936134815216\n",
            "step 7979: generator_loss=3.836054801940918, discriminator_loss=0.023494591936469078\n",
            "step 7980: generator_loss=3.8648972511291504, discriminator_loss=0.023114683106541634\n",
            "step 7981: generator_loss=3.9436540603637695, discriminator_loss=0.022235918790102005\n",
            "step 7982: generator_loss=4.020683765411377, discriminator_loss=0.02148287743330002\n",
            "step 7983: generator_loss=4.088171005249023, discriminator_loss=0.02081258036196232\n",
            "step 7984: generator_loss=4.118734836578369, discriminator_loss=0.020515955984592438\n",
            "step 7985: generator_loss=4.111447334289551, discriminator_loss=0.020559094846248627\n",
            "step 7986: generator_loss=4.098667144775391, discriminator_loss=0.020624222233891487\n",
            "step 7987: generator_loss=4.068467617034912, discriminator_loss=0.020859071984887123\n",
            "step 7988: generator_loss=4.0861663818359375, discriminator_loss=0.020663194358348846\n",
            "step 7989: generator_loss=4.128013610839844, discriminator_loss=0.020249972119927406\n",
            "step 7990: generator_loss=4.134334564208984, discriminator_loss=0.020163511857390404\n",
            "step 7991: generator_loss=4.114396572113037, discriminator_loss=0.020274750888347626\n",
            "step 7992: generator_loss=4.0723466873168945, discriminator_loss=0.020563393831253052\n",
            "step 7993: generator_loss=4.0099053382873535, discriminator_loss=0.0210886113345623\n",
            "step 7994: generator_loss=3.9309537410736084, discriminator_loss=0.021769586950540543\n",
            "step 7995: generator_loss=3.84883189201355, discriminator_loss=0.022595666348934174\n",
            "step 7996: generator_loss=3.774611473083496, discriminator_loss=0.0233992338180542\n",
            "step 7997: generator_loss=3.7145707607269287, discriminator_loss=0.024128679186105728\n",
            "step 7998: generator_loss=3.6797826290130615, discriminator_loss=0.02455207332968712\n",
            "step 7999: generator_loss=3.6764943599700928, discriminator_loss=0.024563025683164597\n",
            "step 8000: generator_loss=3.675208330154419, discriminator_loss=0.024594252929091454\n",
            "step 8001: generator_loss=3.6707515716552734, discriminator_loss=0.024659831076860428\n",
            "step 8002: generator_loss=3.6730918884277344, discriminator_loss=0.024658894166350365\n",
            "step 8003: generator_loss=3.6701178550720215, discriminator_loss=0.024722684174776077\n",
            "step 8004: generator_loss=3.697943687438965, discriminator_loss=0.024363573640584946\n",
            "step 8005: generator_loss=3.724870443344116, discriminator_loss=0.02408287674188614\n",
            "step 8006: generator_loss=3.7445993423461914, discriminator_loss=0.023890579119324684\n",
            "step 8007: generator_loss=3.808941125869751, discriminator_loss=0.023208383470773697\n",
            "step 8008: generator_loss=3.8832345008850098, discriminator_loss=0.022431176155805588\n",
            "step 8009: generator_loss=3.923454523086548, discriminator_loss=0.022046517580747604\n",
            "step 8010: generator_loss=3.935074806213379, discriminator_loss=0.02192961797118187\n",
            "step 8011: generator_loss=4.014292240142822, discriminator_loss=0.02119559794664383\n",
            "step 8012: generator_loss=4.073235988616943, discriminator_loss=0.02062857896089554\n",
            "step 8013: generator_loss=4.111256122589111, discriminator_loss=0.020258039236068726\n",
            "step 8014: generator_loss=4.126497745513916, discriminator_loss=0.020098380744457245\n",
            "step 8015: generator_loss=4.1353068351745605, discriminator_loss=0.019946454092860222\n",
            "step 8016: generator_loss=4.181995868682861, discriminator_loss=0.019497696310281754\n",
            "step 8017: generator_loss=4.236437797546387, discriminator_loss=0.01900240033864975\n",
            "step 8018: generator_loss=4.274407386779785, discriminator_loss=0.01857435144484043\n",
            "step 8019: generator_loss=4.292695999145508, discriminator_loss=0.018345944583415985\n",
            "step 8020: generator_loss=4.27420711517334, discriminator_loss=0.018332840874791145\n",
            "step 8021: generator_loss=4.23680305480957, discriminator_loss=0.01847640424966812\n",
            "step 8022: generator_loss=4.166088581085205, discriminator_loss=0.018835389986634254\n",
            "step 8023: generator_loss=4.080052852630615, discriminator_loss=0.019440781325101852\n",
            "step 8024: generator_loss=4.025272369384766, discriminator_loss=0.01978079229593277\n",
            "step 8025: generator_loss=3.9852898120880127, discriminator_loss=0.019992485642433167\n",
            "step 8026: generator_loss=3.963031053543091, discriminator_loss=0.02007169835269451\n",
            "step 8027: generator_loss=3.9439637660980225, discriminator_loss=0.020169097930192947\n",
            "step 8028: generator_loss=3.9418201446533203, discriminator_loss=0.020069658756256104\n",
            "step 8029: generator_loss=3.9484949111938477, discriminator_loss=0.019943423569202423\n",
            "step 8030: generator_loss=3.963270664215088, discriminator_loss=0.01970956102013588\n",
            "step 8031: generator_loss=3.977198839187622, discriminator_loss=0.01950879767537117\n",
            "step 8032: generator_loss=3.977102756500244, discriminator_loss=0.01944766938686371\n",
            "step 8033: generator_loss=3.9607300758361816, discriminator_loss=0.019571499899029732\n",
            "step 8034: generator_loss=3.931227922439575, discriminator_loss=0.019827868789434433\n",
            "step 8035: generator_loss=3.9729182720184326, discriminator_loss=0.01937929540872574\n",
            "step 8036: generator_loss=4.038784503936768, discriminator_loss=0.018757052719593048\n",
            "step 8037: generator_loss=4.120625972747803, discriminator_loss=0.018003281205892563\n",
            "step 8038: generator_loss=4.190240859985352, discriminator_loss=0.017456641420722008\n",
            "step 8039: generator_loss=4.228858947753906, discriminator_loss=0.017140276730060577\n",
            "step 8040: generator_loss=4.222830772399902, discriminator_loss=0.017171110957860947\n",
            "step 8041: generator_loss=4.2066802978515625, discriminator_loss=0.017282847315073013\n",
            "step 8042: generator_loss=4.156328201293945, discriminator_loss=0.017640067264437675\n",
            "step 8043: generator_loss=4.078614711761475, discriminator_loss=0.018279287964105606\n",
            "step 8044: generator_loss=4.054920673370361, discriminator_loss=0.01848622038960457\n",
            "step 8045: generator_loss=4.015913963317871, discriminator_loss=0.018818456679582596\n",
            "step 8046: generator_loss=3.980252742767334, discriminator_loss=0.019144780933856964\n",
            "step 8047: generator_loss=3.950507164001465, discriminator_loss=0.019443482160568237\n",
            "step 8048: generator_loss=3.9199910163879395, discriminator_loss=0.019738707691431046\n",
            "step 8049: generator_loss=3.8872873783111572, discriminator_loss=0.020085901021957397\n",
            "step 8050: generator_loss=3.8718791007995605, discriminator_loss=0.020277729257941246\n",
            "step 8051: generator_loss=3.8793129920959473, discriminator_loss=0.02028806135058403\n",
            "step 8052: generator_loss=3.8627305030822754, discriminator_loss=0.02045220509171486\n",
            "step 8053: generator_loss=3.846147298812866, discriminator_loss=0.02066154032945633\n",
            "step 8054: generator_loss=3.81109881401062, discriminator_loss=0.021106448024511337\n",
            "step 8055: generator_loss=3.800009250640869, discriminator_loss=0.021306151524186134\n",
            "step 8056: generator_loss=3.7800962924957275, discriminator_loss=0.021587232127785683\n",
            "step 8057: generator_loss=3.753460168838501, discriminator_loss=0.02195890247821808\n",
            "step 8058: generator_loss=3.758901834487915, discriminator_loss=0.021969925612211227\n",
            "step 8059: generator_loss=3.82784366607666, discriminator_loss=0.021254777908325195\n",
            "step 8060: generator_loss=3.8921256065368652, discriminator_loss=0.02062881551682949\n",
            "step 8061: generator_loss=3.906174659729004, discriminator_loss=0.020545952022075653\n",
            "step 8062: generator_loss=3.9349446296691895, discriminator_loss=0.02038855291903019\n",
            "step 8063: generator_loss=3.969148874282837, discriminator_loss=0.02002924680709839\n",
            "step 8064: generator_loss=4.0025553703308105, discriminator_loss=0.01976091042160988\n",
            "step 8065: generator_loss=4.0284929275512695, discriminator_loss=0.01954798772931099\n",
            "step 8066: generator_loss=4.022463798522949, discriminator_loss=0.01964234933257103\n",
            "step 8067: generator_loss=4.0132317543029785, discriminator_loss=0.019749265164136887\n",
            "step 8068: generator_loss=4.029884338378906, discriminator_loss=0.0195994284003973\n",
            "step 8069: generator_loss=4.077635765075684, discriminator_loss=0.019149070605635643\n",
            "step 8070: generator_loss=4.117964744567871, discriminator_loss=0.01880205236375332\n",
            "step 8071: generator_loss=4.1153364181518555, discriminator_loss=0.018832214176654816\n",
            "step 8072: generator_loss=4.083681106567383, discriminator_loss=0.019084669649600983\n",
            "step 8073: generator_loss=4.0336689949035645, discriminator_loss=0.01941157691180706\n",
            "step 8074: generator_loss=3.9597930908203125, discriminator_loss=0.02004529908299446\n",
            "step 8075: generator_loss=3.8981189727783203, discriminator_loss=0.020658671855926514\n",
            "step 8076: generator_loss=3.8680577278137207, discriminator_loss=0.02092183567583561\n",
            "step 8077: generator_loss=3.8484652042388916, discriminator_loss=0.02105984464287758\n",
            "step 8078: generator_loss=3.8196263313293457, discriminator_loss=0.021328916773200035\n",
            "step 8079: generator_loss=3.807340383529663, discriminator_loss=0.021430673077702522\n",
            "step 8080: generator_loss=3.8211734294891357, discriminator_loss=0.021250799298286438\n",
            "step 8081: generator_loss=3.852832078933716, discriminator_loss=0.02090250328183174\n",
            "step 8082: generator_loss=3.886801242828369, discriminator_loss=0.020536275580525398\n",
            "step 8083: generator_loss=3.91294527053833, discriminator_loss=0.02024741843342781\n",
            "step 8084: generator_loss=3.989772319793701, discriminator_loss=0.01955677568912506\n",
            "step 8085: generator_loss=4.1495466232299805, discriminator_loss=0.018142094835639\n",
            "step 8086: generator_loss=4.282939910888672, discriminator_loss=0.017199790105223656\n",
            "step 8087: generator_loss=4.372858047485352, discriminator_loss=0.016625327989459038\n",
            "step 8088: generator_loss=4.430255889892578, discriminator_loss=0.016240229830145836\n",
            "step 8089: generator_loss=4.441053867340088, discriminator_loss=0.016191717237234116\n",
            "step 8090: generator_loss=4.401050567626953, discriminator_loss=0.016362685710191727\n",
            "step 8091: generator_loss=4.305994033813477, discriminator_loss=0.016957521438598633\n",
            "step 8092: generator_loss=4.219742298126221, discriminator_loss=0.017492221668362617\n",
            "step 8093: generator_loss=4.217512130737305, discriminator_loss=0.017420142889022827\n",
            "step 8094: generator_loss=4.176438808441162, discriminator_loss=0.017630212008953094\n",
            "step 8095: generator_loss=4.118926048278809, discriminator_loss=0.01806119829416275\n",
            "step 8096: generator_loss=4.129819869995117, discriminator_loss=0.017896242439746857\n",
            "step 8097: generator_loss=4.098143100738525, discriminator_loss=0.018114440143108368\n",
            "step 8098: generator_loss=4.040110111236572, discriminator_loss=0.01848621852695942\n",
            "step 8099: generator_loss=3.9889512062072754, discriminator_loss=0.01895824447274208\n",
            "step 8100: generator_loss=3.9866628646850586, discriminator_loss=0.01890813373029232\n",
            "step 8101: generator_loss=3.922189712524414, discriminator_loss=0.020097333937883377\n",
            "step 8102: generator_loss=3.930377721786499, discriminator_loss=0.020461929962038994\n",
            "step 8103: generator_loss=3.967069625854492, discriminator_loss=0.020174644887447357\n",
            "step 8104: generator_loss=4.032506942749023, discriminator_loss=0.020833883434534073\n",
            "step 8105: generator_loss=4.04145622253418, discriminator_loss=0.02350901998579502\n",
            "step 8106: generator_loss=4.112114906311035, discriminator_loss=0.02452322468161583\n",
            "step 8107: generator_loss=4.089480400085449, discriminator_loss=0.032603248953819275\n",
            "step 8108: generator_loss=4.080550193786621, discriminator_loss=0.03744992986321449\n",
            "step 8109: generator_loss=4.288585662841797, discriminator_loss=0.03984721750020981\n",
            "step 8110: generator_loss=4.431148052215576, discriminator_loss=0.04250521585345268\n",
            "step 8111: generator_loss=4.522742748260498, discriminator_loss=0.05877471715211868\n",
            "step 8112: generator_loss=5.0059099197387695, discriminator_loss=0.05195482447743416\n",
            "step 8113: generator_loss=5.511098384857178, discriminator_loss=0.04278041794896126\n",
            "step 8114: generator_loss=5.801260471343994, discriminator_loss=0.035345904529094696\n",
            "step 8115: generator_loss=6.164752960205078, discriminator_loss=0.030426718294620514\n",
            "step 8116: generator_loss=6.559798240661621, discriminator_loss=0.032488081604242325\n",
            "step 8117: generator_loss=6.418785095214844, discriminator_loss=0.03182128071784973\n",
            "step 8118: generator_loss=6.872251510620117, discriminator_loss=0.025151314213871956\n",
            "step 8119: generator_loss=7.6227288246154785, discriminator_loss=0.02031329646706581\n",
            "step 8120: generator_loss=7.782071113586426, discriminator_loss=0.019859690219163895\n",
            "step 8121: generator_loss=7.555506706237793, discriminator_loss=0.019615862518548965\n",
            "step 8122: generator_loss=7.70196533203125, discriminator_loss=0.019775424152612686\n",
            "step 8123: generator_loss=7.7903852462768555, discriminator_loss=0.019300740212202072\n",
            "step 8124: generator_loss=7.642092227935791, discriminator_loss=0.01959775760769844\n",
            "step 8125: generator_loss=8.067363739013672, discriminator_loss=0.01894121989607811\n",
            "step 8126: generator_loss=7.565062522888184, discriminator_loss=0.020157910883426666\n",
            "step 8127: generator_loss=7.22323751449585, discriminator_loss=0.020701244473457336\n",
            "step 8128: generator_loss=7.227333068847656, discriminator_loss=0.019508784636855125\n",
            "step 8129: generator_loss=7.15396785736084, discriminator_loss=0.01889268308877945\n",
            "step 8130: generator_loss=6.991306781768799, discriminator_loss=0.018807698041200638\n",
            "step 8131: generator_loss=6.930112361907959, discriminator_loss=0.018804127350449562\n",
            "step 8132: generator_loss=6.742657661437988, discriminator_loss=0.018323857337236404\n",
            "step 8133: generator_loss=6.420934677124023, discriminator_loss=0.016717560589313507\n",
            "step 8134: generator_loss=6.428336143493652, discriminator_loss=0.01531157921999693\n",
            "step 8135: generator_loss=6.244480133056641, discriminator_loss=0.014731261879205704\n",
            "step 8136: generator_loss=5.923306465148926, discriminator_loss=0.01536917220801115\n",
            "step 8137: generator_loss=5.663498878479004, discriminator_loss=0.01585479825735092\n",
            "step 8138: generator_loss=5.407071590423584, discriminator_loss=0.015776347368955612\n",
            "step 8139: generator_loss=5.142964839935303, discriminator_loss=0.01722918450832367\n",
            "step 8140: generator_loss=4.935702323913574, discriminator_loss=0.017674323171377182\n",
            "step 8141: generator_loss=4.60251522064209, discriminator_loss=0.01967979036271572\n",
            "step 8142: generator_loss=4.348679542541504, discriminator_loss=0.022198710590600967\n",
            "step 8143: generator_loss=4.282293319702148, discriminator_loss=0.02256951481103897\n",
            "step 8144: generator_loss=4.051398277282715, discriminator_loss=0.023801550269126892\n",
            "step 8145: generator_loss=3.832873821258545, discriminator_loss=0.026043038815259933\n",
            "step 8146: generator_loss=3.7766385078430176, discriminator_loss=0.023837454617023468\n",
            "step 8147: generator_loss=4.004673957824707, discriminator_loss=0.022885264828801155\n",
            "step 8148: generator_loss=3.744321823120117, discriminator_loss=0.024864736944437027\n",
            "step 8149: generator_loss=3.670470714569092, discriminator_loss=0.025232922285795212\n",
            "step 8150: generator_loss=3.823615074157715, discriminator_loss=0.025405388325452805\n",
            "step 8151: generator_loss=3.7730979919433594, discriminator_loss=0.026246702298521996\n",
            "step 8152: generator_loss=3.7887380123138428, discriminator_loss=0.02717127650976181\n",
            "step 8153: generator_loss=3.571521520614624, discriminator_loss=0.02948068454861641\n",
            "step 8154: generator_loss=3.8466877937316895, discriminator_loss=0.028181174769997597\n",
            "step 8155: generator_loss=3.7357606887817383, discriminator_loss=0.029218852519989014\n",
            "step 8156: generator_loss=3.5675554275512695, discriminator_loss=0.030674491077661514\n",
            "step 8157: generator_loss=3.6334445476531982, discriminator_loss=0.02999117411673069\n",
            "step 8158: generator_loss=3.7337822914123535, discriminator_loss=0.030933810397982597\n",
            "step 8159: generator_loss=3.4940719604492188, discriminator_loss=0.032555386424064636\n",
            "step 8160: generator_loss=3.6012539863586426, discriminator_loss=0.031832125037908554\n",
            "step 8161: generator_loss=3.9877405166625977, discriminator_loss=0.030687306076288223\n",
            "step 8162: generator_loss=3.8374176025390625, discriminator_loss=0.031743042171001434\n",
            "step 8163: generator_loss=3.7981507778167725, discriminator_loss=0.03162921220064163\n",
            "step 8164: generator_loss=3.7858569622039795, discriminator_loss=0.03148946166038513\n",
            "step 8165: generator_loss=3.8645176887512207, discriminator_loss=0.03087431937456131\n",
            "step 8166: generator_loss=3.686959743499756, discriminator_loss=0.03233958035707474\n",
            "step 8167: generator_loss=3.876112222671509, discriminator_loss=0.03229038044810295\n",
            "step 8168: generator_loss=3.834083080291748, discriminator_loss=0.03290516138076782\n",
            "step 8169: generator_loss=3.8651888370513916, discriminator_loss=0.03339163959026337\n",
            "step 8170: generator_loss=3.5954999923706055, discriminator_loss=0.036649540066719055\n",
            "step 8171: generator_loss=3.3870649337768555, discriminator_loss=0.03865789622068405\n",
            "step 8172: generator_loss=3.233616352081299, discriminator_loss=0.041077058762311935\n",
            "step 8173: generator_loss=3.4357118606567383, discriminator_loss=0.04017655551433563\n",
            "step 8174: generator_loss=3.3671607971191406, discriminator_loss=0.040405429899692535\n",
            "step 8175: generator_loss=3.3432888984680176, discriminator_loss=0.04061766713857651\n",
            "step 8176: generator_loss=3.3791511058807373, discriminator_loss=0.04092419520020485\n",
            "step 8177: generator_loss=3.377354860305786, discriminator_loss=0.04124218225479126\n",
            "step 8178: generator_loss=3.408139228820801, discriminator_loss=0.041632555425167084\n",
            "step 8179: generator_loss=3.24472975730896, discriminator_loss=0.04311244934797287\n",
            "step 8180: generator_loss=3.1137375831604004, discriminator_loss=0.04528408125042915\n",
            "step 8181: generator_loss=3.2775754928588867, discriminator_loss=0.04462432488799095\n",
            "step 8182: generator_loss=3.1344926357269287, discriminator_loss=0.04563765227794647\n",
            "step 8183: generator_loss=3.1345577239990234, discriminator_loss=0.044884540140628815\n",
            "step 8184: generator_loss=3.303203582763672, discriminator_loss=0.04219379276037216\n",
            "step 8185: generator_loss=3.388805627822876, discriminator_loss=0.04023671895265579\n",
            "step 8186: generator_loss=3.4865877628326416, discriminator_loss=0.038914963603019714\n",
            "step 8187: generator_loss=3.512596607208252, discriminator_loss=0.038201555609703064\n",
            "step 8188: generator_loss=3.563689708709717, discriminator_loss=0.03715040534734726\n",
            "step 8189: generator_loss=3.5823922157287598, discriminator_loss=0.036492958664894104\n",
            "step 8190: generator_loss=3.558079719543457, discriminator_loss=0.03625476732850075\n",
            "step 8191: generator_loss=3.4976601600646973, discriminator_loss=0.0363648384809494\n",
            "step 8192: generator_loss=3.4029042720794678, discriminator_loss=0.03725636005401611\n",
            "step 8193: generator_loss=3.274728775024414, discriminator_loss=0.03877539560198784\n",
            "step 8194: generator_loss=3.1990034580230713, discriminator_loss=0.039560265839099884\n",
            "step 8195: generator_loss=3.1645069122314453, discriminator_loss=0.03978059068322182\n",
            "step 8196: generator_loss=3.1679515838623047, discriminator_loss=0.03930853679776192\n",
            "step 8197: generator_loss=3.2152280807495117, discriminator_loss=0.03814315050840378\n",
            "step 8198: generator_loss=3.263939380645752, discriminator_loss=0.03681845963001251\n",
            "step 8199: generator_loss=3.3342316150665283, discriminator_loss=0.035345401614904404\n",
            "step 8200: generator_loss=3.4299161434173584, discriminator_loss=0.033560849726200104\n",
            "step 8201: generator_loss=3.5905473232269287, discriminator_loss=0.030983006581664085\n",
            "step 8202: generator_loss=3.7744410037994385, discriminator_loss=0.028420068323612213\n",
            "step 8203: generator_loss=3.913705825805664, discriminator_loss=0.026802612468600273\n",
            "step 8204: generator_loss=3.9173903465270996, discriminator_loss=0.02656533569097519\n",
            "step 8205: generator_loss=3.847844123840332, discriminator_loss=0.02713390439748764\n",
            "step 8206: generator_loss=3.7631759643554688, discriminator_loss=0.02781827375292778\n",
            "step 8207: generator_loss=3.6869451999664307, discriminator_loss=0.028455879539251328\n",
            "step 8208: generator_loss=3.61753249168396, discriminator_loss=0.029111750423908234\n",
            "step 8209: generator_loss=3.5793490409851074, discriminator_loss=0.029456187039613724\n",
            "step 8210: generator_loss=3.4958231449127197, discriminator_loss=0.03031650185585022\n",
            "step 8211: generator_loss=3.4546568393707275, discriminator_loss=0.030797846615314484\n",
            "step 8212: generator_loss=3.4543395042419434, discriminator_loss=0.03059435449540615\n",
            "step 8213: generator_loss=3.4787254333496094, discriminator_loss=0.029990514740347862\n",
            "step 8214: generator_loss=3.542584180831909, discriminator_loss=0.028884217143058777\n",
            "step 8215: generator_loss=3.593153715133667, discriminator_loss=0.028013447299599648\n",
            "step 8216: generator_loss=3.598613977432251, discriminator_loss=0.02783503383398056\n",
            "step 8217: generator_loss=3.5690979957580566, discriminator_loss=0.028233379125595093\n",
            "step 8218: generator_loss=3.521331310272217, discriminator_loss=0.02891385182738304\n",
            "step 8219: generator_loss=3.447936534881592, discriminator_loss=0.03014346957206726\n",
            "step 8220: generator_loss=3.425701856613159, discriminator_loss=0.03055255115032196\n",
            "step 8221: generator_loss=3.422318935394287, discriminator_loss=0.030696388334035873\n",
            "step 8222: generator_loss=3.4649014472961426, discriminator_loss=0.03008364886045456\n",
            "step 8223: generator_loss=3.5081980228424072, discriminator_loss=0.029608104377985\n",
            "step 8224: generator_loss=3.595268964767456, discriminator_loss=0.02850327081978321\n",
            "step 8225: generator_loss=3.6669762134552, discriminator_loss=0.027723833918571472\n",
            "step 8226: generator_loss=3.6931347846984863, discriminator_loss=0.027575653046369553\n",
            "step 8227: generator_loss=3.671311378479004, discriminator_loss=0.027999678626656532\n",
            "step 8228: generator_loss=3.6581075191497803, discriminator_loss=0.028307460248470306\n",
            "step 8229: generator_loss=3.6845366954803467, discriminator_loss=0.028137944638729095\n",
            "step 8230: generator_loss=3.675473213195801, discriminator_loss=0.028330225497484207\n",
            "step 8231: generator_loss=3.6704859733581543, discriminator_loss=0.02850644662976265\n",
            "step 8232: generator_loss=3.6824967861175537, discriminator_loss=0.02843279391527176\n",
            "step 8233: generator_loss=3.6940758228302, discriminator_loss=0.02831253409385681\n",
            "step 8234: generator_loss=3.6767778396606445, discriminator_loss=0.028468288481235504\n",
            "step 8235: generator_loss=3.6085269451141357, discriminator_loss=0.029466193169355392\n",
            "step 8236: generator_loss=3.500866651535034, discriminator_loss=0.031035084277391434\n",
            "step 8237: generator_loss=3.4073002338409424, discriminator_loss=0.03250277787446976\n",
            "step 8238: generator_loss=3.353501081466675, discriminator_loss=0.03352687507867813\n",
            "step 8239: generator_loss=3.337327241897583, discriminator_loss=0.03389831259846687\n",
            "step 8240: generator_loss=3.3506879806518555, discriminator_loss=0.033698201179504395\n",
            "step 8241: generator_loss=3.374990224838257, discriminator_loss=0.0334719717502594\n",
            "step 8242: generator_loss=3.455087184906006, discriminator_loss=0.03226032108068466\n",
            "step 8243: generator_loss=3.5667362213134766, discriminator_loss=0.030787821859121323\n",
            "step 8244: generator_loss=3.655639410018921, discriminator_loss=0.02973901852965355\n",
            "step 8245: generator_loss=3.69840145111084, discriminator_loss=0.029366999864578247\n",
            "step 8246: generator_loss=3.696810722351074, discriminator_loss=0.029462099075317383\n",
            "step 8247: generator_loss=3.6686854362487793, discriminator_loss=0.029895657673478127\n",
            "step 8248: generator_loss=3.617173194885254, discriminator_loss=0.030590970069169998\n",
            "step 8249: generator_loss=3.5385749340057373, discriminator_loss=0.03178206831216812\n",
            "step 8250: generator_loss=3.447185516357422, discriminator_loss=0.03324710950255394\n",
            "step 8251: generator_loss=3.3623414039611816, discriminator_loss=0.03463874012231827\n",
            "step 8252: generator_loss=3.308837890625, discriminator_loss=0.03561548888683319\n",
            "step 8253: generator_loss=3.2841005325317383, discriminator_loss=0.03605140000581741\n",
            "step 8254: generator_loss=3.2890284061431885, discriminator_loss=0.03600038215517998\n",
            "step 8255: generator_loss=3.305379629135132, discriminator_loss=0.03572191298007965\n",
            "step 8256: generator_loss=3.351222515106201, discriminator_loss=0.03497062250971794\n",
            "step 8257: generator_loss=3.4561686515808105, discriminator_loss=0.03322461247444153\n",
            "step 8258: generator_loss=3.5372471809387207, discriminator_loss=0.032146818935871124\n",
            "step 8259: generator_loss=3.6260833740234375, discriminator_loss=0.031081922352313995\n",
            "step 8260: generator_loss=3.73622989654541, discriminator_loss=0.029762933030724525\n",
            "step 8261: generator_loss=3.806725025177002, discriminator_loss=0.02908971905708313\n",
            "step 8262: generator_loss=3.8286032676696777, discriminator_loss=0.028864793479442596\n",
            "step 8263: generator_loss=3.782202959060669, discriminator_loss=0.029461117461323738\n",
            "step 8264: generator_loss=3.6725947856903076, discriminator_loss=0.030765557661652565\n",
            "step 8265: generator_loss=3.555349826812744, discriminator_loss=0.03226787596940994\n",
            "step 8266: generator_loss=3.475533962249756, discriminator_loss=0.03342436999082565\n",
            "step 8267: generator_loss=3.466724157333374, discriminator_loss=0.03346555307507515\n",
            "step 8268: generator_loss=3.4650707244873047, discriminator_loss=0.03337360918521881\n",
            "step 8269: generator_loss=3.4682204723358154, discriminator_loss=0.03316393867135048\n",
            "step 8270: generator_loss=3.5051145553588867, discriminator_loss=0.03246825560927391\n",
            "step 8271: generator_loss=3.5829973220825195, discriminator_loss=0.0313001424074173\n",
            "step 8272: generator_loss=3.641176700592041, discriminator_loss=0.030407795682549477\n",
            "step 8273: generator_loss=3.6717710494995117, discriminator_loss=0.029858026653528214\n",
            "step 8274: generator_loss=3.7006888389587402, discriminator_loss=0.029316186904907227\n",
            "step 8275: generator_loss=3.7586722373962402, discriminator_loss=0.028407808393239975\n",
            "step 8276: generator_loss=3.8047101497650146, discriminator_loss=0.027760181576013565\n",
            "step 8277: generator_loss=3.813004732131958, discriminator_loss=0.027592329308390617\n",
            "step 8278: generator_loss=3.7688674926757812, discriminator_loss=0.02777237631380558\n",
            "step 8279: generator_loss=3.7961935997009277, discriminator_loss=0.02745220810174942\n",
            "step 8280: generator_loss=3.7874646186828613, discriminator_loss=0.02724112756550312\n",
            "step 8281: generator_loss=3.8314106464385986, discriminator_loss=0.02658943645656109\n",
            "step 8282: generator_loss=3.8960084915161133, discriminator_loss=0.025786414742469788\n",
            "step 8283: generator_loss=3.9863429069519043, discriminator_loss=0.02469704858958721\n",
            "step 8284: generator_loss=4.058738708496094, discriminator_loss=0.023745335638523102\n",
            "step 8285: generator_loss=4.088794708251953, discriminator_loss=0.023352881893515587\n",
            "step 8286: generator_loss=4.172290802001953, discriminator_loss=0.022568106651306152\n",
            "step 8287: generator_loss=4.185007095336914, discriminator_loss=0.02266096882522106\n",
            "step 8288: generator_loss=4.261275291442871, discriminator_loss=0.021578393876552582\n",
            "step 8289: generator_loss=4.138644695281982, discriminator_loss=0.023000825196504593\n",
            "step 8290: generator_loss=4.171071529388428, discriminator_loss=0.022635985165834427\n",
            "step 8291: generator_loss=4.146478176116943, discriminator_loss=0.02251911163330078\n",
            "step 8292: generator_loss=4.09619665145874, discriminator_loss=0.023913173004984856\n",
            "step 8293: generator_loss=4.188516616821289, discriminator_loss=0.0232536680996418\n",
            "step 8294: generator_loss=4.0801191329956055, discriminator_loss=0.024835675954818726\n",
            "step 8295: generator_loss=4.001195430755615, discriminator_loss=0.029128219932317734\n",
            "step 8296: generator_loss=4.015799522399902, discriminator_loss=0.030189234763383865\n",
            "step 8297: generator_loss=4.420270919799805, discriminator_loss=0.023419056087732315\n",
            "step 8298: generator_loss=4.227528095245361, discriminator_loss=0.033793941140174866\n",
            "step 8299: generator_loss=4.244114875793457, discriminator_loss=0.037542685866355896\n",
            "step 8300: generator_loss=4.627148151397705, discriminator_loss=0.029672708362340927\n",
            "step 8301: generator_loss=4.420046806335449, discriminator_loss=0.04501144587993622\n",
            "step 8302: generator_loss=4.439677715301514, discriminator_loss=0.04908248782157898\n",
            "step 8303: generator_loss=5.112373352050781, discriminator_loss=0.03407778963446617\n",
            "step 8304: generator_loss=4.8510894775390625, discriminator_loss=0.046907179057598114\n",
            "step 8305: generator_loss=5.524068832397461, discriminator_loss=0.03263166546821594\n",
            "step 8306: generator_loss=5.684928894042969, discriminator_loss=0.03058386594057083\n",
            "step 8307: generator_loss=5.805220603942871, discriminator_loss=0.0288070235401392\n",
            "step 8308: generator_loss=6.001475811004639, discriminator_loss=0.027084657922387123\n",
            "step 8309: generator_loss=6.195213317871094, discriminator_loss=0.031104398891329765\n",
            "step 8310: generator_loss=6.7174835205078125, discriminator_loss=0.02565208449959755\n",
            "step 8311: generator_loss=6.382601737976074, discriminator_loss=0.031782977283000946\n",
            "step 8312: generator_loss=6.519287109375, discriminator_loss=0.02712903544306755\n",
            "step 8313: generator_loss=6.3223114013671875, discriminator_loss=0.035957589745521545\n",
            "step 8314: generator_loss=6.418586254119873, discriminator_loss=0.04454272612929344\n",
            "step 8315: generator_loss=6.625975608825684, discriminator_loss=0.04955657199025154\n",
            "step 8316: generator_loss=6.495306491851807, discriminator_loss=0.035955701023340225\n",
            "step 8317: generator_loss=6.074371814727783, discriminator_loss=0.03294599428772926\n",
            "step 8318: generator_loss=7.5701165199279785, discriminator_loss=0.0193876251578331\n",
            "step 8319: generator_loss=7.249829292297363, discriminator_loss=0.018286103382706642\n",
            "step 8320: generator_loss=7.624469757080078, discriminator_loss=0.017358358949422836\n",
            "step 8321: generator_loss=7.895512580871582, discriminator_loss=0.016959527507424355\n",
            "step 8322: generator_loss=8.203283309936523, discriminator_loss=0.017308779060840607\n",
            "step 8323: generator_loss=8.114750862121582, discriminator_loss=0.019119124859571457\n",
            "step 8324: generator_loss=7.536356449127197, discriminator_loss=0.020399708300828934\n",
            "step 8325: generator_loss=7.328896522521973, discriminator_loss=0.021982986479997635\n",
            "step 8326: generator_loss=7.669306755065918, discriminator_loss=0.021217990666627884\n",
            "step 8327: generator_loss=6.794482231140137, discriminator_loss=0.02396511286497116\n",
            "step 8328: generator_loss=6.972297668457031, discriminator_loss=0.024872880429029465\n",
            "step 8329: generator_loss=6.420665740966797, discriminator_loss=0.025166895240545273\n",
            "step 8330: generator_loss=7.024056911468506, discriminator_loss=0.022131282836198807\n",
            "step 8331: generator_loss=7.0129804611206055, discriminator_loss=0.02031225711107254\n",
            "step 8332: generator_loss=6.187640190124512, discriminator_loss=0.020912835374474525\n",
            "step 8333: generator_loss=6.765144348144531, discriminator_loss=0.020429689437150955\n",
            "step 8334: generator_loss=6.932135581970215, discriminator_loss=0.018565647304058075\n",
            "step 8335: generator_loss=6.426198959350586, discriminator_loss=0.018040731549263\n",
            "step 8336: generator_loss=6.7072978019714355, discriminator_loss=0.016710972413420677\n",
            "step 8337: generator_loss=6.610609531402588, discriminator_loss=0.01634969189763069\n",
            "step 8338: generator_loss=6.2567644119262695, discriminator_loss=0.017396140843629837\n",
            "step 8339: generator_loss=6.704583168029785, discriminator_loss=0.016118813306093216\n",
            "step 8340: generator_loss=6.358078956604004, discriminator_loss=0.01629037596285343\n",
            "step 8341: generator_loss=6.091407775878906, discriminator_loss=0.01809796318411827\n",
            "step 8342: generator_loss=6.400967121124268, discriminator_loss=0.016512589529156685\n",
            "step 8343: generator_loss=5.9156904220581055, discriminator_loss=0.017223551869392395\n",
            "step 8344: generator_loss=5.951253414154053, discriminator_loss=0.015657072886824608\n",
            "step 8345: generator_loss=6.611319541931152, discriminator_loss=0.013919930905103683\n",
            "step 8346: generator_loss=6.224175453186035, discriminator_loss=0.014286154881119728\n",
            "step 8347: generator_loss=6.490041732788086, discriminator_loss=0.013785786926746368\n",
            "step 8348: generator_loss=6.003569602966309, discriminator_loss=0.014591971412301064\n",
            "step 8349: generator_loss=6.010833740234375, discriminator_loss=0.014662854373455048\n",
            "step 8350: generator_loss=5.787817001342773, discriminator_loss=0.015268217772245407\n",
            "step 8351: generator_loss=5.5095624923706055, discriminator_loss=0.01634342409670353\n",
            "step 8352: generator_loss=5.331153869628906, discriminator_loss=0.01706647500395775\n",
            "step 8353: generator_loss=5.296784400939941, discriminator_loss=0.017413154244422913\n",
            "step 8354: generator_loss=5.189094543457031, discriminator_loss=0.016821008175611496\n",
            "step 8355: generator_loss=5.76950740814209, discriminator_loss=0.015877746045589447\n",
            "step 8356: generator_loss=5.737182140350342, discriminator_loss=0.01491573080420494\n",
            "step 8357: generator_loss=5.5633463859558105, discriminator_loss=0.014516136609017849\n",
            "step 8358: generator_loss=5.194929122924805, discriminator_loss=0.015343954786658287\n",
            "step 8359: generator_loss=4.8818159103393555, discriminator_loss=0.016063932329416275\n",
            "step 8360: generator_loss=5.4548659324646, discriminator_loss=0.015035169199109077\n",
            "step 8361: generator_loss=5.259140968322754, discriminator_loss=0.015648063272237778\n",
            "step 8362: generator_loss=4.944952011108398, discriminator_loss=0.01668989285826683\n",
            "step 8363: generator_loss=4.767932415008545, discriminator_loss=0.01683218777179718\n",
            "step 8364: generator_loss=5.0005340576171875, discriminator_loss=0.01664256677031517\n",
            "step 8365: generator_loss=4.809294700622559, discriminator_loss=0.01700134202837944\n",
            "step 8366: generator_loss=4.802394390106201, discriminator_loss=0.016507165506482124\n",
            "step 8367: generator_loss=4.678277492523193, discriminator_loss=0.01642395742237568\n",
            "step 8368: generator_loss=5.471460342407227, discriminator_loss=0.01531381718814373\n",
            "step 8369: generator_loss=5.067865371704102, discriminator_loss=0.014635763131082058\n",
            "step 8370: generator_loss=5.363738536834717, discriminator_loss=0.014780051074922085\n",
            "step 8371: generator_loss=5.10280704498291, discriminator_loss=0.01544972788542509\n",
            "step 8372: generator_loss=5.890527725219727, discriminator_loss=0.01472760085016489\n",
            "step 8373: generator_loss=5.040616035461426, discriminator_loss=0.016480285674333572\n",
            "step 8374: generator_loss=5.118410110473633, discriminator_loss=0.01643713377416134\n",
            "step 8375: generator_loss=5.509946346282959, discriminator_loss=0.01638810522854328\n",
            "step 8376: generator_loss=4.8401336669921875, discriminator_loss=0.017689503729343414\n",
            "step 8377: generator_loss=4.665196418762207, discriminator_loss=0.019006559625267982\n",
            "step 8378: generator_loss=4.674642562866211, discriminator_loss=0.019374236464500427\n",
            "step 8379: generator_loss=4.664290428161621, discriminator_loss=0.020112697035074234\n",
            "step 8380: generator_loss=4.552971363067627, discriminator_loss=0.020313436165452003\n",
            "step 8381: generator_loss=4.526939392089844, discriminator_loss=0.020762324333190918\n",
            "step 8382: generator_loss=4.505249977111816, discriminator_loss=0.02091737650334835\n",
            "step 8383: generator_loss=4.504452705383301, discriminator_loss=0.02145111933350563\n",
            "step 8384: generator_loss=4.762042045593262, discriminator_loss=0.021258868277072906\n",
            "step 8385: generator_loss=4.280195713043213, discriminator_loss=0.02154889889061451\n",
            "step 8386: generator_loss=4.537857532501221, discriminator_loss=0.020548148080706596\n",
            "step 8387: generator_loss=4.173000812530518, discriminator_loss=0.022755004465579987\n",
            "step 8388: generator_loss=4.367712020874023, discriminator_loss=0.021889645606279373\n",
            "step 8389: generator_loss=4.307595252990723, discriminator_loss=0.023608524352312088\n",
            "step 8390: generator_loss=3.957033157348633, discriminator_loss=0.05021761730313301\n",
            "step 8391: generator_loss=4.166010856628418, discriminator_loss=0.05269145965576172\n",
            "step 8392: generator_loss=3.9209911823272705, discriminator_loss=0.0790882483124733\n",
            "step 8393: generator_loss=4.145016193389893, discriminator_loss=0.12129037082195282\n",
            "step 8394: generator_loss=4.74683952331543, discriminator_loss=0.09861845523118973\n",
            "step 8395: generator_loss=4.6688103675842285, discriminator_loss=0.15418632328510284\n",
            "step 8396: generator_loss=5.173554420471191, discriminator_loss=0.10864284634590149\n",
            "step 8397: generator_loss=5.928409099578857, discriminator_loss=0.05626480281352997\n",
            "step 8398: generator_loss=6.258292198181152, discriminator_loss=0.04491894319653511\n",
            "step 8399: generator_loss=6.615762233734131, discriminator_loss=0.03426595404744148\n",
            "step 8400: generator_loss=7.262207984924316, discriminator_loss=0.028113029897212982\n",
            "step 8401: generator_loss=7.799954414367676, discriminator_loss=0.023354332894086838\n",
            "step 8402: generator_loss=7.745258808135986, discriminator_loss=0.02306169643998146\n",
            "step 8403: generator_loss=8.171037673950195, discriminator_loss=0.02282603271305561\n",
            "step 8404: generator_loss=8.224313735961914, discriminator_loss=0.022457148879766464\n",
            "step 8405: generator_loss=7.944655418395996, discriminator_loss=0.02309149131178856\n",
            "step 8406: generator_loss=7.188055515289307, discriminator_loss=0.025476358830928802\n",
            "step 8407: generator_loss=6.977071762084961, discriminator_loss=0.0250982828438282\n",
            "step 8408: generator_loss=6.688528060913086, discriminator_loss=0.025092534720897675\n",
            "step 8409: generator_loss=6.334323883056641, discriminator_loss=0.025675252079963684\n",
            "step 8410: generator_loss=6.087974548339844, discriminator_loss=0.02705463580787182\n",
            "step 8411: generator_loss=5.557460784912109, discriminator_loss=0.029626725241541862\n",
            "step 8412: generator_loss=5.2258501052856445, discriminator_loss=0.03284287825226784\n",
            "step 8413: generator_loss=4.927099704742432, discriminator_loss=0.03275063633918762\n",
            "step 8414: generator_loss=4.80331563949585, discriminator_loss=0.031049374490976334\n",
            "step 8415: generator_loss=4.800522804260254, discriminator_loss=0.02940400317311287\n",
            "step 8416: generator_loss=4.410284996032715, discriminator_loss=0.03138980641961098\n",
            "step 8417: generator_loss=4.257970809936523, discriminator_loss=0.029983175918459892\n",
            "step 8418: generator_loss=4.189209461212158, discriminator_loss=0.028113506734371185\n",
            "step 8419: generator_loss=4.350726127624512, discriminator_loss=0.02510559931397438\n",
            "step 8420: generator_loss=4.224724769592285, discriminator_loss=0.024972205981612206\n",
            "step 8421: generator_loss=4.430170059204102, discriminator_loss=0.023480728268623352\n",
            "step 8422: generator_loss=4.197490692138672, discriminator_loss=0.024445103481411934\n",
            "step 8423: generator_loss=4.169842720031738, discriminator_loss=0.02409668080508709\n",
            "step 8424: generator_loss=4.056971549987793, discriminator_loss=0.024617314338684082\n",
            "step 8425: generator_loss=3.8335561752319336, discriminator_loss=0.026526549831032753\n",
            "step 8426: generator_loss=3.8561136722564697, discriminator_loss=0.026793239638209343\n",
            "step 8427: generator_loss=3.9099860191345215, discriminator_loss=0.026874959468841553\n",
            "step 8428: generator_loss=3.8564648628234863, discriminator_loss=0.02716688998043537\n",
            "step 8429: generator_loss=3.7869625091552734, discriminator_loss=0.027463456615805626\n",
            "step 8430: generator_loss=3.6452560424804688, discriminator_loss=0.028631839901208878\n",
            "step 8431: generator_loss=3.739248514175415, discriminator_loss=0.02729013003408909\n",
            "step 8432: generator_loss=3.5747857093811035, discriminator_loss=0.02866700291633606\n",
            "step 8433: generator_loss=3.7205300331115723, discriminator_loss=0.027639156207442284\n",
            "step 8434: generator_loss=3.5810985565185547, discriminator_loss=0.029201604425907135\n",
            "step 8435: generator_loss=3.5463128089904785, discriminator_loss=0.029150381684303284\n",
            "step 8436: generator_loss=3.619642734527588, discriminator_loss=0.028865206986665726\n",
            "step 8437: generator_loss=3.6123220920562744, discriminator_loss=0.028609447181224823\n",
            "step 8438: generator_loss=3.5713095664978027, discriminator_loss=0.02905445173382759\n",
            "step 8439: generator_loss=3.6014628410339355, discriminator_loss=0.029124075546860695\n",
            "step 8440: generator_loss=3.557891845703125, discriminator_loss=0.029950641095638275\n",
            "step 8441: generator_loss=3.4500885009765625, discriminator_loss=0.031919047236442566\n",
            "step 8442: generator_loss=3.31660795211792, discriminator_loss=0.03396076709032059\n",
            "step 8443: generator_loss=3.2771425247192383, discriminator_loss=0.035094089806079865\n",
            "step 8444: generator_loss=3.2422666549682617, discriminator_loss=0.036377958953380585\n",
            "step 8445: generator_loss=3.188197135925293, discriminator_loss=0.03756364434957504\n",
            "step 8446: generator_loss=3.29677677154541, discriminator_loss=0.037249766290187836\n",
            "step 8447: generator_loss=3.3489017486572266, discriminator_loss=0.03741715848445892\n",
            "step 8448: generator_loss=3.3806262016296387, discriminator_loss=0.036239586770534515\n",
            "step 8449: generator_loss=3.3973782062530518, discriminator_loss=0.03577866032719612\n",
            "step 8450: generator_loss=3.55775785446167, discriminator_loss=0.03393258899450302\n",
            "step 8451: generator_loss=3.5937886238098145, discriminator_loss=0.033982761204242706\n",
            "step 8452: generator_loss=3.507706642150879, discriminator_loss=0.034844137728214264\n",
            "step 8453: generator_loss=3.5483593940734863, discriminator_loss=0.03461277484893799\n",
            "step 8454: generator_loss=3.539699077606201, discriminator_loss=0.03478094935417175\n",
            "step 8455: generator_loss=3.5356764793395996, discriminator_loss=0.03491945564746857\n",
            "step 8456: generator_loss=3.5117440223693848, discriminator_loss=0.035355716943740845\n",
            "step 8457: generator_loss=3.4063022136688232, discriminator_loss=0.03683815896511078\n",
            "step 8458: generator_loss=3.296959638595581, discriminator_loss=0.0384863056242466\n",
            "step 8459: generator_loss=3.1852643489837646, discriminator_loss=0.040461696684360504\n",
            "step 8460: generator_loss=3.094235897064209, discriminator_loss=0.04233989119529724\n",
            "step 8461: generator_loss=3.1065871715545654, discriminator_loss=0.04201164096593857\n",
            "step 8462: generator_loss=3.216785430908203, discriminator_loss=0.03939537703990936\n",
            "step 8463: generator_loss=3.3656744956970215, discriminator_loss=0.036392878741025925\n",
            "step 8464: generator_loss=3.4990017414093018, discriminator_loss=0.03414461016654968\n",
            "step 8465: generator_loss=3.611571788787842, discriminator_loss=0.03243427723646164\n",
            "step 8466: generator_loss=3.6488118171691895, discriminator_loss=0.03175558149814606\n",
            "step 8467: generator_loss=3.7003753185272217, discriminator_loss=0.030813496559858322\n",
            "step 8468: generator_loss=3.756911516189575, discriminator_loss=0.029833689332008362\n",
            "step 8469: generator_loss=3.7416489124298096, discriminator_loss=0.029610102996230125\n",
            "step 8470: generator_loss=3.6880722045898438, discriminator_loss=0.029924456030130386\n",
            "step 8471: generator_loss=3.5738487243652344, discriminator_loss=0.030900733545422554\n",
            "step 8472: generator_loss=3.448391914367676, discriminator_loss=0.03244120627641678\n",
            "step 8473: generator_loss=3.3876092433929443, discriminator_loss=0.033145807683467865\n",
            "step 8474: generator_loss=3.3783271312713623, discriminator_loss=0.03289400786161423\n",
            "step 8475: generator_loss=3.36584734916687, discriminator_loss=0.032855093479156494\n",
            "step 8476: generator_loss=3.3632678985595703, discriminator_loss=0.03262711316347122\n",
            "step 8477: generator_loss=3.369107723236084, discriminator_loss=0.03242306038737297\n",
            "step 8478: generator_loss=3.3419830799102783, discriminator_loss=0.03268296271562576\n",
            "step 8479: generator_loss=3.3121049404144287, discriminator_loss=0.033209383487701416\n",
            "step 8480: generator_loss=3.284546136856079, discriminator_loss=0.03360907733440399\n",
            "step 8481: generator_loss=3.285679817199707, discriminator_loss=0.0336679108440876\n",
            "step 8482: generator_loss=3.3016252517700195, discriminator_loss=0.03351303189992905\n",
            "step 8483: generator_loss=3.3121681213378906, discriminator_loss=0.0332447811961174\n",
            "step 8484: generator_loss=3.347712755203247, discriminator_loss=0.03286735340952873\n",
            "step 8485: generator_loss=3.351101875305176, discriminator_loss=0.03273878991603851\n",
            "step 8486: generator_loss=3.3852479457855225, discriminator_loss=0.03235023468732834\n",
            "step 8487: generator_loss=3.416628360748291, discriminator_loss=0.03188313543796539\n",
            "step 8488: generator_loss=3.4447436332702637, discriminator_loss=0.031587086617946625\n",
            "step 8489: generator_loss=3.4022746086120605, discriminator_loss=0.032334499061107635\n",
            "step 8490: generator_loss=3.3766427040100098, discriminator_loss=0.03293055295944214\n",
            "step 8491: generator_loss=3.347764492034912, discriminator_loss=0.03351058065891266\n",
            "step 8492: generator_loss=3.314185380935669, discriminator_loss=0.03424815833568573\n",
            "step 8493: generator_loss=3.2998836040496826, discriminator_loss=0.03465419262647629\n",
            "step 8494: generator_loss=3.353299140930176, discriminator_loss=0.033789001405239105\n",
            "step 8495: generator_loss=3.3768134117126465, discriminator_loss=0.03343050926923752\n",
            "step 8496: generator_loss=3.3791918754577637, discriminator_loss=0.03357498720288277\n",
            "step 8497: generator_loss=3.3654603958129883, discriminator_loss=0.03397703915834427\n",
            "step 8498: generator_loss=3.3773512840270996, discriminator_loss=0.03386499732732773\n",
            "step 8499: generator_loss=3.437142848968506, discriminator_loss=0.03299376368522644\n",
            "step 8500: generator_loss=3.552677631378174, discriminator_loss=0.031185844913125038\n",
            "step 8501: generator_loss=3.6615724563598633, discriminator_loss=0.029717309400439262\n",
            "step 8502: generator_loss=3.742656707763672, discriminator_loss=0.028667891398072243\n",
            "step 8503: generator_loss=3.7777202129364014, discriminator_loss=0.028148559853434563\n",
            "step 8504: generator_loss=3.782989978790283, discriminator_loss=0.02791600301861763\n",
            "step 8505: generator_loss=3.7502880096435547, discriminator_loss=0.02815895900130272\n",
            "step 8506: generator_loss=3.699465751647949, discriminator_loss=0.02862980030477047\n",
            "step 8507: generator_loss=3.7394769191741943, discriminator_loss=0.027993155643343925\n",
            "step 8508: generator_loss=3.8097074031829834, discriminator_loss=0.026836415752768517\n",
            "step 8509: generator_loss=3.8423047065734863, discriminator_loss=0.026162687689065933\n",
            "step 8510: generator_loss=3.821438789367676, discriminator_loss=0.026152677834033966\n",
            "step 8511: generator_loss=3.7622721195220947, discriminator_loss=0.02649765834212303\n",
            "step 8512: generator_loss=3.687432050704956, discriminator_loss=0.0272088460624218\n",
            "step 8513: generator_loss=3.579596519470215, discriminator_loss=0.028437528759241104\n",
            "step 8514: generator_loss=3.4959044456481934, discriminator_loss=0.029458990320563316\n",
            "step 8515: generator_loss=3.428453207015991, discriminator_loss=0.03044043481349945\n",
            "step 8516: generator_loss=3.370785713195801, discriminator_loss=0.031576745212078094\n",
            "step 8517: generator_loss=3.3671181201934814, discriminator_loss=0.03184172138571739\n",
            "step 8518: generator_loss=3.35992431640625, discriminator_loss=0.032525621354579926\n",
            "step 8519: generator_loss=3.3681702613830566, discriminator_loss=0.03326408192515373\n",
            "step 8520: generator_loss=3.3941149711608887, discriminator_loss=0.034323547035455704\n",
            "step 8521: generator_loss=3.4833664894104004, discriminator_loss=0.03285051882266998\n",
            "step 8522: generator_loss=3.4893124103546143, discriminator_loss=0.034907493740320206\n",
            "step 8523: generator_loss=3.4734301567077637, discriminator_loss=0.03975595533847809\n",
            "step 8524: generator_loss=3.418165922164917, discriminator_loss=0.046172112226486206\n",
            "step 8525: generator_loss=3.788644313812256, discriminator_loss=0.03494509309530258\n",
            "step 8526: generator_loss=3.876768112182617, discriminator_loss=0.04044877737760544\n",
            "step 8527: generator_loss=3.8715524673461914, discriminator_loss=0.04896417260169983\n",
            "step 8528: generator_loss=3.90047550201416, discriminator_loss=0.05935119092464447\n",
            "step 8529: generator_loss=4.397463798522949, discriminator_loss=0.05031093209981918\n",
            "step 8530: generator_loss=4.527410507202148, discriminator_loss=0.054806970059871674\n",
            "step 8531: generator_loss=4.626575469970703, discriminator_loss=0.05967031791806221\n",
            "step 8532: generator_loss=4.798330783843994, discriminator_loss=0.056905753910541534\n",
            "step 8533: generator_loss=4.881706237792969, discriminator_loss=0.055564600974321365\n",
            "step 8534: generator_loss=5.3573503494262695, discriminator_loss=0.04305842146277428\n",
            "step 8535: generator_loss=6.0211381912231445, discriminator_loss=0.03623335063457489\n",
            "step 8536: generator_loss=6.5886616706848145, discriminator_loss=0.025467995554208755\n",
            "step 8537: generator_loss=6.516064643859863, discriminator_loss=0.02713269181549549\n",
            "step 8538: generator_loss=6.583929061889648, discriminator_loss=0.0268267672508955\n",
            "step 8539: generator_loss=6.859163761138916, discriminator_loss=0.024064674973487854\n",
            "step 8540: generator_loss=7.073429107666016, discriminator_loss=0.023459136486053467\n",
            "step 8541: generator_loss=7.11131477355957, discriminator_loss=0.024394039064645767\n",
            "step 8542: generator_loss=7.03333854675293, discriminator_loss=0.023963626474142075\n",
            "step 8543: generator_loss=6.928218841552734, discriminator_loss=0.025010939687490463\n",
            "step 8544: generator_loss=6.590486526489258, discriminator_loss=0.028406865894794464\n",
            "step 8545: generator_loss=6.323443412780762, discriminator_loss=0.030102655291557312\n",
            "step 8546: generator_loss=6.337680816650391, discriminator_loss=0.030085165053606033\n",
            "step 8547: generator_loss=6.024331092834473, discriminator_loss=0.030105814337730408\n",
            "step 8548: generator_loss=6.507781028747559, discriminator_loss=0.025033514946699142\n",
            "step 8549: generator_loss=6.351909637451172, discriminator_loss=0.02501101791858673\n",
            "step 8550: generator_loss=6.4365010261535645, discriminator_loss=0.023486636579036713\n",
            "step 8551: generator_loss=6.534700393676758, discriminator_loss=0.021233264356851578\n",
            "step 8552: generator_loss=6.489001750946045, discriminator_loss=0.019308844581246376\n",
            "step 8553: generator_loss=6.467992782592773, discriminator_loss=0.019496813416481018\n",
            "step 8554: generator_loss=6.218605995178223, discriminator_loss=0.01979808881878853\n",
            "step 8555: generator_loss=6.188144207000732, discriminator_loss=0.018956389278173447\n",
            "step 8556: generator_loss=5.890100479125977, discriminator_loss=0.020864097401499748\n",
            "step 8557: generator_loss=6.287047386169434, discriminator_loss=0.019856300204992294\n",
            "step 8558: generator_loss=5.701950550079346, discriminator_loss=0.021360503509640694\n",
            "step 8559: generator_loss=5.8595662117004395, discriminator_loss=0.01925278827548027\n",
            "step 8560: generator_loss=5.585414409637451, discriminator_loss=0.020947318524122238\n",
            "step 8561: generator_loss=5.341874122619629, discriminator_loss=0.022205498069524765\n",
            "step 8562: generator_loss=4.86149787902832, discriminator_loss=0.024641351774334908\n",
            "step 8563: generator_loss=4.760952472686768, discriminator_loss=0.025334443897008896\n",
            "step 8564: generator_loss=4.703799247741699, discriminator_loss=0.026339219883084297\n",
            "step 8565: generator_loss=4.7263336181640625, discriminator_loss=0.022757690399885178\n",
            "step 8566: generator_loss=4.837698459625244, discriminator_loss=0.021780021488666534\n",
            "step 8567: generator_loss=4.603724956512451, discriminator_loss=0.021006714552640915\n",
            "step 8568: generator_loss=4.740255355834961, discriminator_loss=0.020198963582515717\n",
            "step 8569: generator_loss=4.9895477294921875, discriminator_loss=0.018516426905989647\n",
            "step 8570: generator_loss=4.658592224121094, discriminator_loss=0.020584439858794212\n",
            "step 8571: generator_loss=4.7164306640625, discriminator_loss=0.02089744433760643\n",
            "step 8572: generator_loss=4.479292869567871, discriminator_loss=0.020780161023139954\n",
            "step 8573: generator_loss=4.403627395629883, discriminator_loss=0.022857490926980972\n",
            "step 8574: generator_loss=3.840620517730713, discriminator_loss=0.026247305795550346\n",
            "step 8575: generator_loss=4.027260780334473, discriminator_loss=0.02592024765908718\n",
            "step 8576: generator_loss=3.918515682220459, discriminator_loss=0.02937694452702999\n",
            "step 8577: generator_loss=3.7018396854400635, discriminator_loss=0.03068757802248001\n",
            "step 8578: generator_loss=3.576622486114502, discriminator_loss=0.032179247587919235\n",
            "step 8579: generator_loss=3.469113826751709, discriminator_loss=0.03400620073080063\n",
            "step 8580: generator_loss=3.4058313369750977, discriminator_loss=0.03454594314098358\n",
            "step 8581: generator_loss=3.411971092224121, discriminator_loss=0.03376302495598793\n",
            "step 8582: generator_loss=3.551370143890381, discriminator_loss=0.03193341940641403\n",
            "step 8583: generator_loss=3.3912744522094727, discriminator_loss=0.032090745866298676\n",
            "step 8584: generator_loss=3.5172512531280518, discriminator_loss=0.030065912753343582\n",
            "step 8585: generator_loss=3.489469051361084, discriminator_loss=0.03005952015519142\n",
            "step 8586: generator_loss=3.5987703800201416, discriminator_loss=0.02924158237874508\n",
            "step 8587: generator_loss=3.570639133453369, discriminator_loss=0.029545608907938004\n",
            "step 8588: generator_loss=3.5436744689941406, discriminator_loss=0.030076676979660988\n",
            "step 8589: generator_loss=3.458590030670166, discriminator_loss=0.03154865652322769\n",
            "step 8590: generator_loss=3.3839011192321777, discriminator_loss=0.03328508883714676\n",
            "step 8591: generator_loss=3.2950448989868164, discriminator_loss=0.035190023481845856\n",
            "step 8592: generator_loss=3.2602906227111816, discriminator_loss=0.036453865468502045\n",
            "step 8593: generator_loss=3.240386486053467, discriminator_loss=0.03717987984418869\n",
            "step 8594: generator_loss=3.3157224655151367, discriminator_loss=0.0362313836812973\n",
            "step 8595: generator_loss=3.3787662982940674, discriminator_loss=0.035586290061473846\n",
            "step 8596: generator_loss=3.5103118419647217, discriminator_loss=0.03365129232406616\n",
            "step 8597: generator_loss=3.7157487869262695, discriminator_loss=0.03145868703722954\n",
            "step 8598: generator_loss=3.795437812805176, discriminator_loss=0.030475424602627754\n",
            "step 8599: generator_loss=3.7907137870788574, discriminator_loss=0.030133860185742378\n",
            "step 8600: generator_loss=3.774172306060791, discriminator_loss=0.030246762558817863\n",
            "step 8601: generator_loss=3.6800289154052734, discriminator_loss=0.031167274340987206\n",
            "step 8602: generator_loss=3.564343214035034, discriminator_loss=0.03250579535961151\n",
            "step 8603: generator_loss=3.446668863296509, discriminator_loss=0.03387771546840668\n",
            "step 8604: generator_loss=3.5057437419891357, discriminator_loss=0.03269348293542862\n",
            "step 8605: generator_loss=3.618272066116333, discriminator_loss=0.030687835067510605\n",
            "step 8606: generator_loss=3.7182199954986572, discriminator_loss=0.029026787728071213\n",
            "step 8607: generator_loss=3.754356861114502, discriminator_loss=0.028218738734722137\n",
            "step 8608: generator_loss=3.728240489959717, discriminator_loss=0.02818170189857483\n",
            "step 8609: generator_loss=3.636920690536499, discriminator_loss=0.02905505895614624\n",
            "step 8610: generator_loss=3.5086801052093506, discriminator_loss=0.03060299903154373\n",
            "step 8611: generator_loss=3.4265809059143066, discriminator_loss=0.03154981881380081\n",
            "step 8612: generator_loss=3.3881750106811523, discriminator_loss=0.03196772187948227\n",
            "step 8613: generator_loss=3.4706976413726807, discriminator_loss=0.030458996072411537\n",
            "step 8614: generator_loss=3.594496726989746, discriminator_loss=0.02851954475045204\n",
            "step 8615: generator_loss=3.69520902633667, discriminator_loss=0.027239523828029633\n",
            "step 8616: generator_loss=3.7584095001220703, discriminator_loss=0.02676604874432087\n",
            "step 8617: generator_loss=3.7660727500915527, discriminator_loss=0.026258938014507294\n",
            "step 8618: generator_loss=3.80751633644104, discriminator_loss=0.025756437331438065\n",
            "step 8619: generator_loss=3.7488231658935547, discriminator_loss=0.026714591309428215\n",
            "step 8620: generator_loss=3.72102427482605, discriminator_loss=0.02697208896279335\n",
            "step 8621: generator_loss=3.7007031440734863, discriminator_loss=0.027045927941799164\n",
            "step 8622: generator_loss=3.6967856884002686, discriminator_loss=0.02756424807012081\n",
            "step 8623: generator_loss=3.682425022125244, discriminator_loss=0.027651317417621613\n",
            "step 8624: generator_loss=3.753196954727173, discriminator_loss=0.02698466181755066\n",
            "step 8625: generator_loss=3.747584342956543, discriminator_loss=0.0271310955286026\n",
            "step 8626: generator_loss=3.8265433311462402, discriminator_loss=0.025397688150405884\n",
            "step 8627: generator_loss=3.82287859916687, discriminator_loss=0.026030197739601135\n",
            "step 8628: generator_loss=3.9131643772125244, discriminator_loss=0.024670174345374107\n",
            "step 8629: generator_loss=3.84328556060791, discriminator_loss=0.025226205587387085\n",
            "step 8630: generator_loss=3.912351131439209, discriminator_loss=0.02435619942843914\n",
            "step 8631: generator_loss=3.9853053092956543, discriminator_loss=0.023620184510946274\n",
            "step 8632: generator_loss=3.9698636531829834, discriminator_loss=0.024588890373706818\n",
            "step 8633: generator_loss=3.956138849258423, discriminator_loss=0.025573108345270157\n",
            "step 8634: generator_loss=3.9848430156707764, discriminator_loss=0.027443155646324158\n",
            "step 8635: generator_loss=3.8953423500061035, discriminator_loss=0.034963417798280716\n",
            "step 8636: generator_loss=4.113317489624023, discriminator_loss=0.032402075827121735\n",
            "step 8637: generator_loss=4.038370132446289, discriminator_loss=0.04926906153559685\n",
            "step 8638: generator_loss=4.0982985496521, discriminator_loss=0.05131441727280617\n",
            "step 8639: generator_loss=4.229913234710693, discriminator_loss=0.05659443885087967\n",
            "step 8640: generator_loss=4.7544755935668945, discriminator_loss=0.03705728054046631\n",
            "step 8641: generator_loss=4.977537155151367, discriminator_loss=0.042731333523988724\n",
            "step 8642: generator_loss=5.495766639709473, discriminator_loss=0.028820641338825226\n",
            "step 8643: generator_loss=5.884357452392578, discriminator_loss=0.025440044701099396\n",
            "step 8644: generator_loss=6.362131118774414, discriminator_loss=0.020575854927301407\n",
            "step 8645: generator_loss=6.6797943115234375, discriminator_loss=0.01857743225991726\n",
            "step 8646: generator_loss=6.813048362731934, discriminator_loss=0.018241746351122856\n",
            "step 8647: generator_loss=6.739907741546631, discriminator_loss=0.017753921449184418\n",
            "step 8648: generator_loss=6.877204895019531, discriminator_loss=0.018024757504463196\n",
            "step 8649: generator_loss=6.756061553955078, discriminator_loss=0.01870756410062313\n",
            "step 8650: generator_loss=6.321094989776611, discriminator_loss=0.02364836260676384\n",
            "step 8651: generator_loss=6.677334785461426, discriminator_loss=0.022005388513207436\n",
            "step 8652: generator_loss=6.239841461181641, discriminator_loss=0.023800335824489594\n",
            "step 8653: generator_loss=5.835411071777344, discriminator_loss=0.024019591510295868\n",
            "step 8654: generator_loss=5.5061211585998535, discriminator_loss=0.023418106138706207\n",
            "step 8655: generator_loss=5.749741077423096, discriminator_loss=0.019712720066308975\n",
            "step 8656: generator_loss=5.748692512512207, discriminator_loss=0.018538963049650192\n",
            "step 8657: generator_loss=5.747827529907227, discriminator_loss=0.01762089878320694\n",
            "step 8658: generator_loss=5.553189277648926, discriminator_loss=0.017541322857141495\n",
            "step 8659: generator_loss=5.643850326538086, discriminator_loss=0.01643148623406887\n",
            "step 8660: generator_loss=5.617525100708008, discriminator_loss=0.01595940813422203\n",
            "step 8661: generator_loss=5.115044116973877, discriminator_loss=0.018386922776699066\n",
            "step 8662: generator_loss=4.975842475891113, discriminator_loss=0.01908743754029274\n",
            "step 8663: generator_loss=4.79924201965332, discriminator_loss=0.01999327354133129\n",
            "step 8664: generator_loss=4.762085437774658, discriminator_loss=0.019382193684577942\n",
            "step 8665: generator_loss=4.754937171936035, discriminator_loss=0.01847190037369728\n",
            "step 8666: generator_loss=4.568159580230713, discriminator_loss=0.018951822072267532\n",
            "step 8667: generator_loss=4.526881217956543, discriminator_loss=0.018659688532352448\n",
            "step 8668: generator_loss=4.2202277183532715, discriminator_loss=0.02067781426012516\n",
            "step 8669: generator_loss=4.258865833282471, discriminator_loss=0.020453451201319695\n",
            "step 8670: generator_loss=4.100620269775391, discriminator_loss=0.021656140685081482\n",
            "step 8671: generator_loss=4.027038097381592, discriminator_loss=0.021905317902565002\n",
            "step 8672: generator_loss=3.893409013748169, discriminator_loss=0.02334340289235115\n",
            "step 8673: generator_loss=3.8903942108154297, discriminator_loss=0.023526128381490707\n",
            "step 8674: generator_loss=3.699267625808716, discriminator_loss=0.024572892114520073\n",
            "step 8675: generator_loss=3.909409284591675, discriminator_loss=0.0244365893304348\n",
            "step 8676: generator_loss=3.7118442058563232, discriminator_loss=0.025242120027542114\n",
            "step 8677: generator_loss=4.023144721984863, discriminator_loss=0.024212419986724854\n",
            "step 8678: generator_loss=3.919717311859131, discriminator_loss=0.025201715528964996\n",
            "step 8679: generator_loss=3.7235851287841797, discriminator_loss=0.02586459182202816\n",
            "step 8680: generator_loss=3.875092029571533, discriminator_loss=0.02560906484723091\n",
            "step 8681: generator_loss=4.104870796203613, discriminator_loss=0.024763045832514763\n",
            "step 8682: generator_loss=3.9310507774353027, discriminator_loss=0.025663338601589203\n",
            "step 8683: generator_loss=3.9210686683654785, discriminator_loss=0.026472240686416626\n",
            "step 8684: generator_loss=4.071024417877197, discriminator_loss=0.027319133281707764\n",
            "step 8685: generator_loss=3.588146448135376, discriminator_loss=0.030122986063361168\n",
            "step 8686: generator_loss=3.7553718090057373, discriminator_loss=0.030872998759150505\n",
            "step 8687: generator_loss=3.559298038482666, discriminator_loss=0.0326983779668808\n",
            "step 8688: generator_loss=3.4294238090515137, discriminator_loss=0.034432247281074524\n",
            "step 8689: generator_loss=3.806168556213379, discriminator_loss=0.0331404022872448\n",
            "step 8690: generator_loss=3.7983367443084717, discriminator_loss=0.035143475979566574\n",
            "step 8691: generator_loss=3.4919779300689697, discriminator_loss=0.03804805874824524\n",
            "step 8692: generator_loss=3.363191843032837, discriminator_loss=0.038275085389614105\n",
            "step 8693: generator_loss=3.613711357116699, discriminator_loss=0.03651967644691467\n",
            "step 8694: generator_loss=3.554475784301758, discriminator_loss=0.03683271259069443\n",
            "step 8695: generator_loss=3.426609754562378, discriminator_loss=0.03722349554300308\n",
            "step 8696: generator_loss=3.4329833984375, discriminator_loss=0.037236906588077545\n",
            "step 8697: generator_loss=3.470951557159424, discriminator_loss=0.036712002009153366\n",
            "step 8698: generator_loss=3.5847878456115723, discriminator_loss=0.035757098346948624\n",
            "step 8699: generator_loss=3.524343967437744, discriminator_loss=0.03648436814546585\n",
            "step 8700: generator_loss=3.4526286125183105, discriminator_loss=0.037753306329250336\n",
            "step 8701: generator_loss=3.3809943199157715, discriminator_loss=0.039054397493600845\n",
            "step 8702: generator_loss=3.314343214035034, discriminator_loss=0.039894528687000275\n",
            "step 8703: generator_loss=3.315330743789673, discriminator_loss=0.03997744619846344\n",
            "step 8704: generator_loss=3.2813806533813477, discriminator_loss=0.04048996418714523\n",
            "step 8705: generator_loss=3.222075939178467, discriminator_loss=0.04170282557606697\n",
            "step 8706: generator_loss=3.2996177673339844, discriminator_loss=0.0400271937251091\n",
            "step 8707: generator_loss=3.394314765930176, discriminator_loss=0.0381842777132988\n",
            "step 8708: generator_loss=3.4721336364746094, discriminator_loss=0.036617860198020935\n",
            "step 8709: generator_loss=3.5375237464904785, discriminator_loss=0.035310134291648865\n",
            "step 8710: generator_loss=3.5663654804229736, discriminator_loss=0.034493155777454376\n",
            "step 8711: generator_loss=3.5452487468719482, discriminator_loss=0.03427042067050934\n",
            "step 8712: generator_loss=3.4999334812164307, discriminator_loss=0.03436700999736786\n",
            "step 8713: generator_loss=3.497377395629883, discriminator_loss=0.03370056301355362\n",
            "step 8714: generator_loss=3.5213801860809326, discriminator_loss=0.032640036195516586\n",
            "step 8715: generator_loss=3.5959391593933105, discriminator_loss=0.030968505889177322\n",
            "step 8716: generator_loss=3.656140089035034, discriminator_loss=0.029531747102737427\n",
            "step 8717: generator_loss=3.698472738265991, discriminator_loss=0.028419747948646545\n",
            "step 8718: generator_loss=3.7290945053100586, discriminator_loss=0.027419906109571457\n",
            "step 8719: generator_loss=3.722663640975952, discriminator_loss=0.02696121856570244\n",
            "step 8720: generator_loss=3.71994686126709, discriminator_loss=0.026609240099787712\n",
            "step 8721: generator_loss=3.7296416759490967, discriminator_loss=0.026066679507493973\n",
            "step 8722: generator_loss=3.711360216140747, discriminator_loss=0.02598128840327263\n",
            "step 8723: generator_loss=3.671633720397949, discriminator_loss=0.026281412690877914\n",
            "step 8724: generator_loss=3.6009039878845215, discriminator_loss=0.026962287724018097\n",
            "step 8725: generator_loss=3.513758897781372, discriminator_loss=0.028086498379707336\n",
            "step 8726: generator_loss=3.466052293777466, discriminator_loss=0.02870788611471653\n",
            "step 8727: generator_loss=3.47012996673584, discriminator_loss=0.028571760281920433\n",
            "step 8728: generator_loss=3.4981985092163086, discriminator_loss=0.028150204569101334\n",
            "step 8729: generator_loss=3.5678248405456543, discriminator_loss=0.027111735194921494\n",
            "step 8730: generator_loss=3.6723556518554688, discriminator_loss=0.025753574445843697\n",
            "step 8731: generator_loss=3.749701499938965, discriminator_loss=0.02481325902044773\n",
            "step 8732: generator_loss=3.8468198776245117, discriminator_loss=0.02376706153154373\n",
            "step 8733: generator_loss=3.9240570068359375, discriminator_loss=0.02303110808134079\n",
            "step 8734: generator_loss=3.9680843353271484, discriminator_loss=0.02258947677910328\n",
            "step 8735: generator_loss=3.9461426734924316, discriminator_loss=0.022801633924245834\n",
            "step 8736: generator_loss=3.9999914169311523, discriminator_loss=0.022227153182029724\n",
            "step 8737: generator_loss=3.99005126953125, discriminator_loss=0.02221478335559368\n",
            "step 8738: generator_loss=3.9727203845977783, discriminator_loss=0.022257376462221146\n",
            "step 8739: generator_loss=3.9516243934631348, discriminator_loss=0.022291384637355804\n",
            "step 8740: generator_loss=3.915241241455078, discriminator_loss=0.0225207582116127\n",
            "step 8741: generator_loss=3.900639772415161, discriminator_loss=0.02252025716006756\n",
            "step 8742: generator_loss=3.854828357696533, discriminator_loss=0.022835517302155495\n",
            "step 8743: generator_loss=3.844663619995117, discriminator_loss=0.022840963676571846\n",
            "step 8744: generator_loss=3.8657045364379883, discriminator_loss=0.02246721088886261\n",
            "step 8745: generator_loss=3.8857436180114746, discriminator_loss=0.02210940606892109\n",
            "step 8746: generator_loss=3.8752379417419434, discriminator_loss=0.02208966389298439\n",
            "step 8747: generator_loss=3.856414794921875, discriminator_loss=0.022259391844272614\n",
            "step 8748: generator_loss=3.8446712493896484, discriminator_loss=0.02224745601415634\n",
            "step 8749: generator_loss=3.8087286949157715, discriminator_loss=0.02258582040667534\n",
            "step 8750: generator_loss=3.8236846923828125, discriminator_loss=0.02235804870724678\n",
            "step 8751: generator_loss=3.847410202026367, discriminator_loss=0.022079691290855408\n",
            "step 8752: generator_loss=3.8891549110412598, discriminator_loss=0.02162339724600315\n",
            "step 8753: generator_loss=3.911445140838623, discriminator_loss=0.021402547135949135\n",
            "step 8754: generator_loss=3.895915985107422, discriminator_loss=0.021575666964054108\n",
            "step 8755: generator_loss=3.867015838623047, discriminator_loss=0.02194155938923359\n",
            "step 8756: generator_loss=3.8421103954315186, discriminator_loss=0.022261861711740494\n",
            "step 8757: generator_loss=3.811520576477051, discriminator_loss=0.022637879475951195\n",
            "step 8758: generator_loss=3.794931650161743, discriminator_loss=0.02292988821864128\n",
            "step 8759: generator_loss=3.774380922317505, discriminator_loss=0.023252539336681366\n",
            "step 8760: generator_loss=3.7343997955322266, discriminator_loss=0.02383733168244362\n",
            "step 8761: generator_loss=3.698434591293335, discriminator_loss=0.024322621524333954\n",
            "step 8762: generator_loss=3.733713388442993, discriminator_loss=0.024101538583636284\n",
            "step 8763: generator_loss=3.756438732147217, discriminator_loss=0.023907601833343506\n",
            "step 8764: generator_loss=3.784423828125, discriminator_loss=0.023667624220252037\n",
            "step 8765: generator_loss=3.820991039276123, discriminator_loss=0.023331664502620697\n",
            "step 8766: generator_loss=3.841797351837158, discriminator_loss=0.02316020242869854\n",
            "step 8767: generator_loss=3.8415040969848633, discriminator_loss=0.023213662207126617\n",
            "step 8768: generator_loss=3.8648834228515625, discriminator_loss=0.02298833429813385\n",
            "step 8769: generator_loss=3.900153160095215, discriminator_loss=0.02281266450881958\n",
            "step 8770: generator_loss=3.896419048309326, discriminator_loss=0.022737978026270866\n",
            "step 8771: generator_loss=3.8674283027648926, discriminator_loss=0.02304946817457676\n",
            "step 8772: generator_loss=3.789720058441162, discriminator_loss=0.023876868188381195\n",
            "step 8773: generator_loss=3.7011971473693848, discriminator_loss=0.024961920455098152\n",
            "step 8774: generator_loss=3.642171859741211, discriminator_loss=0.025698501616716385\n",
            "step 8775: generator_loss=3.6011312007904053, discriminator_loss=0.0263264961540699\n",
            "step 8776: generator_loss=3.5987679958343506, discriminator_loss=0.026391737163066864\n",
            "step 8777: generator_loss=3.604557514190674, discriminator_loss=0.02635573223233223\n",
            "step 8778: generator_loss=3.6551313400268555, discriminator_loss=0.02571934275329113\n",
            "step 8779: generator_loss=3.697902202606201, discriminator_loss=0.025270015001296997\n",
            "step 8780: generator_loss=3.730536460876465, discriminator_loss=0.024975109845399857\n",
            "step 8781: generator_loss=3.780636787414551, discriminator_loss=0.02455100603401661\n",
            "step 8782: generator_loss=3.8149499893188477, discriminator_loss=0.024160943925380707\n",
            "step 8783: generator_loss=3.8684959411621094, discriminator_loss=0.023606838658452034\n",
            "step 8784: generator_loss=3.8985958099365234, discriminator_loss=0.02331511303782463\n",
            "step 8785: generator_loss=3.9003746509552, discriminator_loss=0.023367440328001976\n",
            "step 8786: generator_loss=3.8736674785614014, discriminator_loss=0.023701898753643036\n",
            "step 8787: generator_loss=3.826169013977051, discriminator_loss=0.024166129529476166\n",
            "step 8788: generator_loss=3.824052572250366, discriminator_loss=0.02420070394873619\n",
            "step 8789: generator_loss=3.8156678676605225, discriminator_loss=0.024359343573451042\n",
            "step 8790: generator_loss=3.8451154232025146, discriminator_loss=0.024059630930423737\n",
            "step 8791: generator_loss=3.857969045639038, discriminator_loss=0.02392146736383438\n",
            "step 8792: generator_loss=3.85456919670105, discriminator_loss=0.024013284593820572\n",
            "step 8793: generator_loss=3.8351030349731445, discriminator_loss=0.024220559746026993\n",
            "step 8794: generator_loss=3.8257670402526855, discriminator_loss=0.024368803948163986\n",
            "step 8795: generator_loss=3.8491225242614746, discriminator_loss=0.024111486971378326\n",
            "step 8796: generator_loss=3.8329365253448486, discriminator_loss=0.024316519498825073\n",
            "step 8797: generator_loss=3.803114414215088, discriminator_loss=0.02456916868686676\n",
            "step 8798: generator_loss=3.797907590866089, discriminator_loss=0.02458571270108223\n",
            "step 8799: generator_loss=3.8122658729553223, discriminator_loss=0.024388788267970085\n",
            "step 8800: generator_loss=3.7979769706726074, discriminator_loss=0.024532252922654152\n",
            "step 8801: generator_loss=3.7482528686523438, discriminator_loss=0.025031480938196182\n",
            "step 8802: generator_loss=3.6888153553009033, discriminator_loss=0.025691870599985123\n",
            "step 8803: generator_loss=3.669121742248535, discriminator_loss=0.025889821350574493\n",
            "step 8804: generator_loss=3.681220769882202, discriminator_loss=0.025681335479021072\n",
            "step 8805: generator_loss=3.7092132568359375, discriminator_loss=0.0252990685403347\n",
            "step 8806: generator_loss=3.7259602546691895, discriminator_loss=0.025067798793315887\n",
            "step 8807: generator_loss=3.7194995880126953, discriminator_loss=0.025100048631429672\n",
            "step 8808: generator_loss=3.7098889350891113, discriminator_loss=0.02525903657078743\n",
            "step 8809: generator_loss=3.706416130065918, discriminator_loss=0.025279495865106583\n",
            "step 8810: generator_loss=3.6815102100372314, discriminator_loss=0.025611355900764465\n",
            "step 8811: generator_loss=3.63877534866333, discriminator_loss=0.02615932933986187\n",
            "step 8812: generator_loss=3.5971288681030273, discriminator_loss=0.026746567338705063\n",
            "step 8813: generator_loss=3.5542049407958984, discriminator_loss=0.027399135753512383\n",
            "step 8814: generator_loss=3.5558927059173584, discriminator_loss=0.027394242584705353\n",
            "step 8815: generator_loss=3.582282543182373, discriminator_loss=0.02714460715651512\n",
            "step 8816: generator_loss=3.6142518520355225, discriminator_loss=0.026787441223859787\n",
            "step 8817: generator_loss=3.6749842166900635, discriminator_loss=0.026099918410182\n",
            "step 8818: generator_loss=3.7647504806518555, discriminator_loss=0.025111138820648193\n",
            "step 8819: generator_loss=3.869905948638916, discriminator_loss=0.02404092624783516\n",
            "step 8820: generator_loss=3.984312057495117, discriminator_loss=0.023008592426776886\n",
            "step 8821: generator_loss=4.039231300354004, discriminator_loss=0.022577200084924698\n",
            "step 8822: generator_loss=4.027902603149414, discriminator_loss=0.022636208683252335\n",
            "step 8823: generator_loss=3.9670217037200928, discriminator_loss=0.023317478597164154\n",
            "step 8824: generator_loss=3.912242889404297, discriminator_loss=0.023831071332097054\n",
            "step 8825: generator_loss=3.8440351486206055, discriminator_loss=0.024392805993556976\n",
            "step 8826: generator_loss=3.7580273151397705, discriminator_loss=0.025226876139640808\n",
            "step 8827: generator_loss=3.6790060997009277, discriminator_loss=0.02616388164460659\n",
            "step 8828: generator_loss=3.6434690952301025, discriminator_loss=0.026547923684120178\n",
            "step 8829: generator_loss=3.579101085662842, discriminator_loss=0.027665767818689346\n",
            "step 8830: generator_loss=3.5733962059020996, discriminator_loss=0.02788613736629486\n",
            "step 8831: generator_loss=3.535752534866333, discriminator_loss=0.02896467223763466\n",
            "step 8832: generator_loss=3.6067121028900146, discriminator_loss=0.02942950278520584\n",
            "step 8833: generator_loss=3.5100297927856445, discriminator_loss=0.034279756247997284\n",
            "step 8834: generator_loss=3.7303695678710938, discriminator_loss=0.02852652594447136\n",
            "step 8835: generator_loss=3.7042219638824463, discriminator_loss=0.03595558926463127\n",
            "step 8836: generator_loss=3.8821229934692383, discriminator_loss=0.038712650537490845\n",
            "step 8837: generator_loss=4.078807830810547, discriminator_loss=0.038544945418834686\n",
            "step 8838: generator_loss=4.154294967651367, discriminator_loss=0.04830074682831764\n",
            "step 8839: generator_loss=4.484301567077637, discriminator_loss=0.04790670797228813\n",
            "step 8840: generator_loss=5.020122528076172, discriminator_loss=0.031500108540058136\n",
            "step 8841: generator_loss=4.87550163269043, discriminator_loss=0.050494078546762466\n",
            "step 8842: generator_loss=5.337439060211182, discriminator_loss=0.03839961439371109\n",
            "step 8843: generator_loss=5.863719463348389, discriminator_loss=0.028453543782234192\n",
            "step 8844: generator_loss=6.1450042724609375, discriminator_loss=0.027082178741693497\n",
            "step 8845: generator_loss=6.586141586303711, discriminator_loss=0.024102400988340378\n",
            "step 8846: generator_loss=6.646865367889404, discriminator_loss=0.025708992034196854\n",
            "step 8847: generator_loss=7.158995628356934, discriminator_loss=0.023493774235248566\n",
            "step 8848: generator_loss=7.294173240661621, discriminator_loss=0.024580340832471848\n",
            "step 8849: generator_loss=7.451398849487305, discriminator_loss=0.025435470044612885\n",
            "step 8850: generator_loss=7.075620174407959, discriminator_loss=0.027949245646595955\n",
            "step 8851: generator_loss=7.7751545906066895, discriminator_loss=0.024598803371191025\n",
            "step 8852: generator_loss=7.451247215270996, discriminator_loss=0.026183554902672768\n",
            "step 8853: generator_loss=7.378922462463379, discriminator_loss=0.031685009598731995\n",
            "step 8854: generator_loss=7.429764747619629, discriminator_loss=0.029317058622837067\n",
            "step 8855: generator_loss=7.5718889236450195, discriminator_loss=0.023503366857767105\n",
            "step 8856: generator_loss=7.916954040527344, discriminator_loss=0.019438687711954117\n",
            "step 8857: generator_loss=7.349454879760742, discriminator_loss=0.020914876833558083\n",
            "step 8858: generator_loss=8.124150276184082, discriminator_loss=0.016705915331840515\n",
            "step 8859: generator_loss=7.533236980438232, discriminator_loss=0.017407357692718506\n",
            "step 8860: generator_loss=8.068548202514648, discriminator_loss=0.01568021811544895\n",
            "step 8861: generator_loss=8.222370147705078, discriminator_loss=0.014465139247477055\n",
            "step 8862: generator_loss=8.427298545837402, discriminator_loss=0.013369030319154263\n",
            "step 8863: generator_loss=7.9565019607543945, discriminator_loss=0.01382162794470787\n",
            "step 8864: generator_loss=7.709761142730713, discriminator_loss=0.014216659590601921\n",
            "step 8865: generator_loss=7.344252586364746, discriminator_loss=0.014680571854114532\n",
            "step 8866: generator_loss=7.431963920593262, discriminator_loss=0.014843052253127098\n",
            "step 8867: generator_loss=6.891648292541504, discriminator_loss=0.01646921969950199\n",
            "step 8868: generator_loss=6.546907424926758, discriminator_loss=0.018032606691122055\n",
            "step 8869: generator_loss=6.294686317443848, discriminator_loss=0.01789851114153862\n",
            "step 8870: generator_loss=6.338162422180176, discriminator_loss=0.01658996008336544\n",
            "step 8871: generator_loss=5.89700984954834, discriminator_loss=0.01916634850203991\n",
            "step 8872: generator_loss=5.841293811798096, discriminator_loss=0.018235936760902405\n",
            "step 8873: generator_loss=5.667433738708496, discriminator_loss=0.01800500601530075\n",
            "step 8874: generator_loss=5.44345760345459, discriminator_loss=0.01814795657992363\n",
            "step 8875: generator_loss=5.84138298034668, discriminator_loss=0.013735074549913406\n",
            "step 8876: generator_loss=5.551748275756836, discriminator_loss=0.01378371100872755\n",
            "step 8877: generator_loss=5.5420427322387695, discriminator_loss=0.013254420831799507\n",
            "step 8878: generator_loss=5.56550407409668, discriminator_loss=0.0131206214427948\n",
            "step 8879: generator_loss=5.1635894775390625, discriminator_loss=0.014776675030589104\n",
            "step 8880: generator_loss=5.133802890777588, discriminator_loss=0.014430191367864609\n",
            "step 8881: generator_loss=5.003562927246094, discriminator_loss=0.014550352469086647\n",
            "step 8882: generator_loss=4.90286922454834, discriminator_loss=0.014677360653877258\n",
            "step 8883: generator_loss=4.793605804443359, discriminator_loss=0.01539914682507515\n",
            "step 8884: generator_loss=4.422993183135986, discriminator_loss=0.016595926135778427\n",
            "step 8885: generator_loss=4.311771869659424, discriminator_loss=0.018539827316999435\n",
            "step 8886: generator_loss=4.250484466552734, discriminator_loss=0.019604777917265892\n",
            "step 8887: generator_loss=4.085112571716309, discriminator_loss=0.01907660812139511\n",
            "step 8888: generator_loss=3.9777019023895264, discriminator_loss=0.02043171599507332\n",
            "step 8889: generator_loss=3.759538173675537, discriminator_loss=0.02306179329752922\n",
            "step 8890: generator_loss=3.698072671890259, discriminator_loss=0.023332607001066208\n",
            "step 8891: generator_loss=3.7055506706237793, discriminator_loss=0.024229558184742928\n",
            "step 8892: generator_loss=3.5729317665100098, discriminator_loss=0.02439623698592186\n",
            "step 8893: generator_loss=3.54565691947937, discriminator_loss=0.024671312421560287\n",
            "step 8894: generator_loss=3.6828224658966064, discriminator_loss=0.02368924207985401\n",
            "step 8895: generator_loss=3.6500110626220703, discriminator_loss=0.022875625640153885\n",
            "step 8896: generator_loss=3.9501051902770996, discriminator_loss=0.02081076242029667\n",
            "step 8897: generator_loss=4.18714714050293, discriminator_loss=0.01914217323064804\n",
            "step 8898: generator_loss=4.137744903564453, discriminator_loss=0.018522020429372787\n",
            "step 8899: generator_loss=4.254879951477051, discriminator_loss=0.017822638154029846\n",
            "step 8900: generator_loss=4.263156414031982, discriminator_loss=0.017915694043040276\n",
            "step 8901: generator_loss=4.199131488800049, discriminator_loss=0.01823241077363491\n",
            "step 8902: generator_loss=4.1999406814575195, discriminator_loss=0.0190142635256052\n",
            "step 8903: generator_loss=4.102819442749023, discriminator_loss=0.019776249304413795\n",
            "step 8904: generator_loss=3.868370294570923, discriminator_loss=0.021587403491139412\n",
            "step 8905: generator_loss=3.803382635116577, discriminator_loss=0.022766156122088432\n",
            "step 8906: generator_loss=3.7272796630859375, discriminator_loss=0.02413204312324524\n",
            "step 8907: generator_loss=3.6618435382843018, discriminator_loss=0.024928146973252296\n",
            "step 8908: generator_loss=3.5765316486358643, discriminator_loss=0.02593259885907173\n",
            "step 8909: generator_loss=3.4908337593078613, discriminator_loss=0.02720828540623188\n",
            "step 8910: generator_loss=3.4873557090759277, discriminator_loss=0.02756456285715103\n",
            "step 8911: generator_loss=3.5854239463806152, discriminator_loss=0.027271907776594162\n",
            "step 8912: generator_loss=3.530996322631836, discriminator_loss=0.02757864072918892\n",
            "step 8913: generator_loss=3.6508114337921143, discriminator_loss=0.02652721479535103\n",
            "step 8914: generator_loss=3.605710506439209, discriminator_loss=0.027221279218792915\n",
            "step 8915: generator_loss=3.6450257301330566, discriminator_loss=0.02657845988869667\n",
            "step 8916: generator_loss=3.769308090209961, discriminator_loss=0.02563084289431572\n",
            "step 8917: generator_loss=3.6852025985717773, discriminator_loss=0.026840444654226303\n",
            "step 8918: generator_loss=3.632876396179199, discriminator_loss=0.028631344437599182\n",
            "step 8919: generator_loss=3.6558165550231934, discriminator_loss=0.029330462217330933\n",
            "step 8920: generator_loss=3.611907958984375, discriminator_loss=0.03211798518896103\n",
            "step 8921: generator_loss=3.572683811187744, discriminator_loss=0.039786145091056824\n",
            "step 8922: generator_loss=3.593926429748535, discriminator_loss=0.046333156526088715\n",
            "step 8923: generator_loss=3.6249992847442627, discriminator_loss=0.04165433719754219\n",
            "step 8924: generator_loss=3.539461851119995, discriminator_loss=0.08279797434806824\n",
            "step 8925: generator_loss=3.6357245445251465, discriminator_loss=0.09721936285495758\n",
            "step 8926: generator_loss=3.829631805419922, discriminator_loss=0.10616495460271835\n",
            "step 8927: generator_loss=4.105481147766113, discriminator_loss=0.09419712424278259\n",
            "step 8928: generator_loss=4.12054967880249, discriminator_loss=0.11915349215269089\n",
            "step 8929: generator_loss=4.509205341339111, discriminator_loss=0.08482661843299866\n",
            "step 8930: generator_loss=4.916125774383545, discriminator_loss=0.0725606232881546\n",
            "step 8931: generator_loss=5.461905479431152, discriminator_loss=0.045187488198280334\n",
            "step 8932: generator_loss=6.279417037963867, discriminator_loss=0.03300899267196655\n",
            "step 8933: generator_loss=6.55259895324707, discriminator_loss=0.031189225614070892\n",
            "step 8934: generator_loss=7.225594520568848, discriminator_loss=0.02852180041372776\n",
            "step 8935: generator_loss=7.464776515960693, discriminator_loss=0.028454773128032684\n",
            "step 8936: generator_loss=7.345077037811279, discriminator_loss=0.027429454028606415\n",
            "step 8937: generator_loss=7.205888271331787, discriminator_loss=0.029797453433275223\n",
            "step 8938: generator_loss=7.970698833465576, discriminator_loss=0.026345999911427498\n",
            "step 8939: generator_loss=7.527505874633789, discriminator_loss=0.029407760128378868\n",
            "step 8940: generator_loss=7.145604133605957, discriminator_loss=0.03232189267873764\n",
            "step 8941: generator_loss=6.439596176147461, discriminator_loss=0.037457387894392014\n",
            "step 8942: generator_loss=6.925102233886719, discriminator_loss=0.03216015547513962\n",
            "step 8943: generator_loss=6.432670593261719, discriminator_loss=0.0334823876619339\n",
            "step 8944: generator_loss=6.166172981262207, discriminator_loss=0.033217962831258774\n",
            "step 8945: generator_loss=6.549156665802002, discriminator_loss=0.024658706039190292\n",
            "step 8946: generator_loss=6.613519668579102, discriminator_loss=0.022806216031312943\n",
            "step 8947: generator_loss=6.242098808288574, discriminator_loss=0.023577678948640823\n",
            "step 8948: generator_loss=6.201160430908203, discriminator_loss=0.021288618445396423\n",
            "step 8949: generator_loss=6.29095458984375, discriminator_loss=0.020986368879675865\n",
            "step 8950: generator_loss=6.463827610015869, discriminator_loss=0.019477153196930885\n",
            "step 8951: generator_loss=6.01725435256958, discriminator_loss=0.021087590605020523\n",
            "step 8952: generator_loss=5.939492702484131, discriminator_loss=0.022005291655659676\n",
            "step 8953: generator_loss=5.483789443969727, discriminator_loss=0.023776743561029434\n",
            "step 8954: generator_loss=5.452324390411377, discriminator_loss=0.0231233611702919\n",
            "step 8955: generator_loss=5.290271282196045, discriminator_loss=0.022472333163022995\n",
            "step 8956: generator_loss=5.491034030914307, discriminator_loss=0.019411524757742882\n",
            "step 8957: generator_loss=5.169952392578125, discriminator_loss=0.020311720669269562\n",
            "step 8958: generator_loss=5.179671764373779, discriminator_loss=0.0205509215593338\n",
            "step 8959: generator_loss=5.269168853759766, discriminator_loss=0.018795790150761604\n",
            "step 8960: generator_loss=4.965794563293457, discriminator_loss=0.019418906420469284\n",
            "step 8961: generator_loss=5.148932456970215, discriminator_loss=0.017698317766189575\n",
            "step 8962: generator_loss=4.91123104095459, discriminator_loss=0.018589530140161514\n",
            "step 8963: generator_loss=4.826511383056641, discriminator_loss=0.018081046640872955\n",
            "step 8964: generator_loss=4.462869644165039, discriminator_loss=0.020592866465449333\n",
            "step 8965: generator_loss=4.400206565856934, discriminator_loss=0.02047612890601158\n",
            "step 8966: generator_loss=4.064957141876221, discriminator_loss=0.023033931851387024\n",
            "step 8967: generator_loss=3.9291837215423584, discriminator_loss=0.024962062016129494\n",
            "step 8968: generator_loss=3.925956964492798, discriminator_loss=0.024011895060539246\n",
            "step 8969: generator_loss=3.8199234008789062, discriminator_loss=0.025196321308612823\n",
            "step 8970: generator_loss=4.069007396697998, discriminator_loss=0.02350601740181446\n",
            "step 8971: generator_loss=4.066196918487549, discriminator_loss=0.022553764283657074\n",
            "step 8972: generator_loss=3.9158337116241455, discriminator_loss=0.022716429084539413\n",
            "step 8973: generator_loss=4.272099494934082, discriminator_loss=0.021264171227812767\n",
            "step 8974: generator_loss=4.232311725616455, discriminator_loss=0.020648952573537827\n",
            "step 8975: generator_loss=4.364089012145996, discriminator_loss=0.019794069230556488\n",
            "step 8976: generator_loss=4.227841377258301, discriminator_loss=0.01969864033162594\n",
            "step 8977: generator_loss=4.156580924987793, discriminator_loss=0.020335722714662552\n",
            "step 8978: generator_loss=4.294251918792725, discriminator_loss=0.02027510665357113\n",
            "step 8979: generator_loss=4.0976881980896, discriminator_loss=0.02080087549984455\n",
            "step 8980: generator_loss=4.237841606140137, discriminator_loss=0.021247699856758118\n",
            "step 8981: generator_loss=3.9825949668884277, discriminator_loss=0.022361963987350464\n",
            "step 8982: generator_loss=3.87520170211792, discriminator_loss=0.02344202622771263\n",
            "step 8983: generator_loss=3.8133621215820312, discriminator_loss=0.024500947445631027\n",
            "step 8984: generator_loss=3.7732458114624023, discriminator_loss=0.02530841901898384\n",
            "step 8985: generator_loss=3.622368335723877, discriminator_loss=0.02625284716486931\n",
            "step 8986: generator_loss=3.720979690551758, discriminator_loss=0.025774460285902023\n",
            "step 8987: generator_loss=3.7717716693878174, discriminator_loss=0.025787662714719772\n",
            "step 8988: generator_loss=3.63775634765625, discriminator_loss=0.026406172662973404\n",
            "step 8989: generator_loss=3.796215295791626, discriminator_loss=0.025452423840761185\n",
            "step 8990: generator_loss=3.6383845806121826, discriminator_loss=0.026332752779126167\n",
            "step 8991: generator_loss=3.8157739639282227, discriminator_loss=0.025517405942082405\n",
            "step 8992: generator_loss=3.8728199005126953, discriminator_loss=0.0248187817633152\n",
            "step 8993: generator_loss=3.886723279953003, discriminator_loss=0.024178216233849525\n",
            "step 8994: generator_loss=4.010129928588867, discriminator_loss=0.023458275943994522\n",
            "step 8995: generator_loss=4.141229152679443, discriminator_loss=0.023065583780407906\n",
            "step 8996: generator_loss=3.881723165512085, discriminator_loss=0.02484227530658245\n",
            "step 8997: generator_loss=3.9810640811920166, discriminator_loss=0.024612747132778168\n",
            "step 8998: generator_loss=3.788422107696533, discriminator_loss=0.027776489034295082\n",
            "step 8999: generator_loss=3.8976218700408936, discriminator_loss=0.027874499559402466\n",
            "step 9000: generator_loss=3.804661512374878, discriminator_loss=0.02948908507823944\n",
            "step 9001: generator_loss=3.9654736518859863, discriminator_loss=0.030108632519841194\n",
            "step 9002: generator_loss=3.87974214553833, discriminator_loss=0.03638601303100586\n",
            "step 9003: generator_loss=3.5887837409973145, discriminator_loss=0.06071409955620766\n",
            "step 9004: generator_loss=3.7431936264038086, discriminator_loss=0.0702388733625412\n",
            "step 9005: generator_loss=3.7588095664978027, discriminator_loss=0.10140558332204819\n",
            "step 9006: generator_loss=3.981818675994873, discriminator_loss=0.13890857994556427\n",
            "step 9007: generator_loss=4.30460262298584, discriminator_loss=0.15093469619750977\n",
            "step 9008: generator_loss=5.128578186035156, discriminator_loss=0.09609096497297287\n",
            "step 9009: generator_loss=5.444677352905273, discriminator_loss=0.08717764914035797\n",
            "step 9010: generator_loss=6.262667655944824, discriminator_loss=0.05039175599813461\n",
            "step 9011: generator_loss=7.5380144119262695, discriminator_loss=0.03425019234418869\n",
            "step 9012: generator_loss=7.970757484436035, discriminator_loss=0.028762344270944595\n",
            "step 9013: generator_loss=8.07619571685791, discriminator_loss=0.02891373634338379\n",
            "step 9014: generator_loss=8.656173706054688, discriminator_loss=0.028707675635814667\n",
            "step 9015: generator_loss=8.885673522949219, discriminator_loss=0.02804192155599594\n",
            "step 9016: generator_loss=8.662067413330078, discriminator_loss=0.029232680797576904\n",
            "step 9017: generator_loss=8.352518081665039, discriminator_loss=0.03377154469490051\n",
            "step 9018: generator_loss=7.895735263824463, discriminator_loss=0.03913826867938042\n",
            "step 9019: generator_loss=7.900179862976074, discriminator_loss=0.03643709421157837\n",
            "step 9020: generator_loss=7.381505966186523, discriminator_loss=0.038637395948171616\n",
            "step 9021: generator_loss=7.387969017028809, discriminator_loss=0.033049385994672775\n",
            "step 9022: generator_loss=7.137737274169922, discriminator_loss=0.034323304891586304\n",
            "step 9023: generator_loss=6.740470886230469, discriminator_loss=0.03350085765123367\n",
            "step 9024: generator_loss=6.442038536071777, discriminator_loss=0.03162621706724167\n",
            "step 9025: generator_loss=6.049334526062012, discriminator_loss=0.028681686148047447\n",
            "step 9026: generator_loss=6.275730133056641, discriminator_loss=0.025397930294275284\n",
            "step 9027: generator_loss=5.901506423950195, discriminator_loss=0.02543465420603752\n",
            "step 9028: generator_loss=5.555522441864014, discriminator_loss=0.025346912443637848\n",
            "step 9029: generator_loss=5.497217178344727, discriminator_loss=0.02475568652153015\n",
            "step 9030: generator_loss=5.148090362548828, discriminator_loss=0.02608005702495575\n",
            "step 9031: generator_loss=4.804285049438477, discriminator_loss=0.028575021773576736\n",
            "step 9032: generator_loss=4.554022789001465, discriminator_loss=0.031112302094697952\n",
            "step 9033: generator_loss=4.161377906799316, discriminator_loss=0.03990975767374039\n",
            "step 9034: generator_loss=4.163128852844238, discriminator_loss=0.03782004490494728\n",
            "step 9035: generator_loss=3.8639376163482666, discriminator_loss=0.04868904501199722\n",
            "step 9036: generator_loss=3.7404677867889404, discriminator_loss=0.054069582372903824\n",
            "step 9037: generator_loss=3.962209463119507, discriminator_loss=0.05480960011482239\n",
            "step 9038: generator_loss=4.112432479858398, discriminator_loss=0.05121985450387001\n",
            "step 9039: generator_loss=4.438111305236816, discriminator_loss=0.051583223044872284\n",
            "step 9040: generator_loss=5.007961750030518, discriminator_loss=0.040371909737586975\n",
            "step 9041: generator_loss=5.393259048461914, discriminator_loss=0.03533700853586197\n",
            "step 9042: generator_loss=5.443208694458008, discriminator_loss=0.037152111530303955\n",
            "step 9043: generator_loss=5.675661087036133, discriminator_loss=0.0330725833773613\n",
            "step 9044: generator_loss=5.989486217498779, discriminator_loss=0.02783145383000374\n",
            "step 9045: generator_loss=6.111432075500488, discriminator_loss=0.027141166850924492\n",
            "step 9046: generator_loss=6.384490966796875, discriminator_loss=0.02576339617371559\n",
            "step 9047: generator_loss=6.637818813323975, discriminator_loss=0.02566390298306942\n",
            "step 9048: generator_loss=6.045519828796387, discriminator_loss=0.029319513589143753\n",
            "step 9049: generator_loss=6.018281936645508, discriminator_loss=0.027874218299984932\n",
            "step 9050: generator_loss=5.5060505867004395, discriminator_loss=0.02977188117802143\n",
            "step 9051: generator_loss=5.331540107727051, discriminator_loss=0.031101113185286522\n",
            "step 9052: generator_loss=5.260858535766602, discriminator_loss=0.03254467621445656\n",
            "step 9053: generator_loss=4.517434120178223, discriminator_loss=0.03676886111497879\n",
            "step 9054: generator_loss=4.751918792724609, discriminator_loss=0.035012394189834595\n",
            "step 9055: generator_loss=4.7529096603393555, discriminator_loss=0.032845668494701385\n",
            "step 9056: generator_loss=4.729978561401367, discriminator_loss=0.028768468648195267\n",
            "step 9057: generator_loss=4.924339771270752, discriminator_loss=0.02820035070180893\n",
            "step 9058: generator_loss=4.700766563415527, discriminator_loss=0.02765529975295067\n",
            "step 9059: generator_loss=4.734837532043457, discriminator_loss=0.02682419866323471\n",
            "step 9060: generator_loss=4.195298194885254, discriminator_loss=0.03162000700831413\n",
            "step 9061: generator_loss=4.54469108581543, discriminator_loss=0.027909502387046814\n",
            "step 9062: generator_loss=4.521582126617432, discriminator_loss=0.02657473273575306\n",
            "step 9063: generator_loss=4.547382354736328, discriminator_loss=0.026698850095272064\n",
            "step 9064: generator_loss=4.510030746459961, discriminator_loss=0.026383452117443085\n",
            "step 9065: generator_loss=4.446719169616699, discriminator_loss=0.02549809031188488\n",
            "step 9066: generator_loss=4.425333023071289, discriminator_loss=0.02571251057088375\n",
            "step 9067: generator_loss=4.376031875610352, discriminator_loss=0.027688466012477875\n",
            "step 9068: generator_loss=3.963357448577881, discriminator_loss=0.029974373057484627\n",
            "step 9069: generator_loss=4.159785270690918, discriminator_loss=0.029020335525274277\n",
            "step 9070: generator_loss=4.16986083984375, discriminator_loss=0.028418544679880142\n",
            "step 9071: generator_loss=3.977843761444092, discriminator_loss=0.02947428449988365\n",
            "step 9072: generator_loss=4.013454437255859, discriminator_loss=0.02896115928888321\n",
            "step 9073: generator_loss=4.004739761352539, discriminator_loss=0.02897566370666027\n",
            "step 9074: generator_loss=4.086816787719727, discriminator_loss=0.02748599648475647\n",
            "step 9075: generator_loss=4.122592926025391, discriminator_loss=0.02726583555340767\n",
            "step 9076: generator_loss=4.3486247062683105, discriminator_loss=0.026462208479642868\n",
            "step 9077: generator_loss=4.146789073944092, discriminator_loss=0.02677980065345764\n",
            "step 9078: generator_loss=4.068281173706055, discriminator_loss=0.027163473889231682\n",
            "step 9079: generator_loss=4.0552568435668945, discriminator_loss=0.027400124818086624\n",
            "step 9080: generator_loss=3.8498973846435547, discriminator_loss=0.028414882719516754\n",
            "step 9081: generator_loss=3.8697919845581055, discriminator_loss=0.02885899692773819\n",
            "step 9082: generator_loss=3.868891716003418, discriminator_loss=0.02849535644054413\n",
            "step 9083: generator_loss=3.6968605518341064, discriminator_loss=0.029820680618286133\n",
            "step 9084: generator_loss=3.6846871376037598, discriminator_loss=0.029648998752236366\n",
            "step 9085: generator_loss=3.653862953186035, discriminator_loss=0.029492616653442383\n",
            "step 9086: generator_loss=3.668128252029419, discriminator_loss=0.029420150443911552\n",
            "step 9087: generator_loss=3.6043643951416016, discriminator_loss=0.02991241216659546\n",
            "step 9088: generator_loss=3.5939903259277344, discriminator_loss=0.0300779826939106\n",
            "step 9089: generator_loss=3.5787172317504883, discriminator_loss=0.03034069947898388\n",
            "step 9090: generator_loss=3.543058395385742, discriminator_loss=0.03082696907222271\n",
            "step 9091: generator_loss=3.488496780395508, discriminator_loss=0.03180525451898575\n",
            "step 9092: generator_loss=3.454256057739258, discriminator_loss=0.03250988572835922\n",
            "step 9093: generator_loss=3.47027850151062, discriminator_loss=0.03262830898165703\n",
            "step 9094: generator_loss=3.5068886280059814, discriminator_loss=0.03199741989374161\n",
            "step 9095: generator_loss=3.6351499557495117, discriminator_loss=0.030443109571933746\n",
            "step 9096: generator_loss=3.7206790447235107, discriminator_loss=0.029635323211550713\n",
            "step 9097: generator_loss=3.7756567001342773, discriminator_loss=0.029005732387304306\n",
            "step 9098: generator_loss=3.813164234161377, discriminator_loss=0.028602756559848785\n",
            "step 9099: generator_loss=3.8681344985961914, discriminator_loss=0.02803325280547142\n",
            "step 9100: generator_loss=3.898205280303955, discriminator_loss=0.027573589235544205\n",
            "step 9101: generator_loss=3.935441255569458, discriminator_loss=0.027056893333792686\n",
            "step 9102: generator_loss=3.9504613876342773, discriminator_loss=0.026647359132766724\n",
            "step 9103: generator_loss=3.9525980949401855, discriminator_loss=0.026407085359096527\n",
            "step 9104: generator_loss=3.946502685546875, discriminator_loss=0.02612234093248844\n",
            "step 9105: generator_loss=3.8973538875579834, discriminator_loss=0.026346925646066666\n",
            "step 9106: generator_loss=3.8125953674316406, discriminator_loss=0.02684210240840912\n",
            "step 9107: generator_loss=3.731201171875, discriminator_loss=0.02752237394452095\n",
            "step 9108: generator_loss=3.635669231414795, discriminator_loss=0.028504755347967148\n",
            "step 9109: generator_loss=3.598361015319824, discriminator_loss=0.028784573078155518\n",
            "step 9110: generator_loss=3.6291632652282715, discriminator_loss=0.028205152601003647\n",
            "step 9111: generator_loss=3.68725848197937, discriminator_loss=0.027270428836345673\n",
            "step 9112: generator_loss=3.7244904041290283, discriminator_loss=0.026742303743958473\n",
            "step 9113: generator_loss=3.712089776992798, discriminator_loss=0.026800015941262245\n",
            "step 9114: generator_loss=3.6675148010253906, discriminator_loss=0.02727210521697998\n",
            "step 9115: generator_loss=3.640530824661255, discriminator_loss=0.02769070491194725\n",
            "step 9116: generator_loss=3.624544858932495, discriminator_loss=0.027960805222392082\n",
            "step 9117: generator_loss=3.619410991668701, discriminator_loss=0.028016231954097748\n",
            "step 9118: generator_loss=3.665339469909668, discriminator_loss=0.027609987184405327\n",
            "step 9119: generator_loss=3.6760566234588623, discriminator_loss=0.027420450001955032\n",
            "step 9120: generator_loss=3.6596601009368896, discriminator_loss=0.02766149863600731\n",
            "step 9121: generator_loss=3.6638505458831787, discriminator_loss=0.02773965522646904\n",
            "step 9122: generator_loss=3.6698555946350098, discriminator_loss=0.027833957225084305\n",
            "step 9123: generator_loss=3.670077323913574, discriminator_loss=0.02791396901011467\n",
            "step 9124: generator_loss=3.6865203380584717, discriminator_loss=0.02772066742181778\n",
            "step 9125: generator_loss=3.698305606842041, discriminator_loss=0.02768711745738983\n",
            "step 9126: generator_loss=3.672983169555664, discriminator_loss=0.028089873492717743\n",
            "step 9127: generator_loss=3.6311731338500977, discriminator_loss=0.028708919882774353\n",
            "step 9128: generator_loss=3.580683469772339, discriminator_loss=0.029529033228754997\n",
            "step 9129: generator_loss=3.524393320083618, discriminator_loss=0.03045796789228916\n",
            "step 9130: generator_loss=3.4854280948638916, discriminator_loss=0.03111371025443077\n",
            "step 9131: generator_loss=3.4628610610961914, discriminator_loss=0.03168259933590889\n",
            "step 9132: generator_loss=3.458402156829834, discriminator_loss=0.03191986680030823\n",
            "step 9133: generator_loss=3.4678242206573486, discriminator_loss=0.032033100724220276\n",
            "step 9134: generator_loss=3.5125651359558105, discriminator_loss=0.03149937465786934\n",
            "step 9135: generator_loss=3.577385663986206, discriminator_loss=0.030835412442684174\n",
            "step 9136: generator_loss=3.710301399230957, discriminator_loss=0.02929873578250408\n",
            "step 9137: generator_loss=3.7899274826049805, discriminator_loss=0.028431439772248268\n",
            "step 9138: generator_loss=3.77870512008667, discriminator_loss=0.02871014177799225\n",
            "step 9139: generator_loss=3.7176103591918945, discriminator_loss=0.02954930067062378\n",
            "step 9140: generator_loss=3.7218587398529053, discriminator_loss=0.029419276863336563\n",
            "step 9141: generator_loss=3.7598745822906494, discriminator_loss=0.028844308108091354\n",
            "step 9142: generator_loss=3.774792432785034, discriminator_loss=0.028500065207481384\n",
            "step 9143: generator_loss=3.815061092376709, discriminator_loss=0.0278200451284647\n",
            "step 9144: generator_loss=3.816488742828369, discriminator_loss=0.027681555598974228\n",
            "step 9145: generator_loss=3.788682460784912, discriminator_loss=0.027681736275553703\n",
            "step 9146: generator_loss=3.7398486137390137, discriminator_loss=0.028019074350595474\n",
            "step 9147: generator_loss=3.6944658756256104, discriminator_loss=0.02836957946419716\n",
            "step 9148: generator_loss=3.65539288520813, discriminator_loss=0.02858777716755867\n",
            "step 9149: generator_loss=3.606262445449829, discriminator_loss=0.029028303921222687\n",
            "step 9150: generator_loss=3.6037564277648926, discriminator_loss=0.028892725706100464\n",
            "step 9151: generator_loss=3.6133415699005127, discriminator_loss=0.02864145115017891\n",
            "step 9152: generator_loss=3.6069018840789795, discriminator_loss=0.028623536229133606\n",
            "step 9153: generator_loss=3.5817878246307373, discriminator_loss=0.029035814106464386\n",
            "step 9154: generator_loss=3.550013780593872, discriminator_loss=0.029476236552000046\n",
            "step 9155: generator_loss=3.537022590637207, discriminator_loss=0.029711678624153137\n",
            "step 9156: generator_loss=3.5293490886688232, discriminator_loss=0.0298820361495018\n",
            "step 9157: generator_loss=3.51253604888916, discriminator_loss=0.030283603817224503\n",
            "step 9158: generator_loss=3.5177407264709473, discriminator_loss=0.03034956194460392\n",
            "step 9159: generator_loss=3.548603057861328, discriminator_loss=0.03010132536292076\n",
            "step 9160: generator_loss=3.603377103805542, discriminator_loss=0.029451988637447357\n",
            "step 9161: generator_loss=3.641451120376587, discriminator_loss=0.029074300080537796\n",
            "step 9162: generator_loss=3.6809701919555664, discriminator_loss=0.028911713510751724\n",
            "step 9163: generator_loss=3.747117519378662, discriminator_loss=0.0281803198158741\n",
            "step 9164: generator_loss=3.8103041648864746, discriminator_loss=0.027603479102253914\n",
            "step 9165: generator_loss=3.8403701782226562, discriminator_loss=0.027374953031539917\n",
            "step 9166: generator_loss=3.8438384532928467, discriminator_loss=0.027375776320695877\n",
            "step 9167: generator_loss=3.8190362453460693, discriminator_loss=0.027609914541244507\n",
            "step 9168: generator_loss=3.759464740753174, discriminator_loss=0.028327012434601784\n",
            "step 9169: generator_loss=3.7210960388183594, discriminator_loss=0.02867424115538597\n",
            "step 9170: generator_loss=3.776507616043091, discriminator_loss=0.02795557677745819\n",
            "step 9171: generator_loss=3.8375537395477295, discriminator_loss=0.02715383842587471\n",
            "step 9172: generator_loss=3.890103340148926, discriminator_loss=0.026476025581359863\n",
            "step 9173: generator_loss=3.9254398345947266, discriminator_loss=0.02600141055881977\n",
            "step 9174: generator_loss=3.948225975036621, discriminator_loss=0.025678111240267754\n",
            "step 9175: generator_loss=3.964428424835205, discriminator_loss=0.025306429713964462\n",
            "step 9176: generator_loss=3.9725074768066406, discriminator_loss=0.025059225037693977\n",
            "step 9177: generator_loss=3.9430456161499023, discriminator_loss=0.025205746293067932\n",
            "step 9178: generator_loss=3.9043381214141846, discriminator_loss=0.025380857288837433\n",
            "step 9179: generator_loss=3.9240424633026123, discriminator_loss=0.02505117654800415\n",
            "step 9180: generator_loss=3.904094696044922, discriminator_loss=0.025037147104740143\n",
            "step 9181: generator_loss=3.8383595943450928, discriminator_loss=0.02559446170926094\n",
            "step 9182: generator_loss=3.7645504474639893, discriminator_loss=0.02621808834373951\n",
            "step 9183: generator_loss=3.73073673248291, discriminator_loss=0.02650051936507225\n",
            "step 9184: generator_loss=3.7313761711120605, discriminator_loss=0.026371821761131287\n",
            "step 9185: generator_loss=3.7535228729248047, discriminator_loss=0.02608196809887886\n",
            "step 9186: generator_loss=3.8079380989074707, discriminator_loss=0.025393791496753693\n",
            "step 9187: generator_loss=3.889702081680298, discriminator_loss=0.02451050467789173\n",
            "step 9188: generator_loss=4.0295820236206055, discriminator_loss=0.023140646517276764\n",
            "step 9189: generator_loss=4.160131931304932, discriminator_loss=0.022008713334798813\n",
            "step 9190: generator_loss=4.227199554443359, discriminator_loss=0.02143768221139908\n",
            "step 9191: generator_loss=4.247722148895264, discriminator_loss=0.021160989999771118\n",
            "step 9192: generator_loss=4.330080032348633, discriminator_loss=0.020516222342848778\n",
            "step 9193: generator_loss=4.414539813995361, discriminator_loss=0.019816158339381218\n",
            "step 9194: generator_loss=4.4430975914001465, discriminator_loss=0.019459260627627373\n",
            "step 9195: generator_loss=4.430413246154785, discriminator_loss=0.019347038120031357\n",
            "step 9196: generator_loss=4.3796706199646, discriminator_loss=0.019504636526107788\n",
            "step 9197: generator_loss=4.306419849395752, discriminator_loss=0.019725875928997993\n",
            "step 9198: generator_loss=4.192357063293457, discriminator_loss=0.020353805273771286\n",
            "step 9199: generator_loss=4.088146209716797, discriminator_loss=0.02096521109342575\n",
            "step 9200: generator_loss=3.9686331748962402, discriminator_loss=0.021893128752708435\n",
            "step 9201: generator_loss=3.877427577972412, discriminator_loss=0.022681374102830887\n",
            "step 9202: generator_loss=3.8441169261932373, discriminator_loss=0.02288210578262806\n",
            "step 9203: generator_loss=3.8195345401763916, discriminator_loss=0.023068103939294815\n",
            "step 9204: generator_loss=3.812511920928955, discriminator_loss=0.02306271344423294\n",
            "step 9205: generator_loss=3.823141574859619, discriminator_loss=0.022836171090602875\n",
            "step 9206: generator_loss=3.8200628757476807, discriminator_loss=0.022834476083517075\n",
            "step 9207: generator_loss=3.8251521587371826, discriminator_loss=0.022706501185894012\n",
            "step 9208: generator_loss=3.894270896911621, discriminator_loss=0.021974772214889526\n",
            "step 9209: generator_loss=3.995983362197876, discriminator_loss=0.020944148302078247\n",
            "step 9210: generator_loss=4.089300155639648, discriminator_loss=0.020112846046686172\n",
            "step 9211: generator_loss=4.152397632598877, discriminator_loss=0.01963752880692482\n",
            "step 9212: generator_loss=4.188797950744629, discriminator_loss=0.019362615421414375\n",
            "step 9213: generator_loss=4.198736667633057, discriminator_loss=0.0193104837089777\n",
            "step 9214: generator_loss=4.2473602294921875, discriminator_loss=0.018947910517454147\n",
            "step 9215: generator_loss=4.2819085121154785, discriminator_loss=0.018677333369851112\n",
            "step 9216: generator_loss=4.290780067443848, discriminator_loss=0.018598943948745728\n",
            "step 9217: generator_loss=4.301098823547363, discriminator_loss=0.01849643513560295\n",
            "step 9218: generator_loss=4.304995536804199, discriminator_loss=0.01844150573015213\n",
            "step 9219: generator_loss=4.317164897918701, discriminator_loss=0.018286334350705147\n",
            "step 9220: generator_loss=4.337226867675781, discriminator_loss=0.01805618591606617\n",
            "step 9221: generator_loss=4.354907035827637, discriminator_loss=0.017892222851514816\n",
            "step 9222: generator_loss=4.387153625488281, discriminator_loss=0.017578108236193657\n",
            "step 9223: generator_loss=4.39457368850708, discriminator_loss=0.017451290041208267\n",
            "step 9224: generator_loss=4.364536285400391, discriminator_loss=0.017545975744724274\n",
            "step 9225: generator_loss=4.333077430725098, discriminator_loss=0.017640724778175354\n",
            "step 9226: generator_loss=4.270832538604736, discriminator_loss=0.017969530075788498\n",
            "step 9227: generator_loss=4.211756229400635, discriminator_loss=0.018278349190950394\n",
            "step 9228: generator_loss=4.164645195007324, discriminator_loss=0.018545523285865784\n",
            "step 9229: generator_loss=4.110459327697754, discriminator_loss=0.018883386626839638\n",
            "step 9230: generator_loss=4.092057704925537, discriminator_loss=0.018951063975691795\n",
            "step 9231: generator_loss=4.156689643859863, discriminator_loss=0.01836494915187359\n",
            "step 9232: generator_loss=4.21937370300293, discriminator_loss=0.0177978053689003\n",
            "step 9233: generator_loss=4.253128528594971, discriminator_loss=0.01745769754052162\n",
            "step 9234: generator_loss=4.288680553436279, discriminator_loss=0.017173349857330322\n",
            "step 9235: generator_loss=4.318573951721191, discriminator_loss=0.01689518801867962\n",
            "step 9236: generator_loss=4.380300045013428, discriminator_loss=0.016481347382068634\n",
            "step 9237: generator_loss=4.414312362670898, discriminator_loss=0.016212377697229385\n",
            "step 9238: generator_loss=4.3951263427734375, discriminator_loss=0.016302332282066345\n",
            "step 9239: generator_loss=4.392330169677734, discriminator_loss=0.01628325693309307\n",
            "step 9240: generator_loss=4.417135238647461, discriminator_loss=0.01611935906112194\n",
            "step 9241: generator_loss=4.481248378753662, discriminator_loss=0.015707682818174362\n",
            "step 9242: generator_loss=4.513912200927734, discriminator_loss=0.01544194482266903\n",
            "step 9243: generator_loss=4.539523601531982, discriminator_loss=0.015290540643036366\n",
            "step 9244: generator_loss=4.513963222503662, discriminator_loss=0.015334611758589745\n",
            "step 9245: generator_loss=4.472558498382568, discriminator_loss=0.015495041385293007\n",
            "step 9246: generator_loss=4.447843551635742, discriminator_loss=0.01557888276875019\n",
            "step 9247: generator_loss=4.362505912780762, discriminator_loss=0.016020240262150764\n",
            "step 9248: generator_loss=4.234687328338623, discriminator_loss=0.01681518368422985\n",
            "step 9249: generator_loss=4.086780548095703, discriminator_loss=0.017916865646839142\n",
            "step 9250: generator_loss=3.9743552207946777, discriminator_loss=0.01887773908674717\n",
            "step 9251: generator_loss=3.939500093460083, discriminator_loss=0.019146084785461426\n",
            "step 9252: generator_loss=3.9670135974884033, discriminator_loss=0.018874630331993103\n",
            "step 9253: generator_loss=4.014220714569092, discriminator_loss=0.01839936524629593\n",
            "step 9254: generator_loss=4.072418689727783, discriminator_loss=0.017899546772241592\n",
            "step 9255: generator_loss=4.1370439529418945, discriminator_loss=0.017374960705637932\n",
            "step 9256: generator_loss=4.181725978851318, discriminator_loss=0.0170424934476614\n",
            "step 9257: generator_loss=4.235114574432373, discriminator_loss=0.016657089814543724\n",
            "step 9258: generator_loss=4.281350612640381, discriminator_loss=0.016341816633939743\n",
            "step 9259: generator_loss=4.265239238739014, discriminator_loss=0.016474008560180664\n",
            "step 9260: generator_loss=4.216917514801025, discriminator_loss=0.01683405041694641\n",
            "step 9261: generator_loss=4.1570539474487305, discriminator_loss=0.01729370281100273\n",
            "step 9262: generator_loss=4.079687595367432, discriminator_loss=0.01794983074069023\n",
            "step 9263: generator_loss=3.9768612384796143, discriminator_loss=0.018866751343011856\n",
            "step 9264: generator_loss=3.8980772495269775, discriminator_loss=0.019707899540662766\n",
            "step 9265: generator_loss=3.865713596343994, discriminator_loss=0.020064111799001694\n",
            "step 9266: generator_loss=3.9089930057525635, discriminator_loss=0.01969388872385025\n",
            "step 9267: generator_loss=3.9383606910705566, discriminator_loss=0.019442446529865265\n",
            "step 9268: generator_loss=3.9529623985290527, discriminator_loss=0.019385460764169693\n",
            "step 9269: generator_loss=3.9506938457489014, discriminator_loss=0.01946614682674408\n",
            "step 9270: generator_loss=3.9248077869415283, discriminator_loss=0.019805893301963806\n",
            "step 9271: generator_loss=3.9105777740478516, discriminator_loss=0.020001264289021492\n",
            "step 9272: generator_loss=3.8974525928497314, discriminator_loss=0.02022698149085045\n",
            "step 9273: generator_loss=3.9219274520874023, discriminator_loss=0.020035888999700546\n",
            "step 9274: generator_loss=4.023204326629639, discriminator_loss=0.019116690382361412\n",
            "step 9275: generator_loss=4.14608907699585, discriminator_loss=0.018102582544088364\n",
            "step 9276: generator_loss=4.241647720336914, discriminator_loss=0.017369741573929787\n",
            "step 9277: generator_loss=4.3313307762146, discriminator_loss=0.01673268899321556\n",
            "step 9278: generator_loss=4.364616870880127, discriminator_loss=0.016474949195981026\n",
            "step 9279: generator_loss=4.357580661773682, discriminator_loss=0.01646195724606514\n",
            "step 9280: generator_loss=4.311208724975586, discriminator_loss=0.01662359945476055\n",
            "step 9281: generator_loss=4.2392354011535645, discriminator_loss=0.016997622326016426\n",
            "step 9282: generator_loss=4.112783432006836, discriminator_loss=0.01786622777581215\n",
            "step 9283: generator_loss=3.9892077445983887, discriminator_loss=0.018749631941318512\n",
            "step 9284: generator_loss=3.8493356704711914, discriminator_loss=0.020027074962854385\n",
            "step 9285: generator_loss=3.720393180847168, discriminator_loss=0.021415283903479576\n",
            "step 9286: generator_loss=3.6240735054016113, discriminator_loss=0.02252275124192238\n",
            "step 9287: generator_loss=3.5931031703948975, discriminator_loss=0.022891990840435028\n",
            "step 9288: generator_loss=3.6275594234466553, discriminator_loss=0.022314369678497314\n",
            "step 9289: generator_loss=3.696091651916504, discriminator_loss=0.02141166478395462\n",
            "step 9290: generator_loss=3.758880138397217, discriminator_loss=0.020613260567188263\n",
            "step 9291: generator_loss=3.817030668258667, discriminator_loss=0.019942209124565125\n",
            "step 9292: generator_loss=3.8636085987091064, discriminator_loss=0.01950150728225708\n",
            "step 9293: generator_loss=3.889547824859619, discriminator_loss=0.01923244446516037\n",
            "step 9294: generator_loss=3.913479804992676, discriminator_loss=0.019087694585323334\n",
            "step 9295: generator_loss=3.984326124191284, discriminator_loss=0.018477093428373337\n",
            "step 9296: generator_loss=4.056530475616455, discriminator_loss=0.01787949725985527\n",
            "step 9297: generator_loss=4.092828750610352, discriminator_loss=0.01768484339118004\n",
            "step 9298: generator_loss=4.100601673126221, discriminator_loss=0.017688676714897156\n",
            "step 9299: generator_loss=4.091798782348633, discriminator_loss=0.01781035214662552\n",
            "step 9300: generator_loss=4.10129976272583, discriminator_loss=0.01775909587740898\n",
            "step 9301: generator_loss=4.062500953674316, discriminator_loss=0.018132459372282028\n",
            "step 9302: generator_loss=4.073355674743652, discriminator_loss=0.018038786947727203\n",
            "step 9303: generator_loss=4.100826263427734, discriminator_loss=0.017790354788303375\n",
            "step 9304: generator_loss=4.104557991027832, discriminator_loss=0.01769942045211792\n",
            "step 9305: generator_loss=4.151675224304199, discriminator_loss=0.017316918820142746\n",
            "step 9306: generator_loss=4.217455863952637, discriminator_loss=0.016718808561563492\n",
            "step 9307: generator_loss=4.2547454833984375, discriminator_loss=0.016384681686758995\n",
            "step 9308: generator_loss=4.269074440002441, discriminator_loss=0.016191918402910233\n",
            "step 9309: generator_loss=4.268749713897705, discriminator_loss=0.016105709597468376\n",
            "step 9310: generator_loss=4.264636039733887, discriminator_loss=0.01602284424006939\n",
            "step 9311: generator_loss=4.232819557189941, discriminator_loss=0.0161218773573637\n",
            "step 9312: generator_loss=4.170439720153809, discriminator_loss=0.016459420323371887\n",
            "step 9313: generator_loss=4.09047794342041, discriminator_loss=0.016989268362522125\n",
            "step 9314: generator_loss=4.01921272277832, discriminator_loss=0.017506983131170273\n",
            "step 9315: generator_loss=4.019707679748535, discriminator_loss=0.017394037917256355\n",
            "step 9316: generator_loss=4.144282341003418, discriminator_loss=0.016229450702667236\n",
            "step 9317: generator_loss=4.264476776123047, discriminator_loss=0.015241757966578007\n",
            "step 9318: generator_loss=4.381417274475098, discriminator_loss=0.014345433562994003\n",
            "step 9319: generator_loss=4.437645435333252, discriminator_loss=0.013898467645049095\n",
            "step 9320: generator_loss=4.431596279144287, discriminator_loss=0.01387641578912735\n",
            "step 9321: generator_loss=4.37847900390625, discriminator_loss=0.014122569002211094\n",
            "step 9322: generator_loss=4.310757637023926, discriminator_loss=0.014450283721089363\n",
            "step 9323: generator_loss=4.29574728012085, discriminator_loss=0.014489399269223213\n",
            "step 9324: generator_loss=4.295184135437012, discriminator_loss=0.014430683106184006\n",
            "step 9325: generator_loss=4.269148826599121, discriminator_loss=0.014546653255820274\n",
            "step 9326: generator_loss=4.210562229156494, discriminator_loss=0.014916728250682354\n",
            "step 9327: generator_loss=4.164372444152832, discriminator_loss=0.015223468653857708\n",
            "step 9328: generator_loss=4.122217655181885, discriminator_loss=0.015506891533732414\n",
            "step 9329: generator_loss=4.082736492156982, discriminator_loss=0.0157797671854496\n",
            "step 9330: generator_loss=4.040085315704346, discriminator_loss=0.016139492392539978\n",
            "step 9331: generator_loss=4.003595352172852, discriminator_loss=0.0164320208132267\n",
            "step 9332: generator_loss=4.011931419372559, discriminator_loss=0.016317781060934067\n",
            "step 9333: generator_loss=4.069390773773193, discriminator_loss=0.015783365815877914\n",
            "step 9334: generator_loss=4.123724460601807, discriminator_loss=0.015311820432543755\n",
            "step 9335: generator_loss=4.210104465484619, discriminator_loss=0.014627417549490929\n",
            "step 9336: generator_loss=4.295915603637695, discriminator_loss=0.013984909281134605\n",
            "step 9337: generator_loss=4.346644401550293, discriminator_loss=0.013628542423248291\n",
            "step 9338: generator_loss=4.359339714050293, discriminator_loss=0.013552572578191757\n",
            "step 9339: generator_loss=4.327215194702148, discriminator_loss=0.013724945485591888\n",
            "step 9340: generator_loss=4.250492572784424, discriminator_loss=0.014257386326789856\n",
            "step 9341: generator_loss=4.183281421661377, discriminator_loss=0.014727598056197166\n",
            "step 9342: generator_loss=4.131392478942871, discriminator_loss=0.01512722484767437\n",
            "step 9343: generator_loss=4.07269811630249, discriminator_loss=0.015613390132784843\n",
            "step 9344: generator_loss=4.022098541259766, discriminator_loss=0.016094937920570374\n",
            "step 9345: generator_loss=4.014969825744629, discriminator_loss=0.01611839421093464\n",
            "step 9346: generator_loss=4.050422668457031, discriminator_loss=0.015834568068385124\n",
            "step 9347: generator_loss=4.085340976715088, discriminator_loss=0.015569740906357765\n",
            "step 9348: generator_loss=4.111189842224121, discriminator_loss=0.015401933342218399\n",
            "step 9349: generator_loss=4.114293575286865, discriminator_loss=0.015426106750965118\n",
            "step 9350: generator_loss=4.1248779296875, discriminator_loss=0.015363985672593117\n",
            "step 9351: generator_loss=4.183761119842529, discriminator_loss=0.014957831241190434\n",
            "step 9352: generator_loss=4.231220722198486, discriminator_loss=0.01459516491740942\n",
            "step 9353: generator_loss=4.311293125152588, discriminator_loss=0.01408536545932293\n",
            "step 9354: generator_loss=4.346547603607178, discriminator_loss=0.013891059905290604\n",
            "step 9355: generator_loss=4.325967311859131, discriminator_loss=0.014058161526918411\n",
            "step 9356: generator_loss=4.30282735824585, discriminator_loss=0.014240331947803497\n",
            "step 9357: generator_loss=4.306999206542969, discriminator_loss=0.014214569702744484\n",
            "step 9358: generator_loss=4.306275367736816, discriminator_loss=0.014230135828256607\n",
            "step 9359: generator_loss=4.317990779876709, discriminator_loss=0.014169408939778805\n",
            "step 9360: generator_loss=4.312932014465332, discriminator_loss=0.014198743738234043\n",
            "step 9361: generator_loss=4.28781795501709, discriminator_loss=0.014324262738227844\n",
            "step 9362: generator_loss=4.227287292480469, discriminator_loss=0.014771539717912674\n",
            "step 9363: generator_loss=4.139274597167969, discriminator_loss=0.015423414297401905\n",
            "step 9364: generator_loss=4.052672863006592, discriminator_loss=0.016144465655088425\n",
            "step 9365: generator_loss=3.9585940837860107, discriminator_loss=0.017021620646119118\n",
            "step 9366: generator_loss=3.888979434967041, discriminator_loss=0.017729703336954117\n",
            "step 9367: generator_loss=3.932624101638794, discriminator_loss=0.01732620596885681\n",
            "step 9368: generator_loss=4.030241966247559, discriminator_loss=0.016408205032348633\n",
            "step 9369: generator_loss=4.132507801055908, discriminator_loss=0.01554927509278059\n",
            "step 9370: generator_loss=4.1945061683654785, discriminator_loss=0.015092556364834309\n",
            "step 9371: generator_loss=4.2392497062683105, discriminator_loss=0.014761893078684807\n",
            "step 9372: generator_loss=4.298366546630859, discriminator_loss=0.014410173520445824\n",
            "step 9373: generator_loss=4.338099002838135, discriminator_loss=0.014161983504891396\n",
            "step 9374: generator_loss=4.380289077758789, discriminator_loss=0.013896111398935318\n",
            "step 9375: generator_loss=4.381427764892578, discriminator_loss=0.013927610591053963\n",
            "step 9376: generator_loss=4.342820644378662, discriminator_loss=0.014192681759595871\n",
            "step 9377: generator_loss=4.355935573577881, discriminator_loss=0.014093415811657906\n",
            "step 9378: generator_loss=4.392418384552002, discriminator_loss=0.01390161830931902\n",
            "step 9379: generator_loss=4.385508060455322, discriminator_loss=0.013931643217802048\n",
            "step 9380: generator_loss=4.338730335235596, discriminator_loss=0.014251685701310635\n",
            "step 9381: generator_loss=4.257388114929199, discriminator_loss=0.014779806137084961\n",
            "step 9382: generator_loss=4.181751251220703, discriminator_loss=0.015350114554166794\n",
            "step 9383: generator_loss=4.17782735824585, discriminator_loss=0.015419564209878445\n",
            "step 9384: generator_loss=4.218103408813477, discriminator_loss=0.015111971646547318\n",
            "step 9385: generator_loss=4.237059593200684, discriminator_loss=0.014979149214923382\n",
            "step 9386: generator_loss=4.265881538391113, discriminator_loss=0.014776978641748428\n",
            "step 9387: generator_loss=4.278257369995117, discriminator_loss=0.014697464182972908\n",
            "step 9388: generator_loss=4.31749963760376, discriminator_loss=0.014436712488532066\n",
            "step 9389: generator_loss=4.388668060302734, discriminator_loss=0.013966534286737442\n",
            "step 9390: generator_loss=4.470304012298584, discriminator_loss=0.013498565182089806\n",
            "step 9391: generator_loss=4.499005317687988, discriminator_loss=0.01331582386046648\n",
            "step 9392: generator_loss=4.472408771514893, discriminator_loss=0.013470979407429695\n",
            "step 9393: generator_loss=4.402266502380371, discriminator_loss=0.013834042474627495\n",
            "step 9394: generator_loss=4.324182987213135, discriminator_loss=0.014352861791849136\n",
            "step 9395: generator_loss=4.316679954528809, discriminator_loss=0.014356579631567001\n",
            "step 9396: generator_loss=4.387011528015137, discriminator_loss=0.01387060061097145\n",
            "step 9397: generator_loss=4.432464599609375, discriminator_loss=0.013575345277786255\n",
            "step 9398: generator_loss=4.44329309463501, discriminator_loss=0.013502426445484161\n",
            "step 9399: generator_loss=4.389235019683838, discriminator_loss=0.013808177784085274\n",
            "step 9400: generator_loss=4.369285583496094, discriminator_loss=0.013905739411711693\n",
            "step 9401: generator_loss=4.4097676277160645, discriminator_loss=0.013642670586705208\n",
            "step 9402: generator_loss=4.432839870452881, discriminator_loss=0.013465194031596184\n",
            "step 9403: generator_loss=4.417425632476807, discriminator_loss=0.013540330342948437\n",
            "step 9404: generator_loss=4.402871608734131, discriminator_loss=0.013597778044641018\n",
            "step 9405: generator_loss=4.396345138549805, discriminator_loss=0.013609666377305984\n",
            "step 9406: generator_loss=4.392157077789307, discriminator_loss=0.01360347680747509\n",
            "step 9407: generator_loss=4.375787258148193, discriminator_loss=0.013656804338097572\n",
            "step 9408: generator_loss=4.339505672454834, discriminator_loss=0.01386075560003519\n",
            "step 9409: generator_loss=4.283045768737793, discriminator_loss=0.014224886894226074\n",
            "step 9410: generator_loss=4.228884696960449, discriminator_loss=0.014554974623024464\n",
            "step 9411: generator_loss=4.2129034996032715, discriminator_loss=0.014672168530523777\n",
            "step 9412: generator_loss=4.23908805847168, discriminator_loss=0.014508859254419804\n",
            "step 9413: generator_loss=4.271924018859863, discriminator_loss=0.014263473451137543\n",
            "step 9414: generator_loss=4.287695407867432, discriminator_loss=0.014153745025396347\n",
            "step 9415: generator_loss=4.2999467849731445, discriminator_loss=0.0141091737896204\n",
            "step 9416: generator_loss=4.338386535644531, discriminator_loss=0.01388954184949398\n",
            "step 9417: generator_loss=4.38067626953125, discriminator_loss=0.013661026954650879\n",
            "step 9418: generator_loss=4.42234468460083, discriminator_loss=0.0134055744856596\n",
            "step 9419: generator_loss=4.462367534637451, discriminator_loss=0.013188345357775688\n",
            "step 9420: generator_loss=4.474647045135498, discriminator_loss=0.013166623190045357\n",
            "step 9421: generator_loss=4.49023962020874, discriminator_loss=0.013069305568933487\n",
            "step 9422: generator_loss=4.519698143005371, discriminator_loss=0.0129380002617836\n",
            "step 9423: generator_loss=4.551004409790039, discriminator_loss=0.012761201709508896\n",
            "step 9424: generator_loss=4.574983596801758, discriminator_loss=0.012602157890796661\n",
            "step 9425: generator_loss=4.564205169677734, discriminator_loss=0.012657089158892632\n",
            "step 9426: generator_loss=4.57089376449585, discriminator_loss=0.012577654793858528\n",
            "step 9427: generator_loss=4.664791584014893, discriminator_loss=0.012094329111278057\n",
            "step 9428: generator_loss=4.732354164123535, discriminator_loss=0.011733980849385262\n",
            "step 9429: generator_loss=4.764438152313232, discriminator_loss=0.011501584202051163\n",
            "step 9430: generator_loss=4.756008148193359, discriminator_loss=0.011510342359542847\n",
            "step 9431: generator_loss=4.7391133308410645, discriminator_loss=0.01149803027510643\n",
            "step 9432: generator_loss=4.703288555145264, discriminator_loss=0.011595139279961586\n",
            "step 9433: generator_loss=4.667239665985107, discriminator_loss=0.011706974357366562\n",
            "step 9434: generator_loss=4.644320011138916, discriminator_loss=0.011761462315917015\n",
            "step 9435: generator_loss=4.625871658325195, discriminator_loss=0.011770602315664291\n",
            "step 9436: generator_loss=4.6295671463012695, discriminator_loss=0.011700278148055077\n",
            "step 9437: generator_loss=4.660045623779297, discriminator_loss=0.011480511166155338\n",
            "step 9438: generator_loss=4.6955060958862305, discriminator_loss=0.01126448716968298\n",
            "step 9439: generator_loss=4.7037882804870605, discriminator_loss=0.011164813302457333\n",
            "step 9440: generator_loss=4.705905437469482, discriminator_loss=0.011110655963420868\n",
            "step 9441: generator_loss=4.689774036407471, discriminator_loss=0.01113881915807724\n",
            "step 9442: generator_loss=4.687205791473389, discriminator_loss=0.011110993102192879\n",
            "step 9443: generator_loss=4.708804607391357, discriminator_loss=0.01097671128809452\n",
            "step 9444: generator_loss=4.706381797790527, discriminator_loss=0.01096008438616991\n",
            "step 9445: generator_loss=4.676031112670898, discriminator_loss=0.011040709912776947\n",
            "step 9446: generator_loss=4.657536506652832, discriminator_loss=0.011109933257102966\n",
            "step 9447: generator_loss=4.6702375411987305, discriminator_loss=0.01103671733289957\n",
            "step 9448: generator_loss=4.6898016929626465, discriminator_loss=0.0109129399061203\n",
            "step 9449: generator_loss=4.699272155761719, discriminator_loss=0.010864002630114555\n",
            "step 9450: generator_loss=4.706053733825684, discriminator_loss=0.010817214846611023\n",
            "step 9451: generator_loss=4.702019691467285, discriminator_loss=0.010808403603732586\n",
            "step 9452: generator_loss=4.68364143371582, discriminator_loss=0.010892912745475769\n",
            "step 9453: generator_loss=4.664777755737305, discriminator_loss=0.010970836505293846\n",
            "step 9454: generator_loss=4.632976531982422, discriminator_loss=0.01110963337123394\n",
            "step 9455: generator_loss=4.608952045440674, discriminator_loss=0.011230956763029099\n",
            "step 9456: generator_loss=4.578953266143799, discriminator_loss=0.01135195977985859\n",
            "step 9457: generator_loss=4.5656352043151855, discriminator_loss=0.011432994157075882\n",
            "step 9458: generator_loss=4.5272650718688965, discriminator_loss=0.011607499793171883\n",
            "step 9459: generator_loss=4.482079982757568, discriminator_loss=0.01187867857515812\n",
            "step 9460: generator_loss=4.4483418464660645, discriminator_loss=0.012043515220284462\n",
            "step 9461: generator_loss=4.431746959686279, discriminator_loss=0.012151136063039303\n",
            "step 9462: generator_loss=4.401088714599609, discriminator_loss=0.012318179942667484\n",
            "step 9463: generator_loss=4.3629350662231445, discriminator_loss=0.012565823271870613\n",
            "step 9464: generator_loss=4.292752265930176, discriminator_loss=0.013029065914452076\n",
            "step 9465: generator_loss=4.240865230560303, discriminator_loss=0.013411888852715492\n",
            "step 9466: generator_loss=4.261912822723389, discriminator_loss=0.01327142957597971\n",
            "step 9467: generator_loss=4.350159168243408, discriminator_loss=0.012676969170570374\n",
            "step 9468: generator_loss=4.483874320983887, discriminator_loss=0.011879127472639084\n",
            "step 9469: generator_loss=4.598598003387451, discriminator_loss=0.011274253949522972\n",
            "step 9470: generator_loss=4.662845611572266, discriminator_loss=0.011007441207766533\n",
            "step 9471: generator_loss=4.750566482543945, discriminator_loss=0.010619647800922394\n",
            "step 9472: generator_loss=4.781219005584717, discriminator_loss=0.01051880419254303\n",
            "step 9473: generator_loss=4.791112422943115, discriminator_loss=0.010451966896653175\n",
            "step 9474: generator_loss=4.757306098937988, discriminator_loss=0.010633559897542\n",
            "step 9475: generator_loss=4.698379039764404, discriminator_loss=0.01088150218129158\n",
            "step 9476: generator_loss=4.649598121643066, discriminator_loss=0.0111042819917202\n",
            "step 9477: generator_loss=4.581265926361084, discriminator_loss=0.011455532163381577\n",
            "step 9478: generator_loss=4.570093154907227, discriminator_loss=0.011503858491778374\n",
            "step 9479: generator_loss=4.530376434326172, discriminator_loss=0.011717138811945915\n",
            "step 9480: generator_loss=4.476816177368164, discriminator_loss=0.012001346796751022\n",
            "step 9481: generator_loss=4.442984580993652, discriminator_loss=0.01219650637358427\n",
            "step 9482: generator_loss=4.432175636291504, discriminator_loss=0.012261411175131798\n",
            "step 9483: generator_loss=4.451201915740967, discriminator_loss=0.01214643009006977\n",
            "step 9484: generator_loss=4.483155250549316, discriminator_loss=0.011968127451837063\n",
            "step 9485: generator_loss=4.550345420837402, discriminator_loss=0.011595522984862328\n",
            "step 9486: generator_loss=4.580039024353027, discriminator_loss=0.011458152905106544\n",
            "step 9487: generator_loss=4.5749311447143555, discriminator_loss=0.011469326913356781\n",
            "step 9488: generator_loss=4.545076847076416, discriminator_loss=0.011626238003373146\n",
            "step 9489: generator_loss=4.489536285400391, discriminator_loss=0.01195679884403944\n",
            "step 9490: generator_loss=4.47723913192749, discriminator_loss=0.012025456875562668\n",
            "step 9491: generator_loss=4.491244316101074, discriminator_loss=0.011945271864533424\n",
            "step 9492: generator_loss=4.5386528968811035, discriminator_loss=0.011689460836350918\n",
            "step 9493: generator_loss=4.581860065460205, discriminator_loss=0.011478256434202194\n",
            "step 9494: generator_loss=4.58204460144043, discriminator_loss=0.011471403762698174\n",
            "step 9495: generator_loss=4.576082706451416, discriminator_loss=0.011502962559461594\n",
            "step 9496: generator_loss=4.560425758361816, discriminator_loss=0.011550426483154297\n",
            "step 9497: generator_loss=4.526330471038818, discriminator_loss=0.011731632053852081\n",
            "step 9498: generator_loss=4.4687957763671875, discriminator_loss=0.01203586533665657\n",
            "step 9499: generator_loss=4.396522045135498, discriminator_loss=0.012479586526751518\n",
            "step 9500: generator_loss=4.318824291229248, discriminator_loss=0.012962771579623222\n",
            "step 9501: generator_loss=4.271912574768066, discriminator_loss=0.013283925130963326\n",
            "step 9502: generator_loss=4.244069576263428, discriminator_loss=0.013521484099328518\n",
            "step 9503: generator_loss=4.239579200744629, discriminator_loss=0.013563219457864761\n",
            "step 9504: generator_loss=4.255456447601318, discriminator_loss=0.0134563148021698\n",
            "step 9505: generator_loss=4.3083038330078125, discriminator_loss=0.013138120993971825\n",
            "step 9506: generator_loss=4.37239933013916, discriminator_loss=0.012754103168845177\n",
            "step 9507: generator_loss=4.409770965576172, discriminator_loss=0.012537909671664238\n",
            "step 9508: generator_loss=4.477751731872559, discriminator_loss=0.012209266424179077\n",
            "step 9509: generator_loss=4.541059970855713, discriminator_loss=0.011894265189766884\n",
            "step 9510: generator_loss=4.554697513580322, discriminator_loss=0.011890625581145287\n",
            "step 9511: generator_loss=4.506808280944824, discriminator_loss=0.012202966958284378\n",
            "step 9512: generator_loss=4.446876049041748, discriminator_loss=0.012618422508239746\n",
            "step 9513: generator_loss=4.42839241027832, discriminator_loss=0.012744194827973843\n",
            "step 9514: generator_loss=4.39445686340332, discriminator_loss=0.01300753466784954\n",
            "step 9515: generator_loss=4.363580703735352, discriminator_loss=0.013254344463348389\n",
            "step 9516: generator_loss=4.3158369064331055, discriminator_loss=0.013669847510755062\n",
            "step 9517: generator_loss=4.2450456619262695, discriminator_loss=0.014731080271303654\n",
            "step 9518: generator_loss=4.222960472106934, discriminator_loss=0.01564771682024002\n",
            "step 9519: generator_loss=4.215752601623535, discriminator_loss=0.016399923712015152\n",
            "step 9520: generator_loss=4.090542793273926, discriminator_loss=0.019856682047247887\n",
            "step 9521: generator_loss=4.131224632263184, discriminator_loss=0.020801516249775887\n",
            "step 9522: generator_loss=4.160165786743164, discriminator_loss=0.021963171660900116\n",
            "step 9523: generator_loss=3.9429593086242676, discriminator_loss=0.043678928166627884\n",
            "step 9524: generator_loss=4.145408630371094, discriminator_loss=0.04436556249856949\n",
            "step 9525: generator_loss=4.36216402053833, discriminator_loss=0.0588405504822731\n",
            "step 9526: generator_loss=4.489869117736816, discriminator_loss=0.0936085656285286\n",
            "step 9527: generator_loss=4.753086090087891, discriminator_loss=0.0886354148387909\n",
            "step 9528: generator_loss=5.045671463012695, discriminator_loss=0.08919285237789154\n",
            "step 9529: generator_loss=5.6036376953125, discriminator_loss=0.06481412798166275\n",
            "step 9530: generator_loss=5.888911724090576, discriminator_loss=0.07275674492120743\n",
            "step 9531: generator_loss=6.580120086669922, discriminator_loss=0.04274329915642738\n",
            "step 9532: generator_loss=6.766778945922852, discriminator_loss=0.03679843991994858\n",
            "step 9533: generator_loss=7.740850448608398, discriminator_loss=0.020347334444522858\n",
            "step 9534: generator_loss=8.156342506408691, discriminator_loss=0.01879117079079151\n",
            "step 9535: generator_loss=7.81687068939209, discriminator_loss=0.022484976798295975\n",
            "step 9536: generator_loss=8.532877922058105, discriminator_loss=0.017919132485985756\n",
            "step 9537: generator_loss=8.81495189666748, discriminator_loss=0.017443763092160225\n",
            "step 9538: generator_loss=8.602739334106445, discriminator_loss=0.02170034497976303\n",
            "step 9539: generator_loss=9.17734146118164, discriminator_loss=0.01847604475915432\n",
            "step 9540: generator_loss=8.961257934570312, discriminator_loss=0.01941782236099243\n",
            "step 9541: generator_loss=9.04697322845459, discriminator_loss=0.02152065560221672\n",
            "step 9542: generator_loss=9.037315368652344, discriminator_loss=0.021986808627843857\n",
            "step 9543: generator_loss=8.480247497558594, discriminator_loss=0.02354596182703972\n",
            "step 9544: generator_loss=8.6205415725708, discriminator_loss=0.024316702038049698\n",
            "step 9545: generator_loss=8.141852378845215, discriminator_loss=0.02546970546245575\n",
            "step 9546: generator_loss=8.344520568847656, discriminator_loss=0.025324484333395958\n",
            "step 9547: generator_loss=8.326828002929688, discriminator_loss=0.025259491056203842\n",
            "step 9548: generator_loss=8.671246528625488, discriminator_loss=0.02156309224665165\n",
            "step 9549: generator_loss=8.080787658691406, discriminator_loss=0.022856807336211205\n",
            "step 9550: generator_loss=8.159645080566406, discriminator_loss=0.023512912914156914\n",
            "step 9551: generator_loss=7.607422828674316, discriminator_loss=0.024260010570287704\n",
            "step 9552: generator_loss=7.648759841918945, discriminator_loss=0.021339811384677887\n",
            "step 9553: generator_loss=7.57159948348999, discriminator_loss=0.023761974647641182\n",
            "step 9554: generator_loss=7.621560096740723, discriminator_loss=0.020962530747056007\n",
            "step 9555: generator_loss=6.821873188018799, discriminator_loss=0.024012688547372818\n",
            "step 9556: generator_loss=7.1870927810668945, discriminator_loss=0.020170368254184723\n",
            "step 9557: generator_loss=6.795001029968262, discriminator_loss=0.018648937344551086\n",
            "step 9558: generator_loss=6.676483154296875, discriminator_loss=0.02095932699739933\n",
            "step 9559: generator_loss=6.373294353485107, discriminator_loss=0.025509512051939964\n",
            "step 9560: generator_loss=5.9334306716918945, discriminator_loss=0.028127742931246758\n",
            "step 9561: generator_loss=6.059826374053955, discriminator_loss=0.03405114263296127\n",
            "step 9562: generator_loss=6.573470115661621, discriminator_loss=0.03440749645233154\n",
            "step 9563: generator_loss=6.036698341369629, discriminator_loss=0.05225429683923721\n",
            "step 9564: generator_loss=6.112593650817871, discriminator_loss=0.043908171355724335\n",
            "step 9565: generator_loss=6.206713676452637, discriminator_loss=0.04410063475370407\n",
            "step 9566: generator_loss=6.497142791748047, discriminator_loss=0.03249848634004593\n",
            "step 9567: generator_loss=6.093125343322754, discriminator_loss=0.04538369178771973\n",
            "step 9568: generator_loss=6.463454246520996, discriminator_loss=0.034441523253917694\n",
            "step 9569: generator_loss=7.015951156616211, discriminator_loss=0.028127210214734077\n",
            "step 9570: generator_loss=6.84816837310791, discriminator_loss=0.029888514429330826\n",
            "step 9571: generator_loss=6.828912734985352, discriminator_loss=0.0270853228867054\n",
            "step 9572: generator_loss=7.730081558227539, discriminator_loss=0.01769498735666275\n",
            "step 9573: generator_loss=7.709777355194092, discriminator_loss=0.01776752807199955\n",
            "step 9574: generator_loss=7.645130634307861, discriminator_loss=0.018142815679311752\n",
            "step 9575: generator_loss=7.887471675872803, discriminator_loss=0.015707796439528465\n",
            "step 9576: generator_loss=7.6694231033325195, discriminator_loss=0.015571868047118187\n",
            "step 9577: generator_loss=7.722472667694092, discriminator_loss=0.014476316049695015\n",
            "step 9578: generator_loss=7.599684715270996, discriminator_loss=0.015427185222506523\n",
            "step 9579: generator_loss=6.944289207458496, discriminator_loss=0.01917707361280918\n",
            "step 9580: generator_loss=7.138755798339844, discriminator_loss=0.018614787608385086\n",
            "step 9581: generator_loss=6.655162811279297, discriminator_loss=0.022005673497915268\n",
            "step 9582: generator_loss=6.777672290802002, discriminator_loss=0.01939106360077858\n",
            "step 9583: generator_loss=6.689120292663574, discriminator_loss=0.019442997872829437\n",
            "step 9584: generator_loss=6.734375953674316, discriminator_loss=0.01814703270792961\n",
            "step 9585: generator_loss=6.506202220916748, discriminator_loss=0.017731111496686935\n",
            "step 9586: generator_loss=6.5632734298706055, discriminator_loss=0.017335858196020126\n",
            "step 9587: generator_loss=6.5635271072387695, discriminator_loss=0.01769516058266163\n",
            "step 9588: generator_loss=6.224728584289551, discriminator_loss=0.01775513030588627\n",
            "step 9589: generator_loss=6.095719814300537, discriminator_loss=0.018603000789880753\n",
            "step 9590: generator_loss=6.264862060546875, discriminator_loss=0.017996754497289658\n",
            "step 9591: generator_loss=5.902092933654785, discriminator_loss=0.01808004081249237\n",
            "step 9592: generator_loss=6.066376209259033, discriminator_loss=0.01648637279868126\n",
            "step 9593: generator_loss=6.000833511352539, discriminator_loss=0.015073105692863464\n",
            "step 9594: generator_loss=5.92796516418457, discriminator_loss=0.015140831470489502\n",
            "step 9595: generator_loss=5.6325364112854, discriminator_loss=0.015606814995408058\n",
            "step 9596: generator_loss=5.818472862243652, discriminator_loss=0.015227343887090683\n",
            "step 9597: generator_loss=5.694101333618164, discriminator_loss=0.015060661360621452\n",
            "step 9598: generator_loss=5.63765287399292, discriminator_loss=0.014575261622667313\n",
            "step 9599: generator_loss=5.342836380004883, discriminator_loss=0.015692081302404404\n",
            "step 9600: generator_loss=5.338216304779053, discriminator_loss=0.015519375912845135\n",
            "step 9601: generator_loss=5.073410987854004, discriminator_loss=0.016215765848755836\n",
            "step 9602: generator_loss=5.172844409942627, discriminator_loss=0.01502341590821743\n",
            "step 9603: generator_loss=4.950267791748047, discriminator_loss=0.01625039428472519\n",
            "step 9604: generator_loss=4.817583084106445, discriminator_loss=0.016591235995292664\n",
            "step 9605: generator_loss=4.827923774719238, discriminator_loss=0.016409212723374367\n",
            "step 9606: generator_loss=4.852315902709961, discriminator_loss=0.01620393805205822\n",
            "step 9607: generator_loss=4.735936164855957, discriminator_loss=0.01735963486135006\n",
            "step 9608: generator_loss=4.80570125579834, discriminator_loss=0.01658792607486248\n",
            "step 9609: generator_loss=4.826369285583496, discriminator_loss=0.016373861581087112\n",
            "step 9610: generator_loss=4.469324111938477, discriminator_loss=0.018440794199705124\n",
            "step 9611: generator_loss=4.453563213348389, discriminator_loss=0.018575794994831085\n",
            "step 9612: generator_loss=4.224856376647949, discriminator_loss=0.019787343218922615\n",
            "step 9613: generator_loss=4.413751602172852, discriminator_loss=0.019489016383886337\n",
            "step 9614: generator_loss=4.328322887420654, discriminator_loss=0.019066981971263885\n",
            "step 9615: generator_loss=4.600927829742432, discriminator_loss=0.01768864504992962\n",
            "step 9616: generator_loss=4.903567314147949, discriminator_loss=0.01655868999660015\n",
            "step 9617: generator_loss=4.61490535736084, discriminator_loss=0.017117103561758995\n",
            "step 9618: generator_loss=4.713221549987793, discriminator_loss=0.01627359353005886\n",
            "step 9619: generator_loss=4.939061641693115, discriminator_loss=0.015899278223514557\n",
            "step 9620: generator_loss=4.944718360900879, discriminator_loss=0.015435504727065563\n",
            "step 9621: generator_loss=4.734735012054443, discriminator_loss=0.016360895708203316\n",
            "step 9622: generator_loss=4.889334678649902, discriminator_loss=0.015517421998083591\n",
            "step 9623: generator_loss=4.718414783477783, discriminator_loss=0.01582932099699974\n",
            "step 9624: generator_loss=4.8006696701049805, discriminator_loss=0.015984198078513145\n",
            "step 9625: generator_loss=4.663506507873535, discriminator_loss=0.016235968098044395\n",
            "step 9626: generator_loss=4.756272315979004, discriminator_loss=0.01660054363310337\n",
            "step 9627: generator_loss=4.539807319641113, discriminator_loss=0.01712007075548172\n",
            "step 9628: generator_loss=4.641956329345703, discriminator_loss=0.01736680418252945\n",
            "step 9629: generator_loss=4.457976341247559, discriminator_loss=0.018634680658578873\n",
            "step 9630: generator_loss=4.525726318359375, discriminator_loss=0.01801898702979088\n",
            "step 9631: generator_loss=4.505239009857178, discriminator_loss=0.018562905490398407\n",
            "step 9632: generator_loss=4.272790908813477, discriminator_loss=0.019129116088151932\n",
            "step 9633: generator_loss=4.514546871185303, discriminator_loss=0.01820233277976513\n",
            "step 9634: generator_loss=4.452342987060547, discriminator_loss=0.018969479948282242\n",
            "step 9635: generator_loss=4.277334213256836, discriminator_loss=0.018370263278484344\n",
            "step 9636: generator_loss=4.3386383056640625, discriminator_loss=0.018062807619571686\n",
            "step 9637: generator_loss=4.6035614013671875, discriminator_loss=0.0167301706969738\n",
            "step 9638: generator_loss=4.525352478027344, discriminator_loss=0.016890987753868103\n",
            "step 9639: generator_loss=4.456572532653809, discriminator_loss=0.01636957749724388\n",
            "step 9640: generator_loss=4.411726951599121, discriminator_loss=0.016660910099744797\n",
            "step 9641: generator_loss=4.464344024658203, discriminator_loss=0.016766125336289406\n",
            "step 9642: generator_loss=4.530206680297852, discriminator_loss=0.015947047621011734\n",
            "step 9643: generator_loss=4.559505462646484, discriminator_loss=0.015959471464157104\n",
            "step 9644: generator_loss=4.400130271911621, discriminator_loss=0.01719237118959427\n",
            "step 9645: generator_loss=4.551538467407227, discriminator_loss=0.01616710238158703\n",
            "step 9646: generator_loss=4.398416519165039, discriminator_loss=0.017032569274306297\n",
            "step 9647: generator_loss=4.44791316986084, discriminator_loss=0.016365913674235344\n",
            "step 9648: generator_loss=4.493563652038574, discriminator_loss=0.016716275364160538\n",
            "step 9649: generator_loss=4.4922566413879395, discriminator_loss=0.01630844920873642\n",
            "step 9650: generator_loss=4.541126251220703, discriminator_loss=0.016185158863663673\n",
            "step 9651: generator_loss=4.39296817779541, discriminator_loss=0.016872819513082504\n",
            "step 9652: generator_loss=4.423047065734863, discriminator_loss=0.016926471143960953\n",
            "step 9653: generator_loss=4.578126907348633, discriminator_loss=0.01656254567205906\n",
            "step 9654: generator_loss=4.5726213455200195, discriminator_loss=0.016384687274694443\n",
            "step 9655: generator_loss=4.339888095855713, discriminator_loss=0.017149679362773895\n",
            "step 9656: generator_loss=4.628664016723633, discriminator_loss=0.014867168851196766\n",
            "step 9657: generator_loss=4.494619369506836, discriminator_loss=0.015913236886262894\n",
            "step 9658: generator_loss=4.376066207885742, discriminator_loss=0.0167778879404068\n",
            "step 9659: generator_loss=4.438721656799316, discriminator_loss=0.01659843698143959\n",
            "step 9660: generator_loss=4.337660312652588, discriminator_loss=0.017191996797919273\n",
            "step 9661: generator_loss=4.423851490020752, discriminator_loss=0.016191743314266205\n",
            "step 9662: generator_loss=4.418832778930664, discriminator_loss=0.01615045592188835\n",
            "step 9663: generator_loss=4.533489227294922, discriminator_loss=0.015256058424711227\n",
            "step 9664: generator_loss=4.554285526275635, discriminator_loss=0.015476793982088566\n",
            "step 9665: generator_loss=4.599466800689697, discriminator_loss=0.014765520580112934\n",
            "step 9666: generator_loss=4.466468811035156, discriminator_loss=0.015664352104067802\n",
            "step 9667: generator_loss=4.573073387145996, discriminator_loss=0.015008090063929558\n",
            "step 9668: generator_loss=4.481088638305664, discriminator_loss=0.01504778116941452\n",
            "step 9669: generator_loss=4.4769606590271, discriminator_loss=0.015429506078362465\n",
            "step 9670: generator_loss=4.434118270874023, discriminator_loss=0.015847807750105858\n",
            "step 9671: generator_loss=4.506510257720947, discriminator_loss=0.01541528757661581\n",
            "step 9672: generator_loss=4.349490165710449, discriminator_loss=0.015377591364085674\n",
            "step 9673: generator_loss=4.214761257171631, discriminator_loss=0.016189197078347206\n",
            "step 9674: generator_loss=4.255740642547607, discriminator_loss=0.01598510704934597\n",
            "step 9675: generator_loss=4.278951644897461, discriminator_loss=0.015627309679985046\n",
            "step 9676: generator_loss=4.317815780639648, discriminator_loss=0.015145913697779179\n",
            "step 9677: generator_loss=4.344359397888184, discriminator_loss=0.014711810275912285\n",
            "step 9678: generator_loss=4.392847061157227, discriminator_loss=0.014957182109355927\n",
            "step 9679: generator_loss=4.3177971839904785, discriminator_loss=0.015277625992894173\n",
            "step 9680: generator_loss=4.161605358123779, discriminator_loss=0.015742279589176178\n",
            "step 9681: generator_loss=4.343754768371582, discriminator_loss=0.015583830885589123\n",
            "step 9682: generator_loss=4.2387590408325195, discriminator_loss=0.016006864607334137\n",
            "step 9683: generator_loss=4.100410461425781, discriminator_loss=0.016519714146852493\n",
            "step 9684: generator_loss=4.161313056945801, discriminator_loss=0.016688156872987747\n",
            "step 9685: generator_loss=4.074829578399658, discriminator_loss=0.017025316134095192\n",
            "step 9686: generator_loss=4.050434112548828, discriminator_loss=0.017280615866184235\n",
            "step 9687: generator_loss=4.1026291847229, discriminator_loss=0.016963044181466103\n",
            "step 9688: generator_loss=4.152488708496094, discriminator_loss=0.016635838896036148\n",
            "step 9689: generator_loss=4.093539237976074, discriminator_loss=0.016778364777565002\n",
            "step 9690: generator_loss=4.116339683532715, discriminator_loss=0.01662614569067955\n",
            "step 9691: generator_loss=4.123918533325195, discriminator_loss=0.01642497442662716\n",
            "step 9692: generator_loss=4.1948723793029785, discriminator_loss=0.015894673764705658\n",
            "step 9693: generator_loss=4.14542818069458, discriminator_loss=0.015952907502651215\n",
            "step 9694: generator_loss=4.206281661987305, discriminator_loss=0.015639616176486015\n",
            "step 9695: generator_loss=4.262495040893555, discriminator_loss=0.015196121297776699\n",
            "step 9696: generator_loss=4.391120910644531, discriminator_loss=0.014479887671768665\n",
            "step 9697: generator_loss=4.414216041564941, discriminator_loss=0.01429077424108982\n",
            "step 9698: generator_loss=4.444217681884766, discriminator_loss=0.014171046204864979\n",
            "step 9699: generator_loss=4.452201843261719, discriminator_loss=0.01412879303097725\n",
            "step 9700: generator_loss=4.418236255645752, discriminator_loss=0.014344130642712116\n",
            "step 9701: generator_loss=4.374689102172852, discriminator_loss=0.014574164524674416\n",
            "step 9702: generator_loss=4.341567039489746, discriminator_loss=0.0147579750046134\n",
            "step 9703: generator_loss=4.282004356384277, discriminator_loss=0.01515115611255169\n",
            "step 9704: generator_loss=4.233262062072754, discriminator_loss=0.01548023708164692\n",
            "step 9705: generator_loss=4.179965496063232, discriminator_loss=0.015778176486492157\n",
            "step 9706: generator_loss=4.115605354309082, discriminator_loss=0.016340048983693123\n",
            "step 9707: generator_loss=4.132406711578369, discriminator_loss=0.01656218245625496\n",
            "step 9708: generator_loss=4.123550891876221, discriminator_loss=0.017628923058509827\n",
            "step 9709: generator_loss=4.16375207901001, discriminator_loss=0.016857560724020004\n",
            "step 9710: generator_loss=4.027716636657715, discriminator_loss=0.021824847906827927\n",
            "step 9711: generator_loss=3.978201389312744, discriminator_loss=0.03572945296764374\n",
            "step 9712: generator_loss=3.9433536529541016, discriminator_loss=0.05613300949335098\n",
            "step 9713: generator_loss=4.204440116882324, discriminator_loss=0.05335656926035881\n",
            "step 9714: generator_loss=4.416752815246582, discriminator_loss=0.07078222185373306\n",
            "step 9715: generator_loss=4.418328285217285, discriminator_loss=0.14396986365318298\n",
            "step 9716: generator_loss=4.907928466796875, discriminator_loss=0.11121935397386551\n",
            "step 9717: generator_loss=6.207060813903809, discriminator_loss=0.04644351452589035\n",
            "step 9718: generator_loss=6.712810516357422, discriminator_loss=0.04033990204334259\n",
            "step 9719: generator_loss=7.8732709884643555, discriminator_loss=0.020342912524938583\n",
            "step 9720: generator_loss=8.117055892944336, discriminator_loss=0.019939586520195007\n",
            "step 9721: generator_loss=8.451547622680664, discriminator_loss=0.020788220688700676\n",
            "step 9722: generator_loss=8.429125785827637, discriminator_loss=0.02130405604839325\n",
            "step 9723: generator_loss=9.251274108886719, discriminator_loss=0.01817646622657776\n",
            "step 9724: generator_loss=9.059806823730469, discriminator_loss=0.019847601652145386\n",
            "step 9725: generator_loss=8.438600540161133, discriminator_loss=0.02159331552684307\n",
            "step 9726: generator_loss=8.7940673828125, discriminator_loss=0.022180207073688507\n",
            "step 9727: generator_loss=9.120881080627441, discriminator_loss=0.01978055015206337\n",
            "step 9728: generator_loss=7.830083847045898, discriminator_loss=0.026734773069620132\n",
            "step 9729: generator_loss=7.918102741241455, discriminator_loss=0.02883816324174404\n",
            "step 9730: generator_loss=7.505978584289551, discriminator_loss=0.029169728979468346\n",
            "step 9731: generator_loss=7.328108787536621, discriminator_loss=0.023378387093544006\n",
            "step 9732: generator_loss=7.264683246612549, discriminator_loss=0.01944568008184433\n",
            "step 9733: generator_loss=7.187833309173584, discriminator_loss=0.017393896356225014\n",
            "step 9734: generator_loss=6.9661712646484375, discriminator_loss=0.01690773293375969\n",
            "step 9735: generator_loss=6.552624702453613, discriminator_loss=0.017527587711811066\n",
            "step 9736: generator_loss=6.3030548095703125, discriminator_loss=0.01696840114891529\n",
            "step 9737: generator_loss=6.095103740692139, discriminator_loss=0.017820309847593307\n",
            "step 9738: generator_loss=5.628525733947754, discriminator_loss=0.018505161628127098\n",
            "step 9739: generator_loss=5.331284999847412, discriminator_loss=0.019934017211198807\n",
            "step 9740: generator_loss=5.119596004486084, discriminator_loss=0.01930437982082367\n",
            "step 9741: generator_loss=4.768008708953857, discriminator_loss=0.021510522812604904\n",
            "step 9742: generator_loss=4.636306285858154, discriminator_loss=0.02135605737566948\n",
            "step 9743: generator_loss=4.397775650024414, discriminator_loss=0.021434038877487183\n",
            "step 9744: generator_loss=4.241042137145996, discriminator_loss=0.02177850715816021\n",
            "step 9745: generator_loss=4.131501197814941, discriminator_loss=0.021412577480077744\n",
            "step 9746: generator_loss=4.031096458435059, discriminator_loss=0.02155226096510887\n",
            "step 9747: generator_loss=4.111474514007568, discriminator_loss=0.021432608366012573\n",
            "step 9748: generator_loss=4.0608673095703125, discriminator_loss=0.021402888000011444\n",
            "step 9749: generator_loss=3.9053971767425537, discriminator_loss=0.022560756653547287\n",
            "step 9750: generator_loss=3.8447072505950928, discriminator_loss=0.02300678938627243\n",
            "step 9751: generator_loss=3.8854455947875977, discriminator_loss=0.023680396378040314\n",
            "step 9752: generator_loss=3.826662540435791, discriminator_loss=0.02421734109520912\n",
            "step 9753: generator_loss=3.7900705337524414, discriminator_loss=0.02468106336891651\n",
            "step 9754: generator_loss=3.695861339569092, discriminator_loss=0.026112861931324005\n",
            "step 9755: generator_loss=3.843442678451538, discriminator_loss=0.024521151557564735\n",
            "step 9756: generator_loss=4.012148857116699, discriminator_loss=0.022620517760515213\n",
            "step 9757: generator_loss=3.998344898223877, discriminator_loss=0.022689733654260635\n",
            "step 9758: generator_loss=4.11529541015625, discriminator_loss=0.021735168993473053\n",
            "step 9759: generator_loss=4.1576151847839355, discriminator_loss=0.021352993324398994\n",
            "step 9760: generator_loss=4.076754570007324, discriminator_loss=0.021799948066473007\n",
            "step 9761: generator_loss=4.212190628051758, discriminator_loss=0.02156311273574829\n",
            "step 9762: generator_loss=4.0286664962768555, discriminator_loss=0.02249050885438919\n",
            "step 9763: generator_loss=3.937448263168335, discriminator_loss=0.023178821429610252\n",
            "step 9764: generator_loss=3.8640379905700684, discriminator_loss=0.024156518280506134\n",
            "step 9765: generator_loss=3.895318031311035, discriminator_loss=0.024210812523961067\n",
            "step 9766: generator_loss=3.8006954193115234, discriminator_loss=0.02506871148943901\n",
            "step 9767: generator_loss=3.72754168510437, discriminator_loss=0.02584294229745865\n",
            "step 9768: generator_loss=3.6742236614227295, discriminator_loss=0.026083147153258324\n",
            "step 9769: generator_loss=3.85213041305542, discriminator_loss=0.025086045265197754\n",
            "step 9770: generator_loss=3.6864774227142334, discriminator_loss=0.02626458927989006\n",
            "step 9771: generator_loss=3.690556287765503, discriminator_loss=0.026328708976507187\n",
            "step 9772: generator_loss=3.7524161338806152, discriminator_loss=0.026197755709290504\n",
            "step 9773: generator_loss=3.7780561447143555, discriminator_loss=0.026569731533527374\n",
            "step 9774: generator_loss=3.7886199951171875, discriminator_loss=0.02658269554376602\n",
            "step 9775: generator_loss=3.746015787124634, discriminator_loss=0.026923056691884995\n",
            "step 9776: generator_loss=3.818060874938965, discriminator_loss=0.026435798034071922\n",
            "step 9777: generator_loss=3.853672504425049, discriminator_loss=0.026681790128350258\n",
            "step 9778: generator_loss=3.6927754878997803, discriminator_loss=0.02759636752307415\n",
            "step 9779: generator_loss=3.6737074851989746, discriminator_loss=0.027908865362405777\n",
            "step 9780: generator_loss=3.562467575073242, discriminator_loss=0.029302485287189484\n",
            "step 9781: generator_loss=3.7547521591186523, discriminator_loss=0.028301827609539032\n",
            "step 9782: generator_loss=3.648160457611084, discriminator_loss=0.030193887650966644\n",
            "step 9783: generator_loss=3.698350429534912, discriminator_loss=0.03036060929298401\n",
            "step 9784: generator_loss=3.6596179008483887, discriminator_loss=0.03265995904803276\n",
            "step 9785: generator_loss=3.7260327339172363, discriminator_loss=0.03937012329697609\n",
            "step 9786: generator_loss=3.5442023277282715, discriminator_loss=0.052457042038440704\n",
            "step 9787: generator_loss=3.570601463317871, discriminator_loss=0.08205512166023254\n",
            "step 9788: generator_loss=3.8946235179901123, discriminator_loss=0.06971285492181778\n",
            "step 9789: generator_loss=3.7393386363983154, discriminator_loss=0.11191114783287048\n",
            "step 9790: generator_loss=4.129967212677002, discriminator_loss=0.14212583005428314\n",
            "step 9791: generator_loss=4.2275495529174805, discriminator_loss=0.19099348783493042\n",
            "step 9792: generator_loss=4.5943922996521, discriminator_loss=0.13898395001888275\n",
            "step 9793: generator_loss=5.200963020324707, discriminator_loss=0.11299968510866165\n",
            "step 9794: generator_loss=5.2944746017456055, discriminator_loss=0.07790202647447586\n",
            "step 9795: generator_loss=5.278134346008301, discriminator_loss=0.12446732819080353\n",
            "step 9796: generator_loss=4.674015045166016, discriminator_loss=0.15462952852249146\n",
            "step 9797: generator_loss=5.518867015838623, discriminator_loss=0.06885966658592224\n",
            "step 9798: generator_loss=5.980160236358643, discriminator_loss=0.0491379052400589\n",
            "step 9799: generator_loss=5.97702169418335, discriminator_loss=0.0419783815741539\n",
            "step 9800: generator_loss=6.253553867340088, discriminator_loss=0.03775845468044281\n",
            "step 9801: generator_loss=6.606481075286865, discriminator_loss=0.0361323282122612\n",
            "step 9802: generator_loss=6.871303558349609, discriminator_loss=0.035874538123607635\n",
            "step 9803: generator_loss=7.122686862945557, discriminator_loss=0.03506074845790863\n",
            "step 9804: generator_loss=6.590690612792969, discriminator_loss=0.038505785167217255\n",
            "step 9805: generator_loss=6.080595016479492, discriminator_loss=0.045767903327941895\n",
            "step 9806: generator_loss=6.028290271759033, discriminator_loss=0.04793541878461838\n",
            "step 9807: generator_loss=5.810799598693848, discriminator_loss=0.049943652004003525\n",
            "step 9808: generator_loss=5.6994218826293945, discriminator_loss=0.04949719458818436\n",
            "step 9809: generator_loss=5.056913375854492, discriminator_loss=0.05855307728052139\n",
            "step 9810: generator_loss=4.935810089111328, discriminator_loss=0.04920698702335358\n",
            "step 9811: generator_loss=5.016844749450684, discriminator_loss=0.03978976979851723\n",
            "step 9812: generator_loss=5.132548809051514, discriminator_loss=0.035249724984169006\n",
            "step 9813: generator_loss=5.052284240722656, discriminator_loss=0.03371699899435043\n",
            "step 9814: generator_loss=4.634208679199219, discriminator_loss=0.036223869770765305\n",
            "step 9815: generator_loss=4.508479118347168, discriminator_loss=0.03642422705888748\n",
            "step 9816: generator_loss=4.453474521636963, discriminator_loss=0.03436925634741783\n",
            "step 9817: generator_loss=4.133623123168945, discriminator_loss=0.03660934790968895\n",
            "step 9818: generator_loss=3.8831748962402344, discriminator_loss=0.03782956302165985\n",
            "step 9819: generator_loss=3.638202667236328, discriminator_loss=0.03899981826543808\n",
            "step 9820: generator_loss=3.658238410949707, discriminator_loss=0.039949931204319\n",
            "step 9821: generator_loss=3.5904464721679688, discriminator_loss=0.04032490402460098\n",
            "step 9822: generator_loss=3.7769861221313477, discriminator_loss=0.03647053986787796\n",
            "step 9823: generator_loss=3.764869451522827, discriminator_loss=0.03534477949142456\n",
            "step 9824: generator_loss=3.9311163425445557, discriminator_loss=0.032962363213300705\n",
            "step 9825: generator_loss=4.099742412567139, discriminator_loss=0.0314721018075943\n",
            "step 9826: generator_loss=4.055291652679443, discriminator_loss=0.03157845884561539\n",
            "step 9827: generator_loss=4.172640323638916, discriminator_loss=0.030888739973306656\n",
            "step 9828: generator_loss=4.1204657554626465, discriminator_loss=0.03101169504225254\n",
            "step 9829: generator_loss=4.033640384674072, discriminator_loss=0.031119581311941147\n",
            "step 9830: generator_loss=3.8900489807128906, discriminator_loss=0.031493209302425385\n",
            "step 9831: generator_loss=3.905374765396118, discriminator_loss=0.03163392096757889\n",
            "step 9832: generator_loss=3.925887107849121, discriminator_loss=0.03110065497457981\n",
            "step 9833: generator_loss=3.7881557941436768, discriminator_loss=0.03154653310775757\n",
            "step 9834: generator_loss=3.9160239696502686, discriminator_loss=0.03022885136306286\n",
            "step 9835: generator_loss=3.9727940559387207, discriminator_loss=0.028964810073375702\n",
            "step 9836: generator_loss=4.000272750854492, discriminator_loss=0.028693130239844322\n",
            "step 9837: generator_loss=4.0507588386535645, discriminator_loss=0.028115151450037956\n",
            "step 9838: generator_loss=4.032844543457031, discriminator_loss=0.02813110686838627\n",
            "step 9839: generator_loss=3.981797695159912, discriminator_loss=0.02836119569838047\n",
            "step 9840: generator_loss=3.980320692062378, discriminator_loss=0.028136620298027992\n",
            "step 9841: generator_loss=3.9983015060424805, discriminator_loss=0.028491679579019547\n",
            "step 9842: generator_loss=3.8556125164031982, discriminator_loss=0.029211949557065964\n",
            "step 9843: generator_loss=3.859198570251465, discriminator_loss=0.029705243185162544\n",
            "step 9844: generator_loss=3.8569719791412354, discriminator_loss=0.0297542791813612\n",
            "step 9845: generator_loss=3.8455100059509277, discriminator_loss=0.03146976977586746\n",
            "step 9846: generator_loss=3.8209800720214844, discriminator_loss=0.031975820660591125\n",
            "step 9847: generator_loss=3.8643484115600586, discriminator_loss=0.031537365168333054\n",
            "step 9848: generator_loss=3.7792086601257324, discriminator_loss=0.035769157111644745\n",
            "step 9849: generator_loss=3.7605466842651367, discriminator_loss=0.03743761032819748\n",
            "step 9850: generator_loss=3.9099643230438232, discriminator_loss=0.03673788532614708\n",
            "step 9851: generator_loss=3.9341859817504883, discriminator_loss=0.04048063978552818\n",
            "step 9852: generator_loss=4.020664215087891, discriminator_loss=0.041199930012226105\n",
            "step 9853: generator_loss=4.300298690795898, discriminator_loss=0.03656437620520592\n",
            "step 9854: generator_loss=4.340290546417236, discriminator_loss=0.040005750954151154\n",
            "step 9855: generator_loss=4.42228889465332, discriminator_loss=0.04172239452600479\n",
            "step 9856: generator_loss=4.670045852661133, discriminator_loss=0.03635186702013016\n",
            "step 9857: generator_loss=4.759815216064453, discriminator_loss=0.03534723073244095\n",
            "step 9858: generator_loss=4.687542915344238, discriminator_loss=0.039577409625053406\n",
            "step 9859: generator_loss=4.860903739929199, discriminator_loss=0.036697711795568466\n",
            "step 9860: generator_loss=4.825689315795898, discriminator_loss=0.03684867545962334\n",
            "step 9861: generator_loss=5.121650695800781, discriminator_loss=0.03427790105342865\n",
            "step 9862: generator_loss=4.982965469360352, discriminator_loss=0.036606818437576294\n",
            "step 9863: generator_loss=5.4123382568359375, discriminator_loss=0.0354800708591938\n",
            "step 9864: generator_loss=5.4425835609436035, discriminator_loss=0.03592574596405029\n",
            "step 9865: generator_loss=5.651453018188477, discriminator_loss=0.034247979521751404\n",
            "step 9866: generator_loss=6.002202033996582, discriminator_loss=0.03333357721567154\n",
            "step 9867: generator_loss=6.125005722045898, discriminator_loss=0.03147327899932861\n",
            "step 9868: generator_loss=6.389397621154785, discriminator_loss=0.02911650761961937\n",
            "step 9869: generator_loss=5.731346130371094, discriminator_loss=0.033106423914432526\n",
            "step 9870: generator_loss=5.744254112243652, discriminator_loss=0.031799767166376114\n",
            "step 9871: generator_loss=6.323675632476807, discriminator_loss=0.027010131627321243\n",
            "step 9872: generator_loss=7.040119647979736, discriminator_loss=0.023557953536510468\n",
            "step 9873: generator_loss=6.748779296875, discriminator_loss=0.024008657783269882\n",
            "step 9874: generator_loss=6.263692855834961, discriminator_loss=0.025226425379514694\n",
            "step 9875: generator_loss=6.223531723022461, discriminator_loss=0.02583843097090721\n",
            "step 9876: generator_loss=5.8528361320495605, discriminator_loss=0.02726096659898758\n",
            "step 9877: generator_loss=5.731873989105225, discriminator_loss=0.027442824095487595\n",
            "step 9878: generator_loss=5.855900764465332, discriminator_loss=0.024917418137192726\n",
            "step 9879: generator_loss=5.754087924957275, discriminator_loss=0.022826313972473145\n",
            "step 9880: generator_loss=5.634420394897461, discriminator_loss=0.02225324511528015\n",
            "step 9881: generator_loss=5.550179958343506, discriminator_loss=0.0209790226072073\n",
            "step 9882: generator_loss=5.47688102722168, discriminator_loss=0.020468711853027344\n",
            "step 9883: generator_loss=5.269120216369629, discriminator_loss=0.020210590213537216\n",
            "step 9884: generator_loss=5.260541915893555, discriminator_loss=0.019935157150030136\n",
            "step 9885: generator_loss=4.8918280601501465, discriminator_loss=0.021381350234150887\n",
            "step 9886: generator_loss=4.811924934387207, discriminator_loss=0.021146878600120544\n",
            "step 9887: generator_loss=4.720433712005615, discriminator_loss=0.02182277850806713\n",
            "step 9888: generator_loss=4.57481575012207, discriminator_loss=0.022205576300621033\n",
            "step 9889: generator_loss=4.363082408905029, discriminator_loss=0.023461539298295975\n",
            "step 9890: generator_loss=4.37483549118042, discriminator_loss=0.022707780823111534\n",
            "step 9891: generator_loss=4.389958381652832, discriminator_loss=0.021864376962184906\n",
            "step 9892: generator_loss=4.531179428100586, discriminator_loss=0.020623741671442986\n",
            "step 9893: generator_loss=4.570295333862305, discriminator_loss=0.020237287506461143\n",
            "step 9894: generator_loss=4.573264122009277, discriminator_loss=0.019798889756202698\n",
            "step 9895: generator_loss=4.693480491638184, discriminator_loss=0.019401662051677704\n",
            "step 9896: generator_loss=4.642637729644775, discriminator_loss=0.019223690032958984\n",
            "step 9897: generator_loss=4.611252784729004, discriminator_loss=0.01955300197005272\n",
            "step 9898: generator_loss=4.53175163269043, discriminator_loss=0.01989077217876911\n",
            "step 9899: generator_loss=4.487247943878174, discriminator_loss=0.020238522440195084\n",
            "step 9900: generator_loss=4.51982307434082, discriminator_loss=0.020122576504945755\n",
            "step 9901: generator_loss=4.265901565551758, discriminator_loss=0.02116350270807743\n",
            "step 9902: generator_loss=4.157625675201416, discriminator_loss=0.021864384412765503\n",
            "step 9903: generator_loss=4.209870338439941, discriminator_loss=0.02155505307018757\n",
            "step 9904: generator_loss=4.143617630004883, discriminator_loss=0.02184992842376232\n",
            "step 9905: generator_loss=4.265967845916748, discriminator_loss=0.020921144634485245\n",
            "step 9906: generator_loss=4.256130218505859, discriminator_loss=0.020817700773477554\n",
            "step 9907: generator_loss=4.219051837921143, discriminator_loss=0.02083035744726658\n",
            "step 9908: generator_loss=4.306597709655762, discriminator_loss=0.02024427056312561\n",
            "step 9909: generator_loss=4.326785564422607, discriminator_loss=0.019824396818876266\n",
            "step 9910: generator_loss=4.293137550354004, discriminator_loss=0.019930459558963776\n",
            "step 9911: generator_loss=4.295644760131836, discriminator_loss=0.019783657044172287\n",
            "step 9912: generator_loss=4.279304027557373, discriminator_loss=0.019855093210935593\n",
            "step 9913: generator_loss=4.242060661315918, discriminator_loss=0.020049160346388817\n",
            "step 9914: generator_loss=4.1688313484191895, discriminator_loss=0.02049427479505539\n",
            "step 9915: generator_loss=4.1302571296691895, discriminator_loss=0.020707396790385246\n",
            "step 9916: generator_loss=4.153167724609375, discriminator_loss=0.020460646599531174\n",
            "step 9917: generator_loss=4.158974647521973, discriminator_loss=0.02032604068517685\n",
            "step 9918: generator_loss=4.122859954833984, discriminator_loss=0.020587362349033356\n",
            "step 9919: generator_loss=4.103778839111328, discriminator_loss=0.020711351186037064\n",
            "step 9920: generator_loss=4.035693645477295, discriminator_loss=0.021128015592694283\n",
            "step 9921: generator_loss=4.033936500549316, discriminator_loss=0.021131064742803574\n",
            "step 9922: generator_loss=4.072932243347168, discriminator_loss=0.020719315856695175\n",
            "step 9923: generator_loss=4.128573894500732, discriminator_loss=0.02008116990327835\n",
            "step 9924: generator_loss=4.19405460357666, discriminator_loss=0.019509632140398026\n",
            "step 9925: generator_loss=4.175957202911377, discriminator_loss=0.01952672004699707\n",
            "step 9926: generator_loss=4.14122200012207, discriminator_loss=0.01977630890905857\n",
            "step 9927: generator_loss=4.091916084289551, discriminator_loss=0.02020994946360588\n",
            "step 9928: generator_loss=4.00685453414917, discriminator_loss=0.020769193768501282\n",
            "step 9929: generator_loss=3.9728646278381348, discriminator_loss=0.021055931225419044\n",
            "step 9930: generator_loss=3.9301671981811523, discriminator_loss=0.02146286517381668\n",
            "step 9931: generator_loss=3.8792314529418945, discriminator_loss=0.021978408098220825\n",
            "step 9932: generator_loss=3.850576162338257, discriminator_loss=0.022271569818258286\n",
            "step 9933: generator_loss=3.837379217147827, discriminator_loss=0.02243959531188011\n",
            "step 9934: generator_loss=3.8141236305236816, discriminator_loss=0.022700324654579163\n",
            "step 9935: generator_loss=3.8284342288970947, discriminator_loss=0.02276112139225006\n",
            "step 9936: generator_loss=3.894930839538574, discriminator_loss=0.02211010456085205\n",
            "step 9937: generator_loss=4.037811279296875, discriminator_loss=0.0208749882876873\n",
            "step 9938: generator_loss=4.11409854888916, discriminator_loss=0.02033351920545101\n",
            "step 9939: generator_loss=4.072447776794434, discriminator_loss=0.020765013992786407\n",
            "step 9940: generator_loss=4.013843536376953, discriminator_loss=0.021425515413284302\n",
            "step 9941: generator_loss=3.9507827758789062, discriminator_loss=0.022105585783720016\n",
            "step 9942: generator_loss=3.941451072692871, discriminator_loss=0.022319938987493515\n",
            "step 9943: generator_loss=3.936232089996338, discriminator_loss=0.02239839732646942\n",
            "step 9944: generator_loss=3.9549717903137207, discriminator_loss=0.022287899628281593\n",
            "step 9945: generator_loss=3.975470781326294, discriminator_loss=0.022200800478458405\n",
            "step 9946: generator_loss=3.9775357246398926, discriminator_loss=0.022222787141799927\n",
            "step 9947: generator_loss=3.9685990810394287, discriminator_loss=0.02236013486981392\n",
            "step 9948: generator_loss=3.983410358428955, discriminator_loss=0.02223925292491913\n",
            "step 9949: generator_loss=3.9767730236053467, discriminator_loss=0.022283609956502914\n",
            "step 9950: generator_loss=3.90683650970459, discriminator_loss=0.022837786003947258\n",
            "step 9951: generator_loss=3.8262157440185547, discriminator_loss=0.023637039586901665\n",
            "step 9952: generator_loss=3.7648580074310303, discriminator_loss=0.02422412857413292\n",
            "step 9953: generator_loss=3.7450175285339355, discriminator_loss=0.024387823417782784\n",
            "step 9954: generator_loss=3.795271396636963, discriminator_loss=0.023706426844000816\n",
            "step 9955: generator_loss=3.833327293395996, discriminator_loss=0.023196157068014145\n",
            "step 9956: generator_loss=3.8488221168518066, discriminator_loss=0.022975528612732887\n",
            "step 9957: generator_loss=3.8523778915405273, discriminator_loss=0.02302766963839531\n",
            "step 9958: generator_loss=3.8498313426971436, discriminator_loss=0.023126576095819473\n",
            "step 9959: generator_loss=3.891927719116211, discriminator_loss=0.022802287712693214\n",
            "step 9960: generator_loss=3.9334921836853027, discriminator_loss=0.02246563881635666\n",
            "step 9961: generator_loss=3.933771848678589, discriminator_loss=0.022550906985998154\n",
            "step 9962: generator_loss=3.900075674057007, discriminator_loss=0.022981751710176468\n",
            "step 9963: generator_loss=3.9391891956329346, discriminator_loss=0.022666221484541893\n",
            "step 9964: generator_loss=3.9971842765808105, discriminator_loss=0.0221550352871418\n",
            "step 9965: generator_loss=4.010900497436523, discriminator_loss=0.02201809361577034\n",
            "step 9966: generator_loss=3.9784674644470215, discriminator_loss=0.022334538400173187\n",
            "step 9967: generator_loss=3.89713716506958, discriminator_loss=0.023160455748438835\n",
            "step 9968: generator_loss=3.853455066680908, discriminator_loss=0.0234354417771101\n",
            "step 9969: generator_loss=3.844794750213623, discriminator_loss=0.023498468101024628\n",
            "step 9970: generator_loss=3.910308599472046, discriminator_loss=0.022695578634738922\n",
            "step 9971: generator_loss=4.155098915100098, discriminator_loss=0.020387213677167892\n",
            "step 9972: generator_loss=4.387205600738525, discriminator_loss=0.018772441893815994\n",
            "step 9973: generator_loss=4.596297264099121, discriminator_loss=0.017287885770201683\n",
            "step 9974: generator_loss=4.696252822875977, discriminator_loss=0.016612371429800987\n",
            "step 9975: generator_loss=4.694988250732422, discriminator_loss=0.01629190519452095\n",
            "step 9976: generator_loss=4.619687080383301, discriminator_loss=0.016338728368282318\n",
            "step 9977: generator_loss=4.44105339050293, discriminator_loss=0.017271574586629868\n",
            "step 9978: generator_loss=4.29270601272583, discriminator_loss=0.017850300297141075\n",
            "step 9979: generator_loss=4.214437007904053, discriminator_loss=0.018345024436712265\n",
            "step 9980: generator_loss=4.133852005004883, discriminator_loss=0.019456911832094193\n",
            "step 9981: generator_loss=4.18861722946167, discriminator_loss=0.018011242151260376\n",
            "step 9982: generator_loss=4.183534622192383, discriminator_loss=0.018560081720352173\n",
            "step 9983: generator_loss=4.260321617126465, discriminator_loss=0.01843337155878544\n",
            "step 9984: generator_loss=4.370619773864746, discriminator_loss=0.016921255737543106\n",
            "step 9985: generator_loss=4.451136589050293, discriminator_loss=0.017128560692071915\n",
            "step 9986: generator_loss=4.426746368408203, discriminator_loss=0.01980464905500412\n",
            "step 9987: generator_loss=4.516256332397461, discriminator_loss=0.020642265677452087\n",
            "step 9988: generator_loss=4.5774312019348145, discriminator_loss=0.024318089708685875\n",
            "step 9989: generator_loss=4.496335983276367, discriminator_loss=0.026642682030797005\n",
            "step 9990: generator_loss=4.597403526306152, discriminator_loss=0.0305547546595335\n",
            "step 9991: generator_loss=4.637988567352295, discriminator_loss=0.03192216902971268\n",
            "step 9992: generator_loss=4.517797470092773, discriminator_loss=0.03843202441930771\n",
            "step 9993: generator_loss=4.716829299926758, discriminator_loss=0.035548701882362366\n",
            "step 9994: generator_loss=4.6179728507995605, discriminator_loss=0.04403989017009735\n",
            "step 9995: generator_loss=4.616078853607178, discriminator_loss=0.046535640954971313\n",
            "step 9996: generator_loss=5.195317268371582, discriminator_loss=0.032468438148498535\n",
            "step 9997: generator_loss=5.188457012176514, discriminator_loss=0.03797115758061409\n",
            "step 9998: generator_loss=5.43843412399292, discriminator_loss=0.03282618150115013\n",
            "step 9999: generator_loss=5.757081985473633, discriminator_loss=0.02760891430079937\n",
            "step 10000: generator_loss=5.7503581047058105, discriminator_loss=0.030189931392669678\n"
          ]
        }
      ],
      "source": [
        "# Continue training from the most recent checkpoint\n",
        "# Assumes only that `train_data` has already been defined.\n",
        "\n",
        "# --- Recreate GAN / optimizer hyperparameters to match original run ---\n",
        "learning_rate = 1e-3\n",
        "batch_size = 128\n",
        "latent_dim = 64\n",
        "loss_type = \"nonsaturating\"\n",
        "steps_per_save = 250\n",
        "optimizer = optax.adam(learning_rate)\n",
        "\n",
        "# --- Resume from last checkpoint and continue training ---\n",
        "extra_steps = 5_000  # number of additional training steps\n",
        "resume_seed = 1  # RNG seed for resumed training\n",
        "key = jax.random.key(resume_seed)\n",
        "\n",
        "experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "steps = sorted(ckpt_manager.all_steps())\n",
        "if not steps:\n",
        "    raise ValueError(f\"No checkpoints found for experiment_name={experiment_name!r}.\")\n",
        "\n",
        "last_step = steps[-1]\n",
        "print(f\"Resuming training from global step {last_step} (experiment_name={experiment_name!r})\")\n",
        "\n",
        "# Template states for restoring checkpoints\n",
        "gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "    optimizer=optimizer,\n",
        "    key=jax.random.key(0),\n",
        "    latent_dim=latent_dim,\n",
        ")\n",
        "restored = ckpt_manager.restore(\n",
        "    last_step,\n",
        "    args=ocp.args.StandardRestore(\n",
        "        item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "    ),\n",
        ")\n",
        "\n",
        "generator_training_state = restored[\"generator\"]\n",
        "discriminator_training_state = restored[\"discriminator\"]\n",
        "\n",
        "# Run additional training and keep saving checkpoints with global step indices\n",
        "generator_training_state, discriminator_training_state, key = run_training_gan(\n",
        "    train_data=train_data,\n",
        "    n_steps=extra_steps,\n",
        "    generator_training_state=generator_training_state,\n",
        "    discriminator_training_state=discriminator_training_state,\n",
        "    key=key,\n",
        "    steps_per_save=steps_per_save,\n",
        "    checkpoint_manager=ckpt_manager,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        "    start_step=last_step,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1bd0cbe4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot real vs generated histogram after extended training (up to 10,000 steps)\n",
        "\n",
        "plot_real_vs_generated_histogram_from_last_checkpoint(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        "    y_max=0.02,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3bab6254",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot GAN training dynamics using all available checkpoints\n",
        "# (original run + extended training)\n",
        "\n",
        "plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
