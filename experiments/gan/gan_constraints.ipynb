{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 182,
      "id": "181bca2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax import vmap\n",
        "\n",
        "import numpy as np\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.collections import LineCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2741d19",
      "metadata": {},
      "source": [
        "### Mixture of Gaussians\n",
        "\n",
        "First we're going to generate a mixture of two Gaussians that the generator is supposed to match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "ccb3e26d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pmf_gaussian_single(\n",
        "    mu: float,\n",
        "    sigma: float,\n",
        "    n_pixels: int = 784,\n",
        "):\n",
        "    \"\"\"\n",
        "    Discrete Gaussian over 784 bins\n",
        "    Uses probability densities at bin centres rather than \n",
        "    integrated probability masses over the bin intervals\n",
        "    \"\"\"\n",
        "    pixel_midpoints = jnp.arange(n_pixels) + 0.5\n",
        "    pixel_frequencies = jnp.exp(-0.5 * ((pixel_midpoints - mu) / sigma) ** 2)\n",
        "    pixel_probabilities = pixel_frequencies / pixel_frequencies.sum()\n",
        "    return pixel_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "f1e74b31",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pmf_gaussian_mixed(\n",
        "    mu1: float,\n",
        "    sigma1: float,\n",
        "    mu2: float,\n",
        "    sigma2: float,\n",
        "    alpha: float,\n",
        "    n_pixels: int = 784,\n",
        "):\n",
        "    \"\"\"pmf for α*N(mean1, std1) + (1-α)*N(mean2, std2) on discrete bins.\"\"\"\n",
        "    p1 = pmf_gaussian_single(mu1, sigma1, n_pixels)\n",
        "    p2 = pmf_gaussian_single(mu2, sigma2, n_pixels)\n",
        "    probs = alpha * p1 + (1.0 - alpha) * p2\n",
        "    return probs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "064ed82b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def r_gaussian_mixed(\n",
        "    key,\n",
        "    mu1: float,\n",
        "    sigma1: float,\n",
        "    mu2: float,\n",
        "    sigma2: float,\n",
        "    alpha: float,\n",
        "    n_pixels: int = 784,\n",
        "    n_samples: int = 100,\n",
        "    n_training_samples: int = 10,\n",
        "):\n",
        "    \"\"\"Draw histograms from the 2-Gaussian mixture.\n",
        "\n",
        "    Returns an array of shape (n_training_samples, n_pixels), where each row\n",
        "    is a histogram (normalized to sum to 1) from `n_samples` draws.\n",
        "    \"\"\"\n",
        "    probs = pmf_gaussian_mixed(mu1, sigma1, mu2, sigma2, alpha, n_pixels)\n",
        "    probs = jnp.broadcast_to(probs, (n_training_samples, n_pixels))\n",
        "    counts = jr.multinomial(key, n=n_samples, p=probs)\n",
        "    return counts / n_samples\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "9813eac5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = nn.Dense(\n",
        "                layer_size,\n",
        "                kernel_init=nn.initializers.normal(0.1),\n",
        "                bias_init=nn.initializers.normal(0.1)\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "275f18e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LowRankDense(nn.Module):\n",
        "    \"\"\"Low-rank dense layer implemented with two factors and einsum.\n",
        "\n",
        "    Parameters are U in R^{in_features x rank} and V in R^{rank x features}.\n",
        "    The forward pass computes y = (x @ U) @ V + b using einsum.\n",
        "    \"\"\"\n",
        "    features: int\n",
        "    rank: int\n",
        "    use_bias: bool = True\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: [batch, in_features]\n",
        "        in_features = inputs.shape[-1]\n",
        "\n",
        "        U = self.param(\n",
        "            \"U\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (in_features, self.rank),\n",
        "        )\n",
        "        V = self.param(\n",
        "            \"V\",\n",
        "            nn.initializers.normal(0.1),\n",
        "            (self.rank, self.features),\n",
        "        )\n",
        "\n",
        "        hidden = jnp.einsum(\"bi,ir->br\", inputs, U)\n",
        "        y = jnp.einsum(\"br,rf->bf\", hidden, V)\n",
        "\n",
        "        if self.use_bias:\n",
        "            bias = self.param(\n",
        "                \"bias\",\n",
        "                nn.initializers.normal(0.1),\n",
        "                (self.features,),\n",
        "            )\n",
        "            y = y + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class LowRankMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Every layer uses the same low-rank dimension rank (=\"rank\")\n",
        "    \"\"\"\n",
        "    layer_sizes: Sequence[int]\n",
        "    rank: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, activations):\n",
        "        for layer_number, layer_size in enumerate(self.layer_sizes):\n",
        "            activations = LowRankDense(\n",
        "                features=layer_size,\n",
        "                rank=self.rank,\n",
        "                use_bias=True,\n",
        "            )(activations)\n",
        "\n",
        "            if layer_number != (len(self.layer_sizes) - 1):\n",
        "                activations = nn.relu(activations)\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "35a91c33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialise_network_params(model, input_layer_size, key):\n",
        "    \"\"\"Initialize all layers for a fully-connected neural network\"\"\"\n",
        "    input_shape_dummy = jnp.ones((1, input_layer_size))\n",
        "    params = model.init(key, input_shape_dummy)[\"params\"]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "id": "8f245d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_state(layer_sizes, optimizer, key, use_lowrank: bool = False, rank: int | None = None):\n",
        "    input_layer_size = layer_sizes[0]\n",
        "    network_layer_sizes = layer_sizes[1:]\n",
        "\n",
        "    if use_lowrank:\n",
        "        if rank is None:\n",
        "            raise ValueError(\"rank must be provided when use_lowrank=True\")\n",
        "        model = LowRankMLP(layer_sizes=network_layer_sizes, rank=rank)\n",
        "    else:\n",
        "        model = MLP(layer_sizes=network_layer_sizes)\n",
        "\n",
        "    apply_fn = model.apply\n",
        "    params = initialise_network_params(model, input_layer_size, key)\n",
        "    training_state = train_state.TrainState.create(\n",
        "        apply_fn=apply_fn,\n",
        "        params=params,\n",
        "        tx=optimizer,\n",
        "    )\n",
        "    return training_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "15a25657",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss_single_label(logits, label):\n",
        "    targets = jnp.full_like(logits, label)\n",
        "    return optax.sigmoid_binary_cross_entropy(logits, targets).mean()\n",
        "\n",
        "def generator_loss_nonsaturating(logits_real_given_fake):\n",
        "    \"\"\"Objective: maximise p(predicted real | fake)\"\"\"\n",
        "    return + cross_entropy_loss_single_label(logits=logits_real_given_fake, label=1)\n",
        "\n",
        "def generator_loss_saturating(logits_real_given_fake):\n",
        "    \"\"\"Objective: minimise p(predicted fake | fake)\"\"\"\n",
        "    return - cross_entropy_loss_single_label(logits=logits_real_given_fake, label=0)\n",
        "\n",
        "def discriminator_loss(logits_real_given_real, logits_real_given_fake):\n",
        "    loss_given_real = cross_entropy_loss_single_label(logits=logits_real_given_real, label=1)\n",
        "    loss_given_fake = cross_entropy_loss_single_label(logits=logits_real_given_fake, label=0)\n",
        "    return (loss_given_real + loss_given_fake) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "b22555e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_generator_loss(generator_params, discriminator_params, generator_apply_fn, discriminator_apply_fn, z_vector, loss_type=\"nonsaturating\"):\n",
        "    fake_images = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    logits_real_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    if loss_type == \"nonsaturating\":\n",
        "        return generator_loss_nonsaturating(logits_real_given_fake)\n",
        "    elif loss_type == \"saturating\":\n",
        "        return generator_loss_saturating(logits_real_given_fake)\n",
        "    else:\n",
        "        raise ValueError(f\"incorrect loss type specified: {loss_type}\")\n",
        "\n",
        "\n",
        "def calculate_discriminator_loss(discriminator_params, generator_params, generator_apply_fn, discriminator_apply_fn, z_vector, real_images):\n",
        "    fake_images = generator_apply_fn({\"params\": generator_params}, z_vector)\n",
        "    logits_real_given_fake = discriminator_apply_fn({\"params\": discriminator_params}, fake_images)\n",
        "    logits_real_given_real = discriminator_apply_fn({\"params\": discriminator_params}, real_images)\n",
        "    return discriminator_loss(logits_real_given_real, logits_real_given_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "efbab3ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def take_generator_step_nonsaturating(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        \"nonsaturating\",\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_generator_step_saturating(generator, discriminator, z_vector):\n",
        "    grads_by_params_fn = jax.grad(calculate_generator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        generator.params,\n",
        "        discriminator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        \"saturating\",\n",
        "    )\n",
        "    return generator.apply_gradients(grads=grads_by_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def take_discriminator_step(generator, discriminator, z_vector, real_images):\n",
        "    grads_by_params_fn = jax.grad(calculate_discriminator_loss)\n",
        "    grads_by_params = grads_by_params_fn(\n",
        "        discriminator.params,\n",
        "        generator.params,\n",
        "        generator.apply_fn,\n",
        "        discriminator.apply_fn,\n",
        "        z_vector,\n",
        "        real_images,\n",
        "    )\n",
        "    return discriminator.apply_gradients(grads=grads_by_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "a156d045",
      "metadata": {},
      "outputs": [],
      "source": [
        "def subsample_images_for_batch(key, images_full_sample, batch_size):\n",
        "    image_ids = jax.random.randint(key, (batch_size,), 0, images_full_sample.shape[0])\n",
        "    return images_full_sample[image_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "id": "e79102c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training_gan(\n",
        "    train_data,\n",
        "    n_steps,\n",
        "    generator_training_state,\n",
        "    discriminator_training_state,\n",
        "    key,\n",
        "    steps_per_save,\n",
        "    checkpoint_manager,\n",
        "    batch_size: int,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "):\n",
        "    \"\"\"Train a GAN using random mini-batches of real images.\n",
        "\n",
        "    Shapes:\n",
        "      - train_data[\"image\"]: (N, n_pixels)\n",
        "      - real_images_batch:   (batch_size, n_pixels)\n",
        "      - z_vectors:           (batch_size, latent_dim)\n",
        "    \"\"\"\n",
        "    real_images = train_data\n",
        "\n",
        "    if loss_type == \"nonsaturating\":\n",
        "        take_generator_step = take_generator_step_nonsaturating\n",
        "    elif loss_type == \"saturating\":\n",
        "        take_generator_step = take_generator_step_saturating\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
        "\n",
        "    for step in range(1, n_steps + 1):\n",
        "        key, key_z_generation, key_real_subsample = jax.random.split(key, 3)\n",
        "\n",
        "        # Random mini-batch of real images\n",
        "        real_images_batch = subsample_images_for_batch(\n",
        "            key_real_subsample,\n",
        "            real_images,\n",
        "            batch_size,\n",
        "        )\n",
        "\n",
        "        # Latent vectors for generator\n",
        "        z_vectors = jax.random.normal(\n",
        "            key_z_generation,\n",
        "            (batch_size, latent_dim),\n",
        "        )\n",
        "\n",
        "        # Update discriminator then generator\n",
        "        discriminator_training_state = take_discriminator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "        generator_training_state = take_generator_step(\n",
        "            generator_training_state,\n",
        "            discriminator_training_state,\n",
        "            z_vectors,\n",
        "        )\n",
        "\n",
        "        # Monitor losses\n",
        "        generator_loss_value = calculate_generator_loss(\n",
        "            generator_training_state.params,\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            loss_type=loss_type,\n",
        "        )\n",
        "        discriminator_loss_value = calculate_discriminator_loss(\n",
        "            discriminator_training_state.params,\n",
        "            generator_training_state.params,\n",
        "            generator_training_state.apply_fn,\n",
        "            discriminator_training_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "        print(\n",
        "            f\"step {step}: generator_loss={generator_loss_value}, \"\n",
        "            f\"discriminator_loss={discriminator_loss_value}\"\n",
        "        )\n",
        "\n",
        "        if step == 1 or step % steps_per_save == 0:\n",
        "            checkpoint_manager.save(\n",
        "                step,\n",
        "                args=ocp.args.StandardSave(\n",
        "                    {\n",
        "                        \"generator\": generator_training_state,\n",
        "                        \"discriminator\": discriminator_training_state,\n",
        "                    }\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "id": "d4d625f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_batch(images, labels, n_batches):\n",
        "    \"\"\"Drops the last set of samples if they're not the right length\"\"\"\n",
        "    n_samples = len(images)\n",
        "    assert len(images) == len(labels)\n",
        "    assert n_samples >= n_batches\n",
        "    assert n_batches > 0\n",
        "    n_samples_per_batch = n_samples // n_batches\n",
        "    start = 0\n",
        "    end = n_samples_per_batch\n",
        "    while end <= n_samples: \n",
        "        yield (images[start:end], labels[start:end])\n",
        "        start += n_samples_per_batch\n",
        "        end += n_samples_per_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "4b0c85d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_gan_training(optimizer, key, latent_dim):\n",
        "    N_PIXELS = 784\n",
        "    N_HIDDEN_LAYER = 128\n",
        "    N_BINARY_CATEGORIES = 1\n",
        "\n",
        "    # Generator maps from latent space (latent_dim) to image space (N_PIXELS)\n",
        "    layer_sizes_generator = [latent_dim, N_HIDDEN_LAYER, N_PIXELS]\n",
        "    layer_sizes_discriminator = [N_PIXELS, N_HIDDEN_LAYER, N_BINARY_CATEGORIES]\n",
        "\n",
        "    g_key, d_key = jax.random.split(key)\n",
        "    generator_training_state = create_training_state(layer_sizes_generator, optimizer, g_key)\n",
        "    discriminator_training_state = create_training_state(layer_sizes_discriminator, optimizer, d_key)\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "id": "6007df0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_experiment_name(optimizer):\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"gan_{opt_name}\"\n",
        "\n",
        "\n",
        "def make_constraints_experiment_name(optimizer, loss_type: str) -> str:\n",
        "    \"\"\"Experiment name for this notebook's mixture-of-Gaussians GAN runs.\"\"\"\n",
        "    opt_name = optimizer.__class__.__name__\n",
        "    return f\"gan_constraints_{opt_name}_{loss_type}\"\n",
        "\n",
        "\n",
        "def initialise_checkpoint_manager(experiment_name: str = \"gan\", max_to_keep=20):\n",
        "    project_root = Path().resolve()\n",
        "    base_dir = project_root / \"checkpoints\"\n",
        "    checkpoint_dir = base_dir / experiment_name\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    checkpoint_manager = ocp.CheckpointManager(\n",
        "        directory=str(checkpoint_dir),\n",
        "        options=ocp.CheckpointManagerOptions(max_to_keep=max_to_keep),\n",
        "    )\n",
        "    return checkpoint_manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "id": "42aaa206",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_gan(\n",
        "    train_data, \n",
        "    optimizer, \n",
        "    n_steps=10**3, \n",
        "    steps_per_save=100, \n",
        "    key=jax.random.key(0),\n",
        "    batch_size: int = 128,\n",
        "    latent_dim: int = 64,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "    ):\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    checkpoint_manager = initialise_checkpoint_manager(experiment_name)\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=key,\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "\n",
        "    generator_training_state, discriminator_training_state, key = run_training_gan(\n",
        "        train_data=train_data,\n",
        "        n_steps=n_steps,\n",
        "        generator_training_state=generator_training_state,\n",
        "        discriminator_training_state=discriminator_training_state,\n",
        "        key=key,\n",
        "        steps_per_save=steps_per_save,\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        batch_size=batch_size,\n",
        "        latent_dim=latent_dim,\n",
        "        loss_type=loss_type,\n",
        "    )\n",
        "    return generator_training_state, discriminator_training_state, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "id": "4575e629",
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data = prepare_data(mnist_data[\"train\"], subsample_size=10**4) \n",
        "# test_data = prepare_data(mnist_data[\"test\"], subsample_size=10**3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "d1ad17b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jr.key(0)\n",
        "n_training_samples = 10_000\n",
        "n_pixels = 784\n",
        "n_samples = 10_000\n",
        "\n",
        "mu1 = 200\n",
        "sigma1 = 50\n",
        "mu2 = 350\n",
        "sigma2 = 50 \n",
        "alpha = 0.5\n",
        "train_data = r_gaussian_mixed(key, mu1, sigma1, mu2, sigma2, alpha, n_pixels, n_samples, n_training_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "aa6a1022",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1: generator_loss=0.2309345155954361, discriminator_loss=1.3712272644042969\n",
            "step 2: generator_loss=0.16097933053970337, discriminator_loss=1.5459084510803223\n",
            "step 3: generator_loss=0.13246533274650574, discriminator_loss=1.6685304641723633\n",
            "step 4: generator_loss=0.12622371315956116, discriminator_loss=1.7081263065338135\n",
            "step 5: generator_loss=0.08316975831985474, discriminator_loss=1.870084285736084\n",
            "step 6: generator_loss=0.06533394753932953, discriminator_loss=2.092430591583252\n",
            "step 7: generator_loss=0.04134879261255264, discriminator_loss=2.2522802352905273\n",
            "step 8: generator_loss=0.03975551575422287, discriminator_loss=2.286003351211548\n",
            "step 9: generator_loss=0.055900685489177704, discriminator_loss=2.1255462169647217\n",
            "step 10: generator_loss=0.05546342581510544, discriminator_loss=2.076913356781006\n",
            "step 11: generator_loss=0.05941787362098694, discriminator_loss=2.036372661590576\n",
            "step 12: generator_loss=0.07214837521314621, discriminator_loss=1.9659740924835205\n",
            "step 13: generator_loss=0.0718962550163269, discriminator_loss=1.9601988792419434\n",
            "step 14: generator_loss=0.10487005114555359, discriminator_loss=1.780239462852478\n",
            "step 15: generator_loss=0.1012478917837143, discriminator_loss=1.8027660846710205\n",
            "step 16: generator_loss=0.11640757322311401, discriminator_loss=1.7067281007766724\n",
            "step 17: generator_loss=0.14146466553211212, discriminator_loss=1.6095446348190308\n",
            "step 18: generator_loss=0.1445244550704956, discriminator_loss=1.6315562725067139\n",
            "step 19: generator_loss=0.15461723506450653, discriminator_loss=1.5626533031463623\n",
            "step 20: generator_loss=0.19202393293380737, discriminator_loss=1.4888732433319092\n",
            "step 21: generator_loss=0.20649844408035278, discriminator_loss=1.498916745185852\n",
            "step 22: generator_loss=0.20760703086853027, discriminator_loss=1.4543678760528564\n",
            "step 23: generator_loss=0.20590102672576904, discriminator_loss=1.5055451393127441\n",
            "step 24: generator_loss=0.2041027843952179, discriminator_loss=1.4682477712631226\n",
            "step 25: generator_loss=0.20505645871162415, discriminator_loss=1.4823952913284302\n",
            "step 26: generator_loss=0.23659509420394897, discriminator_loss=1.4568475484848022\n",
            "step 27: generator_loss=0.20285436511039734, discriminator_loss=1.588182806968689\n",
            "step 28: generator_loss=0.18462564051151276, discriminator_loss=1.504342794418335\n",
            "step 29: generator_loss=0.17792169749736786, discriminator_loss=1.5211422443389893\n",
            "step 30: generator_loss=0.17945867776870728, discriminator_loss=1.5757484436035156\n",
            "step 31: generator_loss=0.18257278203964233, discriminator_loss=1.5328209400177002\n",
            "step 32: generator_loss=0.2319028675556183, discriminator_loss=1.5049711465835571\n",
            "step 33: generator_loss=0.1884908378124237, discriminator_loss=1.5531892776489258\n",
            "step 34: generator_loss=0.19135794043540955, discriminator_loss=1.5605461597442627\n",
            "step 35: generator_loss=0.22711387276649475, discriminator_loss=1.4813120365142822\n",
            "step 36: generator_loss=0.22655421495437622, discriminator_loss=1.4727766513824463\n",
            "step 37: generator_loss=0.20983010530471802, discriminator_loss=1.5387036800384521\n",
            "step 38: generator_loss=0.26225829124450684, discriminator_loss=1.3938679695129395\n",
            "step 39: generator_loss=0.3036637604236603, discriminator_loss=1.3004571199417114\n",
            "step 40: generator_loss=0.3052382171154022, discriminator_loss=1.2615675926208496\n",
            "step 41: generator_loss=0.3226510286331177, discriminator_loss=1.2381149530410767\n",
            "step 42: generator_loss=0.3471081852912903, discriminator_loss=1.1860231161117554\n",
            "step 43: generator_loss=0.47851240634918213, discriminator_loss=1.0512070655822754\n",
            "step 44: generator_loss=0.5173665881156921, discriminator_loss=0.996528685092926\n",
            "step 45: generator_loss=0.6205812692642212, discriminator_loss=0.9150745868682861\n",
            "step 46: generator_loss=0.6743077039718628, discriminator_loss=0.8483102917671204\n",
            "step 47: generator_loss=0.831407904624939, discriminator_loss=0.7991119623184204\n",
            "step 48: generator_loss=0.9367043972015381, discriminator_loss=0.7105979919433594\n",
            "step 49: generator_loss=1.253965139389038, discriminator_loss=0.6229598522186279\n",
            "step 50: generator_loss=1.219306230545044, discriminator_loss=0.604932427406311\n",
            "step 51: generator_loss=1.424879789352417, discriminator_loss=0.5592082738876343\n",
            "step 52: generator_loss=1.6218769550323486, discriminator_loss=0.5382763147354126\n",
            "step 53: generator_loss=1.847104787826538, discriminator_loss=0.48466023802757263\n",
            "step 54: generator_loss=1.9055399894714355, discriminator_loss=0.4800579249858856\n",
            "step 55: generator_loss=2.042538642883301, discriminator_loss=0.4545765519142151\n",
            "step 56: generator_loss=2.243995189666748, discriminator_loss=0.4419064521789551\n",
            "step 57: generator_loss=2.2338905334472656, discriminator_loss=0.43462419509887695\n",
            "step 58: generator_loss=2.414841651916504, discriminator_loss=0.4249802827835083\n",
            "step 59: generator_loss=2.3610329627990723, discriminator_loss=0.42452186346054077\n",
            "step 60: generator_loss=2.4648280143737793, discriminator_loss=0.4127829372882843\n",
            "step 61: generator_loss=2.736334800720215, discriminator_loss=0.39595726132392883\n",
            "step 62: generator_loss=2.690748453140259, discriminator_loss=0.3950918912887573\n",
            "step 63: generator_loss=2.6682844161987305, discriminator_loss=0.4004805088043213\n",
            "step 64: generator_loss=2.8059284687042236, discriminator_loss=0.3856084942817688\n",
            "step 65: generator_loss=2.827486753463745, discriminator_loss=0.3871583938598633\n",
            "step 66: generator_loss=2.9834160804748535, discriminator_loss=0.37441977858543396\n",
            "step 67: generator_loss=2.94832706451416, discriminator_loss=0.3799426853656769\n",
            "step 68: generator_loss=3.029232978820801, discriminator_loss=0.3680075705051422\n",
            "step 69: generator_loss=3.034280300140381, discriminator_loss=0.37287992238998413\n",
            "step 70: generator_loss=3.0041470527648926, discriminator_loss=0.37414199113845825\n",
            "step 71: generator_loss=3.053638458251953, discriminator_loss=0.36796700954437256\n",
            "step 72: generator_loss=3.2249507904052734, discriminator_loss=0.35747379064559937\n",
            "step 73: generator_loss=3.1144237518310547, discriminator_loss=0.3549031615257263\n",
            "step 74: generator_loss=3.1058177947998047, discriminator_loss=0.35400837659835815\n",
            "step 75: generator_loss=3.230134963989258, discriminator_loss=0.350929856300354\n",
            "step 76: generator_loss=3.283878803253174, discriminator_loss=0.34442058205604553\n",
            "step 77: generator_loss=3.3657314777374268, discriminator_loss=0.3479776978492737\n",
            "step 78: generator_loss=3.330843687057495, discriminator_loss=0.3369726836681366\n",
            "step 79: generator_loss=3.4181301593780518, discriminator_loss=0.33351317048072815\n",
            "step 80: generator_loss=3.783625602722168, discriminator_loss=0.32835081219673157\n",
            "step 81: generator_loss=3.603428840637207, discriminator_loss=0.33249008655548096\n",
            "step 82: generator_loss=3.5318660736083984, discriminator_loss=0.3256220519542694\n",
            "step 83: generator_loss=3.7132577896118164, discriminator_loss=0.3258460462093353\n",
            "step 84: generator_loss=3.728868007659912, discriminator_loss=0.3191879987716675\n",
            "step 85: generator_loss=3.9535202980041504, discriminator_loss=0.32117849588394165\n",
            "step 86: generator_loss=3.7168657779693604, discriminator_loss=0.3180757761001587\n",
            "step 87: generator_loss=3.615114212036133, discriminator_loss=0.3186236023902893\n",
            "step 88: generator_loss=3.585284948348999, discriminator_loss=0.3144657015800476\n",
            "step 89: generator_loss=3.721792221069336, discriminator_loss=0.3143143355846405\n",
            "step 90: generator_loss=3.581195592880249, discriminator_loss=0.31907379627227783\n",
            "step 91: generator_loss=3.681837558746338, discriminator_loss=0.31182897090911865\n",
            "step 92: generator_loss=3.6909775733947754, discriminator_loss=0.31616902351379395\n",
            "step 93: generator_loss=3.869673252105713, discriminator_loss=0.3065882921218872\n",
            "step 94: generator_loss=3.9923954010009766, discriminator_loss=0.3079145848751068\n",
            "step 95: generator_loss=4.354138374328613, discriminator_loss=0.2958716154098511\n",
            "step 96: generator_loss=4.236023902893066, discriminator_loss=0.29167935252189636\n",
            "step 97: generator_loss=4.189749717712402, discriminator_loss=0.2928421199321747\n",
            "step 98: generator_loss=4.035361289978027, discriminator_loss=0.2959485650062561\n",
            "step 99: generator_loss=4.174637794494629, discriminator_loss=0.2902759313583374\n",
            "step 100: generator_loss=4.140946388244629, discriminator_loss=0.288642942905426\n",
            "step 101: generator_loss=4.293050765991211, discriminator_loss=0.2825643718242645\n",
            "step 102: generator_loss=4.055825233459473, discriminator_loss=0.28568175435066223\n",
            "step 103: generator_loss=4.2893548011779785, discriminator_loss=0.28163278102874756\n",
            "step 104: generator_loss=4.3509321212768555, discriminator_loss=0.2748258113861084\n",
            "step 105: generator_loss=4.586503028869629, discriminator_loss=0.27307960391044617\n",
            "step 106: generator_loss=4.73262357711792, discriminator_loss=0.27069708704948425\n",
            "step 107: generator_loss=4.82620096206665, discriminator_loss=0.26885923743247986\n",
            "step 108: generator_loss=4.748188495635986, discriminator_loss=0.2665117383003235\n",
            "step 109: generator_loss=4.875361442565918, discriminator_loss=0.2638959288597107\n",
            "step 110: generator_loss=4.615473747253418, discriminator_loss=0.26698970794677734\n",
            "step 111: generator_loss=4.734767436981201, discriminator_loss=0.2619308531284332\n",
            "step 112: generator_loss=4.4539031982421875, discriminator_loss=0.26135602593421936\n",
            "step 113: generator_loss=4.731020450592041, discriminator_loss=0.2600915729999542\n",
            "step 114: generator_loss=4.946103096008301, discriminator_loss=0.2569701373577118\n",
            "step 115: generator_loss=4.68813943862915, discriminator_loss=0.25696104764938354\n",
            "step 116: generator_loss=4.739267349243164, discriminator_loss=0.25499168038368225\n",
            "step 117: generator_loss=4.68137264251709, discriminator_loss=0.2544232904911041\n",
            "step 118: generator_loss=4.724268913269043, discriminator_loss=0.2542295455932617\n",
            "step 119: generator_loss=4.400676727294922, discriminator_loss=0.2523733973503113\n",
            "step 120: generator_loss=4.600804328918457, discriminator_loss=0.24960169196128845\n",
            "step 121: generator_loss=4.79948091506958, discriminator_loss=0.2472764253616333\n",
            "step 122: generator_loss=4.918092727661133, discriminator_loss=0.24533052742481232\n",
            "step 123: generator_loss=4.678013801574707, discriminator_loss=0.24610576033592224\n",
            "step 124: generator_loss=5.152431488037109, discriminator_loss=0.2422507256269455\n",
            "step 125: generator_loss=4.787982940673828, discriminator_loss=0.24357068538665771\n",
            "step 126: generator_loss=5.031857013702393, discriminator_loss=0.240473672747612\n",
            "step 127: generator_loss=4.976137161254883, discriminator_loss=0.239409938454628\n",
            "step 128: generator_loss=5.185166358947754, discriminator_loss=0.236810564994812\n",
            "step 129: generator_loss=4.907234191894531, discriminator_loss=0.23680081963539124\n",
            "step 130: generator_loss=4.82036828994751, discriminator_loss=0.23722398281097412\n",
            "step 131: generator_loss=5.008814811706543, discriminator_loss=0.23482535779476166\n",
            "step 132: generator_loss=4.8883867263793945, discriminator_loss=0.23519207537174225\n",
            "step 133: generator_loss=4.810650825500488, discriminator_loss=0.2346799224615097\n",
            "step 134: generator_loss=4.935277938842773, discriminator_loss=0.23133261501789093\n",
            "step 135: generator_loss=5.064827919006348, discriminator_loss=0.22954759001731873\n",
            "step 136: generator_loss=5.049178123474121, discriminator_loss=0.22838138043880463\n",
            "step 137: generator_loss=5.271172046661377, discriminator_loss=0.22537463903427124\n",
            "step 138: generator_loss=5.390637397766113, discriminator_loss=0.22412048280239105\n",
            "step 139: generator_loss=5.244537353515625, discriminator_loss=0.22394439578056335\n",
            "step 140: generator_loss=5.256522178649902, discriminator_loss=0.22191111743450165\n",
            "step 141: generator_loss=4.7129011154174805, discriminator_loss=0.22534236311912537\n",
            "step 142: generator_loss=4.963324546813965, discriminator_loss=0.22162483632564545\n",
            "step 143: generator_loss=4.919997692108154, discriminator_loss=0.22130829095840454\n",
            "step 144: generator_loss=4.892350196838379, discriminator_loss=0.2212280035018921\n",
            "step 145: generator_loss=4.763195037841797, discriminator_loss=0.2196495234966278\n",
            "step 146: generator_loss=4.795588493347168, discriminator_loss=0.21827752888202667\n",
            "step 147: generator_loss=4.942580699920654, discriminator_loss=0.2164343148469925\n",
            "step 148: generator_loss=4.873615741729736, discriminator_loss=0.21547363698482513\n",
            "step 149: generator_loss=4.799458980560303, discriminator_loss=0.21469171345233917\n",
            "step 150: generator_loss=4.782420635223389, discriminator_loss=0.21467263996601105\n",
            "step 151: generator_loss=4.968807220458984, discriminator_loss=0.2127259373664856\n",
            "step 152: generator_loss=4.919550895690918, discriminator_loss=0.21169401705265045\n",
            "step 153: generator_loss=5.111968517303467, discriminator_loss=0.210125133395195\n",
            "step 154: generator_loss=5.018946647644043, discriminator_loss=0.2087279111146927\n",
            "step 155: generator_loss=5.26398229598999, discriminator_loss=0.20595844089984894\n",
            "step 156: generator_loss=5.2621965408325195, discriminator_loss=0.2052268385887146\n",
            "step 157: generator_loss=5.0743255615234375, discriminator_loss=0.20525439083576202\n",
            "step 158: generator_loss=4.950392723083496, discriminator_loss=0.20475506782531738\n",
            "step 159: generator_loss=4.76242733001709, discriminator_loss=0.20539388060569763\n",
            "step 160: generator_loss=5.154055118560791, discriminator_loss=0.20175090432167053\n",
            "step 161: generator_loss=5.153646945953369, discriminator_loss=0.20085206627845764\n",
            "step 162: generator_loss=4.8072733879089355, discriminator_loss=0.20171068608760834\n",
            "step 163: generator_loss=4.946316242218018, discriminator_loss=0.20029424130916595\n",
            "step 164: generator_loss=4.828713417053223, discriminator_loss=0.19986800849437714\n",
            "step 165: generator_loss=4.936391830444336, discriminator_loss=0.19761468470096588\n",
            "step 166: generator_loss=5.128504753112793, discriminator_loss=0.19617803394794464\n",
            "step 167: generator_loss=4.8154706954956055, discriminator_loss=0.1965457797050476\n",
            "step 168: generator_loss=5.058721542358398, discriminator_loss=0.19408132135868073\n",
            "step 169: generator_loss=5.035681247711182, discriminator_loss=0.19269360601902008\n",
            "step 170: generator_loss=4.853116989135742, discriminator_loss=0.19323429465293884\n",
            "step 171: generator_loss=5.00878381729126, discriminator_loss=0.19127075374126434\n",
            "step 172: generator_loss=4.956234931945801, discriminator_loss=0.19094307720661163\n",
            "step 173: generator_loss=5.161986351013184, discriminator_loss=0.18859204649925232\n",
            "step 174: generator_loss=4.985730171203613, discriminator_loss=0.18809472024440765\n",
            "step 175: generator_loss=5.0897932052612305, discriminator_loss=0.18732070922851562\n",
            "step 176: generator_loss=5.206299304962158, discriminator_loss=0.1848607063293457\n",
            "step 177: generator_loss=5.027519702911377, discriminator_loss=0.18532903492450714\n",
            "step 178: generator_loss=4.874886989593506, discriminator_loss=0.18397384881973267\n",
            "step 179: generator_loss=4.838913440704346, discriminator_loss=0.18329215049743652\n",
            "step 180: generator_loss=4.725101470947266, discriminator_loss=0.18297941982746124\n",
            "step 181: generator_loss=4.627065181732178, discriminator_loss=0.18360763788223267\n",
            "step 182: generator_loss=4.998238563537598, discriminator_loss=0.18014511466026306\n",
            "step 183: generator_loss=4.8184967041015625, discriminator_loss=0.1791149526834488\n",
            "step 184: generator_loss=4.627399444580078, discriminator_loss=0.18041904270648956\n",
            "step 185: generator_loss=4.623793601989746, discriminator_loss=0.17936979234218597\n",
            "step 186: generator_loss=4.549319267272949, discriminator_loss=0.178457111120224\n",
            "step 187: generator_loss=4.716729164123535, discriminator_loss=0.17629265785217285\n",
            "step 188: generator_loss=4.709902286529541, discriminator_loss=0.17557917535305023\n",
            "step 189: generator_loss=4.631369590759277, discriminator_loss=0.17497341334819794\n",
            "step 190: generator_loss=4.537951946258545, discriminator_loss=0.17460514605045319\n",
            "step 191: generator_loss=4.537755012512207, discriminator_loss=0.17393559217453003\n",
            "step 192: generator_loss=4.479595184326172, discriminator_loss=0.17292018234729767\n",
            "step 193: generator_loss=4.585555076599121, discriminator_loss=0.1718868613243103\n",
            "step 194: generator_loss=4.526317596435547, discriminator_loss=0.1705804467201233\n",
            "step 195: generator_loss=4.554306983947754, discriminator_loss=0.16970491409301758\n",
            "step 196: generator_loss=4.412557601928711, discriminator_loss=0.1701286882162094\n",
            "step 197: generator_loss=4.455318450927734, discriminator_loss=0.16896866261959076\n",
            "step 198: generator_loss=4.4083051681518555, discriminator_loss=0.16828420758247375\n",
            "step 199: generator_loss=4.406913757324219, discriminator_loss=0.16769839823246002\n",
            "step 200: generator_loss=4.1278533935546875, discriminator_loss=0.16931219398975372\n",
            "step 201: generator_loss=4.382284164428711, discriminator_loss=0.16720730066299438\n",
            "step 202: generator_loss=4.458574295043945, discriminator_loss=0.16541306674480438\n",
            "step 203: generator_loss=4.4007978439331055, discriminator_loss=0.16375210881233215\n",
            "step 204: generator_loss=4.337236404418945, discriminator_loss=0.1662680208683014\n",
            "step 205: generator_loss=4.311260223388672, discriminator_loss=0.16401566565036774\n",
            "step 206: generator_loss=4.2022600173950195, discriminator_loss=0.16367074847221375\n",
            "step 207: generator_loss=4.38187313079834, discriminator_loss=0.16229330003261566\n",
            "step 208: generator_loss=4.120431423187256, discriminator_loss=0.162568137049675\n",
            "step 209: generator_loss=4.169382095336914, discriminator_loss=0.1617221236228943\n",
            "step 210: generator_loss=4.168129920959473, discriminator_loss=0.16115297377109528\n",
            "step 211: generator_loss=4.175187587738037, discriminator_loss=0.16108505427837372\n",
            "step 212: generator_loss=4.0633344650268555, discriminator_loss=0.16187222301959991\n",
            "step 213: generator_loss=4.0662994384765625, discriminator_loss=0.16006389260292053\n",
            "step 214: generator_loss=4.2066450119018555, discriminator_loss=0.15801593661308289\n",
            "step 215: generator_loss=4.0543107986450195, discriminator_loss=0.15916410088539124\n",
            "step 216: generator_loss=3.944624900817871, discriminator_loss=0.1610347032546997\n",
            "step 217: generator_loss=4.0083699226379395, discriminator_loss=0.15883192420005798\n",
            "step 218: generator_loss=4.201277256011963, discriminator_loss=0.15494515001773834\n",
            "step 219: generator_loss=3.983837366104126, discriminator_loss=0.15774226188659668\n",
            "step 220: generator_loss=4.251527786254883, discriminator_loss=0.1531568318605423\n",
            "step 221: generator_loss=4.15568208694458, discriminator_loss=0.15209834277629852\n",
            "step 222: generator_loss=3.916250705718994, discriminator_loss=0.155140221118927\n",
            "step 223: generator_loss=4.054537773132324, discriminator_loss=0.15268826484680176\n",
            "step 224: generator_loss=4.103403091430664, discriminator_loss=0.1517537385225296\n",
            "step 225: generator_loss=3.975773572921753, discriminator_loss=0.1530878096818924\n",
            "step 226: generator_loss=3.8732833862304688, discriminator_loss=0.15305478870868683\n",
            "step 227: generator_loss=3.967444896697998, discriminator_loss=0.15136122703552246\n",
            "step 228: generator_loss=3.922041177749634, discriminator_loss=0.1509176343679428\n",
            "step 229: generator_loss=4.087681293487549, discriminator_loss=0.14786793291568756\n",
            "step 230: generator_loss=3.8494458198547363, discriminator_loss=0.1518639177083969\n",
            "step 231: generator_loss=3.8443236351013184, discriminator_loss=0.1496724635362625\n",
            "step 232: generator_loss=3.9278175830841064, discriminator_loss=0.14553852379322052\n",
            "step 233: generator_loss=3.889721155166626, discriminator_loss=0.14715805649757385\n",
            "step 234: generator_loss=3.764054775238037, discriminator_loss=0.14766594767570496\n",
            "step 235: generator_loss=3.4950668811798096, discriminator_loss=0.15191656351089478\n",
            "step 236: generator_loss=3.682450771331787, discriminator_loss=0.14614351093769073\n",
            "step 237: generator_loss=3.614236354827881, discriminator_loss=0.14762310683727264\n",
            "step 238: generator_loss=3.5963680744171143, discriminator_loss=0.14597739279270172\n",
            "step 239: generator_loss=3.7481117248535156, discriminator_loss=0.1453273445367813\n",
            "step 240: generator_loss=3.6546409130096436, discriminator_loss=0.14558862149715424\n",
            "step 241: generator_loss=3.6733169555664062, discriminator_loss=0.14219897985458374\n",
            "step 242: generator_loss=3.557206392288208, discriminator_loss=0.14401854574680328\n",
            "step 243: generator_loss=3.5904018878936768, discriminator_loss=0.1428721845149994\n",
            "step 244: generator_loss=3.5640316009521484, discriminator_loss=0.14293748140335083\n",
            "step 245: generator_loss=3.5790772438049316, discriminator_loss=0.14080990850925446\n",
            "step 246: generator_loss=3.608313798904419, discriminator_loss=0.13967591524124146\n",
            "step 247: generator_loss=3.6055908203125, discriminator_loss=0.14000503718852997\n",
            "step 248: generator_loss=3.4804604053497314, discriminator_loss=0.14259299635887146\n",
            "step 249: generator_loss=3.630049705505371, discriminator_loss=0.13660836219787598\n",
            "step 250: generator_loss=3.598987579345703, discriminator_loss=0.137538343667984\n",
            "step 251: generator_loss=3.390719175338745, discriminator_loss=0.13851499557495117\n",
            "step 252: generator_loss=3.5230414867401123, discriminator_loss=0.13523074984550476\n",
            "step 253: generator_loss=3.3178656101226807, discriminator_loss=0.13794419169425964\n",
            "step 254: generator_loss=3.418257713317871, discriminator_loss=0.13530248403549194\n",
            "step 255: generator_loss=3.2488484382629395, discriminator_loss=0.13935908675193787\n",
            "step 256: generator_loss=3.2848520278930664, discriminator_loss=0.13731105625629425\n",
            "step 257: generator_loss=3.263176679611206, discriminator_loss=0.1383012980222702\n",
            "step 258: generator_loss=3.3034262657165527, discriminator_loss=0.134160578250885\n",
            "step 259: generator_loss=3.2294609546661377, discriminator_loss=0.13398867845535278\n",
            "step 260: generator_loss=3.2314529418945312, discriminator_loss=0.13340523838996887\n",
            "step 261: generator_loss=3.128230094909668, discriminator_loss=0.1358126401901245\n",
            "step 262: generator_loss=3.1850666999816895, discriminator_loss=0.13237911462783813\n",
            "step 263: generator_loss=3.0350685119628906, discriminator_loss=0.1352936178445816\n",
            "step 264: generator_loss=3.218752861022949, discriminator_loss=0.1324269026517868\n",
            "step 265: generator_loss=3.0693860054016113, discriminator_loss=0.13369616866111755\n",
            "step 266: generator_loss=3.1157619953155518, discriminator_loss=0.13181471824645996\n",
            "step 267: generator_loss=3.1083436012268066, discriminator_loss=0.13096588850021362\n",
            "step 268: generator_loss=3.0850753784179688, discriminator_loss=0.13098397850990295\n",
            "step 269: generator_loss=3.0197596549987793, discriminator_loss=0.13113652169704437\n",
            "step 270: generator_loss=3.0727498531341553, discriminator_loss=0.130662739276886\n",
            "step 271: generator_loss=2.917477607727051, discriminator_loss=0.1333209127187729\n",
            "step 272: generator_loss=2.9507033824920654, discriminator_loss=0.13178950548171997\n",
            "step 273: generator_loss=2.9265432357788086, discriminator_loss=0.13171574473381042\n",
            "step 274: generator_loss=2.914213180541992, discriminator_loss=0.1306230127811432\n",
            "step 275: generator_loss=2.906101703643799, discriminator_loss=0.13031618297100067\n",
            "step 276: generator_loss=2.9575724601745605, discriminator_loss=0.1284390389919281\n",
            "step 277: generator_loss=2.88653302192688, discriminator_loss=0.1292733997106552\n",
            "step 278: generator_loss=2.9772889614105225, discriminator_loss=0.12668214738368988\n",
            "step 279: generator_loss=2.8349595069885254, discriminator_loss=0.12913675606250763\n",
            "step 280: generator_loss=2.8105483055114746, discriminator_loss=0.1292022466659546\n",
            "step 281: generator_loss=2.871229887008667, discriminator_loss=0.12719129025936127\n",
            "step 282: generator_loss=2.8596203327178955, discriminator_loss=0.12613803148269653\n",
            "step 283: generator_loss=2.7843739986419678, discriminator_loss=0.12810790538787842\n",
            "step 284: generator_loss=2.7093820571899414, discriminator_loss=0.12939125299453735\n",
            "step 285: generator_loss=2.811694860458374, discriminator_loss=0.12561282515525818\n",
            "step 286: generator_loss=2.748424530029297, discriminator_loss=0.12709254026412964\n",
            "step 287: generator_loss=2.769198417663574, discriminator_loss=0.12621253728866577\n",
            "step 288: generator_loss=2.7074265480041504, discriminator_loss=0.1277191936969757\n",
            "step 289: generator_loss=2.633354663848877, discriminator_loss=0.12953004240989685\n",
            "step 290: generator_loss=2.739527702331543, discriminator_loss=0.125334233045578\n",
            "step 291: generator_loss=2.7323222160339355, discriminator_loss=0.1243172362446785\n",
            "step 292: generator_loss=2.6503512859344482, discriminator_loss=0.1270131915807724\n",
            "step 293: generator_loss=2.665073871612549, discriminator_loss=0.12585557997226715\n",
            "step 294: generator_loss=2.6737663745880127, discriminator_loss=0.12496403604745865\n",
            "step 295: generator_loss=2.7016239166259766, discriminator_loss=0.12298041582107544\n",
            "step 296: generator_loss=2.6046252250671387, discriminator_loss=0.12617439031600952\n",
            "step 297: generator_loss=2.6263556480407715, discriminator_loss=0.12528181076049805\n",
            "step 298: generator_loss=2.6283984184265137, discriminator_loss=0.12478525191545486\n",
            "step 299: generator_loss=2.59608793258667, discriminator_loss=0.12522348761558533\n",
            "step 300: generator_loss=2.63552188873291, discriminator_loss=0.1235535517334938\n",
            "step 301: generator_loss=2.6084656715393066, discriminator_loss=0.12378951162099838\n",
            "step 302: generator_loss=2.583252429962158, discriminator_loss=0.12533539533615112\n",
            "step 303: generator_loss=2.582735061645508, discriminator_loss=0.12459778040647507\n",
            "step 304: generator_loss=2.5995917320251465, discriminator_loss=0.1236569806933403\n",
            "step 305: generator_loss=2.589378595352173, discriminator_loss=0.12415020912885666\n",
            "step 306: generator_loss=2.6581203937530518, discriminator_loss=0.11941257119178772\n",
            "step 307: generator_loss=2.600593090057373, discriminator_loss=0.12229249626398087\n",
            "step 308: generator_loss=2.5590672492980957, discriminator_loss=0.12355750054121017\n",
            "step 309: generator_loss=2.5272092819213867, discriminator_loss=0.12555231153964996\n",
            "step 310: generator_loss=2.5142009258270264, discriminator_loss=0.1256483644247055\n",
            "step 311: generator_loss=2.4954166412353516, discriminator_loss=0.12670794129371643\n",
            "step 312: generator_loss=2.489625930786133, discriminator_loss=0.12615284323692322\n",
            "step 313: generator_loss=2.504727840423584, discriminator_loss=0.125315859913826\n",
            "step 314: generator_loss=2.4880900382995605, discriminator_loss=0.12552325427532196\n",
            "step 315: generator_loss=2.505924940109253, discriminator_loss=0.12539492547512054\n",
            "step 316: generator_loss=2.5223894119262695, discriminator_loss=0.12375257909297943\n",
            "step 317: generator_loss=2.540912628173828, discriminator_loss=0.12138655036687851\n",
            "step 318: generator_loss=2.4661736488342285, discriminator_loss=0.12609705328941345\n",
            "step 319: generator_loss=2.475396156311035, discriminator_loss=0.12549424171447754\n",
            "step 320: generator_loss=2.486880302429199, discriminator_loss=0.12521837651729584\n",
            "step 321: generator_loss=2.4609904289245605, discriminator_loss=0.12713924050331116\n",
            "step 322: generator_loss=2.397306203842163, discriminator_loss=0.13049748539924622\n",
            "step 323: generator_loss=2.493546962738037, discriminator_loss=0.12301459163427353\n",
            "step 324: generator_loss=2.437602996826172, discriminator_loss=0.12693655490875244\n",
            "step 325: generator_loss=2.3412704467773438, discriminator_loss=0.13056424260139465\n",
            "step 326: generator_loss=2.4598047733306885, discriminator_loss=0.12324153631925583\n",
            "step 327: generator_loss=2.3878865242004395, discriminator_loss=0.1282319724559784\n",
            "step 328: generator_loss=2.361931562423706, discriminator_loss=0.1300644874572754\n",
            "step 329: generator_loss=2.4465198516845703, discriminator_loss=0.12354579567909241\n",
            "step 330: generator_loss=2.3898911476135254, discriminator_loss=0.12467361986637115\n",
            "step 331: generator_loss=2.374885082244873, discriminator_loss=0.12821772694587708\n",
            "step 332: generator_loss=2.331714630126953, discriminator_loss=0.12757442891597748\n",
            "step 333: generator_loss=2.3636796474456787, discriminator_loss=0.12498761713504791\n",
            "step 334: generator_loss=2.4074435234069824, discriminator_loss=0.12229377031326294\n",
            "step 335: generator_loss=2.3447299003601074, discriminator_loss=0.12527881562709808\n",
            "step 336: generator_loss=2.3242924213409424, discriminator_loss=0.12746185064315796\n",
            "step 337: generator_loss=2.4046812057495117, discriminator_loss=0.12095747888088226\n",
            "step 338: generator_loss=2.3627090454101562, discriminator_loss=0.12288856506347656\n",
            "step 339: generator_loss=2.396409511566162, discriminator_loss=0.12058904767036438\n",
            "step 340: generator_loss=2.353203773498535, discriminator_loss=0.12262129038572311\n",
            "step 341: generator_loss=2.363995313644409, discriminator_loss=0.1229763850569725\n",
            "step 342: generator_loss=2.3354885578155518, discriminator_loss=0.12278449535369873\n",
            "step 343: generator_loss=2.342416286468506, discriminator_loss=0.12177621573209763\n",
            "step 344: generator_loss=2.3145947456359863, discriminator_loss=0.12382803112268448\n",
            "step 345: generator_loss=2.233649730682373, discriminator_loss=0.1278533637523651\n",
            "step 346: generator_loss=2.301687479019165, discriminator_loss=0.12355963885784149\n",
            "step 347: generator_loss=2.319349765777588, discriminator_loss=0.12182340025901794\n",
            "step 348: generator_loss=2.267000675201416, discriminator_loss=0.12653088569641113\n",
            "step 349: generator_loss=2.354301929473877, discriminator_loss=0.11800194531679153\n",
            "step 350: generator_loss=2.293727159500122, discriminator_loss=0.12416495382785797\n",
            "step 351: generator_loss=2.260892391204834, discriminator_loss=0.12572172284126282\n",
            "step 352: generator_loss=2.2641568183898926, discriminator_loss=0.12479296326637268\n",
            "step 353: generator_loss=2.316502809524536, discriminator_loss=0.12058477103710175\n",
            "step 354: generator_loss=2.2595138549804688, discriminator_loss=0.12306791543960571\n",
            "step 355: generator_loss=2.2840399742126465, discriminator_loss=0.12196581065654755\n",
            "step 356: generator_loss=2.2542808055877686, discriminator_loss=0.12355200946331024\n",
            "step 357: generator_loss=2.288058280944824, discriminator_loss=0.12087452411651611\n",
            "step 358: generator_loss=2.2478318214416504, discriminator_loss=0.12308535724878311\n",
            "step 359: generator_loss=2.290773868560791, discriminator_loss=0.11897195875644684\n",
            "step 360: generator_loss=2.256174087524414, discriminator_loss=0.1210494339466095\n",
            "step 361: generator_loss=2.2091970443725586, discriminator_loss=0.12399625033140182\n",
            "step 362: generator_loss=2.172985076904297, discriminator_loss=0.12587898969650269\n",
            "step 363: generator_loss=2.259519100189209, discriminator_loss=0.11849108338356018\n",
            "step 364: generator_loss=2.2196435928344727, discriminator_loss=0.1209779679775238\n",
            "step 365: generator_loss=2.2231056690216064, discriminator_loss=0.12058188021183014\n",
            "step 366: generator_loss=2.1756560802459717, discriminator_loss=0.12312132865190506\n",
            "step 367: generator_loss=2.2061610221862793, discriminator_loss=0.1201704740524292\n",
            "step 368: generator_loss=2.1292495727539062, discriminator_loss=0.12554317712783813\n",
            "step 369: generator_loss=2.1055335998535156, discriminator_loss=0.12679390609264374\n",
            "step 370: generator_loss=2.1285862922668457, discriminator_loss=0.12435297667980194\n",
            "step 371: generator_loss=2.146453857421875, discriminator_loss=0.12258529663085938\n",
            "step 372: generator_loss=2.106204032897949, discriminator_loss=0.1253838986158371\n",
            "step 373: generator_loss=2.079224109649658, discriminator_loss=0.12683117389678955\n",
            "step 374: generator_loss=2.034646511077881, discriminator_loss=0.1289764940738678\n",
            "step 375: generator_loss=2.0669026374816895, discriminator_loss=0.12634092569351196\n",
            "step 376: generator_loss=2.0559823513031006, discriminator_loss=0.12665337324142456\n",
            "step 377: generator_loss=2.0407748222351074, discriminator_loss=0.1272200047969818\n",
            "step 378: generator_loss=2.0161266326904297, discriminator_loss=0.129146009683609\n",
            "step 379: generator_loss=2.0070767402648926, discriminator_loss=0.1291297972202301\n",
            "step 380: generator_loss=2.001115322113037, discriminator_loss=0.12980249524116516\n",
            "step 381: generator_loss=2.010578155517578, discriminator_loss=0.12872862815856934\n",
            "step 382: generator_loss=1.9741239547729492, discriminator_loss=0.13126035034656525\n",
            "step 383: generator_loss=1.99534273147583, discriminator_loss=0.12945261597633362\n",
            "step 384: generator_loss=1.9915709495544434, discriminator_loss=0.12954393029212952\n",
            "step 385: generator_loss=1.977455973625183, discriminator_loss=0.1305966079235077\n",
            "step 386: generator_loss=1.9166924953460693, discriminator_loss=0.13531461358070374\n",
            "step 387: generator_loss=1.9187363386154175, discriminator_loss=0.13586601614952087\n",
            "step 388: generator_loss=1.9683032035827637, discriminator_loss=0.13225069642066956\n",
            "step 389: generator_loss=1.9032292366027832, discriminator_loss=0.13708345592021942\n",
            "step 390: generator_loss=1.9325220584869385, discriminator_loss=0.1357513964176178\n",
            "step 391: generator_loss=1.9087319374084473, discriminator_loss=0.13684076070785522\n",
            "step 392: generator_loss=1.9013454914093018, discriminator_loss=0.14162075519561768\n",
            "step 393: generator_loss=1.8435676097869873, discriminator_loss=0.14576244354248047\n",
            "step 394: generator_loss=1.8551928997039795, discriminator_loss=0.14511053264141083\n",
            "step 395: generator_loss=1.8599238395690918, discriminator_loss=0.14395928382873535\n",
            "step 396: generator_loss=1.848850131034851, discriminator_loss=0.15112075209617615\n",
            "step 397: generator_loss=1.8468879461288452, discriminator_loss=0.1506934016942978\n",
            "step 398: generator_loss=1.927785038948059, discriminator_loss=0.14168089628219604\n",
            "step 399: generator_loss=1.7724794149398804, discriminator_loss=0.15885627269744873\n",
            "step 400: generator_loss=1.9261234998703003, discriminator_loss=0.14124375581741333\n",
            "step 401: generator_loss=1.9517626762390137, discriminator_loss=0.14392998814582825\n",
            "step 402: generator_loss=1.913423776626587, discriminator_loss=0.14492353796958923\n",
            "step 403: generator_loss=1.8726003170013428, discriminator_loss=0.15300318598747253\n",
            "step 404: generator_loss=1.8559751510620117, discriminator_loss=0.1546896994113922\n",
            "step 405: generator_loss=1.961279034614563, discriminator_loss=0.1461842805147171\n",
            "step 406: generator_loss=2.028210401535034, discriminator_loss=0.1422111690044403\n",
            "step 407: generator_loss=1.9381343126296997, discriminator_loss=0.1465064287185669\n",
            "step 408: generator_loss=1.9399197101593018, discriminator_loss=0.1496589183807373\n",
            "step 409: generator_loss=2.001866579055786, discriminator_loss=0.14214172959327698\n",
            "step 410: generator_loss=1.9459989070892334, discriminator_loss=0.14851973950862885\n",
            "step 411: generator_loss=1.9904630184173584, discriminator_loss=0.1430528312921524\n",
            "step 412: generator_loss=2.0087857246398926, discriminator_loss=0.13856878876686096\n",
            "step 413: generator_loss=2.1370372772216797, discriminator_loss=0.12910649180412292\n",
            "step 414: generator_loss=2.0806057453155518, discriminator_loss=0.13257184624671936\n",
            "step 415: generator_loss=2.112328052520752, discriminator_loss=0.13116788864135742\n",
            "step 416: generator_loss=2.084505558013916, discriminator_loss=0.1301271915435791\n",
            "step 417: generator_loss=2.191347599029541, discriminator_loss=0.12012971937656403\n",
            "step 418: generator_loss=2.113034725189209, discriminator_loss=0.12900292873382568\n",
            "step 419: generator_loss=2.1158924102783203, discriminator_loss=0.12576812505722046\n",
            "step 420: generator_loss=2.1874735355377197, discriminator_loss=0.12081678956747055\n",
            "step 421: generator_loss=2.173048973083496, discriminator_loss=0.12038886547088623\n",
            "step 422: generator_loss=2.1526174545288086, discriminator_loss=0.1226358711719513\n",
            "step 423: generator_loss=2.2489047050476074, discriminator_loss=0.11423808336257935\n",
            "step 424: generator_loss=2.1750543117523193, discriminator_loss=0.12194390594959259\n",
            "step 425: generator_loss=2.104954957962036, discriminator_loss=0.1223636120557785\n",
            "step 426: generator_loss=2.142263412475586, discriminator_loss=0.12135349959135056\n",
            "step 427: generator_loss=2.098771572113037, discriminator_loss=0.12265181541442871\n",
            "step 428: generator_loss=2.1246089935302734, discriminator_loss=0.12066366523504257\n",
            "step 429: generator_loss=2.1946253776550293, discriminator_loss=0.11908785998821259\n",
            "step 430: generator_loss=2.1545634269714355, discriminator_loss=0.12020359933376312\n",
            "step 431: generator_loss=2.1048779487609863, discriminator_loss=0.12380039691925049\n",
            "step 432: generator_loss=2.1292145252227783, discriminator_loss=0.12180408835411072\n",
            "step 433: generator_loss=1.9836924076080322, discriminator_loss=0.12830151617527008\n",
            "step 434: generator_loss=2.015106201171875, discriminator_loss=0.12811723351478577\n",
            "step 435: generator_loss=2.0152692794799805, discriminator_loss=0.12803992629051208\n",
            "step 436: generator_loss=2.0702552795410156, discriminator_loss=0.12517784535884857\n",
            "step 437: generator_loss=2.0203514099121094, discriminator_loss=0.12775838375091553\n",
            "step 438: generator_loss=1.9850423336029053, discriminator_loss=0.13130684196949005\n",
            "step 439: generator_loss=1.971222162246704, discriminator_loss=0.13194109499454498\n",
            "step 440: generator_loss=2.0209226608276367, discriminator_loss=0.12884141504764557\n",
            "step 441: generator_loss=1.9742662906646729, discriminator_loss=0.12832669913768768\n",
            "step 442: generator_loss=1.803017020225525, discriminator_loss=0.14341579377651215\n",
            "step 443: generator_loss=1.8704835176467896, discriminator_loss=0.1377382129430771\n",
            "step 444: generator_loss=1.7924096584320068, discriminator_loss=0.14442360401153564\n",
            "step 445: generator_loss=1.80918550491333, discriminator_loss=0.14135625958442688\n",
            "step 446: generator_loss=1.731179118156433, discriminator_loss=0.1495494544506073\n",
            "step 447: generator_loss=1.845862627029419, discriminator_loss=0.13987284898757935\n",
            "step 448: generator_loss=1.8630146980285645, discriminator_loss=0.13602617383003235\n",
            "step 449: generator_loss=1.8091686964035034, discriminator_loss=0.14286872744560242\n",
            "step 450: generator_loss=1.8056902885437012, discriminator_loss=0.14150701463222504\n",
            "step 451: generator_loss=1.7669048309326172, discriminator_loss=0.14530807733535767\n",
            "step 452: generator_loss=1.7412474155426025, discriminator_loss=0.14745116233825684\n",
            "step 453: generator_loss=1.686995506286621, discriminator_loss=0.15372590720653534\n",
            "step 454: generator_loss=1.6792232990264893, discriminator_loss=0.15518522262573242\n",
            "step 455: generator_loss=1.6793208122253418, discriminator_loss=0.15336298942565918\n",
            "step 456: generator_loss=1.655683994293213, discriminator_loss=0.1572127342224121\n",
            "step 457: generator_loss=1.6780198812484741, discriminator_loss=0.1544049233198166\n",
            "step 458: generator_loss=1.6259715557098389, discriminator_loss=0.16061295568943024\n",
            "step 459: generator_loss=1.589457631111145, discriminator_loss=0.16444645822048187\n",
            "step 460: generator_loss=1.5594176054000854, discriminator_loss=0.16867050528526306\n",
            "step 461: generator_loss=1.5594240427017212, discriminator_loss=0.1687052845954895\n",
            "step 462: generator_loss=1.5228486061096191, discriminator_loss=0.17358101904392242\n",
            "step 463: generator_loss=1.4981350898742676, discriminator_loss=0.17751353979110718\n",
            "step 464: generator_loss=1.540653944015503, discriminator_loss=0.1726292371749878\n",
            "step 465: generator_loss=1.5057194232940674, discriminator_loss=0.17811763286590576\n",
            "step 466: generator_loss=1.4929680824279785, discriminator_loss=0.17907479405403137\n",
            "step 467: generator_loss=1.474006175994873, discriminator_loss=0.18287788331508636\n",
            "step 468: generator_loss=1.4043793678283691, discriminator_loss=0.19441553950309753\n",
            "step 469: generator_loss=1.4243332147598267, discriminator_loss=0.19183726608753204\n",
            "step 470: generator_loss=1.4283742904663086, discriminator_loss=0.19390356540679932\n",
            "step 471: generator_loss=1.42642343044281, discriminator_loss=0.19634412229061127\n",
            "step 472: generator_loss=1.377361536026001, discriminator_loss=0.20304660499095917\n",
            "step 473: generator_loss=1.32159423828125, discriminator_loss=0.2146657556295395\n",
            "step 474: generator_loss=1.2730016708374023, discriminator_loss=0.22665347158908844\n",
            "step 475: generator_loss=1.2534531354904175, discriminator_loss=0.23449698090553284\n",
            "step 476: generator_loss=1.181480050086975, discriminator_loss=0.24880364537239075\n",
            "step 477: generator_loss=1.1678298711776733, discriminator_loss=0.2585843503475189\n",
            "step 478: generator_loss=1.102134108543396, discriminator_loss=0.27636006474494934\n",
            "step 479: generator_loss=0.9794782400131226, discriminator_loss=0.3228311240673065\n",
            "step 480: generator_loss=0.9680156707763672, discriminator_loss=0.32214367389678955\n",
            "step 481: generator_loss=0.8754009008407593, discriminator_loss=0.3699507415294647\n",
            "step 482: generator_loss=0.757529616355896, discriminator_loss=0.42263171076774597\n",
            "step 483: generator_loss=0.6984531283378601, discriminator_loss=0.45077794790267944\n",
            "step 484: generator_loss=0.595032811164856, discriminator_loss=0.5227760076522827\n",
            "step 485: generator_loss=0.525945782661438, discriminator_loss=0.5841646790504456\n",
            "step 486: generator_loss=0.49213486909866333, discriminator_loss=0.6151954531669617\n",
            "step 487: generator_loss=0.4051577150821686, discriminator_loss=0.7091968059539795\n",
            "step 488: generator_loss=0.4155459403991699, discriminator_loss=0.711823046207428\n",
            "step 489: generator_loss=0.3880477547645569, discriminator_loss=0.7309683561325073\n",
            "step 490: generator_loss=0.3730129897594452, discriminator_loss=0.7745649218559265\n",
            "step 491: generator_loss=0.3787892758846283, discriminator_loss=0.780057966709137\n",
            "step 492: generator_loss=0.39510029554367065, discriminator_loss=0.7536184787750244\n",
            "step 493: generator_loss=0.4023730158805847, discriminator_loss=0.7435898780822754\n",
            "step 494: generator_loss=0.4557596445083618, discriminator_loss=0.7152833938598633\n",
            "step 495: generator_loss=0.5201162695884705, discriminator_loss=0.669002115726471\n",
            "step 496: generator_loss=0.5126742124557495, discriminator_loss=0.6611548662185669\n",
            "step 497: generator_loss=0.5826293230056763, discriminator_loss=0.6204389333724976\n",
            "step 498: generator_loss=0.7229427099227905, discriminator_loss=0.5374075770378113\n",
            "step 499: generator_loss=0.7462050914764404, discriminator_loss=0.536557137966156\n",
            "step 500: generator_loss=0.7579736709594727, discriminator_loss=0.5367876887321472\n",
            "step 501: generator_loss=0.8442507386207581, discriminator_loss=0.4872460961341858\n",
            "step 502: generator_loss=0.8398370146751404, discriminator_loss=0.48755502700805664\n",
            "step 503: generator_loss=0.903383731842041, discriminator_loss=0.4741421937942505\n",
            "step 504: generator_loss=0.9349696636199951, discriminator_loss=0.47209322452545166\n",
            "step 505: generator_loss=0.9045015573501587, discriminator_loss=0.49329033493995667\n",
            "step 506: generator_loss=0.9676848649978638, discriminator_loss=0.48218053579330444\n",
            "step 507: generator_loss=0.9778931736946106, discriminator_loss=0.4802054464817047\n",
            "step 508: generator_loss=1.0411957502365112, discriminator_loss=0.4729885160923004\n",
            "step 509: generator_loss=1.0095700025558472, discriminator_loss=0.5035266876220703\n",
            "step 510: generator_loss=1.0097951889038086, discriminator_loss=0.5090118050575256\n",
            "step 511: generator_loss=1.021494746208191, discriminator_loss=0.5188225507736206\n",
            "step 512: generator_loss=1.0353024005889893, discriminator_loss=0.5192960500717163\n",
            "step 513: generator_loss=1.0474250316619873, discriminator_loss=0.5280522704124451\n",
            "step 514: generator_loss=1.064105749130249, discriminator_loss=0.5340546369552612\n",
            "step 515: generator_loss=1.0558526515960693, discriminator_loss=0.5441590547561646\n",
            "step 516: generator_loss=1.0598315000534058, discriminator_loss=0.5697239637374878\n",
            "step 517: generator_loss=1.0163209438323975, discriminator_loss=0.5776245594024658\n",
            "step 518: generator_loss=0.9533852338790894, discriminator_loss=0.6240137815475464\n",
            "step 519: generator_loss=0.9994926452636719, discriminator_loss=0.6584752798080444\n",
            "step 520: generator_loss=1.058143138885498, discriminator_loss=0.6271584033966064\n",
            "step 521: generator_loss=1.079556941986084, discriminator_loss=0.652480959892273\n",
            "step 522: generator_loss=1.0494390726089478, discriminator_loss=0.7144023180007935\n",
            "step 523: generator_loss=1.0005991458892822, discriminator_loss=0.8004248142242432\n",
            "step 524: generator_loss=1.0206620693206787, discriminator_loss=0.7888047695159912\n",
            "step 525: generator_loss=0.9723718762397766, discriminator_loss=0.8400859832763672\n",
            "step 526: generator_loss=1.0946834087371826, discriminator_loss=0.9829302430152893\n",
            "step 527: generator_loss=1.128830909729004, discriminator_loss=0.971711277961731\n",
            "step 528: generator_loss=1.1164400577545166, discriminator_loss=0.9758785367012024\n",
            "step 529: generator_loss=1.1698663234710693, discriminator_loss=1.0835635662078857\n",
            "step 530: generator_loss=1.335498571395874, discriminator_loss=1.1235803365707397\n",
            "step 531: generator_loss=1.4138318300247192, discriminator_loss=1.1040000915527344\n",
            "step 532: generator_loss=1.5722031593322754, discriminator_loss=1.115050196647644\n",
            "step 533: generator_loss=1.5086220502853394, discriminator_loss=1.1757216453552246\n",
            "step 534: generator_loss=1.792896032333374, discriminator_loss=1.0414717197418213\n",
            "step 535: generator_loss=1.3721511363983154, discriminator_loss=1.1698719263076782\n",
            "step 536: generator_loss=1.7639983892440796, discriminator_loss=1.0752723217010498\n",
            "step 537: generator_loss=1.8891987800598145, discriminator_loss=0.9680914282798767\n",
            "step 538: generator_loss=2.2417354583740234, discriminator_loss=0.9640920758247375\n",
            "step 539: generator_loss=2.2137842178344727, discriminator_loss=0.8328683376312256\n",
            "step 540: generator_loss=2.466623067855835, discriminator_loss=0.8509862422943115\n",
            "step 541: generator_loss=2.1426327228546143, discriminator_loss=0.8431989550590515\n",
            "step 542: generator_loss=2.472795248031616, discriminator_loss=0.8101154565811157\n",
            "step 543: generator_loss=2.092620849609375, discriminator_loss=0.8714753985404968\n",
            "step 544: generator_loss=2.82658052444458, discriminator_loss=0.6492196321487427\n",
            "step 545: generator_loss=2.5575318336486816, discriminator_loss=0.7144635319709778\n",
            "step 546: generator_loss=2.3981637954711914, discriminator_loss=0.7074335813522339\n",
            "step 547: generator_loss=2.6490161418914795, discriminator_loss=0.6306902766227722\n",
            "step 548: generator_loss=2.738253116607666, discriminator_loss=0.5562058687210083\n",
            "step 549: generator_loss=2.651111125946045, discriminator_loss=0.559755802154541\n",
            "step 550: generator_loss=3.0888404846191406, discriminator_loss=0.4302715063095093\n",
            "step 551: generator_loss=2.863145351409912, discriminator_loss=0.42893898487091064\n",
            "step 552: generator_loss=2.9278793334960938, discriminator_loss=0.3911612629890442\n",
            "step 553: generator_loss=3.0281152725219727, discriminator_loss=0.3860907554626465\n",
            "step 554: generator_loss=2.8618521690368652, discriminator_loss=0.3843025863170624\n",
            "step 555: generator_loss=3.2425639629364014, discriminator_loss=0.3339688181877136\n",
            "step 556: generator_loss=2.959620475769043, discriminator_loss=0.3474990129470825\n",
            "step 557: generator_loss=2.9680442810058594, discriminator_loss=0.343660444021225\n",
            "step 558: generator_loss=3.229257106781006, discriminator_loss=0.32975178956985474\n",
            "step 559: generator_loss=3.1662211418151855, discriminator_loss=0.32521864771842957\n",
            "step 560: generator_loss=3.0240769386291504, discriminator_loss=0.3343244194984436\n",
            "step 561: generator_loss=2.8449277877807617, discriminator_loss=0.3636060357093811\n",
            "step 562: generator_loss=2.840488910675049, discriminator_loss=0.3687300682067871\n",
            "step 563: generator_loss=3.200463056564331, discriminator_loss=0.32923534512519836\n",
            "step 564: generator_loss=2.8311383724212646, discriminator_loss=0.380809485912323\n",
            "step 565: generator_loss=2.983926773071289, discriminator_loss=0.3757334351539612\n",
            "step 566: generator_loss=2.8261539936065674, discriminator_loss=0.42560526728630066\n",
            "step 567: generator_loss=2.825108289718628, discriminator_loss=0.42178285121917725\n",
            "step 568: generator_loss=2.638108253479004, discriminator_loss=0.45881155133247375\n",
            "step 569: generator_loss=3.418339967727661, discriminator_loss=0.36439216136932373\n",
            "step 570: generator_loss=3.040079355239868, discriminator_loss=0.4547196626663208\n",
            "step 571: generator_loss=3.249546766281128, discriminator_loss=0.3455325961112976\n",
            "step 572: generator_loss=3.003796100616455, discriminator_loss=0.42118018865585327\n",
            "step 573: generator_loss=3.647143840789795, discriminator_loss=0.41529929637908936\n",
            "step 574: generator_loss=3.837285041809082, discriminator_loss=0.401467502117157\n",
            "step 575: generator_loss=4.025603771209717, discriminator_loss=0.3665871024131775\n",
            "step 576: generator_loss=3.5496420860290527, discriminator_loss=0.41946175694465637\n",
            "step 577: generator_loss=4.252781867980957, discriminator_loss=0.39243292808532715\n",
            "step 578: generator_loss=4.120530605316162, discriminator_loss=0.33916208148002625\n",
            "step 579: generator_loss=4.362554550170898, discriminator_loss=0.36260420083999634\n",
            "step 580: generator_loss=4.244803428649902, discriminator_loss=0.29549524188041687\n",
            "step 581: generator_loss=4.2852864265441895, discriminator_loss=0.3313145339488983\n",
            "step 582: generator_loss=4.025786399841309, discriminator_loss=0.2859605550765991\n",
            "step 583: generator_loss=4.277022361755371, discriminator_loss=0.3053773045539856\n",
            "step 584: generator_loss=4.2308244705200195, discriminator_loss=0.27370694279670715\n",
            "step 585: generator_loss=4.686106204986572, discriminator_loss=0.2372734248638153\n",
            "step 586: generator_loss=4.533347129821777, discriminator_loss=0.23216208815574646\n",
            "step 587: generator_loss=4.660206317901611, discriminator_loss=0.22896936535835266\n",
            "step 588: generator_loss=4.292135715484619, discriminator_loss=0.2287675440311432\n",
            "step 589: generator_loss=4.572879791259766, discriminator_loss=0.21763549745082855\n",
            "step 590: generator_loss=4.147250175476074, discriminator_loss=0.2217869758605957\n",
            "step 591: generator_loss=3.952439546585083, discriminator_loss=0.2232811450958252\n",
            "step 592: generator_loss=4.114667892456055, discriminator_loss=0.2153111696243286\n",
            "step 593: generator_loss=3.8187389373779297, discriminator_loss=0.22057278454303741\n",
            "step 594: generator_loss=3.6404449939727783, discriminator_loss=0.21931913495063782\n",
            "step 595: generator_loss=3.229645252227783, discriminator_loss=0.22811061143875122\n",
            "step 596: generator_loss=3.31314754486084, discriminator_loss=0.23344913125038147\n",
            "step 597: generator_loss=2.974766254425049, discriminator_loss=0.2424805760383606\n",
            "step 598: generator_loss=2.8489129543304443, discriminator_loss=0.24983292818069458\n",
            "step 599: generator_loss=2.66120982170105, discriminator_loss=0.25078481435775757\n",
            "step 600: generator_loss=2.5626330375671387, discriminator_loss=0.26127445697784424\n",
            "step 601: generator_loss=2.656649351119995, discriminator_loss=0.24733147025108337\n",
            "step 602: generator_loss=2.686176061630249, discriminator_loss=0.2503833770751953\n",
            "step 603: generator_loss=2.6924962997436523, discriminator_loss=0.2823715806007385\n",
            "step 604: generator_loss=2.9988691806793213, discriminator_loss=0.23950865864753723\n",
            "step 605: generator_loss=3.0586588382720947, discriminator_loss=0.24807843565940857\n",
            "step 606: generator_loss=3.40022349357605, discriminator_loss=0.2186288833618164\n",
            "step 607: generator_loss=3.405318260192871, discriminator_loss=0.2211829125881195\n",
            "step 608: generator_loss=3.5935425758361816, discriminator_loss=0.21463534235954285\n",
            "step 609: generator_loss=3.9766712188720703, discriminator_loss=0.21411864459514618\n",
            "step 610: generator_loss=4.051143169403076, discriminator_loss=0.20838923752307892\n",
            "step 611: generator_loss=3.8982200622558594, discriminator_loss=0.2134712040424347\n",
            "step 612: generator_loss=4.196732044219971, discriminator_loss=0.2087080478668213\n",
            "step 613: generator_loss=4.079802989959717, discriminator_loss=0.21597132086753845\n",
            "step 614: generator_loss=4.143760681152344, discriminator_loss=0.20598891377449036\n",
            "step 615: generator_loss=4.226899147033691, discriminator_loss=0.20857937633991241\n",
            "step 616: generator_loss=4.077727317810059, discriminator_loss=0.2337135374546051\n",
            "step 617: generator_loss=4.33261251449585, discriminator_loss=0.21393626928329468\n",
            "step 618: generator_loss=4.306483268737793, discriminator_loss=0.2193848341703415\n",
            "step 619: generator_loss=4.143098831176758, discriminator_loss=0.2238515317440033\n",
            "step 620: generator_loss=4.503665924072266, discriminator_loss=0.21500079333782196\n",
            "step 621: generator_loss=4.254133224487305, discriminator_loss=0.2158781886100769\n",
            "step 622: generator_loss=4.348444938659668, discriminator_loss=0.23080211877822876\n",
            "step 623: generator_loss=4.418240547180176, discriminator_loss=0.21021032333374023\n",
            "step 624: generator_loss=4.366626739501953, discriminator_loss=0.20158299803733826\n",
            "step 625: generator_loss=4.339786052703857, discriminator_loss=0.22068116068840027\n",
            "step 626: generator_loss=4.340339660644531, discriminator_loss=0.20411604642868042\n",
            "step 627: generator_loss=4.05738639831543, discriminator_loss=0.21698957681655884\n",
            "step 628: generator_loss=4.1622395515441895, discriminator_loss=0.20416705310344696\n",
            "step 629: generator_loss=4.058492660522461, discriminator_loss=0.21347102522850037\n",
            "step 630: generator_loss=4.298579216003418, discriminator_loss=0.1969471275806427\n",
            "step 631: generator_loss=3.9371581077575684, discriminator_loss=0.21062932908535004\n",
            "step 632: generator_loss=4.078248500823975, discriminator_loss=0.20631331205368042\n",
            "step 633: generator_loss=4.067564964294434, discriminator_loss=0.2046341598033905\n",
            "step 634: generator_loss=4.0994110107421875, discriminator_loss=0.19365838170051575\n",
            "step 635: generator_loss=3.99704647064209, discriminator_loss=0.19608812034130096\n",
            "step 636: generator_loss=4.264162540435791, discriminator_loss=0.18873991072177887\n",
            "step 637: generator_loss=3.956234931945801, discriminator_loss=0.19329184293746948\n",
            "step 638: generator_loss=4.121382713317871, discriminator_loss=0.19018903374671936\n",
            "step 639: generator_loss=3.729515552520752, discriminator_loss=0.19358880817890167\n",
            "step 640: generator_loss=3.91141939163208, discriminator_loss=0.18735069036483765\n",
            "step 641: generator_loss=3.6099114418029785, discriminator_loss=0.19754339754581451\n",
            "step 642: generator_loss=3.6698834896087646, discriminator_loss=0.19597341120243073\n",
            "step 643: generator_loss=3.641010046005249, discriminator_loss=0.1936003565788269\n",
            "step 644: generator_loss=3.7080559730529785, discriminator_loss=0.195283904671669\n",
            "step 645: generator_loss=3.2907700538635254, discriminator_loss=0.20510414242744446\n",
            "step 646: generator_loss=3.2864840030670166, discriminator_loss=0.20092201232910156\n",
            "step 647: generator_loss=3.441532850265503, discriminator_loss=0.20449447631835938\n",
            "step 648: generator_loss=3.3533596992492676, discriminator_loss=0.20521830022335052\n",
            "step 649: generator_loss=3.0943472385406494, discriminator_loss=0.20813891291618347\n",
            "step 650: generator_loss=2.978030204772949, discriminator_loss=0.21583372354507446\n",
            "step 651: generator_loss=3.038086414337158, discriminator_loss=0.21380388736724854\n",
            "step 652: generator_loss=3.203456401824951, discriminator_loss=0.2072247713804245\n",
            "step 653: generator_loss=3.13613224029541, discriminator_loss=0.20566493272781372\n",
            "step 654: generator_loss=3.137472629547119, discriminator_loss=0.21227994561195374\n",
            "step 655: generator_loss=3.0906898975372314, discriminator_loss=0.21390505135059357\n",
            "step 656: generator_loss=3.010101795196533, discriminator_loss=0.21673698723316193\n",
            "step 657: generator_loss=3.166071891784668, discriminator_loss=0.21332629024982452\n",
            "step 658: generator_loss=3.126408576965332, discriminator_loss=0.21622757613658905\n",
            "step 659: generator_loss=2.980762481689453, discriminator_loss=0.21598008275032043\n",
            "step 660: generator_loss=2.954707384109497, discriminator_loss=0.2241845577955246\n",
            "step 661: generator_loss=3.1489791870117188, discriminator_loss=0.21392375230789185\n",
            "step 662: generator_loss=3.0847017765045166, discriminator_loss=0.2144082635641098\n",
            "step 663: generator_loss=3.0047638416290283, discriminator_loss=0.21918630599975586\n",
            "step 664: generator_loss=3.0809617042541504, discriminator_loss=0.2166650891304016\n",
            "step 665: generator_loss=3.261953353881836, discriminator_loss=0.2078111618757248\n",
            "step 666: generator_loss=3.3384110927581787, discriminator_loss=0.20297476649284363\n",
            "step 667: generator_loss=3.288797378540039, discriminator_loss=0.2047332227230072\n",
            "step 668: generator_loss=3.5228841304779053, discriminator_loss=0.19654548168182373\n",
            "step 669: generator_loss=3.474632501602173, discriminator_loss=0.20169727504253387\n",
            "step 670: generator_loss=3.4402916431427, discriminator_loss=0.2000698745250702\n",
            "step 671: generator_loss=3.379288911819458, discriminator_loss=0.19908952713012695\n",
            "step 672: generator_loss=3.190373182296753, discriminator_loss=0.20872461795806885\n",
            "step 673: generator_loss=3.125101089477539, discriminator_loss=0.2063349485397339\n",
            "step 674: generator_loss=3.198495388031006, discriminator_loss=0.21334640681743622\n",
            "step 675: generator_loss=3.151370048522949, discriminator_loss=0.2060883641242981\n",
            "step 676: generator_loss=3.2263314723968506, discriminator_loss=0.207602858543396\n",
            "step 677: generator_loss=2.930366039276123, discriminator_loss=0.21095135807991028\n",
            "step 678: generator_loss=3.010756492614746, discriminator_loss=0.2117883712053299\n",
            "step 679: generator_loss=2.7930827140808105, discriminator_loss=0.21836254000663757\n",
            "step 680: generator_loss=2.8310227394104004, discriminator_loss=0.219595804810524\n",
            "step 681: generator_loss=3.086658239364624, discriminator_loss=0.20368757843971252\n",
            "step 682: generator_loss=3.022325277328491, discriminator_loss=0.21193179488182068\n",
            "step 683: generator_loss=2.9967212677001953, discriminator_loss=0.21107009053230286\n",
            "step 684: generator_loss=2.7847063541412354, discriminator_loss=0.215773344039917\n",
            "step 685: generator_loss=3.019021511077881, discriminator_loss=0.20619504153728485\n",
            "step 686: generator_loss=2.742452621459961, discriminator_loss=0.213483065366745\n",
            "step 687: generator_loss=3.0622615814208984, discriminator_loss=0.20431137084960938\n",
            "step 688: generator_loss=2.936955451965332, discriminator_loss=0.21500010788440704\n",
            "step 689: generator_loss=3.113403558731079, discriminator_loss=0.20311316847801208\n",
            "step 690: generator_loss=2.7061424255371094, discriminator_loss=0.2134772390127182\n",
            "step 691: generator_loss=2.877912998199463, discriminator_loss=0.2049148976802826\n",
            "step 692: generator_loss=2.86125111579895, discriminator_loss=0.2082349956035614\n",
            "step 693: generator_loss=2.782822608947754, discriminator_loss=0.21211223304271698\n",
            "step 694: generator_loss=2.9926986694335938, discriminator_loss=0.20494350790977478\n",
            "step 695: generator_loss=2.730229616165161, discriminator_loss=0.21028770506381989\n",
            "step 696: generator_loss=2.7391247749328613, discriminator_loss=0.21553927659988403\n",
            "step 697: generator_loss=2.747847557067871, discriminator_loss=0.21037523448467255\n",
            "step 698: generator_loss=2.568039894104004, discriminator_loss=0.2205706536769867\n",
            "step 699: generator_loss=2.71895170211792, discriminator_loss=0.20942828059196472\n",
            "step 700: generator_loss=2.8770852088928223, discriminator_loss=0.21101021766662598\n",
            "step 701: generator_loss=2.654078483581543, discriminator_loss=0.2107410430908203\n",
            "step 702: generator_loss=2.6153464317321777, discriminator_loss=0.2115793377161026\n",
            "step 703: generator_loss=2.4267401695251465, discriminator_loss=0.22314152121543884\n",
            "step 704: generator_loss=2.5896902084350586, discriminator_loss=0.21922028064727783\n",
            "step 705: generator_loss=2.6289172172546387, discriminator_loss=0.2182680070400238\n",
            "step 706: generator_loss=2.6579501628875732, discriminator_loss=0.21581174433231354\n",
            "step 707: generator_loss=2.443027973175049, discriminator_loss=0.22171244025230408\n",
            "step 708: generator_loss=2.7580971717834473, discriminator_loss=0.21497264504432678\n",
            "step 709: generator_loss=2.422771692276001, discriminator_loss=0.22572457790374756\n",
            "step 710: generator_loss=2.410590410232544, discriminator_loss=0.22550523281097412\n",
            "step 711: generator_loss=2.4269442558288574, discriminator_loss=0.22314439713954926\n",
            "step 712: generator_loss=2.3249351978302, discriminator_loss=0.22780176997184753\n",
            "step 713: generator_loss=2.4558756351470947, discriminator_loss=0.22055894136428833\n",
            "step 714: generator_loss=2.175288200378418, discriminator_loss=0.23393650352954865\n",
            "step 715: generator_loss=2.0587658882141113, discriminator_loss=0.24397405982017517\n",
            "step 716: generator_loss=2.305046558380127, discriminator_loss=0.2290615737438202\n",
            "step 717: generator_loss=2.1831135749816895, discriminator_loss=0.23999494314193726\n",
            "step 718: generator_loss=2.227126121520996, discriminator_loss=0.22889845073223114\n",
            "step 719: generator_loss=2.234928846359253, discriminator_loss=0.23192214965820312\n",
            "step 720: generator_loss=2.226621150970459, discriminator_loss=0.24036520719528198\n",
            "step 721: generator_loss=2.173449993133545, discriminator_loss=0.2413502186536789\n",
            "step 722: generator_loss=2.130673885345459, discriminator_loss=0.23556242883205414\n",
            "step 723: generator_loss=2.184607982635498, discriminator_loss=0.235681414604187\n",
            "step 724: generator_loss=2.1346988677978516, discriminator_loss=0.23517395555973053\n",
            "step 725: generator_loss=1.9890419244766235, discriminator_loss=0.2421320229768753\n",
            "step 726: generator_loss=2.137329578399658, discriminator_loss=0.23182126879692078\n",
            "step 727: generator_loss=2.183037281036377, discriminator_loss=0.23068484663963318\n",
            "step 728: generator_loss=1.9992256164550781, discriminator_loss=0.23813121020793915\n",
            "step 729: generator_loss=2.051135301589966, discriminator_loss=0.2346210479736328\n",
            "step 730: generator_loss=2.124912977218628, discriminator_loss=0.23165886104106903\n",
            "step 731: generator_loss=2.171330690383911, discriminator_loss=0.23097294569015503\n",
            "step 732: generator_loss=2.0007236003875732, discriminator_loss=0.23809918761253357\n",
            "step 733: generator_loss=2.063112258911133, discriminator_loss=0.23254276812076569\n",
            "step 734: generator_loss=2.1634130477905273, discriminator_loss=0.22146382927894592\n",
            "step 735: generator_loss=2.1528778076171875, discriminator_loss=0.22463712096214294\n",
            "step 736: generator_loss=2.0289430618286133, discriminator_loss=0.22909463942050934\n",
            "step 737: generator_loss=2.148010492324829, discriminator_loss=0.22301708161830902\n",
            "step 738: generator_loss=2.126695394515991, discriminator_loss=0.22597458958625793\n",
            "step 739: generator_loss=2.1044812202453613, discriminator_loss=0.2227027714252472\n",
            "step 740: generator_loss=2.1651835441589355, discriminator_loss=0.2142421007156372\n",
            "step 741: generator_loss=2.1561079025268555, discriminator_loss=0.21911504864692688\n",
            "step 742: generator_loss=2.103997230529785, discriminator_loss=0.21793708205223083\n",
            "step 743: generator_loss=2.1455302238464355, discriminator_loss=0.220912903547287\n",
            "step 744: generator_loss=2.0328011512756348, discriminator_loss=0.22097176313400269\n",
            "step 745: generator_loss=2.1459155082702637, discriminator_loss=0.21495196223258972\n",
            "step 746: generator_loss=2.151724338531494, discriminator_loss=0.2164076566696167\n",
            "step 747: generator_loss=2.1892125606536865, discriminator_loss=0.21364443004131317\n",
            "step 748: generator_loss=2.2601070404052734, discriminator_loss=0.21016378700733185\n",
            "step 749: generator_loss=2.245723247528076, discriminator_loss=0.20858551561832428\n",
            "step 750: generator_loss=2.284400463104248, discriminator_loss=0.20550407469272614\n",
            "step 751: generator_loss=2.210782527923584, discriminator_loss=0.21090994775295258\n",
            "step 752: generator_loss=2.221848249435425, discriminator_loss=0.20815710723400116\n",
            "step 753: generator_loss=2.186554431915283, discriminator_loss=0.2140469253063202\n",
            "step 754: generator_loss=2.2368335723876953, discriminator_loss=0.2134876251220703\n",
            "step 755: generator_loss=2.1238675117492676, discriminator_loss=0.21368494629859924\n",
            "step 756: generator_loss=2.281309127807617, discriminator_loss=0.20740634202957153\n",
            "step 757: generator_loss=2.2494096755981445, discriminator_loss=0.20852670073509216\n",
            "step 758: generator_loss=2.204354763031006, discriminator_loss=0.20967495441436768\n",
            "step 759: generator_loss=2.2580273151397705, discriminator_loss=0.20568208396434784\n",
            "step 760: generator_loss=2.1863269805908203, discriminator_loss=0.21194060146808624\n",
            "step 761: generator_loss=2.1375656127929688, discriminator_loss=0.21322354674339294\n",
            "step 762: generator_loss=2.12754487991333, discriminator_loss=0.21441352367401123\n",
            "step 763: generator_loss=2.052656412124634, discriminator_loss=0.22030071914196014\n",
            "step 764: generator_loss=2.1726441383361816, discriminator_loss=0.2111506164073944\n",
            "step 765: generator_loss=2.1058549880981445, discriminator_loss=0.2173154652118683\n",
            "step 766: generator_loss=2.082240104675293, discriminator_loss=0.21634170413017273\n",
            "step 767: generator_loss=2.0297346115112305, discriminator_loss=0.22037957608699799\n",
            "step 768: generator_loss=2.1434593200683594, discriminator_loss=0.21431496739387512\n",
            "step 769: generator_loss=1.993124008178711, discriminator_loss=0.22113721072673798\n",
            "step 770: generator_loss=2.0845160484313965, discriminator_loss=0.21462780237197876\n",
            "step 771: generator_loss=2.095264434814453, discriminator_loss=0.21542534232139587\n",
            "step 772: generator_loss=1.9957797527313232, discriminator_loss=0.21987278759479523\n",
            "step 773: generator_loss=2.0346028804779053, discriminator_loss=0.21712681651115417\n",
            "step 774: generator_loss=1.9317013025283813, discriminator_loss=0.2234622836112976\n",
            "step 775: generator_loss=1.8868664503097534, discriminator_loss=0.22973045706748962\n",
            "step 776: generator_loss=1.9602181911468506, discriminator_loss=0.22186757624149323\n",
            "step 777: generator_loss=1.9238556623458862, discriminator_loss=0.22616183757781982\n",
            "step 778: generator_loss=1.8816741704940796, discriminator_loss=0.22779759764671326\n",
            "step 779: generator_loss=1.878872036933899, discriminator_loss=0.22758224606513977\n",
            "step 780: generator_loss=1.9048582315444946, discriminator_loss=0.22407682240009308\n",
            "step 781: generator_loss=1.844921350479126, discriminator_loss=0.22988355159759521\n",
            "step 782: generator_loss=1.8138015270233154, discriminator_loss=0.23209768533706665\n",
            "step 783: generator_loss=1.7965338230133057, discriminator_loss=0.23566894233226776\n",
            "step 784: generator_loss=1.7874298095703125, discriminator_loss=0.23356646299362183\n",
            "step 785: generator_loss=1.6605000495910645, discriminator_loss=0.24372811615467072\n",
            "step 786: generator_loss=1.7357176542282104, discriminator_loss=0.24004890024662018\n",
            "step 787: generator_loss=1.666359782218933, discriminator_loss=0.2446713149547577\n",
            "step 788: generator_loss=1.7100989818572998, discriminator_loss=0.24138358235359192\n",
            "step 789: generator_loss=1.5972564220428467, discriminator_loss=0.2518198490142822\n",
            "step 790: generator_loss=1.5570158958435059, discriminator_loss=0.2560793459415436\n",
            "step 791: generator_loss=1.5644629001617432, discriminator_loss=0.2549896240234375\n",
            "step 792: generator_loss=1.5342910289764404, discriminator_loss=0.26008546352386475\n",
            "step 793: generator_loss=1.578718900680542, discriminator_loss=0.25414085388183594\n",
            "step 794: generator_loss=1.5218706130981445, discriminator_loss=0.25964224338531494\n",
            "step 795: generator_loss=1.4635436534881592, discriminator_loss=0.26793205738067627\n",
            "step 796: generator_loss=1.5236399173736572, discriminator_loss=0.2631731331348419\n",
            "step 797: generator_loss=1.4402196407318115, discriminator_loss=0.2731507420539856\n",
            "step 798: generator_loss=1.455567479133606, discriminator_loss=0.27297329902648926\n",
            "step 799: generator_loss=1.403741717338562, discriminator_loss=0.2815203368663788\n",
            "step 800: generator_loss=1.4358702898025513, discriminator_loss=0.2741471529006958\n",
            "step 801: generator_loss=1.4148938655853271, discriminator_loss=0.279845267534256\n",
            "step 802: generator_loss=1.380034327507019, discriminator_loss=0.28608477115631104\n",
            "step 803: generator_loss=1.3988714218139648, discriminator_loss=0.28464022278785706\n",
            "step 804: generator_loss=1.3632447719573975, discriminator_loss=0.29279229044914246\n",
            "step 805: generator_loss=1.4381930828094482, discriminator_loss=0.27981168031692505\n",
            "step 806: generator_loss=1.4646415710449219, discriminator_loss=0.27760863304138184\n",
            "step 807: generator_loss=1.4488168954849243, discriminator_loss=0.27705883979797363\n",
            "step 808: generator_loss=1.4984383583068848, discriminator_loss=0.27071303129196167\n",
            "step 809: generator_loss=1.5044045448303223, discriminator_loss=0.2702326774597168\n",
            "step 810: generator_loss=1.448390245437622, discriminator_loss=0.27392080426216125\n",
            "step 811: generator_loss=1.4561704397201538, discriminator_loss=0.2746734023094177\n",
            "step 812: generator_loss=1.4763164520263672, discriminator_loss=0.2702041268348694\n",
            "step 813: generator_loss=1.4945467710494995, discriminator_loss=0.2672542333602905\n",
            "step 814: generator_loss=1.5051875114440918, discriminator_loss=0.2635575234889984\n",
            "step 815: generator_loss=1.5332624912261963, discriminator_loss=0.26144495606422424\n",
            "step 816: generator_loss=1.6106986999511719, discriminator_loss=0.2498120367527008\n",
            "step 817: generator_loss=1.5048946142196655, discriminator_loss=0.26080042123794556\n",
            "step 818: generator_loss=1.5038206577301025, discriminator_loss=0.26129233837127686\n",
            "step 819: generator_loss=1.5246050357818604, discriminator_loss=0.25692838430404663\n",
            "step 820: generator_loss=1.5296721458435059, discriminator_loss=0.2543489933013916\n",
            "step 821: generator_loss=1.523239254951477, discriminator_loss=0.2563326358795166\n",
            "step 822: generator_loss=1.5563790798187256, discriminator_loss=0.2519902288913727\n",
            "step 823: generator_loss=1.568428874015808, discriminator_loss=0.24785059690475464\n",
            "step 824: generator_loss=1.5304043292999268, discriminator_loss=0.25347697734832764\n",
            "step 825: generator_loss=1.5512722730636597, discriminator_loss=0.2508423626422882\n",
            "step 826: generator_loss=1.5160043239593506, discriminator_loss=0.2549252510070801\n",
            "step 827: generator_loss=1.5177154541015625, discriminator_loss=0.25415265560150146\n",
            "step 828: generator_loss=1.539587378501892, discriminator_loss=0.25199317932128906\n",
            "step 829: generator_loss=1.5360181331634521, discriminator_loss=0.25094953179359436\n",
            "step 830: generator_loss=1.4755233526229858, discriminator_loss=0.26140642166137695\n",
            "step 831: generator_loss=1.5052971839904785, discriminator_loss=0.2572641372680664\n",
            "step 832: generator_loss=1.540848731994629, discriminator_loss=0.25368833541870117\n",
            "step 833: generator_loss=1.477982997894287, discriminator_loss=0.2607879340648651\n",
            "step 834: generator_loss=1.4487046003341675, discriminator_loss=0.2666960656642914\n",
            "step 835: generator_loss=1.5046813488006592, discriminator_loss=0.2575277090072632\n",
            "step 836: generator_loss=1.4634840488433838, discriminator_loss=0.2646171748638153\n",
            "step 837: generator_loss=1.4142587184906006, discriminator_loss=0.2735908627510071\n",
            "step 838: generator_loss=1.4223647117614746, discriminator_loss=0.2744362950325012\n",
            "step 839: generator_loss=1.434064269065857, discriminator_loss=0.2703542411327362\n",
            "step 840: generator_loss=1.3335317373275757, discriminator_loss=0.2911226451396942\n",
            "step 841: generator_loss=1.3415979146957397, discriminator_loss=0.29223182797431946\n",
            "step 842: generator_loss=1.3716247081756592, discriminator_loss=0.2886573076248169\n",
            "step 843: generator_loss=1.3463712930679321, discriminator_loss=0.2925032675266266\n",
            "step 844: generator_loss=1.3243427276611328, discriminator_loss=0.2977818250656128\n",
            "step 845: generator_loss=1.2839553356170654, discriminator_loss=0.30760934948921204\n",
            "step 846: generator_loss=1.3186427354812622, discriminator_loss=0.3018312156200409\n",
            "step 847: generator_loss=1.2823171615600586, discriminator_loss=0.3111477494239807\n",
            "step 848: generator_loss=1.252976894378662, discriminator_loss=0.3216016888618469\n",
            "step 849: generator_loss=1.261580467224121, discriminator_loss=0.3232266306877136\n",
            "step 850: generator_loss=1.2828209400177002, discriminator_loss=0.31145620346069336\n",
            "step 851: generator_loss=1.2491605281829834, discriminator_loss=0.3276423215866089\n",
            "step 852: generator_loss=1.2032641172409058, discriminator_loss=0.3391302824020386\n",
            "step 853: generator_loss=1.167079210281372, discriminator_loss=0.34886717796325684\n",
            "step 854: generator_loss=1.2318115234375, discriminator_loss=0.3342394530773163\n",
            "step 855: generator_loss=1.1272441148757935, discriminator_loss=0.36167043447494507\n",
            "step 856: generator_loss=1.1482758522033691, discriminator_loss=0.3557532727718353\n",
            "step 857: generator_loss=1.169411301612854, discriminator_loss=0.34526219964027405\n",
            "step 858: generator_loss=1.1885590553283691, discriminator_loss=0.3390475809574127\n",
            "step 859: generator_loss=1.2176629304885864, discriminator_loss=0.33419740200042725\n",
            "step 860: generator_loss=1.2037070989608765, discriminator_loss=0.32801553606987\n",
            "step 861: generator_loss=1.1917040348052979, discriminator_loss=0.32991042733192444\n",
            "step 862: generator_loss=1.2373650074005127, discriminator_loss=0.3180239796638489\n",
            "step 863: generator_loss=1.2476484775543213, discriminator_loss=0.3143657147884369\n",
            "step 864: generator_loss=1.181706190109253, discriminator_loss=0.3286592960357666\n",
            "step 865: generator_loss=1.200960636138916, discriminator_loss=0.32222604751586914\n",
            "step 866: generator_loss=1.2092344760894775, discriminator_loss=0.31954264640808105\n",
            "step 867: generator_loss=1.2293813228607178, discriminator_loss=0.31513524055480957\n",
            "step 868: generator_loss=1.2303192615509033, discriminator_loss=0.31508028507232666\n",
            "step 869: generator_loss=1.2210874557495117, discriminator_loss=0.31738096475601196\n",
            "step 870: generator_loss=1.1974338293075562, discriminator_loss=0.32094770669937134\n",
            "step 871: generator_loss=1.1817294359207153, discriminator_loss=0.32497888803482056\n",
            "step 872: generator_loss=1.1730687618255615, discriminator_loss=0.3283360004425049\n",
            "step 873: generator_loss=1.1731281280517578, discriminator_loss=0.32777225971221924\n",
            "step 874: generator_loss=1.205107569694519, discriminator_loss=0.3228132426738739\n",
            "step 875: generator_loss=1.2031925916671753, discriminator_loss=0.3237830698490143\n",
            "step 876: generator_loss=1.1947389841079712, discriminator_loss=0.32599538564682007\n",
            "step 877: generator_loss=1.1907036304473877, discriminator_loss=0.3285377025604248\n",
            "step 878: generator_loss=1.1942569017410278, discriminator_loss=0.3272021412849426\n",
            "step 879: generator_loss=1.1605629920959473, discriminator_loss=0.33705228567123413\n",
            "step 880: generator_loss=1.1821672916412354, discriminator_loss=0.3330979645252228\n",
            "step 881: generator_loss=1.2208445072174072, discriminator_loss=0.3300936222076416\n",
            "step 882: generator_loss=1.2197892665863037, discriminator_loss=0.32965323328971863\n",
            "step 883: generator_loss=1.1851060390472412, discriminator_loss=0.33730441331863403\n",
            "step 884: generator_loss=1.1852275133132935, discriminator_loss=0.3415127694606781\n",
            "step 885: generator_loss=1.125225305557251, discriminator_loss=0.35276007652282715\n",
            "step 886: generator_loss=1.1535650491714478, discriminator_loss=0.3471956253051758\n",
            "step 887: generator_loss=1.1249834299087524, discriminator_loss=0.3581068515777588\n",
            "step 888: generator_loss=1.1366618871688843, discriminator_loss=0.3577253818511963\n",
            "step 889: generator_loss=1.1277494430541992, discriminator_loss=0.3605859875679016\n",
            "step 890: generator_loss=1.1077593564987183, discriminator_loss=0.3639791011810303\n",
            "step 891: generator_loss=1.0742998123168945, discriminator_loss=0.37233826518058777\n",
            "step 892: generator_loss=1.047250747680664, discriminator_loss=0.3792824149131775\n",
            "step 893: generator_loss=1.0461540222167969, discriminator_loss=0.38335680961608887\n",
            "step 894: generator_loss=1.0858713388442993, discriminator_loss=0.37556928396224976\n",
            "step 895: generator_loss=1.0213292837142944, discriminator_loss=0.38910114765167236\n",
            "step 896: generator_loss=0.9863640069961548, discriminator_loss=0.4016650319099426\n",
            "step 897: generator_loss=1.0236246585845947, discriminator_loss=0.394727885723114\n",
            "step 898: generator_loss=0.9795964956283569, discriminator_loss=0.4071059823036194\n",
            "step 899: generator_loss=0.9895980954170227, discriminator_loss=0.40553706884384155\n",
            "step 900: generator_loss=0.9994994401931763, discriminator_loss=0.40306609869003296\n",
            "step 901: generator_loss=0.9832737445831299, discriminator_loss=0.40728551149368286\n",
            "step 902: generator_loss=0.9633536338806152, discriminator_loss=0.4142979681491852\n",
            "step 903: generator_loss=0.9801579713821411, discriminator_loss=0.41081562638282776\n",
            "step 904: generator_loss=0.9545871615409851, discriminator_loss=0.41828736662864685\n",
            "step 905: generator_loss=1.0026602745056152, discriminator_loss=0.4070069193840027\n",
            "step 906: generator_loss=1.0072883367538452, discriminator_loss=0.40606409311294556\n",
            "step 907: generator_loss=1.0038872957229614, discriminator_loss=0.4071798026561737\n",
            "step 908: generator_loss=1.0083732604980469, discriminator_loss=0.4068217873573303\n",
            "step 909: generator_loss=1.0493381023406982, discriminator_loss=0.40009239315986633\n",
            "step 910: generator_loss=1.030083417892456, discriminator_loss=0.4050443172454834\n",
            "step 911: generator_loss=1.0403783321380615, discriminator_loss=0.4021250605583191\n",
            "step 912: generator_loss=1.0426450967788696, discriminator_loss=0.4020776152610779\n",
            "step 913: generator_loss=1.0793873071670532, discriminator_loss=0.3928180932998657\n",
            "step 914: generator_loss=1.074576735496521, discriminator_loss=0.39414507150650024\n",
            "step 915: generator_loss=1.0427699089050293, discriminator_loss=0.4038618206977844\n",
            "step 916: generator_loss=1.0401663780212402, discriminator_loss=0.4018571376800537\n",
            "step 917: generator_loss=1.0640195608139038, discriminator_loss=0.3988072872161865\n",
            "step 918: generator_loss=1.0426771640777588, discriminator_loss=0.4031020402908325\n",
            "step 919: generator_loss=1.0452806949615479, discriminator_loss=0.4020349979400635\n",
            "step 920: generator_loss=1.0593762397766113, discriminator_loss=0.4002020061016083\n",
            "step 921: generator_loss=1.043513298034668, discriminator_loss=0.40517473220825195\n",
            "step 922: generator_loss=1.0446805953979492, discriminator_loss=0.404293954372406\n",
            "step 923: generator_loss=1.049736738204956, discriminator_loss=0.4036339819431305\n",
            "step 924: generator_loss=1.0524311065673828, discriminator_loss=0.4038679599761963\n",
            "step 925: generator_loss=1.0459160804748535, discriminator_loss=0.4056832194328308\n",
            "step 926: generator_loss=1.0613422393798828, discriminator_loss=0.40250587463378906\n",
            "step 927: generator_loss=1.0636911392211914, discriminator_loss=0.40280529856681824\n",
            "step 928: generator_loss=1.0687273740768433, discriminator_loss=0.4021463096141815\n",
            "step 929: generator_loss=1.0770158767700195, discriminator_loss=0.400921493768692\n",
            "step 930: generator_loss=1.0878043174743652, discriminator_loss=0.3989217281341553\n",
            "step 931: generator_loss=1.0857253074645996, discriminator_loss=0.39985358715057373\n",
            "step 932: generator_loss=1.0885205268859863, discriminator_loss=0.39917895197868347\n",
            "step 933: generator_loss=1.0949262380599976, discriminator_loss=0.39803749322891235\n",
            "step 934: generator_loss=1.0887093544006348, discriminator_loss=0.3995136320590973\n",
            "step 935: generator_loss=1.0821945667266846, discriminator_loss=0.40094348788261414\n",
            "step 936: generator_loss=1.0888919830322266, discriminator_loss=0.3991096019744873\n",
            "step 937: generator_loss=1.0770853757858276, discriminator_loss=0.4017723798751831\n",
            "step 938: generator_loss=1.0806827545166016, discriminator_loss=0.40059927105903625\n",
            "step 939: generator_loss=1.0704677104949951, discriminator_loss=0.40278172492980957\n",
            "step 940: generator_loss=1.0582144260406494, discriminator_loss=0.4054044485092163\n",
            "step 941: generator_loss=1.0533322095870972, discriminator_loss=0.406648725271225\n",
            "step 942: generator_loss=1.0380288362503052, discriminator_loss=0.4099939167499542\n",
            "step 943: generator_loss=1.0274674892425537, discriminator_loss=0.4124213457107544\n",
            "step 944: generator_loss=1.012322187423706, discriminator_loss=0.41646432876586914\n",
            "step 945: generator_loss=1.0017294883728027, discriminator_loss=0.41906216740608215\n",
            "step 946: generator_loss=0.9815913438796997, discriminator_loss=0.42468172311782837\n",
            "step 947: generator_loss=0.9642808437347412, discriminator_loss=0.42965611815452576\n",
            "step 948: generator_loss=0.9559122323989868, discriminator_loss=0.43194469809532166\n",
            "step 949: generator_loss=0.9391791820526123, discriminator_loss=0.43747037649154663\n",
            "step 950: generator_loss=0.9256173968315125, discriminator_loss=0.4417409598827362\n",
            "step 951: generator_loss=0.9072067737579346, discriminator_loss=0.44801127910614014\n",
            "step 952: generator_loss=0.883866548538208, discriminator_loss=0.4564323127269745\n",
            "step 953: generator_loss=0.8709121346473694, discriminator_loss=0.4603176712989807\n",
            "step 954: generator_loss=0.8485339879989624, discriminator_loss=0.46902382373809814\n",
            "step 955: generator_loss=0.8362715840339661, discriminator_loss=0.4738343358039856\n",
            "step 956: generator_loss=0.8235114812850952, discriminator_loss=0.4788937270641327\n",
            "step 957: generator_loss=0.8170481324195862, discriminator_loss=0.4813189208507538\n",
            "step 958: generator_loss=0.8118396997451782, discriminator_loss=0.48395127058029175\n",
            "step 959: generator_loss=0.8083863854408264, discriminator_loss=0.48552221059799194\n",
            "step 960: generator_loss=0.8046185374259949, discriminator_loss=0.4868669807910919\n",
            "step 961: generator_loss=0.8304023742675781, discriminator_loss=0.4770617187023163\n",
            "step 962: generator_loss=0.8523539900779724, discriminator_loss=0.46799468994140625\n",
            "step 963: generator_loss=0.853350043296814, discriminator_loss=0.46788692474365234\n",
            "step 964: generator_loss=0.8813966512680054, discriminator_loss=0.4579600989818573\n",
            "step 965: generator_loss=0.9028090238571167, discriminator_loss=0.45022645592689514\n",
            "step 966: generator_loss=0.9404129385948181, discriminator_loss=0.43836510181427\n",
            "step 967: generator_loss=0.9627445340156555, discriminator_loss=0.43109190464019775\n",
            "step 968: generator_loss=1.0025405883789062, discriminator_loss=0.41982313990592957\n",
            "step 969: generator_loss=1.0250928401947021, discriminator_loss=0.41277021169662476\n",
            "step 970: generator_loss=1.048851490020752, discriminator_loss=0.40533366799354553\n",
            "step 971: generator_loss=1.0795929431915283, discriminator_loss=0.396767258644104\n",
            "step 972: generator_loss=1.1048868894577026, discriminator_loss=0.3901766240596771\n",
            "step 973: generator_loss=1.1353694200515747, discriminator_loss=0.3820115923881531\n",
            "step 974: generator_loss=1.1625635623931885, discriminator_loss=0.37577372789382935\n",
            "step 975: generator_loss=1.2168413400650024, discriminator_loss=0.36363616585731506\n",
            "step 976: generator_loss=1.2072526216506958, discriminator_loss=0.36434924602508545\n",
            "step 977: generator_loss=1.2336395978927612, discriminator_loss=0.35813015699386597\n",
            "step 978: generator_loss=1.2230851650238037, discriminator_loss=0.3593887686729431\n",
            "step 979: generator_loss=1.2474777698516846, discriminator_loss=0.35366976261138916\n",
            "step 980: generator_loss=1.2473949193954468, discriminator_loss=0.3532616198062897\n",
            "step 981: generator_loss=1.2534191608428955, discriminator_loss=0.35100027918815613\n",
            "step 982: generator_loss=1.2129652500152588, discriminator_loss=0.3591774106025696\n",
            "step 983: generator_loss=1.1957862377166748, discriminator_loss=0.36274391412734985\n",
            "step 984: generator_loss=1.225249171257019, discriminator_loss=0.3552301526069641\n",
            "step 985: generator_loss=1.2125412225723267, discriminator_loss=0.3576017916202545\n",
            "step 986: generator_loss=1.17206609249115, discriminator_loss=0.36597341299057007\n",
            "step 987: generator_loss=1.1462969779968262, discriminator_loss=0.3712872564792633\n",
            "step 988: generator_loss=1.1235072612762451, discriminator_loss=0.3773500919342041\n",
            "step 989: generator_loss=1.0908278226852417, discriminator_loss=0.3849503993988037\n",
            "step 990: generator_loss=1.0840356349945068, discriminator_loss=0.38566824793815613\n",
            "step 991: generator_loss=1.0670548677444458, discriminator_loss=0.39033347368240356\n",
            "step 992: generator_loss=1.0401479005813599, discriminator_loss=0.3972828686237335\n",
            "step 993: generator_loss=1.0222692489624023, discriminator_loss=0.40185433626174927\n",
            "step 994: generator_loss=1.0100524425506592, discriminator_loss=0.4056498110294342\n",
            "step 995: generator_loss=0.9692007303237915, discriminator_loss=0.4175143241882324\n",
            "step 996: generator_loss=0.9646981954574585, discriminator_loss=0.4190356135368347\n",
            "step 997: generator_loss=0.9571056962013245, discriminator_loss=0.42201897501945496\n",
            "step 998: generator_loss=0.9624248147010803, discriminator_loss=0.42124712467193604\n",
            "step 999: generator_loss=0.9256969690322876, discriminator_loss=0.4328104555606842\n",
            "step 1000: generator_loss=0.9300146698951721, discriminator_loss=0.4323093891143799\n",
            "step 1001: generator_loss=0.9263883233070374, discriminator_loss=0.43336498737335205\n",
            "step 1002: generator_loss=0.9225311875343323, discriminator_loss=0.4350985288619995\n",
            "step 1003: generator_loss=0.912655770778656, discriminator_loss=0.438720166683197\n",
            "step 1004: generator_loss=0.9016159176826477, discriminator_loss=0.44392192363739014\n",
            "step 1005: generator_loss=0.9139628410339355, discriminator_loss=0.44028180837631226\n",
            "step 1006: generator_loss=0.8938436508178711, discriminator_loss=0.4474394917488098\n",
            "step 1007: generator_loss=0.8888875842094421, discriminator_loss=0.4490559697151184\n",
            "step 1008: generator_loss=0.910831868648529, discriminator_loss=0.44244569540023804\n",
            "step 1009: generator_loss=0.9090325832366943, discriminator_loss=0.44170570373535156\n",
            "step 1010: generator_loss=0.9142001867294312, discriminator_loss=0.4412921667098999\n",
            "step 1011: generator_loss=0.8825310468673706, discriminator_loss=0.45358455181121826\n",
            "step 1012: generator_loss=0.9115235209465027, discriminator_loss=0.4425097703933716\n",
            "step 1013: generator_loss=0.9267037510871887, discriminator_loss=0.4378819465637207\n",
            "step 1014: generator_loss=0.943792462348938, discriminator_loss=0.43282902240753174\n",
            "step 1015: generator_loss=0.9602547883987427, discriminator_loss=0.42833200097084045\n",
            "step 1016: generator_loss=0.9580855369567871, discriminator_loss=0.4291120171546936\n",
            "step 1017: generator_loss=0.9747332334518433, discriminator_loss=0.4245840311050415\n",
            "step 1018: generator_loss=1.0104572772979736, discriminator_loss=0.41354501247406006\n",
            "step 1019: generator_loss=1.0234695672988892, discriminator_loss=0.41092225909233093\n",
            "step 1020: generator_loss=1.0476740598678589, discriminator_loss=0.40324535965919495\n",
            "step 1021: generator_loss=1.0225348472595215, discriminator_loss=0.4128020405769348\n",
            "step 1022: generator_loss=1.054145336151123, discriminator_loss=0.4046406149864197\n",
            "step 1023: generator_loss=1.100858211517334, discriminator_loss=0.3905576467514038\n",
            "step 1024: generator_loss=1.1132893562316895, discriminator_loss=0.3894720673561096\n",
            "step 1025: generator_loss=1.1439414024353027, discriminator_loss=0.38094180822372437\n",
            "step 1026: generator_loss=1.1495921611785889, discriminator_loss=0.3787803053855896\n",
            "step 1027: generator_loss=1.1707401275634766, discriminator_loss=0.3735576272010803\n",
            "step 1028: generator_loss=1.2140448093414307, discriminator_loss=0.3644448518753052\n",
            "step 1029: generator_loss=1.2205228805541992, discriminator_loss=0.3635205030441284\n",
            "step 1030: generator_loss=1.2373716831207275, discriminator_loss=0.3588496148586273\n",
            "step 1031: generator_loss=1.2525551319122314, discriminator_loss=0.35546088218688965\n",
            "step 1032: generator_loss=1.2836120128631592, discriminator_loss=0.34904584288597107\n",
            "step 1033: generator_loss=1.2975454330444336, discriminator_loss=0.3455345332622528\n",
            "step 1034: generator_loss=1.3226521015167236, discriminator_loss=0.34069928526878357\n",
            "step 1035: generator_loss=1.3363986015319824, discriminator_loss=0.33818912506103516\n",
            "step 1036: generator_loss=1.3374565839767456, discriminator_loss=0.3371855616569519\n",
            "step 1037: generator_loss=1.355513095855713, discriminator_loss=0.3343183398246765\n",
            "step 1038: generator_loss=1.3494975566864014, discriminator_loss=0.33442431688308716\n",
            "step 1039: generator_loss=1.3402835130691528, discriminator_loss=0.33623790740966797\n",
            "step 1040: generator_loss=1.3403081893920898, discriminator_loss=0.335281640291214\n",
            "step 1041: generator_loss=1.3452683687210083, discriminator_loss=0.3334832191467285\n",
            "step 1042: generator_loss=1.3293207883834839, discriminator_loss=0.3356242775917053\n",
            "step 1043: generator_loss=1.3291410207748413, discriminator_loss=0.33510124683380127\n",
            "step 1044: generator_loss=1.3259971141815186, discriminator_loss=0.3352808356285095\n",
            "step 1045: generator_loss=1.3022410869598389, discriminator_loss=0.3388604521751404\n",
            "step 1046: generator_loss=1.3111987113952637, discriminator_loss=0.33654409646987915\n",
            "step 1047: generator_loss=1.2927696704864502, discriminator_loss=0.3384707272052765\n",
            "step 1048: generator_loss=1.2618958950042725, discriminator_loss=0.3440651297569275\n",
            "step 1049: generator_loss=1.2762126922607422, discriminator_loss=0.34023401141166687\n",
            "step 1050: generator_loss=1.2662034034729004, discriminator_loss=0.3414531946182251\n",
            "step 1051: generator_loss=1.2436716556549072, discriminator_loss=0.34540531039237976\n",
            "step 1052: generator_loss=1.2331745624542236, discriminator_loss=0.34680142998695374\n",
            "step 1053: generator_loss=1.2072184085845947, discriminator_loss=0.35182178020477295\n",
            "step 1054: generator_loss=1.190955400466919, discriminator_loss=0.35536208748817444\n",
            "step 1055: generator_loss=1.170435905456543, discriminator_loss=0.35881125926971436\n",
            "step 1056: generator_loss=1.1465294361114502, discriminator_loss=0.3641819655895233\n",
            "step 1057: generator_loss=1.1470448970794678, discriminator_loss=0.36384880542755127\n",
            "step 1058: generator_loss=1.1320444345474243, discriminator_loss=0.3670153021812439\n",
            "step 1059: generator_loss=1.1296911239624023, discriminator_loss=0.36772090196609497\n",
            "step 1060: generator_loss=1.1210031509399414, discriminator_loss=0.36985641717910767\n",
            "step 1061: generator_loss=1.107344150543213, discriminator_loss=0.37278613448143005\n",
            "step 1062: generator_loss=1.1069856882095337, discriminator_loss=0.374400794506073\n",
            "step 1063: generator_loss=1.1034399271011353, discriminator_loss=0.3752827048301697\n",
            "step 1064: generator_loss=1.0855894088745117, discriminator_loss=0.3803755044937134\n",
            "step 1065: generator_loss=1.0670149326324463, discriminator_loss=0.38506513833999634\n",
            "step 1066: generator_loss=1.0667800903320312, discriminator_loss=0.3858603239059448\n",
            "step 1067: generator_loss=1.0505694150924683, discriminator_loss=0.39045965671539307\n",
            "step 1068: generator_loss=1.0562634468078613, discriminator_loss=0.3899831175804138\n",
            "step 1069: generator_loss=1.023817777633667, discriminator_loss=0.3991393744945526\n",
            "step 1070: generator_loss=1.0271390676498413, discriminator_loss=0.398967444896698\n",
            "step 1071: generator_loss=1.0155510902404785, discriminator_loss=0.4025366008281708\n",
            "step 1072: generator_loss=0.9980701208114624, discriminator_loss=0.40907177329063416\n",
            "step 1073: generator_loss=1.0049346685409546, discriminator_loss=0.40785229206085205\n",
            "step 1074: generator_loss=0.9965256452560425, discriminator_loss=0.41162529587745667\n",
            "step 1075: generator_loss=0.9837080240249634, discriminator_loss=0.41660773754119873\n",
            "step 1076: generator_loss=0.9848686456680298, discriminator_loss=0.416314959526062\n",
            "step 1077: generator_loss=0.9766273498535156, discriminator_loss=0.41993892192840576\n",
            "step 1078: generator_loss=0.9719820022583008, discriminator_loss=0.42148739099502563\n",
            "step 1079: generator_loss=0.9876598119735718, discriminator_loss=0.4173082113265991\n",
            "step 1080: generator_loss=0.9984365105628967, discriminator_loss=0.415080189704895\n",
            "step 1081: generator_loss=1.0303950309753418, discriminator_loss=0.40731868147850037\n",
            "step 1082: generator_loss=1.041668176651001, discriminator_loss=0.4032374620437622\n",
            "step 1083: generator_loss=1.0872678756713867, discriminator_loss=0.3918203115463257\n",
            "step 1084: generator_loss=1.095391035079956, discriminator_loss=0.38957488536834717\n",
            "step 1085: generator_loss=1.1216397285461426, discriminator_loss=0.3833841383457184\n",
            "step 1086: generator_loss=1.1505827903747559, discriminator_loss=0.37672558426856995\n",
            "step 1087: generator_loss=1.1862099170684814, discriminator_loss=0.369118869304657\n",
            "step 1088: generator_loss=1.221964716911316, discriminator_loss=0.3603843152523041\n",
            "step 1089: generator_loss=1.2223362922668457, discriminator_loss=0.35986408591270447\n",
            "step 1090: generator_loss=1.233290672302246, discriminator_loss=0.3565079867839813\n",
            "step 1091: generator_loss=1.2636357545852661, discriminator_loss=0.3505266010761261\n",
            "step 1092: generator_loss=1.2763842344284058, discriminator_loss=0.34729254245758057\n",
            "step 1093: generator_loss=1.294715166091919, discriminator_loss=0.34331950545310974\n",
            "step 1094: generator_loss=1.308597207069397, discriminator_loss=0.3402314782142639\n",
            "step 1095: generator_loss=1.3205451965332031, discriminator_loss=0.33691614866256714\n",
            "step 1096: generator_loss=1.3228075504302979, discriminator_loss=0.33545219898223877\n",
            "step 1097: generator_loss=1.3470475673675537, discriminator_loss=0.33047276735305786\n",
            "step 1098: generator_loss=1.3594392538070679, discriminator_loss=0.32733577489852905\n",
            "step 1099: generator_loss=1.3803000450134277, discriminator_loss=0.32291316986083984\n",
            "step 1100: generator_loss=1.3935699462890625, discriminator_loss=0.31984397768974304\n",
            "step 1101: generator_loss=1.3944063186645508, discriminator_loss=0.31911981105804443\n",
            "step 1102: generator_loss=1.4142065048217773, discriminator_loss=0.3153111934661865\n",
            "step 1103: generator_loss=1.411180019378662, discriminator_loss=0.3151220977306366\n",
            "step 1104: generator_loss=1.4047489166259766, discriminator_loss=0.31544047594070435\n",
            "step 1105: generator_loss=1.394228219985962, discriminator_loss=0.3162539601325989\n",
            "step 1106: generator_loss=1.383072853088379, discriminator_loss=0.31745755672454834\n",
            "step 1107: generator_loss=1.3588279485702515, discriminator_loss=0.32076287269592285\n",
            "step 1108: generator_loss=1.3449205160140991, discriminator_loss=0.3226425051689148\n",
            "step 1109: generator_loss=1.320949673652649, discriminator_loss=0.32623291015625\n",
            "step 1110: generator_loss=1.2911514043807983, discriminator_loss=0.3310835063457489\n",
            "step 1111: generator_loss=1.2727845907211304, discriminator_loss=0.3339528441429138\n",
            "step 1112: generator_loss=1.2501871585845947, discriminator_loss=0.33787524700164795\n",
            "step 1113: generator_loss=1.2331304550170898, discriminator_loss=0.3407123386859894\n",
            "step 1114: generator_loss=1.2129826545715332, discriminator_loss=0.34444713592529297\n",
            "step 1115: generator_loss=1.2000231742858887, discriminator_loss=0.34677547216415405\n",
            "step 1116: generator_loss=1.1859055757522583, discriminator_loss=0.34953826665878296\n",
            "step 1117: generator_loss=1.172980785369873, discriminator_loss=0.35218483209609985\n",
            "step 1118: generator_loss=1.1642793416976929, discriminator_loss=0.35390686988830566\n",
            "step 1119: generator_loss=1.1572003364562988, discriminator_loss=0.3554781377315521\n",
            "step 1120: generator_loss=1.1535840034484863, discriminator_loss=0.35633236169815063\n",
            "step 1121: generator_loss=1.1421277523040771, discriminator_loss=0.3589283227920532\n",
            "step 1122: generator_loss=1.1353881359100342, discriminator_loss=0.3605579137802124\n",
            "step 1123: generator_loss=1.129195213317871, discriminator_loss=0.36196357011795044\n",
            "step 1124: generator_loss=1.1226917505264282, discriminator_loss=0.3635793924331665\n",
            "step 1125: generator_loss=1.1130789518356323, discriminator_loss=0.36593565344810486\n",
            "step 1126: generator_loss=1.1130549907684326, discriminator_loss=0.3661111295223236\n",
            "step 1127: generator_loss=1.107993721961975, discriminator_loss=0.3675611913204193\n",
            "step 1128: generator_loss=1.1083645820617676, discriminator_loss=0.3676787316799164\n",
            "step 1129: generator_loss=1.1138830184936523, discriminator_loss=0.36650675535202026\n",
            "step 1130: generator_loss=1.1189517974853516, discriminator_loss=0.3656211495399475\n",
            "step 1131: generator_loss=1.1275382041931152, discriminator_loss=0.3636348247528076\n",
            "step 1132: generator_loss=1.1336498260498047, discriminator_loss=0.3624480664730072\n",
            "step 1133: generator_loss=1.1418931484222412, discriminator_loss=0.36079728603363037\n",
            "step 1134: generator_loss=1.1524477005004883, discriminator_loss=0.35863053798675537\n",
            "step 1135: generator_loss=1.152831792831421, discriminator_loss=0.3587130308151245\n",
            "step 1136: generator_loss=1.1536811590194702, discriminator_loss=0.358654260635376\n",
            "step 1137: generator_loss=1.1435552835464478, discriminator_loss=0.36104094982147217\n",
            "step 1138: generator_loss=1.1522743701934814, discriminator_loss=0.35930734872817993\n",
            "step 1139: generator_loss=1.149845838546753, discriminator_loss=0.3599553108215332\n",
            "step 1140: generator_loss=1.158665657043457, discriminator_loss=0.3581051230430603\n",
            "step 1141: generator_loss=1.1560614109039307, discriminator_loss=0.3587470054626465\n",
            "step 1142: generator_loss=1.156448245048523, discriminator_loss=0.3586702346801758\n",
            "step 1143: generator_loss=1.1582872867584229, discriminator_loss=0.3583943545818329\n",
            "step 1144: generator_loss=1.1592867374420166, discriminator_loss=0.3583272695541382\n",
            "step 1145: generator_loss=1.160933017730713, discriminator_loss=0.3582803010940552\n",
            "step 1146: generator_loss=1.1678860187530518, discriminator_loss=0.35698139667510986\n",
            "step 1147: generator_loss=1.1744977235794067, discriminator_loss=0.35584187507629395\n",
            "step 1148: generator_loss=1.1823211908340454, discriminator_loss=0.3544395864009857\n",
            "step 1149: generator_loss=1.1938793659210205, discriminator_loss=0.3522324562072754\n",
            "step 1150: generator_loss=1.2028565406799316, discriminator_loss=0.35065650939941406\n",
            "step 1151: generator_loss=1.21430504322052, discriminator_loss=0.34885483980178833\n",
            "step 1152: generator_loss=1.2223865985870361, discriminator_loss=0.3477576673030853\n",
            "step 1153: generator_loss=1.2287944555282593, discriminator_loss=0.347065269947052\n",
            "step 1154: generator_loss=1.2347291707992554, discriminator_loss=0.3464468717575073\n",
            "step 1155: generator_loss=1.2340939044952393, discriminator_loss=0.3472982943058014\n",
            "step 1156: generator_loss=1.2371366024017334, discriminator_loss=0.34738489985466003\n",
            "step 1157: generator_loss=1.234205961227417, discriminator_loss=0.34845662117004395\n",
            "step 1158: generator_loss=1.2331795692443848, discriminator_loss=0.349173903465271\n",
            "step 1159: generator_loss=1.2313945293426514, discriminator_loss=0.35003745555877686\n",
            "step 1160: generator_loss=1.2416801452636719, discriminator_loss=0.3482540249824524\n",
            "step 1161: generator_loss=1.2560333013534546, discriminator_loss=0.3455839157104492\n",
            "step 1162: generator_loss=1.275685429573059, discriminator_loss=0.34190833568573\n",
            "step 1163: generator_loss=1.2982701063156128, discriminator_loss=0.3378129005432129\n",
            "step 1164: generator_loss=1.3143662214279175, discriminator_loss=0.334977924823761\n",
            "step 1165: generator_loss=1.323306679725647, discriminator_loss=0.33318695425987244\n",
            "step 1166: generator_loss=1.3289180994033813, discriminator_loss=0.33216744661331177\n",
            "step 1167: generator_loss=1.3311316967010498, discriminator_loss=0.33154287934303284\n",
            "step 1168: generator_loss=1.3253767490386963, discriminator_loss=0.33209317922592163\n",
            "step 1169: generator_loss=1.315155029296875, discriminator_loss=0.3336523175239563\n",
            "step 1170: generator_loss=1.304753303527832, discriminator_loss=0.33516696095466614\n",
            "step 1171: generator_loss=1.2890948057174683, discriminator_loss=0.3377912938594818\n",
            "step 1172: generator_loss=1.27789306640625, discriminator_loss=0.33961498737335205\n",
            "step 1173: generator_loss=1.266363263130188, discriminator_loss=0.3415653109550476\n",
            "step 1174: generator_loss=1.2556126117706299, discriminator_loss=0.3433815836906433\n",
            "step 1175: generator_loss=1.240041971206665, discriminator_loss=0.3461959958076477\n",
            "step 1176: generator_loss=1.2355051040649414, discriminator_loss=0.34696611762046814\n",
            "step 1177: generator_loss=1.2297395467758179, discriminator_loss=0.34794050455093384\n",
            "step 1178: generator_loss=1.2195240259170532, discriminator_loss=0.3499460220336914\n",
            "step 1179: generator_loss=1.2064064741134644, discriminator_loss=0.352481871843338\n",
            "step 1180: generator_loss=1.1974456310272217, discriminator_loss=0.3543376922607422\n",
            "step 1181: generator_loss=1.1828618049621582, discriminator_loss=0.3574354648590088\n",
            "step 1182: generator_loss=1.173827886581421, discriminator_loss=0.3594150245189667\n",
            "step 1183: generator_loss=1.159926414489746, discriminator_loss=0.36253979802131653\n",
            "step 1184: generator_loss=1.1449609994888306, discriminator_loss=0.3659208416938782\n",
            "step 1185: generator_loss=1.133424997329712, discriminator_loss=0.36868003010749817\n",
            "step 1186: generator_loss=1.1204137802124023, discriminator_loss=0.37195897102355957\n",
            "step 1187: generator_loss=1.106374740600586, discriminator_loss=0.3757144808769226\n",
            "step 1188: generator_loss=1.098013162612915, discriminator_loss=0.3781788945198059\n",
            "step 1189: generator_loss=1.0911766290664673, discriminator_loss=0.38044363260269165\n",
            "step 1190: generator_loss=1.0795042514801025, discriminator_loss=0.3839668333530426\n",
            "step 1191: generator_loss=1.0750010013580322, discriminator_loss=0.38570553064346313\n",
            "step 1192: generator_loss=1.0731918811798096, discriminator_loss=0.38673514127731323\n",
            "step 1193: generator_loss=1.0685627460479736, discriminator_loss=0.3885561227798462\n",
            "step 1194: generator_loss=1.0636935234069824, discriminator_loss=0.3903515040874481\n",
            "step 1195: generator_loss=1.0623823404312134, discriminator_loss=0.39120614528656006\n",
            "step 1196: generator_loss=1.0617774724960327, discriminator_loss=0.3918178379535675\n",
            "step 1197: generator_loss=1.069476842880249, discriminator_loss=0.39024877548217773\n",
            "step 1198: generator_loss=1.0775761604309082, discriminator_loss=0.38850873708724976\n",
            "step 1199: generator_loss=1.0923199653625488, discriminator_loss=0.38511034846305847\n",
            "step 1200: generator_loss=1.1105717420578003, discriminator_loss=0.3810758590698242\n",
            "step 1201: generator_loss=1.1267313957214355, discriminator_loss=0.3774316906929016\n",
            "step 1202: generator_loss=1.1470187902450562, discriminator_loss=0.372999906539917\n",
            "step 1203: generator_loss=1.165355920791626, discriminator_loss=0.36889714002609253\n",
            "step 1204: generator_loss=1.1779539585113525, discriminator_loss=0.3663981556892395\n",
            "step 1205: generator_loss=1.1930580139160156, discriminator_loss=0.363291472196579\n",
            "step 1206: generator_loss=1.201905608177185, discriminator_loss=0.36166253685951233\n",
            "step 1207: generator_loss=1.2184514999389648, discriminator_loss=0.3582972288131714\n",
            "step 1208: generator_loss=1.2347491979599, discriminator_loss=0.35509389638900757\n",
            "step 1209: generator_loss=1.2454205751419067, discriminator_loss=0.3529115319252014\n",
            "step 1210: generator_loss=1.254730463027954, discriminator_loss=0.3509901165962219\n",
            "step 1211: generator_loss=1.2632066011428833, discriminator_loss=0.3493950664997101\n",
            "step 1212: generator_loss=1.2710952758789062, discriminator_loss=0.34792688488960266\n",
            "step 1213: generator_loss=1.2778992652893066, discriminator_loss=0.3467862606048584\n",
            "step 1214: generator_loss=1.285900354385376, discriminator_loss=0.34546971321105957\n",
            "step 1215: generator_loss=1.2893158197402954, discriminator_loss=0.3450719714164734\n",
            "step 1216: generator_loss=1.2941960096359253, discriminator_loss=0.3441814184188843\n",
            "step 1217: generator_loss=1.2976038455963135, discriminator_loss=0.3434864580631256\n",
            "step 1218: generator_loss=1.2984826564788818, discriminator_loss=0.34327438473701477\n",
            "step 1219: generator_loss=1.3024871349334717, discriminator_loss=0.3421754539012909\n",
            "step 1220: generator_loss=1.302823781967163, discriminator_loss=0.3417162299156189\n",
            "step 1221: generator_loss=1.3000624179840088, discriminator_loss=0.3418627083301544\n",
            "step 1222: generator_loss=1.2974958419799805, discriminator_loss=0.3416997790336609\n",
            "step 1223: generator_loss=1.2944257259368896, discriminator_loss=0.341726690530777\n",
            "step 1224: generator_loss=1.288381814956665, discriminator_loss=0.34220170974731445\n",
            "step 1225: generator_loss=1.281576156616211, discriminator_loss=0.34280622005462646\n",
            "step 1226: generator_loss=1.2778205871582031, discriminator_loss=0.3428516089916229\n",
            "step 1227: generator_loss=1.2697863578796387, discriminator_loss=0.3438093662261963\n",
            "step 1228: generator_loss=1.276982307434082, discriminator_loss=0.34163570404052734\n",
            "step 1229: generator_loss=1.2706921100616455, discriminator_loss=0.34210139513015747\n",
            "step 1230: generator_loss=1.2698209285736084, discriminator_loss=0.34152328968048096\n",
            "step 1231: generator_loss=1.2667087316513062, discriminator_loss=0.341439813375473\n",
            "step 1232: generator_loss=1.2602697610855103, discriminator_loss=0.3421155512332916\n",
            "step 1233: generator_loss=1.253007173538208, discriminator_loss=0.34293970465660095\n",
            "step 1234: generator_loss=1.241891860961914, discriminator_loss=0.3444417417049408\n",
            "step 1235: generator_loss=1.2384531497955322, discriminator_loss=0.344588965177536\n",
            "step 1236: generator_loss=1.2313525676727295, discriminator_loss=0.34567344188690186\n",
            "step 1237: generator_loss=1.2286702394485474, discriminator_loss=0.34606361389160156\n",
            "step 1238: generator_loss=1.2260239124298096, discriminator_loss=0.3464793562889099\n",
            "step 1239: generator_loss=1.2227058410644531, discriminator_loss=0.34709468483924866\n",
            "step 1240: generator_loss=1.2159548997879028, discriminator_loss=0.34840691089630127\n",
            "step 1241: generator_loss=1.2108738422393799, discriminator_loss=0.34943848848342896\n",
            "step 1242: generator_loss=1.1987330913543701, discriminator_loss=0.35194510221481323\n",
            "step 1243: generator_loss=1.1861436367034912, discriminator_loss=0.35449671745300293\n",
            "step 1244: generator_loss=1.178515076637268, discriminator_loss=0.35623711347579956\n",
            "step 1245: generator_loss=1.1645982265472412, discriminator_loss=0.35964512825012207\n",
            "step 1246: generator_loss=1.1496870517730713, discriminator_loss=0.36353379487991333\n",
            "step 1247: generator_loss=1.1300170421600342, discriminator_loss=0.36870086193084717\n",
            "step 1248: generator_loss=1.1101734638214111, discriminator_loss=0.37384018301963806\n",
            "step 1249: generator_loss=1.0929090976715088, discriminator_loss=0.37869974970817566\n",
            "step 1250: generator_loss=1.0736287832260132, discriminator_loss=0.38428956270217896\n",
            "step 1251: generator_loss=1.056151032447815, discriminator_loss=0.38961470127105713\n",
            "step 1252: generator_loss=1.0429506301879883, discriminator_loss=0.39398813247680664\n",
            "step 1253: generator_loss=1.0361979007720947, discriminator_loss=0.396767795085907\n",
            "step 1254: generator_loss=1.0358853340148926, discriminator_loss=0.3977205753326416\n",
            "step 1255: generator_loss=1.039963722229004, discriminator_loss=0.3975064754486084\n",
            "step 1256: generator_loss=1.0497443675994873, discriminator_loss=0.39564377069473267\n",
            "step 1257: generator_loss=1.0617315769195557, discriminator_loss=0.3932085633277893\n",
            "step 1258: generator_loss=1.0801347494125366, discriminator_loss=0.38914498686790466\n",
            "step 1259: generator_loss=1.1035834550857544, discriminator_loss=0.3836006224155426\n",
            "step 1260: generator_loss=1.130752444267273, discriminator_loss=0.377554327249527\n",
            "step 1261: generator_loss=1.15377676486969, discriminator_loss=0.3726075291633606\n",
            "step 1262: generator_loss=1.17197847366333, discriminator_loss=0.3687262535095215\n",
            "step 1263: generator_loss=1.1842694282531738, discriminator_loss=0.3660178780555725\n",
            "step 1264: generator_loss=1.1921021938323975, discriminator_loss=0.3643433749675751\n",
            "step 1265: generator_loss=1.1879475116729736, discriminator_loss=0.36551088094711304\n",
            "step 1266: generator_loss=1.196342945098877, discriminator_loss=0.36377599835395813\n",
            "step 1267: generator_loss=1.198921799659729, discriminator_loss=0.36367279291152954\n",
            "step 1268: generator_loss=1.2053189277648926, discriminator_loss=0.3623724579811096\n",
            "step 1269: generator_loss=1.2066210508346558, discriminator_loss=0.36224520206451416\n",
            "step 1270: generator_loss=1.202216625213623, discriminator_loss=0.36344730854034424\n",
            "step 1271: generator_loss=1.2134766578674316, discriminator_loss=0.36127012968063354\n",
            "step 1272: generator_loss=1.2316138744354248, discriminator_loss=0.35777270793914795\n",
            "step 1273: generator_loss=1.2287756204605103, discriminator_loss=0.3582225441932678\n",
            "step 1274: generator_loss=1.2372651100158691, discriminator_loss=0.3563491702079773\n",
            "step 1275: generator_loss=1.243578314781189, discriminator_loss=0.3545566201210022\n",
            "step 1276: generator_loss=1.2386517524719238, discriminator_loss=0.3550415635108948\n",
            "step 1277: generator_loss=1.228227138519287, discriminator_loss=0.35664841532707214\n",
            "step 1278: generator_loss=1.224637508392334, discriminator_loss=0.35652992129325867\n",
            "step 1279: generator_loss=1.2238407135009766, discriminator_loss=0.3559330701828003\n",
            "step 1280: generator_loss=1.2271994352340698, discriminator_loss=0.3546065390110016\n",
            "step 1281: generator_loss=1.2356975078582764, discriminator_loss=0.3520345687866211\n",
            "step 1282: generator_loss=1.255577802658081, discriminator_loss=0.34739693999290466\n",
            "step 1283: generator_loss=1.2825641632080078, discriminator_loss=0.34171533584594727\n",
            "step 1284: generator_loss=1.3116967678070068, discriminator_loss=0.33579033613204956\n",
            "step 1285: generator_loss=1.340677261352539, discriminator_loss=0.3300808370113373\n",
            "step 1286: generator_loss=1.3663220405578613, discriminator_loss=0.32530975341796875\n",
            "step 1287: generator_loss=1.3897147178649902, discriminator_loss=0.32099831104278564\n",
            "step 1288: generator_loss=1.4146922826766968, discriminator_loss=0.31650909781455994\n",
            "step 1289: generator_loss=1.4399774074554443, discriminator_loss=0.3119649589061737\n",
            "step 1290: generator_loss=1.4781156778335571, discriminator_loss=0.30554091930389404\n",
            "step 1291: generator_loss=1.5089008808135986, discriminator_loss=0.30021700263023376\n",
            "step 1292: generator_loss=1.535044550895691, discriminator_loss=0.2957109212875366\n",
            "step 1293: generator_loss=1.5600334405899048, discriminator_loss=0.291294127702713\n",
            "step 1294: generator_loss=1.5800862312316895, discriminator_loss=0.2877185046672821\n",
            "step 1295: generator_loss=1.5942785739898682, discriminator_loss=0.28490808606147766\n",
            "step 1296: generator_loss=1.6040188074111938, discriminator_loss=0.28279921412467957\n",
            "step 1297: generator_loss=1.614172101020813, discriminator_loss=0.2805410623550415\n",
            "step 1298: generator_loss=1.6201095581054688, discriminator_loss=0.2787228524684906\n",
            "step 1299: generator_loss=1.619644045829773, discriminator_loss=0.27775174379348755\n",
            "step 1300: generator_loss=1.612642526626587, discriminator_loss=0.2776031196117401\n",
            "step 1301: generator_loss=1.6087524890899658, discriminator_loss=0.27727749943733215\n",
            "step 1302: generator_loss=1.6013226509094238, discriminator_loss=0.27741217613220215\n",
            "step 1303: generator_loss=1.5914959907531738, discriminator_loss=0.2778646945953369\n",
            "step 1304: generator_loss=1.5822654962539673, discriminator_loss=0.278285950422287\n",
            "step 1305: generator_loss=1.5659549236297607, discriminator_loss=0.27948975563049316\n",
            "step 1306: generator_loss=1.5537374019622803, discriminator_loss=0.2802673578262329\n",
            "step 1307: generator_loss=1.5348869562149048, discriminator_loss=0.2819599509239197\n",
            "step 1308: generator_loss=1.5136117935180664, discriminator_loss=0.283986896276474\n",
            "step 1309: generator_loss=1.4868333339691162, discriminator_loss=0.2870410680770874\n",
            "step 1310: generator_loss=1.4609096050262451, discriminator_loss=0.2901744544506073\n",
            "step 1311: generator_loss=1.436112403869629, discriminator_loss=0.29352372884750366\n",
            "step 1312: generator_loss=1.4049899578094482, discriminator_loss=0.29790636897087097\n",
            "step 1313: generator_loss=1.3782392740249634, discriminator_loss=0.3018779158592224\n",
            "step 1314: generator_loss=1.3572824001312256, discriminator_loss=0.3050476312637329\n",
            "step 1315: generator_loss=1.3390710353851318, discriminator_loss=0.3079821467399597\n",
            "step 1316: generator_loss=1.3251807689666748, discriminator_loss=0.3102763295173645\n",
            "step 1317: generator_loss=1.3112294673919678, discriminator_loss=0.31274378299713135\n",
            "step 1318: generator_loss=1.2979391813278198, discriminator_loss=0.31508368253707886\n",
            "step 1319: generator_loss=1.283872365951538, discriminator_loss=0.3178950548171997\n",
            "step 1320: generator_loss=1.2731393575668335, discriminator_loss=0.3201361298561096\n",
            "step 1321: generator_loss=1.2655988931655884, discriminator_loss=0.3219088315963745\n",
            "step 1322: generator_loss=1.2565038204193115, discriminator_loss=0.324066162109375\n",
            "step 1323: generator_loss=1.2530772686004639, discriminator_loss=0.32515186071395874\n",
            "step 1324: generator_loss=1.2491308450698853, discriminator_loss=0.32616040110588074\n",
            "step 1325: generator_loss=1.2434784173965454, discriminator_loss=0.32764726877212524\n",
            "step 1326: generator_loss=1.2394421100616455, discriminator_loss=0.3287278413772583\n",
            "step 1327: generator_loss=1.2340338230133057, discriminator_loss=0.3301575481891632\n",
            "step 1328: generator_loss=1.2269054651260376, discriminator_loss=0.33193838596343994\n",
            "step 1329: generator_loss=1.2253143787384033, discriminator_loss=0.3326539993286133\n",
            "step 1330: generator_loss=1.2268946170806885, discriminator_loss=0.33273348212242126\n",
            "step 1331: generator_loss=1.2269058227539062, discriminator_loss=0.3332476317882538\n",
            "step 1332: generator_loss=1.2308069467544556, discriminator_loss=0.332890123128891\n",
            "step 1333: generator_loss=1.2375935316085815, discriminator_loss=0.33196306228637695\n",
            "step 1334: generator_loss=1.2473692893981934, discriminator_loss=0.3304798901081085\n",
            "step 1335: generator_loss=1.261335849761963, discriminator_loss=0.3280155658721924\n",
            "step 1336: generator_loss=1.2774293422698975, discriminator_loss=0.3254200220108032\n",
            "step 1337: generator_loss=1.2893965244293213, discriminator_loss=0.3236984610557556\n",
            "step 1338: generator_loss=1.3024100065231323, discriminator_loss=0.32172539830207825\n",
            "step 1339: generator_loss=1.3176662921905518, discriminator_loss=0.3193988502025604\n",
            "step 1340: generator_loss=1.3315531015396118, discriminator_loss=0.3173374831676483\n",
            "step 1341: generator_loss=1.347280740737915, discriminator_loss=0.31473851203918457\n",
            "step 1342: generator_loss=1.363600254058838, discriminator_loss=0.31218260526657104\n",
            "step 1343: generator_loss=1.3828554153442383, discriminator_loss=0.30887699127197266\n",
            "step 1344: generator_loss=1.3961882591247559, discriminator_loss=0.3066689968109131\n",
            "step 1345: generator_loss=1.4022595882415771, discriminator_loss=0.3053467571735382\n",
            "step 1346: generator_loss=1.4029247760772705, discriminator_loss=0.3048386871814728\n",
            "step 1347: generator_loss=1.3984169960021973, discriminator_loss=0.305210679769516\n",
            "step 1348: generator_loss=1.3901917934417725, discriminator_loss=0.30615270137786865\n",
            "step 1349: generator_loss=1.3743793964385986, discriminator_loss=0.3086148500442505\n",
            "step 1350: generator_loss=1.3578968048095703, discriminator_loss=0.3113745152950287\n",
            "step 1351: generator_loss=1.3376119136810303, discriminator_loss=0.3149360716342926\n",
            "step 1352: generator_loss=1.3220566511154175, discriminator_loss=0.3179051876068115\n",
            "step 1353: generator_loss=1.3033101558685303, discriminator_loss=0.3213953673839569\n",
            "step 1354: generator_loss=1.2865030765533447, discriminator_loss=0.32476115226745605\n",
            "step 1355: generator_loss=1.280343770980835, discriminator_loss=0.3259972929954529\n",
            "step 1356: generator_loss=1.275219202041626, discriminator_loss=0.32707011699676514\n",
            "step 1357: generator_loss=1.2819041013717651, discriminator_loss=0.3260241746902466\n",
            "step 1358: generator_loss=1.2965598106384277, discriminator_loss=0.3232553005218506\n",
            "step 1359: generator_loss=1.319983959197998, discriminator_loss=0.31888675689697266\n",
            "step 1360: generator_loss=1.3436700105667114, discriminator_loss=0.314680278301239\n",
            "step 1361: generator_loss=1.3796011209487915, discriminator_loss=0.30864202976226807\n",
            "step 1362: generator_loss=1.4082939624786377, discriminator_loss=0.3037380874156952\n",
            "step 1363: generator_loss=1.4414310455322266, discriminator_loss=0.2982938885688782\n",
            "step 1364: generator_loss=1.4780895709991455, discriminator_loss=0.29240190982818604\n",
            "step 1365: generator_loss=1.5195964574813843, discriminator_loss=0.28625375032424927\n",
            "step 1366: generator_loss=1.5517594814300537, discriminator_loss=0.28162717819213867\n",
            "step 1367: generator_loss=1.571296215057373, discriminator_loss=0.2785434424877167\n",
            "step 1368: generator_loss=1.5797570943832397, discriminator_loss=0.27703583240509033\n",
            "step 1369: generator_loss=1.604933738708496, discriminator_loss=0.27333885431289673\n",
            "step 1370: generator_loss=1.62221097946167, discriminator_loss=0.2707705497741699\n",
            "step 1371: generator_loss=1.6422255039215088, discriminator_loss=0.26782792806625366\n",
            "step 1372: generator_loss=1.6571142673492432, discriminator_loss=0.26567551493644714\n",
            "step 1373: generator_loss=1.6655343770980835, discriminator_loss=0.26436179876327515\n",
            "step 1374: generator_loss=1.6665375232696533, discriminator_loss=0.26377665996551514\n",
            "step 1375: generator_loss=1.6598622798919678, discriminator_loss=0.26427239179611206\n",
            "step 1376: generator_loss=1.6462721824645996, discriminator_loss=0.26553186774253845\n",
            "step 1377: generator_loss=1.6266734600067139, discriminator_loss=0.26744359731674194\n",
            "step 1378: generator_loss=1.6047070026397705, discriminator_loss=0.2696688771247864\n",
            "step 1379: generator_loss=1.578450322151184, discriminator_loss=0.2725892663002014\n",
            "step 1380: generator_loss=1.5466387271881104, discriminator_loss=0.27616727352142334\n",
            "step 1381: generator_loss=1.5139989852905273, discriminator_loss=0.2802686095237732\n",
            "step 1382: generator_loss=1.4721662998199463, discriminator_loss=0.2859577536582947\n",
            "step 1383: generator_loss=1.434172511100769, discriminator_loss=0.29144513607025146\n",
            "step 1384: generator_loss=1.4002145528793335, discriminator_loss=0.2965790629386902\n",
            "step 1385: generator_loss=1.3735377788543701, discriminator_loss=0.3007827401161194\n",
            "step 1386: generator_loss=1.3516247272491455, discriminator_loss=0.30440637469291687\n",
            "step 1387: generator_loss=1.32953679561615, discriminator_loss=0.3083685040473938\n",
            "step 1388: generator_loss=1.3094351291656494, discriminator_loss=0.3122347593307495\n",
            "step 1389: generator_loss=1.29345703125, discriminator_loss=0.315292090177536\n",
            "step 1390: generator_loss=1.2733771800994873, discriminator_loss=0.31918978691101074\n",
            "step 1391: generator_loss=1.2494275569915771, discriminator_loss=0.3243125081062317\n",
            "step 1392: generator_loss=1.230931043624878, discriminator_loss=0.3283705711364746\n",
            "step 1393: generator_loss=1.2222273349761963, discriminator_loss=0.33083993196487427\n",
            "step 1394: generator_loss=1.2239041328430176, discriminator_loss=0.33116722106933594\n",
            "step 1395: generator_loss=1.227964997291565, discriminator_loss=0.33105915784835815\n",
            "step 1396: generator_loss=1.2508213520050049, discriminator_loss=0.3271338641643524\n",
            "step 1397: generator_loss=1.2764661312103271, discriminator_loss=0.32264551520347595\n",
            "step 1398: generator_loss=1.2967051267623901, discriminator_loss=0.3195376992225647\n",
            "step 1399: generator_loss=1.3092496395111084, discriminator_loss=0.31784218549728394\n",
            "step 1400: generator_loss=1.3169807195663452, discriminator_loss=0.3170596957206726\n",
            "step 1401: generator_loss=1.3219484090805054, discriminator_loss=0.31689709424972534\n",
            "step 1402: generator_loss=1.3316307067871094, discriminator_loss=0.3157516121864319\n",
            "step 1403: generator_loss=1.3523595333099365, discriminator_loss=0.312730610370636\n",
            "step 1404: generator_loss=1.3767969608306885, discriminator_loss=0.3091045022010803\n",
            "step 1405: generator_loss=1.4017572402954102, discriminator_loss=0.305320143699646\n",
            "step 1406: generator_loss=1.428244709968567, discriminator_loss=0.3013741075992584\n",
            "step 1407: generator_loss=1.4476515054702759, discriminator_loss=0.2983880341053009\n",
            "step 1408: generator_loss=1.4613032341003418, discriminator_loss=0.2963753640651703\n",
            "step 1409: generator_loss=1.4744716882705688, discriminator_loss=0.2942548990249634\n",
            "step 1410: generator_loss=1.4859979152679443, discriminator_loss=0.2922508716583252\n",
            "step 1411: generator_loss=1.4899671077728271, discriminator_loss=0.29141247272491455\n",
            "step 1412: generator_loss=1.4849998950958252, discriminator_loss=0.2920805513858795\n",
            "step 1413: generator_loss=1.4791852235794067, discriminator_loss=0.2927713990211487\n",
            "step 1414: generator_loss=1.471569299697876, discriminator_loss=0.2939278185367584\n",
            "step 1415: generator_loss=1.4566665887832642, discriminator_loss=0.2962512969970703\n",
            "step 1416: generator_loss=1.4388831853866577, discriminator_loss=0.2989997863769531\n",
            "step 1417: generator_loss=1.426133394241333, discriminator_loss=0.30109119415283203\n",
            "step 1418: generator_loss=1.4145692586898804, discriminator_loss=0.30292457342147827\n",
            "step 1419: generator_loss=1.420506477355957, discriminator_loss=0.3018776774406433\n",
            "step 1420: generator_loss=1.4357085227966309, discriminator_loss=0.299088716506958\n",
            "step 1421: generator_loss=1.44838285446167, discriminator_loss=0.2968105673789978\n",
            "step 1422: generator_loss=1.4649484157562256, discriminator_loss=0.29379719495773315\n",
            "step 1423: generator_loss=1.4763562679290771, discriminator_loss=0.29142680764198303\n",
            "step 1424: generator_loss=1.4946246147155762, discriminator_loss=0.2881239056587219\n",
            "step 1425: generator_loss=1.5169548988342285, discriminator_loss=0.28425610065460205\n",
            "step 1426: generator_loss=1.5408306121826172, discriminator_loss=0.28024226427078247\n",
            "step 1427: generator_loss=1.5600090026855469, discriminator_loss=0.2769497334957123\n",
            "step 1428: generator_loss=1.5713257789611816, discriminator_loss=0.2746652364730835\n",
            "step 1429: generator_loss=1.5829977989196777, discriminator_loss=0.2722119092941284\n",
            "step 1430: generator_loss=1.5906383991241455, discriminator_loss=0.2704501152038574\n",
            "step 1431: generator_loss=1.586531639099121, discriminator_loss=0.2700628638267517\n",
            "step 1432: generator_loss=1.5768711566925049, discriminator_loss=0.2703733742237091\n",
            "step 1433: generator_loss=1.5621651411056519, discriminator_loss=0.2714751958847046\n",
            "step 1434: generator_loss=1.5442678928375244, discriminator_loss=0.273002028465271\n",
            "step 1435: generator_loss=1.5235304832458496, discriminator_loss=0.27512991428375244\n",
            "step 1436: generator_loss=1.4995757341384888, discriminator_loss=0.2778017818927765\n",
            "step 1437: generator_loss=1.4739959239959717, discriminator_loss=0.28097641468048096\n",
            "step 1438: generator_loss=1.453251838684082, discriminator_loss=0.2835272550582886\n",
            "step 1439: generator_loss=1.430861234664917, discriminator_loss=0.2865346074104309\n",
            "step 1440: generator_loss=1.4070777893066406, discriminator_loss=0.2899205684661865\n",
            "step 1441: generator_loss=1.3830137252807617, discriminator_loss=0.2936611771583557\n",
            "step 1442: generator_loss=1.3593199253082275, discriminator_loss=0.2975214123725891\n",
            "step 1443: generator_loss=1.335935115814209, discriminator_loss=0.3015071749687195\n",
            "step 1444: generator_loss=1.3126788139343262, discriminator_loss=0.3055226802825928\n",
            "step 1445: generator_loss=1.2904094457626343, discriminator_loss=0.30956757068634033\n",
            "step 1446: generator_loss=1.2738454341888428, discriminator_loss=0.31274840235710144\n",
            "step 1447: generator_loss=1.258707046508789, discriminator_loss=0.3157675266265869\n",
            "step 1448: generator_loss=1.2478276491165161, discriminator_loss=0.3180989623069763\n",
            "step 1449: generator_loss=1.2472316026687622, discriminator_loss=0.3182425796985626\n",
            "step 1450: generator_loss=1.2503724098205566, discriminator_loss=0.3179166316986084\n",
            "step 1451: generator_loss=1.2560932636260986, discriminator_loss=0.3171544373035431\n",
            "step 1452: generator_loss=1.2634155750274658, discriminator_loss=0.3160494565963745\n",
            "step 1453: generator_loss=1.269669532775879, discriminator_loss=0.3151649236679077\n",
            "step 1454: generator_loss=1.2747178077697754, discriminator_loss=0.3145293593406677\n",
            "step 1455: generator_loss=1.278257966041565, discriminator_loss=0.3142045736312866\n",
            "step 1456: generator_loss=1.2792203426361084, discriminator_loss=0.31431639194488525\n",
            "step 1457: generator_loss=1.2778948545455933, discriminator_loss=0.31511953473091125\n",
            "step 1458: generator_loss=1.2771426439285278, discriminator_loss=0.3158600926399231\n",
            "step 1459: generator_loss=1.2749884128570557, discriminator_loss=0.31672438979148865\n",
            "step 1460: generator_loss=1.2794067859649658, discriminator_loss=0.3164916932582855\n",
            "step 1461: generator_loss=1.2829142808914185, discriminator_loss=0.31618738174438477\n",
            "step 1462: generator_loss=1.2903704643249512, discriminator_loss=0.3152924180030823\n",
            "step 1463: generator_loss=1.297769546508789, discriminator_loss=0.314475953578949\n",
            "step 1464: generator_loss=1.3185220956802368, discriminator_loss=0.31109941005706787\n",
            "step 1465: generator_loss=1.335357904434204, discriminator_loss=0.3086928129196167\n",
            "step 1466: generator_loss=1.3631956577301025, discriminator_loss=0.30409497022628784\n",
            "step 1467: generator_loss=1.385583758354187, discriminator_loss=0.30065444111824036\n",
            "step 1468: generator_loss=1.405031681060791, discriminator_loss=0.29772841930389404\n",
            "step 1469: generator_loss=1.4414751529693604, discriminator_loss=0.29219168424606323\n",
            "step 1470: generator_loss=1.4746408462524414, discriminator_loss=0.28739818930625916\n",
            "step 1471: generator_loss=1.5163068771362305, discriminator_loss=0.2813682556152344\n",
            "step 1472: generator_loss=1.5606399774551392, discriminator_loss=0.27570414543151855\n",
            "step 1473: generator_loss=1.5869245529174805, discriminator_loss=0.27178874611854553\n",
            "step 1474: generator_loss=1.6282286643981934, discriminator_loss=0.2673853635787964\n",
            "step 1475: generator_loss=1.6379117965698242, discriminator_loss=0.2651546597480774\n",
            "step 1476: generator_loss=1.6570332050323486, discriminator_loss=0.2624094784259796\n",
            "step 1477: generator_loss=1.6827783584594727, discriminator_loss=0.25926560163497925\n",
            "step 1478: generator_loss=1.704278826713562, discriminator_loss=0.2570127248764038\n",
            "step 1479: generator_loss=1.6905962228775024, discriminator_loss=0.257251113653183\n",
            "step 1480: generator_loss=1.6975390911102295, discriminator_loss=0.25625601410865784\n",
            "step 1481: generator_loss=1.685019612312317, discriminator_loss=0.25677821040153503\n",
            "step 1482: generator_loss=1.6726839542388916, discriminator_loss=0.2573566734790802\n",
            "step 1483: generator_loss=1.6593482494354248, discriminator_loss=0.2579302191734314\n",
            "step 1484: generator_loss=1.6471154689788818, discriminator_loss=0.25846076011657715\n",
            "step 1485: generator_loss=1.6383063793182373, discriminator_loss=0.25866562128067017\n",
            "step 1486: generator_loss=1.6161683797836304, discriminator_loss=0.2605586647987366\n",
            "step 1487: generator_loss=1.6026471853256226, discriminator_loss=0.2615445852279663\n",
            "step 1488: generator_loss=1.5793323516845703, discriminator_loss=0.26379215717315674\n",
            "step 1489: generator_loss=1.5573105812072754, discriminator_loss=0.2661631405353546\n",
            "step 1490: generator_loss=1.5360760688781738, discriminator_loss=0.2683921754360199\n",
            "step 1491: generator_loss=1.517966866493225, discriminator_loss=0.2703278064727783\n",
            "step 1492: generator_loss=1.5085912942886353, discriminator_loss=0.27115902304649353\n",
            "step 1493: generator_loss=1.4833884239196777, discriminator_loss=0.2742508053779602\n",
            "step 1494: generator_loss=1.4829888343811035, discriminator_loss=0.2741861343383789\n",
            "step 1495: generator_loss=1.491422414779663, discriminator_loss=0.2724579870700836\n",
            "step 1496: generator_loss=1.4873945713043213, discriminator_loss=0.27197903394699097\n",
            "step 1497: generator_loss=1.5077261924743652, discriminator_loss=0.2686123549938202\n",
            "step 1498: generator_loss=1.5427448749542236, discriminator_loss=0.2635977864265442\n",
            "step 1499: generator_loss=1.5615348815917969, discriminator_loss=0.2606329321861267\n",
            "step 1500: generator_loss=1.585237741470337, discriminator_loss=0.2570292353630066\n",
            "step 1501: generator_loss=1.6052618026733398, discriminator_loss=0.25422170758247375\n",
            "step 1502: generator_loss=1.6244118213653564, discriminator_loss=0.2517229914665222\n",
            "step 1503: generator_loss=1.6383020877838135, discriminator_loss=0.24997416138648987\n",
            "step 1504: generator_loss=1.6434863805770874, discriminator_loss=0.2492542564868927\n",
            "step 1505: generator_loss=1.6487561464309692, discriminator_loss=0.24859261512756348\n",
            "step 1506: generator_loss=1.6481318473815918, discriminator_loss=0.2486587017774582\n",
            "step 1507: generator_loss=1.6481014490127563, discriminator_loss=0.2486516237258911\n",
            "step 1508: generator_loss=1.640621304512024, discriminator_loss=0.24942921102046967\n",
            "step 1509: generator_loss=1.6264119148254395, discriminator_loss=0.25116732716560364\n",
            "step 1510: generator_loss=1.6088416576385498, discriminator_loss=0.253174364566803\n",
            "step 1511: generator_loss=1.5832301378250122, discriminator_loss=0.2563585042953491\n",
            "step 1512: generator_loss=1.5777077674865723, discriminator_loss=0.2569352686405182\n",
            "step 1513: generator_loss=1.5554137229919434, discriminator_loss=0.2596849799156189\n",
            "step 1514: generator_loss=1.52899169921875, discriminator_loss=0.2632324993610382\n",
            "step 1515: generator_loss=1.5149704217910767, discriminator_loss=0.2649684548377991\n",
            "step 1516: generator_loss=1.4926719665527344, discriminator_loss=0.2679632306098938\n",
            "step 1517: generator_loss=1.4800268411636353, discriminator_loss=0.2697647213935852\n",
            "step 1518: generator_loss=1.454681158065796, discriminator_loss=0.27326929569244385\n",
            "step 1519: generator_loss=1.4458250999450684, discriminator_loss=0.274537593126297\n",
            "step 1520: generator_loss=1.4484126567840576, discriminator_loss=0.27401480078697205\n",
            "step 1521: generator_loss=1.4357497692108154, discriminator_loss=0.27603715658187866\n",
            "step 1522: generator_loss=1.4373414516448975, discriminator_loss=0.2756160497665405\n",
            "step 1523: generator_loss=1.4420373439788818, discriminator_loss=0.27506452798843384\n",
            "step 1524: generator_loss=1.4258875846862793, discriminator_loss=0.27769050002098083\n",
            "step 1525: generator_loss=1.4218919277191162, discriminator_loss=0.2783120274543762\n",
            "step 1526: generator_loss=1.401239037513733, discriminator_loss=0.28207364678382874\n",
            "step 1527: generator_loss=1.4050099849700928, discriminator_loss=0.2816331684589386\n",
            "step 1528: generator_loss=1.3953431844711304, discriminator_loss=0.28354382514953613\n",
            "step 1529: generator_loss=1.3845099210739136, discriminator_loss=0.2860047221183777\n",
            "step 1530: generator_loss=1.392073154449463, discriminator_loss=0.28455138206481934\n",
            "step 1531: generator_loss=1.3845434188842773, discriminator_loss=0.28602153062820435\n",
            "step 1532: generator_loss=1.383697271347046, discriminator_loss=0.2865331172943115\n",
            "step 1533: generator_loss=1.4028851985931396, discriminator_loss=0.2831646203994751\n",
            "step 1534: generator_loss=1.3992115259170532, discriminator_loss=0.28438088297843933\n",
            "step 1535: generator_loss=1.4018466472625732, discriminator_loss=0.28420722484588623\n",
            "step 1536: generator_loss=1.4245049953460693, discriminator_loss=0.2806260585784912\n",
            "step 1537: generator_loss=1.4310234785079956, discriminator_loss=0.27986305952072144\n",
            "step 1538: generator_loss=1.445948839187622, discriminator_loss=0.27752095460891724\n",
            "step 1539: generator_loss=1.458413004875183, discriminator_loss=0.27556493878364563\n",
            "step 1540: generator_loss=1.4804699420928955, discriminator_loss=0.2726064920425415\n",
            "step 1541: generator_loss=1.501619577407837, discriminator_loss=0.26974159479141235\n",
            "step 1542: generator_loss=1.5337584018707275, discriminator_loss=0.26541396975517273\n",
            "step 1543: generator_loss=1.5452237129211426, discriminator_loss=0.2638283371925354\n",
            "step 1544: generator_loss=1.5781922340393066, discriminator_loss=0.2594883441925049\n",
            "step 1545: generator_loss=1.593957781791687, discriminator_loss=0.2578972578048706\n",
            "step 1546: generator_loss=1.5855296850204468, discriminator_loss=0.25885501503944397\n",
            "step 1547: generator_loss=1.5908945798873901, discriminator_loss=0.25802475214004517\n",
            "step 1548: generator_loss=1.582147479057312, discriminator_loss=0.2594246566295624\n",
            "step 1549: generator_loss=1.596577763557434, discriminator_loss=0.2573612630367279\n",
            "step 1550: generator_loss=1.591854214668274, discriminator_loss=0.25798553228378296\n",
            "step 1551: generator_loss=1.585514783859253, discriminator_loss=0.25885578989982605\n",
            "step 1552: generator_loss=1.575799822807312, discriminator_loss=0.25970277190208435\n",
            "step 1553: generator_loss=1.5693657398223877, discriminator_loss=0.26049694418907166\n",
            "step 1554: generator_loss=1.5585132837295532, discriminator_loss=0.26196956634521484\n",
            "step 1555: generator_loss=1.5702272653579712, discriminator_loss=0.2605877220630646\n",
            "step 1556: generator_loss=1.5775244235992432, discriminator_loss=0.2592376470565796\n",
            "step 1557: generator_loss=1.5481867790222168, discriminator_loss=0.26314854621887207\n",
            "step 1558: generator_loss=1.5606322288513184, discriminator_loss=0.2614377737045288\n",
            "step 1559: generator_loss=1.553924322128296, discriminator_loss=0.2620151937007904\n",
            "step 1560: generator_loss=1.5534169673919678, discriminator_loss=0.2620288133621216\n",
            "step 1561: generator_loss=1.5321083068847656, discriminator_loss=0.26489728689193726\n",
            "step 1562: generator_loss=1.528695821762085, discriminator_loss=0.26509881019592285\n",
            "step 1563: generator_loss=1.5419684648513794, discriminator_loss=0.2631528377532959\n",
            "step 1564: generator_loss=1.5298370122909546, discriminator_loss=0.26460421085357666\n",
            "step 1565: generator_loss=1.532615303993225, discriminator_loss=0.2639123201370239\n",
            "step 1566: generator_loss=1.5027483701705933, discriminator_loss=0.267994225025177\n",
            "step 1567: generator_loss=1.5003695487976074, discriminator_loss=0.26850205659866333\n",
            "step 1568: generator_loss=1.4937105178833008, discriminator_loss=0.26912930607795715\n",
            "step 1569: generator_loss=1.488381266593933, discriminator_loss=0.2695643901824951\n",
            "step 1570: generator_loss=1.4852465391159058, discriminator_loss=0.2699343264102936\n",
            "step 1571: generator_loss=1.476845145225525, discriminator_loss=0.27117329835891724\n",
            "step 1572: generator_loss=1.48122239112854, discriminator_loss=0.27074164152145386\n",
            "step 1573: generator_loss=1.493187427520752, discriminator_loss=0.26901137828826904\n",
            "step 1574: generator_loss=1.4905871152877808, discriminator_loss=0.2692564129829407\n",
            "step 1575: generator_loss=1.4950422048568726, discriminator_loss=0.2684893310070038\n",
            "step 1576: generator_loss=1.4843212366104126, discriminator_loss=0.2698938250541687\n",
            "step 1577: generator_loss=1.4798715114593506, discriminator_loss=0.27037250995635986\n",
            "step 1578: generator_loss=1.4777507781982422, discriminator_loss=0.27044981718063354\n",
            "step 1579: generator_loss=1.4666199684143066, discriminator_loss=0.27182602882385254\n",
            "step 1580: generator_loss=1.4492201805114746, discriminator_loss=0.27418607473373413\n",
            "step 1581: generator_loss=1.4351708889007568, discriminator_loss=0.27608826756477356\n",
            "step 1582: generator_loss=1.4247876405715942, discriminator_loss=0.2776273190975189\n",
            "step 1583: generator_loss=1.4153369665145874, discriminator_loss=0.2790679633617401\n",
            "step 1584: generator_loss=1.4130408763885498, discriminator_loss=0.2794284224510193\n",
            "step 1585: generator_loss=1.4154648780822754, discriminator_loss=0.2790681719779968\n",
            "step 1586: generator_loss=1.4226964712142944, discriminator_loss=0.2779526114463806\n",
            "step 1587: generator_loss=1.4400570392608643, discriminator_loss=0.2753063440322876\n",
            "step 1588: generator_loss=1.4537827968597412, discriminator_loss=0.27328604459762573\n",
            "step 1589: generator_loss=1.4724609851837158, discriminator_loss=0.2704147696495056\n",
            "step 1590: generator_loss=1.4808201789855957, discriminator_loss=0.2691844701766968\n",
            "step 1591: generator_loss=1.4874227046966553, discriminator_loss=0.2683008313179016\n",
            "step 1592: generator_loss=1.4917606115341187, discriminator_loss=0.2677176594734192\n",
            "step 1593: generator_loss=1.4951860904693604, discriminator_loss=0.26727205514907837\n",
            "step 1594: generator_loss=1.4916212558746338, discriminator_loss=0.2679222822189331\n",
            "step 1595: generator_loss=1.4814379215240479, discriminator_loss=0.2695135474205017\n",
            "step 1596: generator_loss=1.4643807411193848, discriminator_loss=0.27231305837631226\n",
            "step 1597: generator_loss=1.4724295139312744, discriminator_loss=0.2712719738483429\n",
            "step 1598: generator_loss=1.4804368019104004, discriminator_loss=0.2702008783817291\n",
            "step 1599: generator_loss=1.4855197668075562, discriminator_loss=0.2695814371109009\n",
            "step 1600: generator_loss=1.4861228466033936, discriminator_loss=0.26950323581695557\n",
            "step 1601: generator_loss=1.4831334352493286, discriminator_loss=0.2700396180152893\n",
            "step 1602: generator_loss=1.4778039455413818, discriminator_loss=0.2709294557571411\n",
            "step 1603: generator_loss=1.462134838104248, discriminator_loss=0.27345800399780273\n",
            "step 1604: generator_loss=1.4563078880310059, discriminator_loss=0.2746261954307556\n",
            "step 1605: generator_loss=1.4459229707717896, discriminator_loss=0.2762775719165802\n",
            "step 1606: generator_loss=1.4391825199127197, discriminator_loss=0.27751263976097107\n",
            "step 1607: generator_loss=1.436769723892212, discriminator_loss=0.2780749797821045\n",
            "step 1608: generator_loss=1.4436547756195068, discriminator_loss=0.27717819809913635\n",
            "step 1609: generator_loss=1.4643478393554688, discriminator_loss=0.27421486377716064\n",
            "step 1610: generator_loss=1.4924468994140625, discriminator_loss=0.2702692151069641\n",
            "step 1611: generator_loss=1.515146255493164, discriminator_loss=0.2672291696071625\n",
            "step 1612: generator_loss=1.5316894054412842, discriminator_loss=0.2650804817676544\n",
            "step 1613: generator_loss=1.5421290397644043, discriminator_loss=0.26383429765701294\n",
            "step 1614: generator_loss=1.5518066883087158, discriminator_loss=0.26258987188339233\n",
            "step 1615: generator_loss=1.5539000034332275, discriminator_loss=0.26228928565979004\n",
            "step 1616: generator_loss=1.554316520690918, discriminator_loss=0.2622467577457428\n",
            "step 1617: generator_loss=1.5620120763778687, discriminator_loss=0.261194109916687\n",
            "step 1618: generator_loss=1.5730465650558472, discriminator_loss=0.259728342294693\n",
            "step 1619: generator_loss=1.5798563957214355, discriminator_loss=0.2588750123977661\n",
            "step 1620: generator_loss=1.5790040493011475, discriminator_loss=0.25888168811798096\n",
            "step 1621: generator_loss=1.5711781978607178, discriminator_loss=0.2600017786026001\n",
            "step 1622: generator_loss=1.558293104171753, discriminator_loss=0.2616179287433624\n",
            "step 1623: generator_loss=1.5398902893066406, discriminator_loss=0.264009952545166\n",
            "step 1624: generator_loss=1.5119690895080566, discriminator_loss=0.2677990198135376\n",
            "step 1625: generator_loss=1.4757111072540283, discriminator_loss=0.2729578912258148\n",
            "step 1626: generator_loss=1.4345176219940186, discriminator_loss=0.2792730927467346\n",
            "step 1627: generator_loss=1.4018824100494385, discriminator_loss=0.2846142649650574\n",
            "step 1628: generator_loss=1.377229928970337, discriminator_loss=0.28899142146110535\n",
            "step 1629: generator_loss=1.3563182353973389, discriminator_loss=0.29300540685653687\n",
            "step 1630: generator_loss=1.3471022844314575, discriminator_loss=0.2950167655944824\n",
            "step 1631: generator_loss=1.3420131206512451, discriminator_loss=0.29645031690597534\n",
            "step 1632: generator_loss=1.3449811935424805, discriminator_loss=0.2965371012687683\n",
            "step 1633: generator_loss=1.3497064113616943, discriminator_loss=0.29623082280158997\n",
            "step 1634: generator_loss=1.3712081909179688, discriminator_loss=0.293174147605896\n",
            "step 1635: generator_loss=1.4133410453796387, discriminator_loss=0.2866820693016052\n",
            "step 1636: generator_loss=1.4541049003601074, discriminator_loss=0.28062957525253296\n",
            "step 1637: generator_loss=1.491401195526123, discriminator_loss=0.2753514051437378\n",
            "step 1638: generator_loss=1.5330756902694702, discriminator_loss=0.2695571482181549\n",
            "step 1639: generator_loss=1.5710821151733398, discriminator_loss=0.26430878043174744\n",
            "step 1640: generator_loss=1.606084942817688, discriminator_loss=0.2597047984600067\n",
            "step 1641: generator_loss=1.6422709226608276, discriminator_loss=0.255291610956192\n",
            "step 1642: generator_loss=1.6686725616455078, discriminator_loss=0.2518618106842041\n",
            "step 1643: generator_loss=1.6864787340164185, discriminator_loss=0.24956965446472168\n",
            "step 1644: generator_loss=1.6979477405548096, discriminator_loss=0.2480563521385193\n",
            "step 1645: generator_loss=1.69417142868042, discriminator_loss=0.24789494276046753\n",
            "step 1646: generator_loss=1.6858354806900024, discriminator_loss=0.2481861412525177\n",
            "step 1647: generator_loss=1.6726093292236328, discriminator_loss=0.24908192455768585\n",
            "step 1648: generator_loss=1.6410561800003052, discriminator_loss=0.2520923614501953\n",
            "step 1649: generator_loss=1.6114023923873901, discriminator_loss=0.255016028881073\n",
            "step 1650: generator_loss=1.5756199359893799, discriminator_loss=0.2589578330516815\n",
            "step 1651: generator_loss=1.5345706939697266, discriminator_loss=0.26375043392181396\n",
            "step 1652: generator_loss=1.5016961097717285, discriminator_loss=0.2678491473197937\n",
            "step 1653: generator_loss=1.4710885286331177, discriminator_loss=0.27192223072052\n",
            "step 1654: generator_loss=1.4460415840148926, discriminator_loss=0.27547499537467957\n",
            "step 1655: generator_loss=1.4253408908843994, discriminator_loss=0.2785528302192688\n",
            "step 1656: generator_loss=1.4036730527877808, discriminator_loss=0.2819734811782837\n",
            "step 1657: generator_loss=1.3805105686187744, discriminator_loss=0.28577572107315063\n",
            "step 1658: generator_loss=1.3636316061019897, discriminator_loss=0.28882351517677307\n",
            "step 1659: generator_loss=1.3567066192626953, discriminator_loss=0.29020124673843384\n",
            "step 1660: generator_loss=1.3548071384429932, discriminator_loss=0.2907099723815918\n",
            "step 1661: generator_loss=1.3631881475448608, discriminator_loss=0.28949645161628723\n",
            "step 1662: generator_loss=1.371886968612671, discriminator_loss=0.28819894790649414\n",
            "step 1663: generator_loss=1.3884949684143066, discriminator_loss=0.2857307195663452\n",
            "step 1664: generator_loss=1.40571928024292, discriminator_loss=0.28323128819465637\n",
            "step 1665: generator_loss=1.4237254858016968, discriminator_loss=0.28078770637512207\n",
            "step 1666: generator_loss=1.4345215559005737, discriminator_loss=0.2795068323612213\n",
            "step 1667: generator_loss=1.4452457427978516, discriminator_loss=0.2780812382698059\n",
            "step 1668: generator_loss=1.4616594314575195, discriminator_loss=0.27601468563079834\n",
            "step 1669: generator_loss=1.4749164581298828, discriminator_loss=0.27437859773635864\n",
            "step 1670: generator_loss=1.482094168663025, discriminator_loss=0.27357733249664307\n",
            "step 1671: generator_loss=1.4906706809997559, discriminator_loss=0.2727145850658417\n",
            "step 1672: generator_loss=1.5021557807922363, discriminator_loss=0.2714432179927826\n",
            "step 1673: generator_loss=1.5112605094909668, discriminator_loss=0.27054691314697266\n",
            "step 1674: generator_loss=1.5140794515609741, discriminator_loss=0.27052512764930725\n",
            "step 1675: generator_loss=1.5155396461486816, discriminator_loss=0.2703632712364197\n",
            "step 1676: generator_loss=1.5212898254394531, discriminator_loss=0.26954221725463867\n",
            "step 1677: generator_loss=1.5338363647460938, discriminator_loss=0.2676289677619934\n",
            "step 1678: generator_loss=1.5460853576660156, discriminator_loss=0.26578372716903687\n",
            "step 1679: generator_loss=1.5580354928970337, discriminator_loss=0.26391318440437317\n",
            "step 1680: generator_loss=1.5718438625335693, discriminator_loss=0.2619059085845947\n",
            "step 1681: generator_loss=1.5854823589324951, discriminator_loss=0.259931743144989\n",
            "step 1682: generator_loss=1.5691848993301392, discriminator_loss=0.26177117228507996\n",
            "step 1683: generator_loss=1.5671378374099731, discriminator_loss=0.2617088258266449\n",
            "step 1684: generator_loss=1.5626269578933716, discriminator_loss=0.26207852363586426\n",
            "step 1685: generator_loss=1.5557446479797363, discriminator_loss=0.26265186071395874\n",
            "step 1686: generator_loss=1.5457963943481445, discriminator_loss=0.2636782228946686\n",
            "step 1687: generator_loss=1.5293771028518677, discriminator_loss=0.2655029594898224\n",
            "step 1688: generator_loss=1.5162601470947266, discriminator_loss=0.2668461203575134\n",
            "step 1689: generator_loss=1.4936621189117432, discriminator_loss=0.2699013948440552\n",
            "step 1690: generator_loss=1.4914188385009766, discriminator_loss=0.2703717052936554\n",
            "step 1691: generator_loss=1.4669221639633179, discriminator_loss=0.27313846349716187\n",
            "step 1692: generator_loss=1.4577791690826416, discriminator_loss=0.27397841215133667\n",
            "step 1693: generator_loss=1.4413714408874512, discriminator_loss=0.27610257267951965\n",
            "step 1694: generator_loss=1.4263478517532349, discriminator_loss=0.2782307267189026\n",
            "step 1695: generator_loss=1.4177734851837158, discriminator_loss=0.2796263098716736\n",
            "step 1696: generator_loss=1.4105194807052612, discriminator_loss=0.2801728844642639\n",
            "step 1697: generator_loss=1.4150769710540771, discriminator_loss=0.2792724668979645\n",
            "step 1698: generator_loss=1.42020583152771, discriminator_loss=0.27837106585502625\n",
            "step 1699: generator_loss=1.444040298461914, discriminator_loss=0.2745409309864044\n",
            "step 1700: generator_loss=1.459782361984253, discriminator_loss=0.2721189260482788\n",
            "step 1701: generator_loss=1.4931669235229492, discriminator_loss=0.26715004444122314\n",
            "step 1702: generator_loss=1.5373852252960205, discriminator_loss=0.2608093023300171\n",
            "step 1703: generator_loss=1.5798184871673584, discriminator_loss=0.2551022171974182\n",
            "step 1704: generator_loss=1.653379201889038, discriminator_loss=0.24583932757377625\n",
            "step 1705: generator_loss=1.6950327157974243, discriminator_loss=0.24097415804862976\n",
            "step 1706: generator_loss=1.752564549446106, discriminator_loss=0.23444177210330963\n",
            "step 1707: generator_loss=1.7502613067626953, discriminator_loss=0.23459969460964203\n",
            "step 1708: generator_loss=1.7612321376800537, discriminator_loss=0.23320603370666504\n",
            "step 1709: generator_loss=1.7485718727111816, discriminator_loss=0.2342304289340973\n",
            "step 1710: generator_loss=1.7600040435791016, discriminator_loss=0.23236584663391113\n",
            "step 1711: generator_loss=1.7676693201065063, discriminator_loss=0.23108243942260742\n",
            "step 1712: generator_loss=1.7262459993362427, discriminator_loss=0.2350202351808548\n",
            "step 1713: generator_loss=1.674041509628296, discriminator_loss=0.24113108217716217\n",
            "step 1714: generator_loss=1.5981826782226562, discriminator_loss=0.2506556510925293\n",
            "step 1715: generator_loss=1.5796657800674438, discriminator_loss=0.25306856632232666\n",
            "step 1716: generator_loss=1.5008889436721802, discriminator_loss=0.2643636465072632\n",
            "step 1717: generator_loss=1.5555338859558105, discriminator_loss=0.2555103302001953\n",
            "step 1718: generator_loss=1.4704042673110962, discriminator_loss=0.26840412616729736\n",
            "step 1719: generator_loss=1.453894853591919, discriminator_loss=0.2703629732131958\n",
            "step 1720: generator_loss=1.4267940521240234, discriminator_loss=0.2754526734352112\n",
            "step 1721: generator_loss=1.4521963596343994, discriminator_loss=0.2708711624145508\n",
            "step 1722: generator_loss=1.4348022937774658, discriminator_loss=0.2738554775714874\n",
            "step 1723: generator_loss=1.4467289447784424, discriminator_loss=0.2718043625354767\n",
            "step 1724: generator_loss=1.4496054649353027, discriminator_loss=0.2709087133407593\n",
            "step 1725: generator_loss=1.5140618085861206, discriminator_loss=0.26096734404563904\n",
            "step 1726: generator_loss=1.5069624185562134, discriminator_loss=0.2624223232269287\n",
            "step 1727: generator_loss=1.5741666555404663, discriminator_loss=0.25254589319229126\n",
            "step 1728: generator_loss=1.5509063005447388, discriminator_loss=0.2565785050392151\n",
            "step 1729: generator_loss=1.5706559419631958, discriminator_loss=0.2545546293258667\n",
            "step 1730: generator_loss=1.6299803256988525, discriminator_loss=0.24702811241149902\n",
            "step 1731: generator_loss=1.6167876720428467, discriminator_loss=0.24948431551456451\n",
            "step 1732: generator_loss=1.6503686904907227, discriminator_loss=0.2453286051750183\n",
            "step 1733: generator_loss=1.655442476272583, discriminator_loss=0.24478326737880707\n",
            "step 1734: generator_loss=1.6957261562347412, discriminator_loss=0.24067683517932892\n",
            "step 1735: generator_loss=1.6993924379348755, discriminator_loss=0.24079757928848267\n",
            "step 1736: generator_loss=1.6513140201568604, discriminator_loss=0.24661396443843842\n",
            "step 1737: generator_loss=1.6696512699127197, discriminator_loss=0.24384848773479462\n",
            "step 1738: generator_loss=1.6364047527313232, discriminator_loss=0.24828171730041504\n",
            "step 1739: generator_loss=1.591118335723877, discriminator_loss=0.25434911251068115\n",
            "step 1740: generator_loss=1.6283531188964844, discriminator_loss=0.24898311495780945\n",
            "step 1741: generator_loss=1.6127309799194336, discriminator_loss=0.2506198287010193\n",
            "step 1742: generator_loss=1.6221860647201538, discriminator_loss=0.24854359030723572\n",
            "step 1743: generator_loss=1.6145987510681152, discriminator_loss=0.24953623116016388\n",
            "step 1744: generator_loss=1.6383397579193115, discriminator_loss=0.2453036904335022\n",
            "step 1745: generator_loss=1.6061997413635254, discriminator_loss=0.2497919499874115\n",
            "step 1746: generator_loss=1.6152987480163574, discriminator_loss=0.24780559539794922\n",
            "step 1747: generator_loss=1.6538407802581787, discriminator_loss=0.2435283064842224\n",
            "step 1748: generator_loss=1.6151962280273438, discriminator_loss=0.24716705083847046\n",
            "step 1749: generator_loss=1.617837905883789, discriminator_loss=0.24666240811347961\n",
            "step 1750: generator_loss=1.6634180545806885, discriminator_loss=0.24419933557510376\n",
            "step 1751: generator_loss=1.6561102867126465, discriminator_loss=0.24304647743701935\n",
            "step 1752: generator_loss=1.658027172088623, discriminator_loss=0.24264128506183624\n",
            "step 1753: generator_loss=1.6474049091339111, discriminator_loss=0.24396169185638428\n",
            "step 1754: generator_loss=1.6207767724990845, discriminator_loss=0.2458617091178894\n",
            "step 1755: generator_loss=1.59982430934906, discriminator_loss=0.24806693196296692\n",
            "step 1756: generator_loss=1.600498914718628, discriminator_loss=0.24846643209457397\n",
            "step 1757: generator_loss=1.579220175743103, discriminator_loss=0.25187164545059204\n",
            "step 1758: generator_loss=1.5901910066604614, discriminator_loss=0.2516905665397644\n",
            "step 1759: generator_loss=1.5760819911956787, discriminator_loss=0.2520432472229004\n",
            "step 1760: generator_loss=1.537226676940918, discriminator_loss=0.2587728500366211\n",
            "step 1761: generator_loss=1.5714731216430664, discriminator_loss=0.2542262077331543\n",
            "step 1762: generator_loss=1.4925932884216309, discriminator_loss=0.26875144243240356\n",
            "step 1763: generator_loss=1.5467723608016968, discriminator_loss=0.26043957471847534\n",
            "step 1764: generator_loss=1.5266437530517578, discriminator_loss=0.2646961808204651\n",
            "step 1765: generator_loss=1.5301543474197388, discriminator_loss=0.2676698863506317\n",
            "step 1766: generator_loss=1.5414502620697021, discriminator_loss=0.2671397924423218\n",
            "step 1767: generator_loss=1.537742018699646, discriminator_loss=0.2704775929450989\n",
            "step 1768: generator_loss=1.5241825580596924, discriminator_loss=0.2723233103752136\n",
            "step 1769: generator_loss=1.5194538831710815, discriminator_loss=0.27670368552207947\n",
            "step 1770: generator_loss=1.4297329187393188, discriminator_loss=0.29350852966308594\n",
            "step 1771: generator_loss=1.47792387008667, discriminator_loss=0.2867451608181\n",
            "step 1772: generator_loss=1.4818713665008545, discriminator_loss=0.28579723834991455\n",
            "step 1773: generator_loss=1.446542739868164, discriminator_loss=0.29515182971954346\n",
            "step 1774: generator_loss=1.4320929050445557, discriminator_loss=0.29355335235595703\n",
            "step 1775: generator_loss=1.4436957836151123, discriminator_loss=0.29508236050605774\n",
            "step 1776: generator_loss=1.4433737993240356, discriminator_loss=0.29064273834228516\n",
            "step 1777: generator_loss=1.4937515258789062, discriminator_loss=0.28155025839805603\n",
            "step 1778: generator_loss=1.4053889513015747, discriminator_loss=0.29413264989852905\n",
            "step 1779: generator_loss=1.531109094619751, discriminator_loss=0.27161675691604614\n",
            "step 1780: generator_loss=1.5486112833023071, discriminator_loss=0.26653873920440674\n",
            "step 1781: generator_loss=1.600733995437622, discriminator_loss=0.2595570981502533\n",
            "step 1782: generator_loss=1.5767014026641846, discriminator_loss=0.2608530521392822\n",
            "step 1783: generator_loss=1.5388853549957275, discriminator_loss=0.2653639614582062\n",
            "step 1784: generator_loss=1.5876070261001587, discriminator_loss=0.2585380971431732\n",
            "step 1785: generator_loss=1.615317702293396, discriminator_loss=0.25506484508514404\n",
            "step 1786: generator_loss=1.5882117748260498, discriminator_loss=0.25669804215431213\n",
            "step 1787: generator_loss=1.6074692010879517, discriminator_loss=0.25425007939338684\n",
            "step 1788: generator_loss=1.6405279636383057, discriminator_loss=0.2497802972793579\n",
            "step 1789: generator_loss=1.6941018104553223, discriminator_loss=0.2427818775177002\n",
            "step 1790: generator_loss=1.6359937191009521, discriminator_loss=0.24986183643341064\n",
            "step 1791: generator_loss=1.6541941165924072, discriminator_loss=0.2464510202407837\n",
            "step 1792: generator_loss=1.6608717441558838, discriminator_loss=0.2457110434770584\n",
            "step 1793: generator_loss=1.6516718864440918, discriminator_loss=0.2465973198413849\n",
            "step 1794: generator_loss=1.6352375745773315, discriminator_loss=0.24709005653858185\n",
            "step 1795: generator_loss=1.6084246635437012, discriminator_loss=0.2503238618373871\n",
            "step 1796: generator_loss=1.5610331296920776, discriminator_loss=0.25678348541259766\n",
            "step 1797: generator_loss=1.5271183252334595, discriminator_loss=0.26064246892929077\n",
            "step 1798: generator_loss=1.4938843250274658, discriminator_loss=0.264599084854126\n",
            "step 1799: generator_loss=1.4734604358673096, discriminator_loss=0.26777738332748413\n",
            "step 1800: generator_loss=1.4597139358520508, discriminator_loss=0.27056002616882324\n",
            "step 1801: generator_loss=1.4649468660354614, discriminator_loss=0.26942670345306396\n",
            "step 1802: generator_loss=1.4181654453277588, discriminator_loss=0.2745268940925598\n",
            "step 1803: generator_loss=1.4196133613586426, discriminator_loss=0.2741876542568207\n",
            "step 1804: generator_loss=1.4065247774124146, discriminator_loss=0.2771751582622528\n",
            "step 1805: generator_loss=1.4193521738052368, discriminator_loss=0.2747465968132019\n",
            "step 1806: generator_loss=1.3867743015289307, discriminator_loss=0.2790250778198242\n",
            "step 1807: generator_loss=1.4020140171051025, discriminator_loss=0.2766895294189453\n",
            "step 1808: generator_loss=1.394596815109253, discriminator_loss=0.2777154743671417\n",
            "step 1809: generator_loss=1.4213008880615234, discriminator_loss=0.27348625659942627\n",
            "step 1810: generator_loss=1.435110092163086, discriminator_loss=0.27164745330810547\n",
            "step 1811: generator_loss=1.4446702003479004, discriminator_loss=0.270464688539505\n",
            "step 1812: generator_loss=1.4595942497253418, discriminator_loss=0.26870468258857727\n",
            "step 1813: generator_loss=1.4645862579345703, discriminator_loss=0.2685852646827698\n",
            "step 1814: generator_loss=1.4634013175964355, discriminator_loss=0.26983189582824707\n",
            "step 1815: generator_loss=1.4550777673721313, discriminator_loss=0.27124157547950745\n",
            "step 1816: generator_loss=1.4325917959213257, discriminator_loss=0.2761523127555847\n",
            "step 1817: generator_loss=1.4608187675476074, discriminator_loss=0.2731103301048279\n",
            "step 1818: generator_loss=1.3778424263000488, discriminator_loss=0.28597116470336914\n",
            "step 1819: generator_loss=1.4050333499908447, discriminator_loss=0.2827661633491516\n",
            "step 1820: generator_loss=1.3168096542358398, discriminator_loss=0.2979818880558014\n",
            "step 1821: generator_loss=1.2992525100708008, discriminator_loss=0.3016400933265686\n",
            "step 1822: generator_loss=1.267582893371582, discriminator_loss=0.3082776963710785\n",
            "step 1823: generator_loss=1.2125885486602783, discriminator_loss=0.31957054138183594\n",
            "step 1824: generator_loss=1.176325798034668, discriminator_loss=0.3275109827518463\n",
            "step 1825: generator_loss=1.117321491241455, discriminator_loss=0.3401486873626709\n",
            "step 1826: generator_loss=1.174682855606079, discriminator_loss=0.3293851912021637\n",
            "step 1827: generator_loss=1.1198968887329102, discriminator_loss=0.3428463935852051\n",
            "step 1828: generator_loss=1.13022780418396, discriminator_loss=0.34112346172332764\n",
            "step 1829: generator_loss=1.1129050254821777, discriminator_loss=0.3453541398048401\n",
            "step 1830: generator_loss=1.0850625038146973, discriminator_loss=0.3519902229309082\n",
            "step 1831: generator_loss=1.0934786796569824, discriminator_loss=0.35167038440704346\n",
            "step 1832: generator_loss=1.0739846229553223, discriminator_loss=0.356561541557312\n",
            "step 1833: generator_loss=1.08375883102417, discriminator_loss=0.3546909689903259\n",
            "step 1834: generator_loss=1.1152422428131104, discriminator_loss=0.34799712896347046\n",
            "step 1835: generator_loss=1.1354303359985352, discriminator_loss=0.34409812092781067\n",
            "step 1836: generator_loss=1.165755033493042, discriminator_loss=0.3380955159664154\n",
            "step 1837: generator_loss=1.2120718955993652, discriminator_loss=0.32951366901397705\n",
            "step 1838: generator_loss=1.2170019149780273, discriminator_loss=0.3291237950325012\n",
            "step 1839: generator_loss=1.2514533996582031, discriminator_loss=0.32327139377593994\n",
            "step 1840: generator_loss=1.2635421752929688, discriminator_loss=0.321975976228714\n",
            "step 1841: generator_loss=1.2920509576797485, discriminator_loss=0.3176230192184448\n",
            "step 1842: generator_loss=1.3364664316177368, discriminator_loss=0.310261607170105\n",
            "step 1843: generator_loss=1.3931858539581299, discriminator_loss=0.30098870396614075\n",
            "step 1844: generator_loss=1.445907473564148, discriminator_loss=0.2930712103843689\n",
            "step 1845: generator_loss=1.4886085987091064, discriminator_loss=0.28674477338790894\n",
            "step 1846: generator_loss=1.53082275390625, discriminator_loss=0.2808821201324463\n",
            "step 1847: generator_loss=1.5648629665374756, discriminator_loss=0.27628466486930847\n",
            "step 1848: generator_loss=1.5762300491333008, discriminator_loss=0.2743784785270691\n",
            "step 1849: generator_loss=1.5817879438400269, discriminator_loss=0.27311134338378906\n",
            "step 1850: generator_loss=1.566038966178894, discriminator_loss=0.27456045150756836\n",
            "step 1851: generator_loss=1.553039312362671, discriminator_loss=0.27525168657302856\n",
            "step 1852: generator_loss=1.5245875120162964, discriminator_loss=0.27827945351600647\n",
            "step 1853: generator_loss=1.5017467737197876, discriminator_loss=0.2806287705898285\n",
            "step 1854: generator_loss=1.480259656906128, discriminator_loss=0.28306248784065247\n",
            "step 1855: generator_loss=1.460310935974121, discriminator_loss=0.2852332293987274\n",
            "step 1856: generator_loss=1.4529948234558105, discriminator_loss=0.28555136919021606\n",
            "step 1857: generator_loss=1.443455696105957, discriminator_loss=0.2860773205757141\n",
            "step 1858: generator_loss=1.440476655960083, discriminator_loss=0.2857745885848999\n",
            "step 1859: generator_loss=1.4316589832305908, discriminator_loss=0.28631648421287537\n",
            "step 1860: generator_loss=1.4373573064804077, discriminator_loss=0.28474971652030945\n",
            "step 1861: generator_loss=1.437983751296997, discriminator_loss=0.2839812934398651\n",
            "step 1862: generator_loss=1.4458332061767578, discriminator_loss=0.28198161721229553\n",
            "step 1863: generator_loss=1.4461002349853516, discriminator_loss=0.281147837638855\n",
            "step 1864: generator_loss=1.4490749835968018, discriminator_loss=0.27982380986213684\n",
            "step 1865: generator_loss=1.4493334293365479, discriminator_loss=0.2790490984916687\n",
            "step 1866: generator_loss=1.458683729171753, discriminator_loss=0.2767966389656067\n",
            "step 1867: generator_loss=1.4646129608154297, discriminator_loss=0.27508634328842163\n",
            "step 1868: generator_loss=1.4754611253738403, discriminator_loss=0.27250319719314575\n",
            "step 1869: generator_loss=1.478632926940918, discriminator_loss=0.27113619446754456\n",
            "step 1870: generator_loss=1.4812171459197998, discriminator_loss=0.269930899143219\n",
            "step 1871: generator_loss=1.4835844039916992, discriminator_loss=0.26897290349006653\n",
            "step 1872: generator_loss=1.4853825569152832, discriminator_loss=0.2679349184036255\n",
            "step 1873: generator_loss=1.4865161180496216, discriminator_loss=0.267418771982193\n",
            "step 1874: generator_loss=1.4833825826644897, discriminator_loss=0.26745593547821045\n",
            "step 1875: generator_loss=1.477031946182251, discriminator_loss=0.26801005005836487\n",
            "step 1876: generator_loss=1.4674142599105835, discriminator_loss=0.2691460847854614\n",
            "step 1877: generator_loss=1.4514119625091553, discriminator_loss=0.2711827754974365\n",
            "step 1878: generator_loss=1.437893271446228, discriminator_loss=0.27305668592453003\n",
            "step 1879: generator_loss=1.4243824481964111, discriminator_loss=0.2750474810600281\n",
            "step 1880: generator_loss=1.4111709594726562, discriminator_loss=0.2769051790237427\n",
            "step 1881: generator_loss=1.3992060422897339, discriminator_loss=0.27864497900009155\n",
            "step 1882: generator_loss=1.3982062339782715, discriminator_loss=0.2788175940513611\n",
            "step 1883: generator_loss=1.4087655544281006, discriminator_loss=0.27708661556243896\n",
            "step 1884: generator_loss=1.4158868789672852, discriminator_loss=0.275798499584198\n",
            "step 1885: generator_loss=1.4207370281219482, discriminator_loss=0.2751004695892334\n",
            "step 1886: generator_loss=1.4224324226379395, discriminator_loss=0.27476441860198975\n",
            "step 1887: generator_loss=1.4394943714141846, discriminator_loss=0.2721322774887085\n",
            "step 1888: generator_loss=1.4578386545181274, discriminator_loss=0.269237756729126\n",
            "step 1889: generator_loss=1.4721602201461792, discriminator_loss=0.26706022024154663\n",
            "step 1890: generator_loss=1.4824833869934082, discriminator_loss=0.26559215784072876\n",
            "step 1891: generator_loss=1.4943246841430664, discriminator_loss=0.2639572322368622\n",
            "step 1892: generator_loss=1.5166571140289307, discriminator_loss=0.2610354423522949\n",
            "step 1893: generator_loss=1.5473891496658325, discriminator_loss=0.2569279074668884\n",
            "step 1894: generator_loss=1.5738425254821777, discriminator_loss=0.25352638959884644\n",
            "step 1895: generator_loss=1.594710111618042, discriminator_loss=0.25079280138015747\n",
            "step 1896: generator_loss=1.6051610708236694, discriminator_loss=0.24941270053386688\n",
            "step 1897: generator_loss=1.6131566762924194, discriminator_loss=0.2484472393989563\n",
            "step 1898: generator_loss=1.6216543912887573, discriminator_loss=0.24729914963245392\n",
            "step 1899: generator_loss=1.6257352828979492, discriminator_loss=0.2465658038854599\n",
            "step 1900: generator_loss=1.632169485092163, discriminator_loss=0.24548491835594177\n",
            "step 1901: generator_loss=1.632944107055664, discriminator_loss=0.24518007040023804\n",
            "step 1902: generator_loss=1.631164312362671, discriminator_loss=0.24496781826019287\n",
            "step 1903: generator_loss=1.6287596225738525, discriminator_loss=0.24476441740989685\n",
            "step 1904: generator_loss=1.6310285329818726, discriminator_loss=0.24405957758426666\n",
            "step 1905: generator_loss=1.6387699842453003, discriminator_loss=0.24281743168830872\n",
            "step 1906: generator_loss=1.6424601078033447, discriminator_loss=0.2420138418674469\n",
            "step 1907: generator_loss=1.645361304283142, discriminator_loss=0.2413056641817093\n",
            "step 1908: generator_loss=1.645655870437622, discriminator_loss=0.24088159203529358\n",
            "step 1909: generator_loss=1.6505900621414185, discriminator_loss=0.2398405820131302\n",
            "step 1910: generator_loss=1.6598035097122192, discriminator_loss=0.23822012543678284\n",
            "step 1911: generator_loss=1.6687188148498535, discriminator_loss=0.2368248701095581\n",
            "step 1912: generator_loss=1.6745576858520508, discriminator_loss=0.23593977093696594\n",
            "step 1913: generator_loss=1.677123785018921, discriminator_loss=0.23536819219589233\n",
            "step 1914: generator_loss=1.6709280014038086, discriminator_loss=0.23577529191970825\n",
            "step 1915: generator_loss=1.6581618785858154, discriminator_loss=0.23698481917381287\n",
            "step 1916: generator_loss=1.6504969596862793, discriminator_loss=0.2376999855041504\n",
            "step 1917: generator_loss=1.6395156383514404, discriminator_loss=0.23882746696472168\n",
            "step 1918: generator_loss=1.6437915563583374, discriminator_loss=0.23804901540279388\n",
            "step 1919: generator_loss=1.6456655263900757, discriminator_loss=0.23773637413978577\n",
            "step 1920: generator_loss=1.657086730003357, discriminator_loss=0.23612292110919952\n",
            "step 1921: generator_loss=1.6798475980758667, discriminator_loss=0.23329445719718933\n",
            "step 1922: generator_loss=1.7051377296447754, discriminator_loss=0.2302069514989853\n",
            "step 1923: generator_loss=1.7244830131530762, discriminator_loss=0.22778834402561188\n",
            "step 1924: generator_loss=1.7399096488952637, discriminator_loss=0.22597545385360718\n",
            "step 1925: generator_loss=1.749050259590149, discriminator_loss=0.2247905731201172\n",
            "step 1926: generator_loss=1.7515895366668701, discriminator_loss=0.22432246804237366\n",
            "step 1927: generator_loss=1.7482883930206299, discriminator_loss=0.22460471093654633\n",
            "step 1928: generator_loss=1.7394521236419678, discriminator_loss=0.2254105508327484\n",
            "step 1929: generator_loss=1.7217519283294678, discriminator_loss=0.22724705934524536\n",
            "step 1930: generator_loss=1.7014997005462646, discriminator_loss=0.22938847541809082\n",
            "step 1931: generator_loss=1.6761138439178467, discriminator_loss=0.2320740520954132\n",
            "step 1932: generator_loss=1.647463083267212, discriminator_loss=0.2353857159614563\n",
            "step 1933: generator_loss=1.6272709369659424, discriminator_loss=0.23779109120368958\n",
            "step 1934: generator_loss=1.6122978925704956, discriminator_loss=0.23961935937404633\n",
            "step 1935: generator_loss=1.5972309112548828, discriminator_loss=0.24153146147727966\n",
            "step 1936: generator_loss=1.5775396823883057, discriminator_loss=0.24410861730575562\n",
            "step 1937: generator_loss=1.5567439794540405, discriminator_loss=0.24689263105392456\n",
            "step 1938: generator_loss=1.5418823957443237, discriminator_loss=0.2489696592092514\n",
            "step 1939: generator_loss=1.5371243953704834, discriminator_loss=0.2499069720506668\n",
            "step 1940: generator_loss=1.532320261001587, discriminator_loss=0.25101345777511597\n",
            "step 1941: generator_loss=1.5302233695983887, discriminator_loss=0.25162339210510254\n",
            "step 1942: generator_loss=1.5283409357070923, discriminator_loss=0.2522360682487488\n",
            "step 1943: generator_loss=1.5209681987762451, discriminator_loss=0.25338608026504517\n",
            "step 1944: generator_loss=1.5241246223449707, discriminator_loss=0.25328513979911804\n",
            "step 1945: generator_loss=1.5290875434875488, discriminator_loss=0.2527851462364197\n",
            "step 1946: generator_loss=1.5450142621994019, discriminator_loss=0.25094228982925415\n",
            "step 1947: generator_loss=1.5628820657730103, discriminator_loss=0.2487800419330597\n",
            "step 1948: generator_loss=1.5854599475860596, discriminator_loss=0.246080219745636\n",
            "step 1949: generator_loss=1.6086935997009277, discriminator_loss=0.24333786964416504\n",
            "step 1950: generator_loss=1.638656735420227, discriminator_loss=0.23991379141807556\n",
            "step 1951: generator_loss=1.6529642343521118, discriminator_loss=0.23818765580654144\n",
            "step 1952: generator_loss=1.6675868034362793, discriminator_loss=0.23631879687309265\n",
            "step 1953: generator_loss=1.6829278469085693, discriminator_loss=0.23446616530418396\n",
            "step 1954: generator_loss=1.7246434688568115, discriminator_loss=0.22966428101062775\n",
            "step 1955: generator_loss=1.7849571704864502, discriminator_loss=0.2229950726032257\n",
            "step 1956: generator_loss=1.8355848789215088, discriminator_loss=0.21761566400527954\n",
            "step 1957: generator_loss=1.876391887664795, discriminator_loss=0.2133302092552185\n",
            "step 1958: generator_loss=1.9046134948730469, discriminator_loss=0.21012087166309357\n",
            "step 1959: generator_loss=1.9194313287734985, discriminator_loss=0.20828519761562347\n",
            "step 1960: generator_loss=1.9192978143692017, discriminator_loss=0.20763587951660156\n",
            "step 1961: generator_loss=1.9108283519744873, discriminator_loss=0.20762231945991516\n",
            "step 1962: generator_loss=1.8963085412979126, discriminator_loss=0.20829425752162933\n",
            "step 1963: generator_loss=1.8773424625396729, discriminator_loss=0.20946848392486572\n",
            "step 1964: generator_loss=1.8672354221343994, discriminator_loss=0.2099587470293045\n",
            "step 1965: generator_loss=1.8625752925872803, discriminator_loss=0.20998099446296692\n",
            "step 1966: generator_loss=1.856746792793274, discriminator_loss=0.209957093000412\n",
            "step 1967: generator_loss=1.845821499824524, discriminator_loss=0.2105758786201477\n",
            "step 1968: generator_loss=1.8375602960586548, discriminator_loss=0.2109844982624054\n",
            "step 1969: generator_loss=1.837965965270996, discriminator_loss=0.210482656955719\n",
            "step 1970: generator_loss=1.8436076641082764, discriminator_loss=0.2095736563205719\n",
            "step 1971: generator_loss=1.8490400314331055, discriminator_loss=0.20894408226013184\n",
            "step 1972: generator_loss=1.8554869890213013, discriminator_loss=0.2082308679819107\n",
            "step 1973: generator_loss=1.8528861999511719, discriminator_loss=0.20821252465248108\n",
            "step 1974: generator_loss=1.8389248847961426, discriminator_loss=0.2094445526599884\n",
            "step 1975: generator_loss=1.8199635744094849, discriminator_loss=0.21114620566368103\n",
            "step 1976: generator_loss=1.7931212186813354, discriminator_loss=0.2136189341545105\n",
            "step 1977: generator_loss=1.7552664279937744, discriminator_loss=0.21730700135231018\n",
            "step 1978: generator_loss=1.7166721820831299, discriminator_loss=0.2213842123746872\n",
            "step 1979: generator_loss=1.6847972869873047, discriminator_loss=0.22488215565681458\n",
            "step 1980: generator_loss=1.669578194618225, discriminator_loss=0.22655294835567474\n",
            "step 1981: generator_loss=1.6574289798736572, discriminator_loss=0.2279457002878189\n",
            "step 1982: generator_loss=1.6488351821899414, discriminator_loss=0.22910037636756897\n",
            "step 1983: generator_loss=1.6300244331359863, discriminator_loss=0.2315172553062439\n",
            "step 1984: generator_loss=1.6133770942687988, discriminator_loss=0.2339068353176117\n",
            "step 1985: generator_loss=1.5866515636444092, discriminator_loss=0.2376723289489746\n",
            "step 1986: generator_loss=1.5503145456314087, discriminator_loss=0.24311180412769318\n",
            "step 1987: generator_loss=1.532935619354248, discriminator_loss=0.24589493870735168\n",
            "step 1988: generator_loss=1.5033693313598633, discriminator_loss=0.25063809752464294\n",
            "step 1989: generator_loss=1.4785630702972412, discriminator_loss=0.25490695238113403\n",
            "step 1990: generator_loss=1.4585479497909546, discriminator_loss=0.25931066274642944\n",
            "step 1991: generator_loss=1.4441184997558594, discriminator_loss=0.2616623044013977\n",
            "step 1992: generator_loss=1.4406569004058838, discriminator_loss=0.26326000690460205\n",
            "step 1993: generator_loss=1.4270392656326294, discriminator_loss=0.2664216458797455\n",
            "step 1994: generator_loss=1.4415521621704102, discriminator_loss=0.2650128901004791\n",
            "step 1995: generator_loss=1.429445505142212, discriminator_loss=0.2676706612110138\n",
            "step 1996: generator_loss=1.4338172674179077, discriminator_loss=0.26834702491760254\n",
            "step 1997: generator_loss=1.4286887645721436, discriminator_loss=0.26943936944007874\n",
            "step 1998: generator_loss=1.4015752077102661, discriminator_loss=0.2739655375480652\n",
            "step 1999: generator_loss=1.3870644569396973, discriminator_loss=0.27692893147468567\n",
            "step 2000: generator_loss=1.405534267425537, discriminator_loss=0.2743494510650635\n",
            "step 2001: generator_loss=1.39347505569458, discriminator_loss=0.2774086892604828\n",
            "step 2002: generator_loss=1.4261895418167114, discriminator_loss=0.2724735736846924\n",
            "step 2003: generator_loss=1.417501449584961, discriminator_loss=0.27410420775413513\n",
            "step 2004: generator_loss=1.443516731262207, discriminator_loss=0.2706935405731201\n",
            "step 2005: generator_loss=1.4722145795822144, discriminator_loss=0.2663884162902832\n",
            "step 2006: generator_loss=1.5283399820327759, discriminator_loss=0.2590281665325165\n",
            "step 2007: generator_loss=1.5498124361038208, discriminator_loss=0.2556365132331848\n",
            "step 2008: generator_loss=1.6028902530670166, discriminator_loss=0.24891187250614166\n",
            "step 2009: generator_loss=1.6508395671844482, discriminator_loss=0.242852121591568\n",
            "step 2010: generator_loss=1.7064567804336548, discriminator_loss=0.236026331782341\n",
            "step 2011: generator_loss=1.7596435546875, discriminator_loss=0.23002077639102936\n",
            "step 2012: generator_loss=1.827622413635254, discriminator_loss=0.22232288122177124\n",
            "step 2013: generator_loss=1.8547444343566895, discriminator_loss=0.21872328221797943\n",
            "step 2014: generator_loss=1.9008524417877197, discriminator_loss=0.21356303989887238\n",
            "step 2015: generator_loss=1.914332389831543, discriminator_loss=0.21152879297733307\n",
            "step 2016: generator_loss=1.9376729726791382, discriminator_loss=0.20875348150730133\n",
            "step 2017: generator_loss=1.9381773471832275, discriminator_loss=0.208039790391922\n",
            "step 2018: generator_loss=1.9346057176589966, discriminator_loss=0.20790541172027588\n",
            "step 2019: generator_loss=1.8763866424560547, discriminator_loss=0.21263915300369263\n",
            "step 2020: generator_loss=1.8856244087219238, discriminator_loss=0.21142664551734924\n",
            "step 2021: generator_loss=1.8441903591156006, discriminator_loss=0.21485747396945953\n",
            "step 2022: generator_loss=1.8286659717559814, discriminator_loss=0.2158052921295166\n",
            "step 2023: generator_loss=1.7630317211151123, discriminator_loss=0.22198718786239624\n",
            "step 2024: generator_loss=1.7627030611038208, discriminator_loss=0.22210639715194702\n",
            "step 2025: generator_loss=1.6868995428085327, discriminator_loss=0.22968201339244843\n",
            "step 2026: generator_loss=1.6338011026382446, discriminator_loss=0.23549999296665192\n",
            "step 2027: generator_loss=1.6414128541946411, discriminator_loss=0.2346148043870926\n",
            "step 2028: generator_loss=1.593745231628418, discriminator_loss=0.23974651098251343\n",
            "step 2029: generator_loss=1.6027541160583496, discriminator_loss=0.2377040684223175\n",
            "step 2030: generator_loss=1.5846140384674072, discriminator_loss=0.23924390971660614\n",
            "step 2031: generator_loss=1.5842325687408447, discriminator_loss=0.23844939470291138\n",
            "step 2032: generator_loss=1.5635404586791992, discriminator_loss=0.2400054782629013\n",
            "step 2033: generator_loss=1.5718367099761963, discriminator_loss=0.23835328221321106\n",
            "step 2034: generator_loss=1.5927079916000366, discriminator_loss=0.23510906100273132\n",
            "step 2035: generator_loss=1.6069890260696411, discriminator_loss=0.23286770284175873\n",
            "step 2036: generator_loss=1.61356520652771, discriminator_loss=0.23151804506778717\n",
            "step 2037: generator_loss=1.649073600769043, discriminator_loss=0.2271910309791565\n",
            "step 2038: generator_loss=1.662939190864563, discriminator_loss=0.22547973692417145\n",
            "step 2039: generator_loss=1.6731129884719849, discriminator_loss=0.22433045506477356\n",
            "step 2040: generator_loss=1.6798064708709717, discriminator_loss=0.2237563133239746\n",
            "step 2041: generator_loss=1.6712888479232788, discriminator_loss=0.2245989441871643\n",
            "step 2042: generator_loss=1.6694384813308716, discriminator_loss=0.2250221073627472\n",
            "step 2043: generator_loss=1.646592617034912, discriminator_loss=0.2276630848646164\n",
            "step 2044: generator_loss=1.624314546585083, discriminator_loss=0.23064561188220978\n",
            "step 2045: generator_loss=1.596480369567871, discriminator_loss=0.23434996604919434\n",
            "step 2046: generator_loss=1.5753730535507202, discriminator_loss=0.23735181987285614\n",
            "step 2047: generator_loss=1.5441503524780273, discriminator_loss=0.2416011542081833\n",
            "step 2048: generator_loss=1.5244790315628052, discriminator_loss=0.24450139701366425\n",
            "step 2049: generator_loss=1.5189851522445679, discriminator_loss=0.2454233169555664\n",
            "step 2050: generator_loss=1.5301257371902466, discriminator_loss=0.24414882063865662\n",
            "step 2051: generator_loss=1.5273334980010986, discriminator_loss=0.24477340281009674\n",
            "step 2052: generator_loss=1.528165340423584, discriminator_loss=0.2451561987400055\n",
            "step 2053: generator_loss=1.5337395668029785, discriminator_loss=0.2447253167629242\n",
            "step 2054: generator_loss=1.5411896705627441, discriminator_loss=0.24410219490528107\n",
            "step 2055: generator_loss=1.5575411319732666, discriminator_loss=0.2424004077911377\n",
            "step 2056: generator_loss=1.5721765756607056, discriminator_loss=0.24093016982078552\n",
            "step 2057: generator_loss=1.6155506372451782, discriminator_loss=0.23583252727985382\n",
            "step 2058: generator_loss=1.6689529418945312, discriminator_loss=0.2300068587064743\n",
            "step 2059: generator_loss=1.739739179611206, discriminator_loss=0.22247587144374847\n",
            "step 2060: generator_loss=1.8372094631195068, discriminator_loss=0.21314886212348938\n",
            "step 2061: generator_loss=1.9536863565444946, discriminator_loss=0.20305819809436798\n",
            "step 2062: generator_loss=2.0634052753448486, discriminator_loss=0.19475224614143372\n",
            "step 2063: generator_loss=2.176835060119629, discriminator_loss=0.18692725896835327\n",
            "step 2064: generator_loss=2.2500970363616943, discriminator_loss=0.18215905129909515\n",
            "step 2065: generator_loss=2.31561279296875, discriminator_loss=0.1780451089143753\n",
            "step 2066: generator_loss=2.33345365524292, discriminator_loss=0.17645128071308136\n",
            "step 2067: generator_loss=2.3221688270568848, discriminator_loss=0.1765134036540985\n",
            "step 2068: generator_loss=2.273017644882202, discriminator_loss=0.17843717336654663\n",
            "step 2069: generator_loss=2.249181032180786, discriminator_loss=0.17906919121742249\n",
            "step 2070: generator_loss=2.244858980178833, discriminator_loss=0.17855750024318695\n",
            "step 2071: generator_loss=2.221510410308838, discriminator_loss=0.17900966107845306\n",
            "step 2072: generator_loss=2.1938934326171875, discriminator_loss=0.17989234626293182\n",
            "step 2073: generator_loss=2.140115737915039, discriminator_loss=0.1823236644268036\n",
            "step 2074: generator_loss=2.054537296295166, discriminator_loss=0.18746110796928406\n",
            "step 2075: generator_loss=1.971958875656128, discriminator_loss=0.19317124783992767\n",
            "step 2076: generator_loss=1.9249756336212158, discriminator_loss=0.19690313935279846\n",
            "step 2077: generator_loss=1.908340334892273, discriminator_loss=0.19792254269123077\n",
            "step 2078: generator_loss=1.9078326225280762, discriminator_loss=0.1977595090866089\n",
            "step 2079: generator_loss=1.8876107931137085, discriminator_loss=0.19898056983947754\n",
            "step 2080: generator_loss=1.8917961120605469, discriminator_loss=0.19906049966812134\n",
            "step 2081: generator_loss=1.8564698696136475, discriminator_loss=0.2017785906791687\n",
            "step 2082: generator_loss=1.7942893505096436, discriminator_loss=0.20734615623950958\n",
            "step 2083: generator_loss=1.770350456237793, discriminator_loss=0.21054428815841675\n",
            "step 2084: generator_loss=1.73063325881958, discriminator_loss=0.21506839990615845\n",
            "step 2085: generator_loss=1.7184792757034302, discriminator_loss=0.2170698046684265\n",
            "step 2086: generator_loss=1.7128753662109375, discriminator_loss=0.21878871321678162\n",
            "step 2087: generator_loss=1.7172406911849976, discriminator_loss=0.21953830122947693\n",
            "step 2088: generator_loss=1.6786856651306152, discriminator_loss=0.22378352284431458\n",
            "step 2089: generator_loss=1.6736490726470947, discriminator_loss=0.2234833836555481\n",
            "step 2090: generator_loss=1.6436192989349365, discriminator_loss=0.22868692874908447\n",
            "step 2091: generator_loss=1.6727192401885986, discriminator_loss=0.2262071967124939\n",
            "step 2092: generator_loss=1.6567142009735107, discriminator_loss=0.22819149494171143\n",
            "step 2093: generator_loss=1.687806248664856, discriminator_loss=0.226322740316391\n",
            "step 2094: generator_loss=1.6812796592712402, discriminator_loss=0.22668984532356262\n",
            "step 2095: generator_loss=1.677344560623169, discriminator_loss=0.22921231389045715\n",
            "step 2096: generator_loss=1.605626106262207, discriminator_loss=0.23710867762565613\n",
            "step 2097: generator_loss=1.5713711977005005, discriminator_loss=0.2404528707265854\n",
            "step 2098: generator_loss=1.5660107135772705, discriminator_loss=0.24128639698028564\n",
            "step 2099: generator_loss=1.5495460033416748, discriminator_loss=0.2436160445213318\n",
            "step 2100: generator_loss=1.5033683776855469, discriminator_loss=0.24978692829608917\n",
            "step 2101: generator_loss=1.4940125942230225, discriminator_loss=0.2508765757083893\n",
            "step 2102: generator_loss=1.459995985031128, discriminator_loss=0.2551669478416443\n",
            "step 2103: generator_loss=1.45574951171875, discriminator_loss=0.25572043657302856\n",
            "step 2104: generator_loss=1.4422962665557861, discriminator_loss=0.25708603858947754\n",
            "step 2105: generator_loss=1.4978474378585815, discriminator_loss=0.24962395429611206\n",
            "step 2106: generator_loss=1.5342755317687988, discriminator_loss=0.24491527676582336\n",
            "step 2107: generator_loss=1.570960521697998, discriminator_loss=0.2392924726009369\n",
            "step 2108: generator_loss=1.657804250717163, discriminator_loss=0.22934174537658691\n",
            "step 2109: generator_loss=1.690190076828003, discriminator_loss=0.22502227127552032\n",
            "step 2110: generator_loss=1.7552605867385864, discriminator_loss=0.21782776713371277\n",
            "step 2111: generator_loss=1.7907077074050903, discriminator_loss=0.2136998176574707\n",
            "step 2112: generator_loss=1.8246915340423584, discriminator_loss=0.21009935438632965\n",
            "step 2113: generator_loss=1.8605632781982422, discriminator_loss=0.20656463503837585\n",
            "step 2114: generator_loss=1.86606764793396, discriminator_loss=0.2053799331188202\n",
            "step 2115: generator_loss=1.8666102886199951, discriminator_loss=0.20483627915382385\n",
            "step 2116: generator_loss=1.863134741783142, discriminator_loss=0.20470881462097168\n",
            "step 2117: generator_loss=1.8445789813995361, discriminator_loss=0.20574162900447845\n",
            "step 2118: generator_loss=1.833286166191101, discriminator_loss=0.20621851086616516\n",
            "step 2119: generator_loss=1.8256605863571167, discriminator_loss=0.20647889375686646\n",
            "step 2120: generator_loss=1.8157899379730225, discriminator_loss=0.2068508416414261\n",
            "step 2121: generator_loss=1.8080940246582031, discriminator_loss=0.20693817734718323\n",
            "step 2122: generator_loss=1.8006970882415771, discriminator_loss=0.20706447958946228\n",
            "step 2123: generator_loss=1.8020665645599365, discriminator_loss=0.2062961906194687\n",
            "step 2124: generator_loss=1.8111647367477417, discriminator_loss=0.20491430163383484\n",
            "step 2125: generator_loss=1.8195316791534424, discriminator_loss=0.20347699522972107\n",
            "step 2126: generator_loss=1.8549716472625732, discriminator_loss=0.19973985850811005\n",
            "step 2127: generator_loss=1.8782546520233154, discriminator_loss=0.19715921580791473\n",
            "step 2128: generator_loss=1.928842306137085, discriminator_loss=0.1922774761915207\n",
            "step 2129: generator_loss=1.9676767587661743, discriminator_loss=0.18858566880226135\n",
            "step 2130: generator_loss=2.0116775035858154, discriminator_loss=0.18478035926818848\n",
            "step 2131: generator_loss=2.0417914390563965, discriminator_loss=0.1820267140865326\n",
            "step 2132: generator_loss=2.0616331100463867, discriminator_loss=0.18003033101558685\n",
            "step 2133: generator_loss=2.0758190155029297, discriminator_loss=0.178470179438591\n",
            "step 2134: generator_loss=2.0922484397888184, discriminator_loss=0.1767820119857788\n",
            "step 2135: generator_loss=2.1034533977508545, discriminator_loss=0.1754116266965866\n",
            "step 2136: generator_loss=2.108860969543457, discriminator_loss=0.17454524338245392\n",
            "step 2137: generator_loss=2.0970892906188965, discriminator_loss=0.17491066455841064\n",
            "step 2138: generator_loss=2.0933799743652344, discriminator_loss=0.17491239309310913\n",
            "step 2139: generator_loss=2.076836347579956, discriminator_loss=0.17576798796653748\n",
            "step 2140: generator_loss=2.0686237812042236, discriminator_loss=0.17613506317138672\n",
            "step 2141: generator_loss=2.0634546279907227, discriminator_loss=0.17636238038539886\n",
            "step 2142: generator_loss=2.0444233417510986, discriminator_loss=0.17756955325603485\n",
            "step 2143: generator_loss=2.030193328857422, discriminator_loss=0.17851173877716064\n",
            "step 2144: generator_loss=2.0009636878967285, discriminator_loss=0.1806429922580719\n",
            "step 2145: generator_loss=1.9611554145812988, discriminator_loss=0.1836162507534027\n",
            "step 2146: generator_loss=1.93180513381958, discriminator_loss=0.18590447306632996\n",
            "step 2147: generator_loss=1.9086195230484009, discriminator_loss=0.18781563639640808\n",
            "step 2148: generator_loss=1.8786609172821045, discriminator_loss=0.19039060175418854\n",
            "step 2149: generator_loss=1.8470162153244019, discriminator_loss=0.1932395100593567\n",
            "step 2150: generator_loss=1.8067848682403564, discriminator_loss=0.19729602336883545\n",
            "step 2151: generator_loss=1.7680174112319946, discriminator_loss=0.2012735903263092\n",
            "step 2152: generator_loss=1.7597126960754395, discriminator_loss=0.20230159163475037\n",
            "step 2153: generator_loss=1.7217488288879395, discriminator_loss=0.20678916573524475\n",
            "step 2154: generator_loss=1.7058489322662354, discriminator_loss=0.2094242423772812\n",
            "step 2155: generator_loss=1.7004722356796265, discriminator_loss=0.21039777994155884\n",
            "step 2156: generator_loss=1.6600046157836914, discriminator_loss=0.21577906608581543\n",
            "step 2157: generator_loss=1.6400752067565918, discriminator_loss=0.21980777382850647\n",
            "step 2158: generator_loss=1.6292163133621216, discriminator_loss=0.22170186042785645\n",
            "step 2159: generator_loss=1.6222108602523804, discriminator_loss=0.22493241727352142\n",
            "step 2160: generator_loss=1.620747447013855, discriminator_loss=0.224786177277565\n",
            "step 2161: generator_loss=1.5569140911102295, discriminator_loss=0.23629671335220337\n",
            "step 2162: generator_loss=1.599509358406067, discriminator_loss=0.23274174332618713\n",
            "step 2163: generator_loss=1.590956211090088, discriminator_loss=0.23936080932617188\n",
            "step 2164: generator_loss=1.6193969249725342, discriminator_loss=0.23542146384716034\n",
            "step 2165: generator_loss=1.566914677619934, discriminator_loss=0.24769681692123413\n",
            "step 2166: generator_loss=1.5564041137695312, discriminator_loss=0.2488775998353958\n",
            "step 2167: generator_loss=1.5914041996002197, discriminator_loss=0.24435116350650787\n",
            "step 2168: generator_loss=1.5369845628738403, discriminator_loss=0.25537627935409546\n",
            "step 2169: generator_loss=1.5671643018722534, discriminator_loss=0.2524423599243164\n",
            "step 2170: generator_loss=1.5480618476867676, discriminator_loss=0.25841110944747925\n",
            "step 2171: generator_loss=1.574462890625, discriminator_loss=0.253696084022522\n",
            "step 2172: generator_loss=1.7680593729019165, discriminator_loss=0.22630856931209564\n",
            "step 2173: generator_loss=1.8356317281723022, discriminator_loss=0.21598342061042786\n",
            "step 2174: generator_loss=1.8475611209869385, discriminator_loss=0.2155991494655609\n",
            "step 2175: generator_loss=2.0202410221099854, discriminator_loss=0.19506517052650452\n",
            "step 2176: generator_loss=2.005476713180542, discriminator_loss=0.19553935527801514\n",
            "step 2177: generator_loss=2.251326322555542, discriminator_loss=0.17258620262145996\n",
            "step 2178: generator_loss=2.13411283493042, discriminator_loss=0.18120026588439941\n",
            "step 2179: generator_loss=2.186429023742676, discriminator_loss=0.17593832314014435\n",
            "step 2180: generator_loss=2.2792062759399414, discriminator_loss=0.17189599573612213\n",
            "step 2181: generator_loss=2.2924938201904297, discriminator_loss=0.16757088899612427\n",
            "step 2182: generator_loss=2.3231329917907715, discriminator_loss=0.16550111770629883\n",
            "step 2183: generator_loss=2.2821261882781982, discriminator_loss=0.16596415638923645\n",
            "step 2184: generator_loss=2.244579553604126, discriminator_loss=0.1665397584438324\n",
            "step 2185: generator_loss=2.2167513370513916, discriminator_loss=0.16817989945411682\n",
            "step 2186: generator_loss=2.1419148445129395, discriminator_loss=0.17208240926265717\n",
            "step 2187: generator_loss=2.1922767162323, discriminator_loss=0.16787107288837433\n",
            "step 2188: generator_loss=2.1444382667541504, discriminator_loss=0.17088696360588074\n",
            "step 2189: generator_loss=2.1112725734710693, discriminator_loss=0.17282411456108093\n",
            "step 2190: generator_loss=2.1129491329193115, discriminator_loss=0.1716538965702057\n",
            "step 2191: generator_loss=2.0690765380859375, discriminator_loss=0.17425444722175598\n",
            "step 2192: generator_loss=2.011112928390503, discriminator_loss=0.17817650735378265\n",
            "step 2193: generator_loss=1.9225351810455322, discriminator_loss=0.18505896627902985\n",
            "step 2194: generator_loss=1.8876636028289795, discriminator_loss=0.18728026747703552\n",
            "step 2195: generator_loss=1.8921457529067993, discriminator_loss=0.18640094995498657\n",
            "step 2196: generator_loss=1.8581762313842773, discriminator_loss=0.18900690972805023\n",
            "step 2197: generator_loss=1.8372828960418701, discriminator_loss=0.19043409824371338\n",
            "step 2198: generator_loss=1.8257083892822266, discriminator_loss=0.19094052910804749\n",
            "step 2199: generator_loss=1.7969636917114258, discriminator_loss=0.193521186709404\n",
            "step 2200: generator_loss=1.7912213802337646, discriminator_loss=0.1936475932598114\n",
            "step 2201: generator_loss=1.7925422191619873, discriminator_loss=0.19320376217365265\n",
            "step 2202: generator_loss=1.8161132335662842, discriminator_loss=0.19096936285495758\n",
            "step 2203: generator_loss=1.7989001274108887, discriminator_loss=0.1922982931137085\n",
            "step 2204: generator_loss=1.8103125095367432, discriminator_loss=0.19114099442958832\n",
            "step 2205: generator_loss=1.816815733909607, discriminator_loss=0.19047221541404724\n",
            "step 2206: generator_loss=1.8170225620269775, discriminator_loss=0.1903538703918457\n",
            "step 2207: generator_loss=1.8202641010284424, discriminator_loss=0.190064936876297\n",
            "step 2208: generator_loss=1.8160938024520874, discriminator_loss=0.19050072133541107\n",
            "step 2209: generator_loss=1.809419870376587, discriminator_loss=0.19114390015602112\n",
            "step 2210: generator_loss=1.7951328754425049, discriminator_loss=0.19252149760723114\n",
            "step 2211: generator_loss=1.7823729515075684, discriminator_loss=0.19386552274227142\n",
            "step 2212: generator_loss=1.785477876663208, discriminator_loss=0.19364327192306519\n",
            "step 2213: generator_loss=1.7957035303115845, discriminator_loss=0.1926604062318802\n",
            "step 2214: generator_loss=1.802026391029358, discriminator_loss=0.19218893349170685\n",
            "step 2215: generator_loss=1.8054094314575195, discriminator_loss=0.19186073541641235\n",
            "step 2216: generator_loss=1.8190358877182007, discriminator_loss=0.1906100958585739\n",
            "step 2217: generator_loss=1.8246212005615234, discriminator_loss=0.19014820456504822\n",
            "step 2218: generator_loss=1.8066859245300293, discriminator_loss=0.19172658026218414\n",
            "step 2219: generator_loss=1.8110030889511108, discriminator_loss=0.19141831994056702\n",
            "step 2220: generator_loss=1.7904731035232544, discriminator_loss=0.1936229169368744\n",
            "step 2221: generator_loss=1.778450608253479, discriminator_loss=0.1949341595172882\n",
            "step 2222: generator_loss=1.7464218139648438, discriminator_loss=0.19866198301315308\n",
            "step 2223: generator_loss=1.744347333908081, discriminator_loss=0.1991214007139206\n",
            "step 2224: generator_loss=1.723994255065918, discriminator_loss=0.20186161994934082\n",
            "step 2225: generator_loss=1.7252265214920044, discriminator_loss=0.2018052488565445\n",
            "step 2226: generator_loss=1.7063311338424683, discriminator_loss=0.20403873920440674\n",
            "step 2227: generator_loss=1.7028770446777344, discriminator_loss=0.20476742088794708\n",
            "step 2228: generator_loss=1.6860016584396362, discriminator_loss=0.20670709013938904\n",
            "step 2229: generator_loss=1.6995172500610352, discriminator_loss=0.20519587397575378\n",
            "step 2230: generator_loss=1.7200520038604736, discriminator_loss=0.2032943069934845\n",
            "step 2231: generator_loss=1.744070053100586, discriminator_loss=0.20063617825508118\n",
            "step 2232: generator_loss=1.7319964170455933, discriminator_loss=0.201582133769989\n",
            "step 2233: generator_loss=1.7426328659057617, discriminator_loss=0.20059964060783386\n",
            "step 2234: generator_loss=1.7012414932250977, discriminator_loss=0.20460164546966553\n",
            "step 2235: generator_loss=1.672452449798584, discriminator_loss=0.2076227217912674\n",
            "step 2236: generator_loss=1.6391170024871826, discriminator_loss=0.21166884899139404\n",
            "step 2237: generator_loss=1.5906598567962646, discriminator_loss=0.21784496307373047\n",
            "step 2238: generator_loss=1.5615965127944946, discriminator_loss=0.2216772437095642\n",
            "step 2239: generator_loss=1.5618467330932617, discriminator_loss=0.22207745909690857\n",
            "step 2240: generator_loss=1.5569422245025635, discriminator_loss=0.22275957465171814\n",
            "step 2241: generator_loss=1.5812137126922607, discriminator_loss=0.21997672319412231\n",
            "step 2242: generator_loss=1.629685401916504, discriminator_loss=0.21402040123939514\n",
            "step 2243: generator_loss=1.6818137168884277, discriminator_loss=0.20805564522743225\n",
            "step 2244: generator_loss=1.7357842922210693, discriminator_loss=0.20208024978637695\n",
            "step 2245: generator_loss=1.8095386028289795, discriminator_loss=0.1945442408323288\n",
            "step 2246: generator_loss=1.867277979850769, discriminator_loss=0.18900242447853088\n",
            "step 2247: generator_loss=1.9105303287506104, discriminator_loss=0.18497046828269958\n",
            "step 2248: generator_loss=1.9362914562225342, discriminator_loss=0.18261586129665375\n",
            "step 2249: generator_loss=1.9338222742080688, discriminator_loss=0.18237125873565674\n",
            "step 2250: generator_loss=1.9153133630752563, discriminator_loss=0.18367639183998108\n",
            "step 2251: generator_loss=1.8884508609771729, discriminator_loss=0.18565189838409424\n",
            "step 2252: generator_loss=1.8482965230941772, discriminator_loss=0.18902000784873962\n",
            "step 2253: generator_loss=1.80646550655365, discriminator_loss=0.19286511838436127\n",
            "step 2254: generator_loss=1.7972874641418457, discriminator_loss=0.19364067912101746\n",
            "step 2255: generator_loss=1.8052458763122559, discriminator_loss=0.19280165433883667\n",
            "step 2256: generator_loss=1.8076422214508057, discriminator_loss=0.192356139421463\n",
            "step 2257: generator_loss=1.8008618354797363, discriminator_loss=0.19284547865390778\n",
            "step 2258: generator_loss=1.7885576486587524, discriminator_loss=0.19393882155418396\n",
            "step 2259: generator_loss=1.7672414779663086, discriminator_loss=0.19588536024093628\n",
            "step 2260: generator_loss=1.7402186393737793, discriminator_loss=0.1985251009464264\n",
            "step 2261: generator_loss=1.7138196229934692, discriminator_loss=0.2013760805130005\n",
            "step 2262: generator_loss=1.6852600574493408, discriminator_loss=0.20454153418540955\n",
            "step 2263: generator_loss=1.6528171300888062, discriminator_loss=0.20841318368911743\n",
            "step 2264: generator_loss=1.6243946552276611, discriminator_loss=0.211942657828331\n",
            "step 2265: generator_loss=1.6004530191421509, discriminator_loss=0.2152390033006668\n",
            "step 2266: generator_loss=1.5738170146942139, discriminator_loss=0.2189023792743683\n",
            "step 2267: generator_loss=1.5455363988876343, discriminator_loss=0.22292503714561462\n",
            "step 2268: generator_loss=1.532450795173645, discriminator_loss=0.22525563836097717\n",
            "step 2269: generator_loss=1.5481431484222412, discriminator_loss=0.22343634068965912\n",
            "step 2270: generator_loss=1.5598938465118408, discriminator_loss=0.22231990098953247\n",
            "step 2271: generator_loss=1.5746426582336426, discriminator_loss=0.22086530923843384\n",
            "step 2272: generator_loss=1.5866529941558838, discriminator_loss=0.21966595947742462\n",
            "step 2273: generator_loss=1.609503984451294, discriminator_loss=0.21699568629264832\n",
            "step 2274: generator_loss=1.641597867012024, discriminator_loss=0.2134896218776703\n",
            "step 2275: generator_loss=1.6889771223068237, discriminator_loss=0.20831778645515442\n",
            "step 2276: generator_loss=1.740563154220581, discriminator_loss=0.2031158208847046\n",
            "step 2277: generator_loss=1.8033527135849, discriminator_loss=0.19693610072135925\n",
            "step 2278: generator_loss=1.8634109497070312, discriminator_loss=0.1914019137620926\n",
            "step 2279: generator_loss=1.9134905338287354, discriminator_loss=0.18715396523475647\n",
            "step 2280: generator_loss=1.9495751857757568, discriminator_loss=0.1839621365070343\n",
            "step 2281: generator_loss=1.96830153465271, discriminator_loss=0.18229497969150543\n",
            "step 2282: generator_loss=1.9708338975906372, discriminator_loss=0.1818021833896637\n",
            "step 2283: generator_loss=1.974471092224121, discriminator_loss=0.1812422275543213\n",
            "step 2284: generator_loss=1.9697353839874268, discriminator_loss=0.18120697140693665\n",
            "step 2285: generator_loss=1.9517998695373535, discriminator_loss=0.1822524219751358\n",
            "step 2286: generator_loss=1.9226903915405273, discriminator_loss=0.18408524990081787\n",
            "step 2287: generator_loss=1.8936095237731934, discriminator_loss=0.18609188497066498\n",
            "step 2288: generator_loss=1.8715578317642212, discriminator_loss=0.18758583068847656\n",
            "step 2289: generator_loss=1.848402738571167, discriminator_loss=0.1894262135028839\n",
            "step 2290: generator_loss=1.825286626815796, discriminator_loss=0.19114243984222412\n",
            "step 2291: generator_loss=1.7969602346420288, discriminator_loss=0.1936427354812622\n",
            "step 2292: generator_loss=1.767726182937622, discriminator_loss=0.196316659450531\n",
            "step 2293: generator_loss=1.7407701015472412, discriminator_loss=0.19887369871139526\n",
            "step 2294: generator_loss=1.7088608741760254, discriminator_loss=0.20209214091300964\n",
            "step 2295: generator_loss=1.6875927448272705, discriminator_loss=0.2043476700782776\n",
            "step 2296: generator_loss=1.6807031631469727, discriminator_loss=0.20513561367988586\n",
            "step 2297: generator_loss=1.6937048435211182, discriminator_loss=0.2036924660205841\n",
            "step 2298: generator_loss=1.7108781337738037, discriminator_loss=0.2020786702632904\n",
            "step 2299: generator_loss=1.7407134771347046, discriminator_loss=0.19923365116119385\n",
            "step 2300: generator_loss=1.7861475944519043, discriminator_loss=0.19487535953521729\n",
            "step 2301: generator_loss=1.8430309295654297, discriminator_loss=0.1895931363105774\n",
            "step 2302: generator_loss=1.8962681293487549, discriminator_loss=0.1848982870578766\n",
            "step 2303: generator_loss=1.9443426132202148, discriminator_loss=0.18091130256652832\n",
            "step 2304: generator_loss=1.9799665212631226, discriminator_loss=0.1780426800251007\n",
            "step 2305: generator_loss=2.0032901763916016, discriminator_loss=0.17614732682704926\n",
            "step 2306: generator_loss=2.0092344284057617, discriminator_loss=0.17549219727516174\n",
            "step 2307: generator_loss=2.0052895545959473, discriminator_loss=0.1756683588027954\n",
            "step 2308: generator_loss=1.959792971611023, discriminator_loss=0.17956653237342834\n",
            "step 2309: generator_loss=1.9038050174713135, discriminator_loss=0.18492399156093597\n",
            "step 2310: generator_loss=1.8549656867980957, discriminator_loss=0.19024479389190674\n",
            "step 2311: generator_loss=1.776427984237671, discriminator_loss=0.2010081708431244\n",
            "step 2312: generator_loss=1.6610255241394043, discriminator_loss=0.21852940320968628\n",
            "step 2313: generator_loss=1.6501410007476807, discriminator_loss=0.22596475481987\n",
            "step 2314: generator_loss=1.5436865091323853, discriminator_loss=0.2526220679283142\n",
            "step 2315: generator_loss=1.5739092826843262, discriminator_loss=0.24988408386707306\n",
            "step 2316: generator_loss=1.4889395236968994, discriminator_loss=0.2732726037502289\n",
            "step 2317: generator_loss=1.5014913082122803, discriminator_loss=0.27694547176361084\n",
            "step 2318: generator_loss=1.540757656097412, discriminator_loss=0.2737468481063843\n",
            "step 2319: generator_loss=1.5954564809799194, discriminator_loss=0.26586008071899414\n",
            "step 2320: generator_loss=1.6538163423538208, discriminator_loss=0.2556992471218109\n",
            "step 2321: generator_loss=1.5265756845474243, discriminator_loss=0.2848646342754364\n",
            "step 2322: generator_loss=1.7149152755737305, discriminator_loss=0.24952134490013123\n",
            "step 2323: generator_loss=1.6916413307189941, discriminator_loss=0.25413456559181213\n",
            "step 2324: generator_loss=1.8920485973358154, discriminator_loss=0.22294320166110992\n",
            "step 2325: generator_loss=1.9013620615005493, discriminator_loss=0.2254464328289032\n",
            "step 2326: generator_loss=2.1315903663635254, discriminator_loss=0.1933182179927826\n",
            "step 2327: generator_loss=2.046213150024414, discriminator_loss=0.20244061946868896\n",
            "step 2328: generator_loss=2.2106595039367676, discriminator_loss=0.18685144186019897\n",
            "step 2329: generator_loss=2.132176399230957, discriminator_loss=0.19102151691913605\n",
            "step 2330: generator_loss=2.3450803756713867, discriminator_loss=0.17385095357894897\n",
            "step 2331: generator_loss=2.3739311695098877, discriminator_loss=0.1697535663843155\n",
            "step 2332: generator_loss=2.3731305599212646, discriminator_loss=0.16637031733989716\n",
            "step 2333: generator_loss=2.4767189025878906, discriminator_loss=0.15988346934318542\n",
            "step 2334: generator_loss=2.4771957397460938, discriminator_loss=0.15843212604522705\n",
            "step 2335: generator_loss=2.4718029499053955, discriminator_loss=0.15729519724845886\n",
            "step 2336: generator_loss=2.4568448066711426, discriminator_loss=0.15768486261367798\n",
            "step 2337: generator_loss=2.5146493911743164, discriminator_loss=0.15278290212154388\n",
            "step 2338: generator_loss=2.3044705390930176, discriminator_loss=0.16520079970359802\n",
            "step 2339: generator_loss=2.4360830783843994, discriminator_loss=0.1562788486480713\n",
            "step 2340: generator_loss=2.312987804412842, discriminator_loss=0.16443905234336853\n",
            "step 2341: generator_loss=2.3145651817321777, discriminator_loss=0.16276969015598297\n",
            "step 2342: generator_loss=2.2590174674987793, discriminator_loss=0.16582414507865906\n",
            "step 2343: generator_loss=2.2704248428344727, discriminator_loss=0.16566181182861328\n",
            "step 2344: generator_loss=2.1656205654144287, discriminator_loss=0.17222777009010315\n",
            "step 2345: generator_loss=2.1500284671783447, discriminator_loss=0.17323103547096252\n",
            "step 2346: generator_loss=2.161128044128418, discriminator_loss=0.17033281922340393\n",
            "step 2347: generator_loss=2.1613473892211914, discriminator_loss=0.1717294454574585\n",
            "step 2348: generator_loss=2.1357662677764893, discriminator_loss=0.1730109304189682\n",
            "step 2349: generator_loss=2.1789021492004395, discriminator_loss=0.16819454729557037\n",
            "step 2350: generator_loss=2.1025328636169434, discriminator_loss=0.1728677600622177\n",
            "step 2351: generator_loss=2.1468803882598877, discriminator_loss=0.16869516670703888\n",
            "step 2352: generator_loss=2.152423858642578, discriminator_loss=0.16729633510112762\n",
            "step 2353: generator_loss=2.220594644546509, discriminator_loss=0.1612178385257721\n",
            "step 2354: generator_loss=2.1372976303100586, discriminator_loss=0.165816068649292\n",
            "step 2355: generator_loss=2.148399829864502, discriminator_loss=0.16500651836395264\n",
            "step 2356: generator_loss=2.1924920082092285, discriminator_loss=0.1588015854358673\n",
            "step 2357: generator_loss=2.1460037231445312, discriminator_loss=0.16307251155376434\n",
            "step 2358: generator_loss=2.0859432220458984, discriminator_loss=0.16605648398399353\n",
            "step 2359: generator_loss=2.0016210079193115, discriminator_loss=0.17105910181999207\n",
            "step 2360: generator_loss=2.0304019451141357, discriminator_loss=0.17197629809379578\n",
            "step 2361: generator_loss=1.9514939785003662, discriminator_loss=0.17516760528087616\n",
            "step 2362: generator_loss=1.9067566394805908, discriminator_loss=0.17964333295822144\n",
            "step 2363: generator_loss=1.86802339553833, discriminator_loss=0.18084797263145447\n",
            "step 2364: generator_loss=1.8760895729064941, discriminator_loss=0.18413913249969482\n",
            "step 2365: generator_loss=1.823791265487671, discriminator_loss=0.18621742725372314\n",
            "step 2366: generator_loss=1.8193371295928955, discriminator_loss=0.1861066371202469\n",
            "step 2367: generator_loss=1.8505468368530273, discriminator_loss=0.18359139561653137\n",
            "step 2368: generator_loss=1.8195072412490845, discriminator_loss=0.18802492320537567\n",
            "step 2369: generator_loss=1.7904462814331055, discriminator_loss=0.19025371968746185\n",
            "step 2370: generator_loss=1.814737319946289, discriminator_loss=0.18879754841327667\n",
            "step 2371: generator_loss=1.77157461643219, discriminator_loss=0.19202522933483124\n",
            "step 2372: generator_loss=1.8274085521697998, discriminator_loss=0.18908637762069702\n",
            "step 2373: generator_loss=1.7458351850509644, discriminator_loss=0.19429543614387512\n",
            "step 2374: generator_loss=1.8037017583847046, discriminator_loss=0.1935855895280838\n",
            "step 2375: generator_loss=1.7085368633270264, discriminator_loss=0.19990122318267822\n",
            "step 2376: generator_loss=1.696218729019165, discriminator_loss=0.2016022503376007\n",
            "step 2377: generator_loss=1.6245536804199219, discriminator_loss=0.20721375942230225\n",
            "step 2378: generator_loss=1.6355774402618408, discriminator_loss=0.20753982663154602\n",
            "step 2379: generator_loss=1.604131817817688, discriminator_loss=0.21123704314231873\n",
            "step 2380: generator_loss=1.5767322778701782, discriminator_loss=0.21484869718551636\n",
            "step 2381: generator_loss=1.558791995048523, discriminator_loss=0.2180662602186203\n",
            "step 2382: generator_loss=1.5092322826385498, discriminator_loss=0.2253228724002838\n",
            "step 2383: generator_loss=1.4767770767211914, discriminator_loss=0.23031000792980194\n",
            "step 2384: generator_loss=1.4157605171203613, discriminator_loss=0.24022257328033447\n",
            "step 2385: generator_loss=1.4154871702194214, discriminator_loss=0.24191069602966309\n",
            "step 2386: generator_loss=1.4174554347991943, discriminator_loss=0.2431626319885254\n",
            "step 2387: generator_loss=1.3586809635162354, discriminator_loss=0.25350117683410645\n",
            "step 2388: generator_loss=1.3581650257110596, discriminator_loss=0.255676805973053\n",
            "step 2389: generator_loss=1.365347146987915, discriminator_loss=0.25677332282066345\n",
            "step 2390: generator_loss=1.3875839710235596, discriminator_loss=0.25577351450920105\n",
            "step 2391: generator_loss=1.3452709913253784, discriminator_loss=0.26274508237838745\n",
            "step 2392: generator_loss=1.3675298690795898, discriminator_loss=0.26026394963264465\n",
            "step 2393: generator_loss=1.3705857992172241, discriminator_loss=0.2612088918685913\n",
            "step 2394: generator_loss=1.3948612213134766, discriminator_loss=0.25805342197418213\n",
            "step 2395: generator_loss=1.462756633758545, discriminator_loss=0.24922756850719452\n",
            "step 2396: generator_loss=1.5394420623779297, discriminator_loss=0.23974356055259705\n",
            "step 2397: generator_loss=1.6310380697250366, discriminator_loss=0.22950896620750427\n",
            "step 2398: generator_loss=1.7165722846984863, discriminator_loss=0.22130413353443146\n",
            "step 2399: generator_loss=1.8009090423583984, discriminator_loss=0.21430648863315582\n",
            "step 2400: generator_loss=1.8590192794799805, discriminator_loss=0.21058370172977448\n",
            "step 2401: generator_loss=1.8565014600753784, discriminator_loss=0.21108217537403107\n",
            "step 2402: generator_loss=1.8743281364440918, discriminator_loss=0.2104097604751587\n",
            "step 2403: generator_loss=1.8939863443374634, discriminator_loss=0.20972256362438202\n",
            "step 2404: generator_loss=1.872961401939392, discriminator_loss=0.21172210574150085\n",
            "step 2405: generator_loss=1.8553996086120605, discriminator_loss=0.21342787146568298\n",
            "step 2406: generator_loss=1.82505202293396, discriminator_loss=0.2158219814300537\n",
            "step 2407: generator_loss=1.794772982597351, discriminator_loss=0.21793970465660095\n",
            "step 2408: generator_loss=1.7883031368255615, discriminator_loss=0.21744436025619507\n",
            "step 2409: generator_loss=1.7702758312225342, discriminator_loss=0.2176988422870636\n",
            "step 2410: generator_loss=1.7557488679885864, discriminator_loss=0.2174130380153656\n",
            "step 2411: generator_loss=1.7330143451690674, discriminator_loss=0.21799640357494354\n",
            "step 2412: generator_loss=1.7062737941741943, discriminator_loss=0.21909606456756592\n",
            "step 2413: generator_loss=1.6883665323257446, discriminator_loss=0.21936511993408203\n",
            "step 2414: generator_loss=1.6619688272476196, discriminator_loss=0.2207944095134735\n",
            "step 2415: generator_loss=1.6557579040527344, discriminator_loss=0.2201620638370514\n",
            "step 2416: generator_loss=1.6474535465240479, discriminator_loss=0.22016417980194092\n",
            "step 2417: generator_loss=1.6588623523712158, discriminator_loss=0.2183561474084854\n",
            "step 2418: generator_loss=1.656653881072998, discriminator_loss=0.2182547152042389\n",
            "step 2419: generator_loss=1.6756618022918701, discriminator_loss=0.215131476521492\n",
            "step 2420: generator_loss=1.6625134944915771, discriminator_loss=0.21823248267173767\n",
            "step 2421: generator_loss=1.666503667831421, discriminator_loss=0.2183057814836502\n",
            "step 2422: generator_loss=1.6533093452453613, discriminator_loss=0.22326502203941345\n",
            "step 2423: generator_loss=1.705974817276001, discriminator_loss=0.21795785427093506\n",
            "step 2424: generator_loss=1.6439909934997559, discriminator_loss=0.22984027862548828\n",
            "step 2425: generator_loss=1.7178034782409668, discriminator_loss=0.2218962013721466\n",
            "step 2426: generator_loss=1.801117181777954, discriminator_loss=0.2130868136882782\n",
            "step 2427: generator_loss=1.8916609287261963, discriminator_loss=0.20618703961372375\n",
            "step 2428: generator_loss=1.8991531133651733, discriminator_loss=0.2064163088798523\n",
            "step 2429: generator_loss=1.7278419733047485, discriminator_loss=0.23540736734867096\n",
            "step 2430: generator_loss=2.001887798309326, discriminator_loss=0.20571771264076233\n",
            "step 2431: generator_loss=2.0707123279571533, discriminator_loss=0.1971830278635025\n",
            "step 2432: generator_loss=1.9930933713912964, discriminator_loss=0.20826208591461182\n",
            "step 2433: generator_loss=2.0538954734802246, discriminator_loss=0.19933798909187317\n",
            "step 2434: generator_loss=2.221449851989746, discriminator_loss=0.18534180521965027\n",
            "step 2435: generator_loss=2.140787124633789, discriminator_loss=0.19104281067848206\n",
            "step 2436: generator_loss=2.1958673000335693, discriminator_loss=0.1870933175086975\n",
            "step 2437: generator_loss=2.2624688148498535, discriminator_loss=0.18046234548091888\n",
            "step 2438: generator_loss=2.293252944946289, discriminator_loss=0.18001596629619598\n",
            "step 2439: generator_loss=2.345322847366333, discriminator_loss=0.17358535528182983\n",
            "step 2440: generator_loss=2.5612049102783203, discriminator_loss=0.1594797670841217\n",
            "step 2441: generator_loss=2.5763089656829834, discriminator_loss=0.1538982391357422\n",
            "step 2442: generator_loss=2.475106954574585, discriminator_loss=0.15998558700084686\n",
            "step 2443: generator_loss=2.5193097591400146, discriminator_loss=0.1566876471042633\n",
            "step 2444: generator_loss=2.4705681800842285, discriminator_loss=0.15688389539718628\n",
            "step 2445: generator_loss=2.532012462615967, discriminator_loss=0.15305015444755554\n",
            "step 2446: generator_loss=2.617135524749756, discriminator_loss=0.14934846758842468\n",
            "step 2447: generator_loss=2.461440324783325, discriminator_loss=0.15665209293365479\n",
            "step 2448: generator_loss=2.4711408615112305, discriminator_loss=0.15223270654678345\n",
            "step 2449: generator_loss=2.567310094833374, discriminator_loss=0.14803698658943176\n",
            "step 2450: generator_loss=2.4184539318084717, discriminator_loss=0.1531696915626526\n",
            "step 2451: generator_loss=2.437007427215576, discriminator_loss=0.1511809378862381\n",
            "step 2452: generator_loss=2.4350156784057617, discriminator_loss=0.14912532269954681\n",
            "step 2453: generator_loss=2.431018829345703, discriminator_loss=0.14890700578689575\n",
            "step 2454: generator_loss=2.2911031246185303, discriminator_loss=0.15610888600349426\n",
            "step 2455: generator_loss=2.3146474361419678, discriminator_loss=0.15427237749099731\n",
            "step 2456: generator_loss=2.175913095474243, discriminator_loss=0.1601364016532898\n",
            "step 2457: generator_loss=2.1289525032043457, discriminator_loss=0.16240859031677246\n",
            "step 2458: generator_loss=2.1605849266052246, discriminator_loss=0.16237185895442963\n",
            "step 2459: generator_loss=2.0392837524414062, discriminator_loss=0.1676434576511383\n",
            "step 2460: generator_loss=2.0409421920776367, discriminator_loss=0.16989222168922424\n",
            "step 2461: generator_loss=1.9010608196258545, discriminator_loss=0.17666129767894745\n",
            "step 2462: generator_loss=1.9839825630187988, discriminator_loss=0.1736442893743515\n",
            "step 2463: generator_loss=1.913332223892212, discriminator_loss=0.18056625127792358\n",
            "step 2464: generator_loss=1.8748209476470947, discriminator_loss=0.1836511492729187\n",
            "step 2465: generator_loss=1.7693108320236206, discriminator_loss=0.1905832588672638\n",
            "step 2466: generator_loss=1.7653416395187378, discriminator_loss=0.19402912259101868\n",
            "step 2467: generator_loss=1.8182562589645386, discriminator_loss=0.19576218724250793\n",
            "step 2468: generator_loss=1.7692307233810425, discriminator_loss=0.2025618553161621\n",
            "step 2469: generator_loss=1.8543221950531006, discriminator_loss=0.19728818535804749\n",
            "step 2470: generator_loss=1.786969780921936, discriminator_loss=0.20487338304519653\n",
            "step 2471: generator_loss=1.6135923862457275, discriminator_loss=0.2170577198266983\n",
            "step 2472: generator_loss=1.6672438383102417, discriminator_loss=0.21610090136528015\n",
            "step 2473: generator_loss=1.61000394821167, discriminator_loss=0.22183926403522491\n",
            "step 2474: generator_loss=1.584201693534851, discriminator_loss=0.22344623506069183\n",
            "step 2475: generator_loss=1.5398693084716797, discriminator_loss=0.22765257954597473\n",
            "step 2476: generator_loss=1.5746469497680664, discriminator_loss=0.22845515608787537\n",
            "step 2477: generator_loss=1.5575878620147705, discriminator_loss=0.2281275987625122\n",
            "step 2478: generator_loss=1.4889609813690186, discriminator_loss=0.23492364585399628\n",
            "step 2479: generator_loss=1.5282235145568848, discriminator_loss=0.23156389594078064\n",
            "step 2480: generator_loss=1.521535873413086, discriminator_loss=0.23255448043346405\n",
            "step 2481: generator_loss=1.4547276496887207, discriminator_loss=0.241621732711792\n",
            "step 2482: generator_loss=1.5205028057098389, discriminator_loss=0.23144081234931946\n",
            "step 2483: generator_loss=1.55613374710083, discriminator_loss=0.22834239900112152\n",
            "step 2484: generator_loss=1.5753114223480225, discriminator_loss=0.22494566440582275\n",
            "step 2485: generator_loss=1.6125911474227905, discriminator_loss=0.2210673689842224\n",
            "step 2486: generator_loss=1.619025707244873, discriminator_loss=0.22093109786510468\n",
            "step 2487: generator_loss=1.667548418045044, discriminator_loss=0.2169371396303177\n",
            "step 2488: generator_loss=1.6703121662139893, discriminator_loss=0.2180616408586502\n",
            "step 2489: generator_loss=1.7409008741378784, discriminator_loss=0.21174314618110657\n",
            "step 2490: generator_loss=1.7143168449401855, discriminator_loss=0.21372675895690918\n",
            "step 2491: generator_loss=1.6782348155975342, discriminator_loss=0.21867170929908752\n",
            "step 2492: generator_loss=1.6258291006088257, discriminator_loss=0.22614817321300507\n",
            "step 2493: generator_loss=1.575242042541504, discriminator_loss=0.23495356738567352\n",
            "step 2494: generator_loss=1.5680317878723145, discriminator_loss=0.2366950511932373\n",
            "step 2495: generator_loss=1.6048252582550049, discriminator_loss=0.23391395807266235\n",
            "step 2496: generator_loss=1.5648701190948486, discriminator_loss=0.24092333018779755\n",
            "step 2497: generator_loss=1.5471402406692505, discriminator_loss=0.24492444097995758\n",
            "step 2498: generator_loss=1.5574896335601807, discriminator_loss=0.24456322193145752\n",
            "step 2499: generator_loss=1.5826630592346191, discriminator_loss=0.2460828721523285\n",
            "step 2500: generator_loss=1.5613999366760254, discriminator_loss=0.25045254826545715\n",
            "step 2501: generator_loss=1.5589042901992798, discriminator_loss=0.2548726201057434\n",
            "step 2502: generator_loss=1.5505661964416504, discriminator_loss=0.2609207332134247\n",
            "step 2503: generator_loss=1.6292171478271484, discriminator_loss=0.2530112862586975\n",
            "step 2504: generator_loss=1.6668574810028076, discriminator_loss=0.25188255310058594\n",
            "step 2505: generator_loss=1.6771092414855957, discriminator_loss=0.25094467401504517\n",
            "step 2506: generator_loss=1.8166898488998413, discriminator_loss=0.23469841480255127\n",
            "step 2507: generator_loss=1.7489535808563232, discriminator_loss=0.23979830741882324\n",
            "step 2508: generator_loss=1.7757498025894165, discriminator_loss=0.2420327067375183\n",
            "step 2509: generator_loss=1.7640388011932373, discriminator_loss=0.24096283316612244\n",
            "step 2510: generator_loss=1.7502343654632568, discriminator_loss=0.2431247979402542\n",
            "step 2511: generator_loss=1.8300318717956543, discriminator_loss=0.23171210289001465\n",
            "step 2512: generator_loss=1.8649182319641113, discriminator_loss=0.22623014450073242\n",
            "step 2513: generator_loss=1.8636014461517334, discriminator_loss=0.22346320748329163\n",
            "step 2514: generator_loss=1.857863187789917, discriminator_loss=0.21971821784973145\n",
            "step 2515: generator_loss=1.9092175960540771, discriminator_loss=0.2126549482345581\n",
            "step 2516: generator_loss=1.9690812826156616, discriminator_loss=0.20542433857917786\n",
            "step 2517: generator_loss=1.9809225797653198, discriminator_loss=0.20101501047611237\n",
            "step 2518: generator_loss=2.0587172508239746, discriminator_loss=0.19366058707237244\n",
            "step 2519: generator_loss=2.056711196899414, discriminator_loss=0.1922726035118103\n",
            "step 2520: generator_loss=2.0898237228393555, discriminator_loss=0.1873575747013092\n",
            "step 2521: generator_loss=2.1368370056152344, discriminator_loss=0.18264064192771912\n",
            "step 2522: generator_loss=2.0701284408569336, discriminator_loss=0.18584084510803223\n",
            "step 2523: generator_loss=2.0969271659851074, discriminator_loss=0.18214288353919983\n",
            "step 2524: generator_loss=2.0907602310180664, discriminator_loss=0.18091191351413727\n",
            "step 2525: generator_loss=2.0971434116363525, discriminator_loss=0.17915822565555573\n",
            "step 2526: generator_loss=2.0964465141296387, discriminator_loss=0.17772802710533142\n",
            "step 2527: generator_loss=2.0670971870422363, discriminator_loss=0.17832908034324646\n",
            "step 2528: generator_loss=2.0517477989196777, discriminator_loss=0.17822211980819702\n",
            "step 2529: generator_loss=1.9955990314483643, discriminator_loss=0.1809164583683014\n",
            "step 2530: generator_loss=1.9539778232574463, discriminator_loss=0.18331274390220642\n",
            "step 2531: generator_loss=1.9326542615890503, discriminator_loss=0.18364989757537842\n",
            "step 2532: generator_loss=1.8988540172576904, discriminator_loss=0.18541006743907928\n",
            "step 2533: generator_loss=1.8562724590301514, discriminator_loss=0.18816961348056793\n",
            "step 2534: generator_loss=1.8452608585357666, discriminator_loss=0.1884121149778366\n",
            "step 2535: generator_loss=1.8492052555084229, discriminator_loss=0.18718594312667847\n",
            "step 2536: generator_loss=1.8387947082519531, discriminator_loss=0.18760210275650024\n",
            "step 2537: generator_loss=1.8503974676132202, discriminator_loss=0.18605275452136993\n",
            "step 2538: generator_loss=1.856744408607483, discriminator_loss=0.18519511818885803\n",
            "step 2539: generator_loss=1.8386542797088623, discriminator_loss=0.18684667348861694\n",
            "step 2540: generator_loss=1.8490865230560303, discriminator_loss=0.18610459566116333\n",
            "step 2541: generator_loss=1.820826768875122, discriminator_loss=0.1886160671710968\n",
            "step 2542: generator_loss=1.8133794069290161, discriminator_loss=0.18932320177555084\n",
            "step 2543: generator_loss=1.831676959991455, discriminator_loss=0.18826055526733398\n",
            "step 2544: generator_loss=1.8463860750198364, discriminator_loss=0.18686003983020782\n",
            "step 2545: generator_loss=1.8270527124404907, discriminator_loss=0.1889508068561554\n",
            "step 2546: generator_loss=1.8169397115707397, discriminator_loss=0.18983539938926697\n",
            "step 2547: generator_loss=1.7920515537261963, discriminator_loss=0.19313809275627136\n",
            "step 2548: generator_loss=1.8043568134307861, discriminator_loss=0.1923639178276062\n",
            "step 2549: generator_loss=1.7296628952026367, discriminator_loss=0.1991540640592575\n",
            "step 2550: generator_loss=1.6944677829742432, discriminator_loss=0.2025321125984192\n",
            "step 2551: generator_loss=1.641231656074524, discriminator_loss=0.20889756083488464\n",
            "step 2552: generator_loss=1.6482658386230469, discriminator_loss=0.20879535377025604\n",
            "step 2553: generator_loss=1.6659135818481445, discriminator_loss=0.20713627338409424\n",
            "step 2554: generator_loss=1.662947654724121, discriminator_loss=0.20788279175758362\n",
            "step 2555: generator_loss=1.7365310192108154, discriminator_loss=0.20001205801963806\n",
            "step 2556: generator_loss=1.710956335067749, discriminator_loss=0.20390541851520538\n",
            "step 2557: generator_loss=1.75544011592865, discriminator_loss=0.19963404536247253\n",
            "step 2558: generator_loss=1.7761352062225342, discriminator_loss=0.19866834580898285\n",
            "step 2559: generator_loss=1.7740525007247925, discriminator_loss=0.1987249255180359\n",
            "step 2560: generator_loss=1.7294353246688843, discriminator_loss=0.20460617542266846\n",
            "step 2561: generator_loss=1.6930744647979736, discriminator_loss=0.20884579420089722\n",
            "step 2562: generator_loss=1.7156054973602295, discriminator_loss=0.2104015350341797\n",
            "step 2563: generator_loss=1.6999564170837402, discriminator_loss=0.2128385305404663\n",
            "step 2564: generator_loss=1.7445939779281616, discriminator_loss=0.20720891654491425\n",
            "step 2565: generator_loss=1.7803698778152466, discriminator_loss=0.2059856355190277\n",
            "step 2566: generator_loss=1.7671302556991577, discriminator_loss=0.20718443393707275\n",
            "step 2567: generator_loss=1.7401865720748901, discriminator_loss=0.2092488408088684\n",
            "step 2568: generator_loss=1.7457504272460938, discriminator_loss=0.20855897665023804\n",
            "step 2569: generator_loss=1.7920247316360474, discriminator_loss=0.20364177227020264\n",
            "step 2570: generator_loss=1.7867988348007202, discriminator_loss=0.20202751457691193\n",
            "step 2571: generator_loss=1.8311505317687988, discriminator_loss=0.1975395530462265\n",
            "step 2572: generator_loss=1.891169786453247, discriminator_loss=0.19170549511909485\n",
            "step 2573: generator_loss=1.9326996803283691, discriminator_loss=0.188076913356781\n",
            "step 2574: generator_loss=2.032658338546753, discriminator_loss=0.18001560866832733\n",
            "step 2575: generator_loss=2.1092891693115234, discriminator_loss=0.1738966405391693\n",
            "step 2576: generator_loss=2.182570457458496, discriminator_loss=0.168610081076622\n",
            "step 2577: generator_loss=2.2234387397766113, discriminator_loss=0.16538235545158386\n",
            "step 2578: generator_loss=2.270285129547119, discriminator_loss=0.16176831722259521\n",
            "step 2579: generator_loss=2.2930803298950195, discriminator_loss=0.1598203480243683\n",
            "step 2580: generator_loss=2.2886276245117188, discriminator_loss=0.159236878156662\n",
            "step 2581: generator_loss=2.290947675704956, discriminator_loss=0.15822410583496094\n",
            "step 2582: generator_loss=2.255476713180542, discriminator_loss=0.1593770980834961\n",
            "step 2583: generator_loss=2.231865167617798, discriminator_loss=0.16019989550113678\n",
            "step 2584: generator_loss=2.180488109588623, discriminator_loss=0.16224288940429688\n",
            "step 2585: generator_loss=2.1530752182006836, discriminator_loss=0.16327789425849915\n",
            "step 2586: generator_loss=2.0722527503967285, discriminator_loss=0.16834263503551483\n",
            "step 2587: generator_loss=2.0007433891296387, discriminator_loss=0.17353501915931702\n",
            "step 2588: generator_loss=1.9677281379699707, discriminator_loss=0.1762530505657196\n",
            "step 2589: generator_loss=1.8879375457763672, discriminator_loss=0.18415208160877228\n",
            "step 2590: generator_loss=1.8181921243667603, discriminator_loss=0.19351044297218323\n",
            "step 2591: generator_loss=1.7365926504135132, discriminator_loss=0.20673751831054688\n",
            "step 2592: generator_loss=1.7012617588043213, discriminator_loss=0.2180350422859192\n",
            "step 2593: generator_loss=1.6312572956085205, discriminator_loss=0.23840297758579254\n",
            "step 2594: generator_loss=1.7222130298614502, discriminator_loss=0.22250065207481384\n",
            "step 2595: generator_loss=1.6614124774932861, discriminator_loss=0.23940765857696533\n",
            "step 2596: generator_loss=1.6619597673416138, discriminator_loss=0.24144603312015533\n",
            "step 2597: generator_loss=1.7421355247497559, discriminator_loss=0.2297654002904892\n",
            "step 2598: generator_loss=1.6865029335021973, discriminator_loss=0.23957763612270355\n",
            "step 2599: generator_loss=1.7748193740844727, discriminator_loss=0.23113203048706055\n",
            "step 2600: generator_loss=1.7820849418640137, discriminator_loss=0.23069438338279724\n",
            "step 2601: generator_loss=1.9400067329406738, discriminator_loss=0.20662672817707062\n",
            "step 2602: generator_loss=1.9671547412872314, discriminator_loss=0.20361962914466858\n",
            "step 2603: generator_loss=2.0743799209594727, discriminator_loss=0.1868506371974945\n",
            "step 2604: generator_loss=2.163173198699951, discriminator_loss=0.17634254693984985\n",
            "step 2605: generator_loss=2.1647472381591797, discriminator_loss=0.1756860911846161\n",
            "step 2606: generator_loss=2.333789587020874, discriminator_loss=0.16280843317508698\n",
            "step 2607: generator_loss=2.3767409324645996, discriminator_loss=0.155747652053833\n",
            "step 2608: generator_loss=2.4679553508758545, discriminator_loss=0.14998501539230347\n",
            "step 2609: generator_loss=2.4774296283721924, discriminator_loss=0.14934268593788147\n",
            "step 2610: generator_loss=2.5124714374542236, discriminator_loss=0.14623713493347168\n",
            "step 2611: generator_loss=2.490793228149414, discriminator_loss=0.14588218927383423\n",
            "step 2612: generator_loss=2.5023984909057617, discriminator_loss=0.14680567383766174\n",
            "step 2613: generator_loss=2.3843297958374023, discriminator_loss=0.15110550820827484\n",
            "step 2614: generator_loss=2.3778834342956543, discriminator_loss=0.15373940765857697\n",
            "step 2615: generator_loss=2.2211976051330566, discriminator_loss=0.1603013277053833\n",
            "step 2616: generator_loss=2.1311426162719727, discriminator_loss=0.16818514466285706\n",
            "step 2617: generator_loss=2.059940814971924, discriminator_loss=0.17379409074783325\n",
            "step 2618: generator_loss=1.884962797164917, discriminator_loss=0.19638803601264954\n",
            "step 2619: generator_loss=1.7605384588241577, discriminator_loss=0.21945233643054962\n",
            "step 2620: generator_loss=1.9399349689483643, discriminator_loss=0.19850027561187744\n",
            "step 2621: generator_loss=1.783372163772583, discriminator_loss=0.21898820996284485\n",
            "step 2622: generator_loss=1.6414794921875, discriminator_loss=0.23921306431293488\n",
            "step 2623: generator_loss=1.697793960571289, discriminator_loss=0.23576809465885162\n",
            "step 2624: generator_loss=1.7260639667510986, discriminator_loss=0.23627980053424835\n",
            "step 2625: generator_loss=1.7219219207763672, discriminator_loss=0.23627497255802155\n",
            "step 2626: generator_loss=1.8338830471038818, discriminator_loss=0.21404048800468445\n",
            "step 2627: generator_loss=1.9012290239334106, discriminator_loss=0.20456290245056152\n",
            "step 2628: generator_loss=1.8623762130737305, discriminator_loss=0.21713116765022278\n",
            "step 2629: generator_loss=1.8465709686279297, discriminator_loss=0.21907122433185577\n",
            "step 2630: generator_loss=1.8363662958145142, discriminator_loss=0.22326257824897766\n",
            "step 2631: generator_loss=1.829355001449585, discriminator_loss=0.2204996645450592\n",
            "step 2632: generator_loss=1.864182472229004, discriminator_loss=0.2179037183523178\n",
            "step 2633: generator_loss=1.8539714813232422, discriminator_loss=0.22385020554065704\n",
            "step 2634: generator_loss=2.0727949142456055, discriminator_loss=0.1970730721950531\n",
            "step 2635: generator_loss=2.0551462173461914, discriminator_loss=0.20251153409481049\n",
            "step 2636: generator_loss=2.088008403778076, discriminator_loss=0.1961258351802826\n",
            "step 2637: generator_loss=1.9564182758331299, discriminator_loss=0.20642995834350586\n",
            "step 2638: generator_loss=2.102189540863037, discriminator_loss=0.19299760460853577\n",
            "step 2639: generator_loss=2.1596827507019043, discriminator_loss=0.18727198243141174\n",
            "step 2640: generator_loss=2.074328899383545, discriminator_loss=0.18981939554214478\n",
            "step 2641: generator_loss=2.125704288482666, discriminator_loss=0.18473118543624878\n",
            "step 2642: generator_loss=2.0795533657073975, discriminator_loss=0.1862688511610031\n",
            "step 2643: generator_loss=2.1577258110046387, discriminator_loss=0.17432014644145966\n",
            "step 2644: generator_loss=2.133016347885132, discriminator_loss=0.17352423071861267\n",
            "step 2645: generator_loss=2.030240535736084, discriminator_loss=0.17710904777050018\n",
            "step 2646: generator_loss=2.03515362739563, discriminator_loss=0.1752648949623108\n",
            "step 2647: generator_loss=2.1117584705352783, discriminator_loss=0.17028869688510895\n",
            "step 2648: generator_loss=1.9820948839187622, discriminator_loss=0.17712469398975372\n",
            "step 2649: generator_loss=2.0107922554016113, discriminator_loss=0.17426162958145142\n",
            "step 2650: generator_loss=1.9818246364593506, discriminator_loss=0.17578157782554626\n",
            "step 2651: generator_loss=1.9456360340118408, discriminator_loss=0.17797665297985077\n",
            "step 2652: generator_loss=1.9533915519714355, discriminator_loss=0.18028730154037476\n",
            "step 2653: generator_loss=1.889906883239746, discriminator_loss=0.18442153930664062\n",
            "step 2654: generator_loss=1.8679357767105103, discriminator_loss=0.18808507919311523\n",
            "step 2655: generator_loss=1.7842705249786377, discriminator_loss=0.19465002417564392\n",
            "step 2656: generator_loss=1.8613414764404297, discriminator_loss=0.18999718129634857\n",
            "step 2657: generator_loss=1.69183349609375, discriminator_loss=0.20085583627223969\n",
            "step 2658: generator_loss=1.6798455715179443, discriminator_loss=0.2036578357219696\n",
            "step 2659: generator_loss=1.7398204803466797, discriminator_loss=0.20652887225151062\n",
            "step 2660: generator_loss=1.6555331945419312, discriminator_loss=0.21433159708976746\n",
            "step 2661: generator_loss=1.5842807292938232, discriminator_loss=0.21621914207935333\n",
            "step 2662: generator_loss=1.6783040761947632, discriminator_loss=0.20943346619606018\n",
            "step 2663: generator_loss=1.684157371520996, discriminator_loss=0.21211519837379456\n",
            "step 2664: generator_loss=1.5881426334381104, discriminator_loss=0.21581053733825684\n",
            "step 2665: generator_loss=1.6740999221801758, discriminator_loss=0.20670056343078613\n",
            "step 2666: generator_loss=1.6522222757339478, discriminator_loss=0.2074533998966217\n",
            "step 2667: generator_loss=1.6956323385238647, discriminator_loss=0.20507271587848663\n",
            "step 2668: generator_loss=1.7553831338882446, discriminator_loss=0.1999860554933548\n",
            "step 2669: generator_loss=1.6795074939727783, discriminator_loss=0.20651251077651978\n",
            "step 2670: generator_loss=1.668736457824707, discriminator_loss=0.20807158946990967\n",
            "step 2671: generator_loss=1.5884132385253906, discriminator_loss=0.21697504818439484\n",
            "step 2672: generator_loss=1.6089383363723755, discriminator_loss=0.21604172885417938\n",
            "step 2673: generator_loss=1.5414217710494995, discriminator_loss=0.22474506497383118\n",
            "step 2674: generator_loss=1.5202157497406006, discriminator_loss=0.22901412844657898\n",
            "step 2675: generator_loss=1.5267138481140137, discriminator_loss=0.2298421859741211\n",
            "step 2676: generator_loss=1.543269157409668, discriminator_loss=0.22958716750144958\n",
            "step 2677: generator_loss=1.5567448139190674, discriminator_loss=0.229539155960083\n",
            "step 2678: generator_loss=1.5915826559066772, discriminator_loss=0.22739921510219574\n",
            "step 2679: generator_loss=1.6123483180999756, discriminator_loss=0.22632426023483276\n",
            "step 2680: generator_loss=1.582815408706665, discriminator_loss=0.2300538420677185\n",
            "step 2681: generator_loss=1.5869780778884888, discriminator_loss=0.2302718162536621\n",
            "step 2682: generator_loss=1.5882306098937988, discriminator_loss=0.23071949183940887\n",
            "step 2683: generator_loss=1.6017600297927856, discriminator_loss=0.23091906309127808\n",
            "step 2684: generator_loss=1.5467958450317383, discriminator_loss=0.2373729944229126\n",
            "step 2685: generator_loss=1.5550975799560547, discriminator_loss=0.23722237348556519\n",
            "step 2686: generator_loss=1.5596492290496826, discriminator_loss=0.23720714449882507\n",
            "step 2687: generator_loss=1.5737054347991943, discriminator_loss=0.23624935746192932\n",
            "step 2688: generator_loss=1.5645415782928467, discriminator_loss=0.23789963126182556\n",
            "step 2689: generator_loss=1.5643296241760254, discriminator_loss=0.2386174499988556\n",
            "step 2690: generator_loss=1.5940864086151123, discriminator_loss=0.23581069707870483\n",
            "step 2691: generator_loss=1.6173653602600098, discriminator_loss=0.233305424451828\n",
            "step 2692: generator_loss=1.6622238159179688, discriminator_loss=0.22847653925418854\n",
            "step 2693: generator_loss=1.7210259437561035, discriminator_loss=0.22231903672218323\n",
            "step 2694: generator_loss=1.7645013332366943, discriminator_loss=0.21788884699344635\n",
            "step 2695: generator_loss=1.8071479797363281, discriminator_loss=0.21354907751083374\n",
            "step 2696: generator_loss=1.8549998998641968, discriminator_loss=0.20876622200012207\n",
            "step 2697: generator_loss=1.8990049362182617, discriminator_loss=0.20418071746826172\n",
            "step 2698: generator_loss=1.9157066345214844, discriminator_loss=0.20239412784576416\n",
            "step 2699: generator_loss=1.911086082458496, discriminator_loss=0.20227399468421936\n",
            "step 2700: generator_loss=1.8828601837158203, discriminator_loss=0.2041696310043335\n",
            "step 2701: generator_loss=1.852280616760254, discriminator_loss=0.2062196433544159\n",
            "step 2702: generator_loss=1.8104617595672607, discriminator_loss=0.20941013097763062\n",
            "step 2703: generator_loss=1.7699767351150513, discriminator_loss=0.21261534094810486\n",
            "step 2704: generator_loss=1.7243821620941162, discriminator_loss=0.21638816595077515\n",
            "step 2705: generator_loss=1.6778299808502197, discriminator_loss=0.2202509492635727\n",
            "step 2706: generator_loss=1.6432586908340454, discriminator_loss=0.22297050058841705\n",
            "step 2707: generator_loss=1.6291773319244385, discriminator_loss=0.2234622836112976\n",
            "step 2708: generator_loss=1.6130597591400146, discriminator_loss=0.22414222359657288\n",
            "step 2709: generator_loss=1.6028423309326172, discriminator_loss=0.22431230545043945\n",
            "step 2710: generator_loss=1.611017107963562, discriminator_loss=0.22237828373908997\n",
            "step 2711: generator_loss=1.6266775131225586, discriminator_loss=0.21987487375736237\n",
            "step 2712: generator_loss=1.6486847400665283, discriminator_loss=0.2166656106710434\n",
            "step 2713: generator_loss=1.6702512502670288, discriminator_loss=0.21351054310798645\n",
            "step 2714: generator_loss=1.6916602849960327, discriminator_loss=0.2107202410697937\n",
            "step 2715: generator_loss=1.743462085723877, discriminator_loss=0.2046302855014801\n",
            "step 2716: generator_loss=1.8155547380447388, discriminator_loss=0.19684448838233948\n",
            "step 2717: generator_loss=1.8910056352615356, discriminator_loss=0.18941333889961243\n",
            "step 2718: generator_loss=1.9532945156097412, discriminator_loss=0.18374869227409363\n",
            "step 2719: generator_loss=1.9995994567871094, discriminator_loss=0.1794302612543106\n",
            "step 2720: generator_loss=2.0245862007141113, discriminator_loss=0.17676782608032227\n",
            "step 2721: generator_loss=2.034203052520752, discriminator_loss=0.1752118617296219\n",
            "step 2722: generator_loss=2.024860382080078, discriminator_loss=0.17506030201911926\n",
            "step 2723: generator_loss=1.998057246208191, discriminator_loss=0.1762617528438568\n",
            "step 2724: generator_loss=1.9612184762954712, discriminator_loss=0.17844364047050476\n",
            "step 2725: generator_loss=1.9096989631652832, discriminator_loss=0.18197816610336304\n",
            "step 2726: generator_loss=1.8486050367355347, discriminator_loss=0.18669769167900085\n",
            "step 2727: generator_loss=1.7891582250595093, discriminator_loss=0.19174161553382874\n",
            "step 2728: generator_loss=1.7335071563720703, discriminator_loss=0.1970151960849762\n",
            "step 2729: generator_loss=1.6832208633422852, discriminator_loss=0.20208817720413208\n",
            "step 2730: generator_loss=1.6580142974853516, discriminator_loss=0.2047751545906067\n",
            "step 2731: generator_loss=1.6467057466506958, discriminator_loss=0.20590254664421082\n",
            "step 2732: generator_loss=1.6514647006988525, discriminator_loss=0.20513460040092468\n",
            "step 2733: generator_loss=1.6720993518829346, discriminator_loss=0.20276284217834473\n",
            "step 2734: generator_loss=1.691802978515625, discriminator_loss=0.2005334198474884\n",
            "step 2735: generator_loss=1.7170228958129883, discriminator_loss=0.19784536957740784\n",
            "step 2736: generator_loss=1.7621634006500244, discriminator_loss=0.19294531643390656\n",
            "step 2737: generator_loss=1.8103809356689453, discriminator_loss=0.1879938542842865\n",
            "step 2738: generator_loss=1.8568845987319946, discriminator_loss=0.183567076921463\n",
            "step 2739: generator_loss=1.8959805965423584, discriminator_loss=0.1799401044845581\n",
            "step 2740: generator_loss=1.9307730197906494, discriminator_loss=0.1770276427268982\n",
            "step 2741: generator_loss=1.950064778327942, discriminator_loss=0.17520837485790253\n",
            "step 2742: generator_loss=1.9574964046478271, discriminator_loss=0.17434433102607727\n",
            "step 2743: generator_loss=1.9733604192733765, discriminator_loss=0.17280061542987823\n",
            "step 2744: generator_loss=2.0041720867156982, discriminator_loss=0.1700441539287567\n",
            "step 2745: generator_loss=2.0321767330169678, discriminator_loss=0.1675102561712265\n",
            "step 2746: generator_loss=2.051705837249756, discriminator_loss=0.1656351089477539\n",
            "step 2747: generator_loss=2.063291072845459, discriminator_loss=0.16436031460762024\n",
            "step 2748: generator_loss=2.057711124420166, discriminator_loss=0.16425156593322754\n",
            "step 2749: generator_loss=2.0409276485443115, discriminator_loss=0.16505083441734314\n",
            "step 2750: generator_loss=2.0288214683532715, discriminator_loss=0.16542664170265198\n",
            "step 2751: generator_loss=2.016641139984131, discriminator_loss=0.16590186953544617\n",
            "step 2752: generator_loss=2.03153920173645, discriminator_loss=0.16421492397785187\n",
            "step 2753: generator_loss=2.050751209259033, discriminator_loss=0.16242024302482605\n",
            "step 2754: generator_loss=2.0577383041381836, discriminator_loss=0.1613132804632187\n",
            "step 2755: generator_loss=2.066333770751953, discriminator_loss=0.16021476686000824\n",
            "step 2756: generator_loss=2.083223342895508, discriminator_loss=0.158500075340271\n",
            "step 2757: generator_loss=2.0955007076263428, discriminator_loss=0.15712830424308777\n",
            "step 2758: generator_loss=2.0998804569244385, discriminator_loss=0.15650951862335205\n",
            "step 2759: generator_loss=2.0939998626708984, discriminator_loss=0.1564752608537674\n",
            "step 2760: generator_loss=2.066617488861084, discriminator_loss=0.1580735146999359\n",
            "step 2761: generator_loss=2.0523500442504883, discriminator_loss=0.15877360105514526\n",
            "step 2762: generator_loss=2.0438294410705566, discriminator_loss=0.1590903103351593\n",
            "step 2763: generator_loss=2.029423236846924, discriminator_loss=0.1600521355867386\n",
            "step 2764: generator_loss=2.0146889686584473, discriminator_loss=0.16109120845794678\n",
            "step 2765: generator_loss=1.9990276098251343, discriminator_loss=0.16255858540534973\n",
            "step 2766: generator_loss=2.035684585571289, discriminator_loss=0.15957671403884888\n",
            "step 2767: generator_loss=2.046319007873535, discriminator_loss=0.1588657647371292\n",
            "step 2768: generator_loss=2.049691677093506, discriminator_loss=0.15912432968616486\n",
            "step 2769: generator_loss=2.063383102416992, discriminator_loss=0.1588379293680191\n",
            "step 2770: generator_loss=2.073244094848633, discriminator_loss=0.15987065434455872\n",
            "step 2771: generator_loss=2.072457790374756, discriminator_loss=0.1625647395849228\n",
            "step 2772: generator_loss=2.0936594009399414, discriminator_loss=0.1642889678478241\n",
            "step 2773: generator_loss=2.0704212188720703, discriminator_loss=0.17144565284252167\n",
            "step 2774: generator_loss=2.054363489151001, discriminator_loss=0.17795664072036743\n",
            "step 2775: generator_loss=2.137723922729492, discriminator_loss=0.16924068331718445\n",
            "step 2776: generator_loss=2.1155805587768555, discriminator_loss=0.1765337437391281\n",
            "step 2777: generator_loss=2.1315953731536865, discriminator_loss=0.17697279155254364\n",
            "step 2778: generator_loss=2.1786420345306396, discriminator_loss=0.17991438508033752\n",
            "step 2779: generator_loss=2.1207141876220703, discriminator_loss=0.18942606449127197\n",
            "step 2780: generator_loss=2.221259593963623, discriminator_loss=0.1851443350315094\n",
            "step 2781: generator_loss=2.1630096435546875, discriminator_loss=0.19730469584465027\n",
            "step 2782: generator_loss=2.05191707611084, discriminator_loss=0.21152332425117493\n",
            "step 2783: generator_loss=2.2219276428222656, discriminator_loss=0.20076262950897217\n",
            "step 2784: generator_loss=2.2621750831604004, discriminator_loss=0.19590085744857788\n",
            "step 2785: generator_loss=2.2226924896240234, discriminator_loss=0.19692480564117432\n",
            "step 2786: generator_loss=2.309798240661621, discriminator_loss=0.18882517516613007\n",
            "step 2787: generator_loss=2.5298399925231934, discriminator_loss=0.16795706748962402\n",
            "step 2788: generator_loss=2.7076759338378906, discriminator_loss=0.15454626083374023\n",
            "step 2789: generator_loss=2.3666152954101562, discriminator_loss=0.17907071113586426\n",
            "step 2790: generator_loss=2.6988887786865234, discriminator_loss=0.15359213948249817\n",
            "step 2791: generator_loss=2.682319164276123, discriminator_loss=0.15157103538513184\n",
            "step 2792: generator_loss=2.9727587699890137, discriminator_loss=0.13777564465999603\n",
            "step 2793: generator_loss=2.6591336727142334, discriminator_loss=0.1490403711795807\n",
            "step 2794: generator_loss=2.8121845722198486, discriminator_loss=0.14140626788139343\n",
            "step 2795: generator_loss=2.901730537414551, discriminator_loss=0.13587512075901031\n",
            "step 2796: generator_loss=2.6914572715759277, discriminator_loss=0.14382490515708923\n",
            "step 2797: generator_loss=2.8500852584838867, discriminator_loss=0.1336607038974762\n",
            "step 2798: generator_loss=2.8577167987823486, discriminator_loss=0.13379380106925964\n",
            "step 2799: generator_loss=2.8380656242370605, discriminator_loss=0.1337025761604309\n",
            "step 2800: generator_loss=2.831190586090088, discriminator_loss=0.13215795159339905\n",
            "step 2801: generator_loss=2.893026828765869, discriminator_loss=0.12880751490592957\n",
            "step 2802: generator_loss=2.8627028465270996, discriminator_loss=0.12960153818130493\n",
            "step 2803: generator_loss=2.743411064147949, discriminator_loss=0.13236631453037262\n",
            "step 2804: generator_loss=2.768738269805908, discriminator_loss=0.13339032232761383\n",
            "step 2805: generator_loss=2.6190433502197266, discriminator_loss=0.13639068603515625\n",
            "step 2806: generator_loss=2.477330207824707, discriminator_loss=0.13925838470458984\n",
            "step 2807: generator_loss=2.3880224227905273, discriminator_loss=0.14325737953186035\n",
            "step 2808: generator_loss=2.3247287273406982, discriminator_loss=0.14839690923690796\n",
            "step 2809: generator_loss=2.2530245780944824, discriminator_loss=0.15074147284030914\n",
            "step 2810: generator_loss=2.1070666313171387, discriminator_loss=0.15799137949943542\n",
            "step 2811: generator_loss=2.1355459690093994, discriminator_loss=0.1621934324502945\n",
            "step 2812: generator_loss=2.0582218170166016, discriminator_loss=0.1655830442905426\n",
            "step 2813: generator_loss=1.9617266654968262, discriminator_loss=0.1691901832818985\n",
            "step 2814: generator_loss=1.8667138814926147, discriminator_loss=0.17763344943523407\n",
            "step 2815: generator_loss=1.8691987991333008, discriminator_loss=0.175266295671463\n",
            "step 2816: generator_loss=1.7451285123825073, discriminator_loss=0.18473432958126068\n",
            "step 2817: generator_loss=1.7950928211212158, discriminator_loss=0.1840941607952118\n",
            "step 2818: generator_loss=1.805528163909912, discriminator_loss=0.18236088752746582\n",
            "step 2819: generator_loss=1.8275471925735474, discriminator_loss=0.1843058466911316\n",
            "step 2820: generator_loss=1.8557324409484863, discriminator_loss=0.18328312039375305\n",
            "step 2821: generator_loss=1.7395145893096924, discriminator_loss=0.18875469267368317\n",
            "step 2822: generator_loss=1.786064624786377, discriminator_loss=0.186237633228302\n",
            "step 2823: generator_loss=1.8907921314239502, discriminator_loss=0.17996472120285034\n",
            "step 2824: generator_loss=1.857893705368042, discriminator_loss=0.181467205286026\n",
            "step 2825: generator_loss=1.8120524883270264, discriminator_loss=0.1842685043811798\n",
            "step 2826: generator_loss=1.8350967168807983, discriminator_loss=0.18486499786376953\n",
            "step 2827: generator_loss=1.796332597732544, discriminator_loss=0.18595537543296814\n",
            "step 2828: generator_loss=1.8389148712158203, discriminator_loss=0.1854179948568344\n",
            "step 2829: generator_loss=1.8143928050994873, discriminator_loss=0.18698439002037048\n",
            "step 2830: generator_loss=1.7567901611328125, discriminator_loss=0.19040809571743011\n",
            "step 2831: generator_loss=1.7747700214385986, discriminator_loss=0.1899431347846985\n",
            "step 2832: generator_loss=1.7949284315109253, discriminator_loss=0.1885402798652649\n",
            "step 2833: generator_loss=1.7932703495025635, discriminator_loss=0.1867268979549408\n",
            "step 2834: generator_loss=1.7771966457366943, discriminator_loss=0.18738584220409393\n",
            "step 2835: generator_loss=1.8188250064849854, discriminator_loss=0.1835097372531891\n",
            "step 2836: generator_loss=1.8171041011810303, discriminator_loss=0.1833377480506897\n",
            "step 2837: generator_loss=1.8329288959503174, discriminator_loss=0.18190008401870728\n",
            "step 2838: generator_loss=1.852516770362854, discriminator_loss=0.18052349984645844\n",
            "step 2839: generator_loss=1.8408421277999878, discriminator_loss=0.18146473169326782\n",
            "step 2840: generator_loss=1.8439009189605713, discriminator_loss=0.18134205043315887\n",
            "step 2841: generator_loss=1.862716794013977, discriminator_loss=0.17986954748630524\n",
            "step 2842: generator_loss=1.862442135810852, discriminator_loss=0.17982281744480133\n",
            "step 2843: generator_loss=1.8493469953536987, discriminator_loss=0.1814655065536499\n",
            "step 2844: generator_loss=1.8303565979003906, discriminator_loss=0.18353106081485748\n",
            "step 2845: generator_loss=1.83562433719635, discriminator_loss=0.18329685926437378\n",
            "step 2846: generator_loss=1.8549726009368896, discriminator_loss=0.18232445418834686\n",
            "step 2847: generator_loss=1.8561726808547974, discriminator_loss=0.18193209171295166\n",
            "step 2848: generator_loss=1.8520030975341797, discriminator_loss=0.18135255575180054\n",
            "step 2849: generator_loss=1.9022293090820312, discriminator_loss=0.17666305601596832\n",
            "step 2850: generator_loss=1.957397699356079, discriminator_loss=0.17192429304122925\n",
            "step 2851: generator_loss=1.9802948236465454, discriminator_loss=0.16973356902599335\n",
            "step 2852: generator_loss=1.996532678604126, discriminator_loss=0.16818666458129883\n",
            "step 2853: generator_loss=2.0287399291992188, discriminator_loss=0.16556225717067719\n",
            "step 2854: generator_loss=2.0607070922851562, discriminator_loss=0.16294339299201965\n",
            "step 2855: generator_loss=2.0951855182647705, discriminator_loss=0.1599717140197754\n",
            "step 2856: generator_loss=2.10685658454895, discriminator_loss=0.1585586816072464\n",
            "step 2857: generator_loss=2.097177505493164, discriminator_loss=0.15874189138412476\n",
            "step 2858: generator_loss=2.0748090744018555, discriminator_loss=0.15954458713531494\n",
            "step 2859: generator_loss=2.0537126064300537, discriminator_loss=0.16070836782455444\n",
            "step 2860: generator_loss=2.04414439201355, discriminator_loss=0.16081607341766357\n",
            "step 2861: generator_loss=2.03419828414917, discriminator_loss=0.16087166965007782\n",
            "step 2862: generator_loss=2.0242509841918945, discriminator_loss=0.16107523441314697\n",
            "step 2863: generator_loss=2.008699417114258, discriminator_loss=0.16159850358963013\n",
            "step 2864: generator_loss=2.0127573013305664, discriminator_loss=0.16070525348186493\n",
            "step 2865: generator_loss=2.018454074859619, discriminator_loss=0.15965545177459717\n",
            "step 2866: generator_loss=2.0260562896728516, discriminator_loss=0.15843284130096436\n",
            "step 2867: generator_loss=2.0412282943725586, discriminator_loss=0.1566295623779297\n",
            "step 2868: generator_loss=2.059084415435791, discriminator_loss=0.15476056933403015\n",
            "step 2869: generator_loss=2.0676960945129395, discriminator_loss=0.1535009741783142\n",
            "step 2870: generator_loss=2.0693602561950684, discriminator_loss=0.15313471853733063\n",
            "step 2871: generator_loss=2.0731253623962402, discriminator_loss=0.15251260995864868\n",
            "step 2872: generator_loss=2.091196060180664, discriminator_loss=0.15101942420005798\n",
            "step 2873: generator_loss=2.1140246391296387, discriminator_loss=0.14927522838115692\n",
            "step 2874: generator_loss=2.1404824256896973, discriminator_loss=0.14735147356987\n",
            "step 2875: generator_loss=2.176939010620117, discriminator_loss=0.1447363793849945\n",
            "step 2876: generator_loss=2.2056479454040527, discriminator_loss=0.14265164732933044\n",
            "step 2877: generator_loss=2.2213234901428223, discriminator_loss=0.14159345626831055\n",
            "step 2878: generator_loss=2.2185211181640625, discriminator_loss=0.14159539341926575\n",
            "step 2879: generator_loss=2.1922855377197266, discriminator_loss=0.14285175502300262\n",
            "step 2880: generator_loss=2.1671581268310547, discriminator_loss=0.14416638016700745\n",
            "step 2881: generator_loss=2.1454110145568848, discriminator_loss=0.14530298113822937\n",
            "step 2882: generator_loss=2.1228785514831543, discriminator_loss=0.14645570516586304\n",
            "step 2883: generator_loss=2.1060235500335693, discriminator_loss=0.14734399318695068\n",
            "step 2884: generator_loss=2.097931146621704, discriminator_loss=0.14766880869865417\n",
            "step 2885: generator_loss=2.086181402206421, discriminator_loss=0.14815816283226013\n",
            "step 2886: generator_loss=2.073033332824707, discriminator_loss=0.14898061752319336\n",
            "step 2887: generator_loss=2.059230327606201, discriminator_loss=0.14981096982955933\n",
            "step 2888: generator_loss=2.0500032901763916, discriminator_loss=0.15033534169197083\n",
            "step 2889: generator_loss=2.0393402576446533, discriminator_loss=0.15102994441986084\n",
            "step 2890: generator_loss=2.0235650539398193, discriminator_loss=0.15209710597991943\n",
            "step 2891: generator_loss=2.0006606578826904, discriminator_loss=0.15379184484481812\n",
            "step 2892: generator_loss=1.9742728471755981, discriminator_loss=0.15585988759994507\n",
            "step 2893: generator_loss=1.9497207403182983, discriminator_loss=0.1578361839056015\n",
            "step 2894: generator_loss=1.922044277191162, discriminator_loss=0.16019776463508606\n",
            "step 2895: generator_loss=1.9025533199310303, discriminator_loss=0.16198331117630005\n",
            "step 2896: generator_loss=1.8814629316329956, discriminator_loss=0.16402873396873474\n",
            "step 2897: generator_loss=1.8664402961730957, discriminator_loss=0.16562891006469727\n",
            "step 2898: generator_loss=1.873410701751709, discriminator_loss=0.16524451971054077\n",
            "step 2899: generator_loss=1.8936989307403564, discriminator_loss=0.16361352801322937\n",
            "step 2900: generator_loss=1.9368882179260254, discriminator_loss=0.15997663140296936\n",
            "step 2901: generator_loss=1.9853242635726929, discriminator_loss=0.15614007413387299\n",
            "step 2902: generator_loss=2.022909164428711, discriminator_loss=0.15328660607337952\n",
            "step 2903: generator_loss=2.0656354427337646, discriminator_loss=0.15013271570205688\n",
            "step 2904: generator_loss=2.115111827850342, discriminator_loss=0.14658531546592712\n",
            "step 2905: generator_loss=2.152712821960449, discriminator_loss=0.14396971464157104\n",
            "step 2906: generator_loss=2.1667120456695557, discriminator_loss=0.14292633533477783\n",
            "step 2907: generator_loss=2.1662468910217285, discriminator_loss=0.1427374929189682\n",
            "step 2908: generator_loss=2.1568429470062256, discriminator_loss=0.14300279319286346\n",
            "step 2909: generator_loss=2.1395819187164307, discriminator_loss=0.14393064379692078\n",
            "step 2910: generator_loss=2.116636276245117, discriminator_loss=0.14515116810798645\n",
            "step 2911: generator_loss=2.087611198425293, discriminator_loss=0.14682060480117798\n",
            "step 2912: generator_loss=2.0564656257629395, discriminator_loss=0.14877328276634216\n",
            "step 2913: generator_loss=2.020996570587158, discriminator_loss=0.1512400209903717\n",
            "step 2914: generator_loss=1.9791769981384277, discriminator_loss=0.15425479412078857\n",
            "step 2915: generator_loss=1.940152645111084, discriminator_loss=0.15735256671905518\n",
            "step 2916: generator_loss=1.9223822355270386, discriminator_loss=0.1587747037410736\n",
            "step 2917: generator_loss=1.9186091423034668, discriminator_loss=0.15906664729118347\n",
            "step 2918: generator_loss=1.926548719406128, discriminator_loss=0.15840351581573486\n",
            "step 2919: generator_loss=1.9635682106018066, discriminator_loss=0.1552533209323883\n",
            "step 2920: generator_loss=2.008794069290161, discriminator_loss=0.15178850293159485\n",
            "step 2921: generator_loss=2.054872512817383, discriminator_loss=0.14845438301563263\n",
            "step 2922: generator_loss=2.092975616455078, discriminator_loss=0.14573830366134644\n",
            "step 2923: generator_loss=2.1172990798950195, discriminator_loss=0.14424782991409302\n",
            "step 2924: generator_loss=2.141599178314209, discriminator_loss=0.14262188971042633\n",
            "step 2925: generator_loss=2.1542532444000244, discriminator_loss=0.14189225435256958\n",
            "step 2926: generator_loss=2.1591763496398926, discriminator_loss=0.14151152968406677\n",
            "step 2927: generator_loss=2.1590826511383057, discriminator_loss=0.14142325520515442\n",
            "step 2928: generator_loss=2.1640114784240723, discriminator_loss=0.1410035490989685\n",
            "step 2929: generator_loss=2.1693742275238037, discriminator_loss=0.14044831693172455\n",
            "step 2930: generator_loss=2.172694444656372, discriminator_loss=0.14001011848449707\n",
            "step 2931: generator_loss=2.175049304962158, discriminator_loss=0.13958606123924255\n",
            "step 2932: generator_loss=2.1759133338928223, discriminator_loss=0.13921304047107697\n",
            "step 2933: generator_loss=2.1644840240478516, discriminator_loss=0.1396322399377823\n",
            "step 2934: generator_loss=2.1633002758026123, discriminator_loss=0.1395300328731537\n",
            "step 2935: generator_loss=2.1608681678771973, discriminator_loss=0.13948430120944977\n",
            "step 2936: generator_loss=2.1766374111175537, discriminator_loss=0.13821130990982056\n",
            "step 2937: generator_loss=2.1981234550476074, discriminator_loss=0.13670268654823303\n",
            "step 2938: generator_loss=2.2162656784057617, discriminator_loss=0.1354626715183258\n",
            "step 2939: generator_loss=2.2252397537231445, discriminator_loss=0.1346905678510666\n",
            "step 2940: generator_loss=2.2424275875091553, discriminator_loss=0.1334344744682312\n",
            "step 2941: generator_loss=2.2534425258636475, discriminator_loss=0.13272500038146973\n",
            "step 2942: generator_loss=2.2498676776885986, discriminator_loss=0.13273224234580994\n",
            "step 2943: generator_loss=2.232090473175049, discriminator_loss=0.13370192050933838\n",
            "step 2944: generator_loss=2.2095072269439697, discriminator_loss=0.13515418767929077\n",
            "step 2945: generator_loss=2.176875591278076, discriminator_loss=0.1372261941432953\n",
            "step 2946: generator_loss=2.140324831008911, discriminator_loss=0.1397046446800232\n",
            "step 2947: generator_loss=2.109821081161499, discriminator_loss=0.1418635994195938\n",
            "step 2948: generator_loss=2.0905094146728516, discriminator_loss=0.14324599504470825\n",
            "step 2949: generator_loss=2.068964958190918, discriminator_loss=0.1448473334312439\n",
            "step 2950: generator_loss=2.0512759685516357, discriminator_loss=0.14624741673469543\n",
            "step 2951: generator_loss=2.0343546867370605, discriminator_loss=0.147609680891037\n",
            "step 2952: generator_loss=2.0504915714263916, discriminator_loss=0.14671045541763306\n",
            "step 2953: generator_loss=2.0818710327148438, discriminator_loss=0.14464625716209412\n",
            "step 2954: generator_loss=2.134378433227539, discriminator_loss=0.14116071164608002\n",
            "step 2955: generator_loss=2.1787309646606445, discriminator_loss=0.1385515332221985\n",
            "step 2956: generator_loss=2.211778163909912, discriminator_loss=0.1365603357553482\n",
            "step 2957: generator_loss=2.2314672470092773, discriminator_loss=0.13547274470329285\n",
            "step 2958: generator_loss=2.2712440490722656, discriminator_loss=0.13308613002300262\n",
            "step 2959: generator_loss=2.296787977218628, discriminator_loss=0.1315493881702423\n",
            "step 2960: generator_loss=2.3121747970581055, discriminator_loss=0.13043484091758728\n",
            "step 2961: generator_loss=2.331305742263794, discriminator_loss=0.12902627885341644\n",
            "step 2962: generator_loss=2.340219020843506, discriminator_loss=0.12809737026691437\n",
            "step 2963: generator_loss=2.331605911254883, discriminator_loss=0.1280122995376587\n",
            "step 2964: generator_loss=2.3075380325317383, discriminator_loss=0.1289103776216507\n",
            "step 2965: generator_loss=2.269516944885254, discriminator_loss=0.1307247132062912\n",
            "step 2966: generator_loss=2.219432830810547, discriminator_loss=0.13334232568740845\n",
            "step 2967: generator_loss=2.1698524951934814, discriminator_loss=0.136216938495636\n",
            "step 2968: generator_loss=2.1304049491882324, discriminator_loss=0.138605996966362\n",
            "step 2969: generator_loss=2.1052701473236084, discriminator_loss=0.14019179344177246\n",
            "step 2970: generator_loss=2.077401638031006, discriminator_loss=0.14204341173171997\n",
            "step 2971: generator_loss=2.057520627975464, discriminator_loss=0.14336836338043213\n",
            "step 2972: generator_loss=2.0521321296691895, discriminator_loss=0.14372298121452332\n",
            "step 2973: generator_loss=2.042393445968628, discriminator_loss=0.14438334107398987\n",
            "step 2974: generator_loss=2.026658058166504, discriminator_loss=0.14552269876003265\n",
            "step 2975: generator_loss=2.006103277206421, discriminator_loss=0.14706462621688843\n",
            "step 2976: generator_loss=1.985557198524475, discriminator_loss=0.14871282875537872\n",
            "step 2977: generator_loss=2.0029897689819336, discriminator_loss=0.14743824303150177\n",
            "step 2978: generator_loss=2.0227556228637695, discriminator_loss=0.14595571160316467\n",
            "step 2979: generator_loss=2.0360589027404785, discriminator_loss=0.1450951099395752\n",
            "step 2980: generator_loss=2.0407228469848633, discriminator_loss=0.14494222402572632\n",
            "step 2981: generator_loss=2.044830560684204, discriminator_loss=0.14473748207092285\n",
            "step 2982: generator_loss=2.0366768836975098, discriminator_loss=0.1456415057182312\n",
            "step 2983: generator_loss=2.018404483795166, discriminator_loss=0.14721107482910156\n",
            "step 2984: generator_loss=1.9978018999099731, discriminator_loss=0.14911052584648132\n",
            "step 2985: generator_loss=1.9854334592819214, discriminator_loss=0.15034882724285126\n",
            "step 2986: generator_loss=1.9796710014343262, discriminator_loss=0.15103618800640106\n",
            "step 2987: generator_loss=1.9836435317993164, discriminator_loss=0.1510169804096222\n",
            "step 2988: generator_loss=2.0001444816589355, discriminator_loss=0.14997604489326477\n",
            "step 2989: generator_loss=2.0171751976013184, discriminator_loss=0.1490471363067627\n",
            "step 2990: generator_loss=2.034837007522583, discriminator_loss=0.1480782926082611\n",
            "step 2991: generator_loss=2.040493965148926, discriminator_loss=0.14814728498458862\n",
            "step 2992: generator_loss=2.0423460006713867, discriminator_loss=0.14819592237472534\n",
            "step 2993: generator_loss=2.0368309020996094, discriminator_loss=0.148810476064682\n",
            "step 2994: generator_loss=2.0211892127990723, discriminator_loss=0.15021564066410065\n",
            "step 2995: generator_loss=2.027322769165039, discriminator_loss=0.14991672337055206\n",
            "step 2996: generator_loss=2.0399389266967773, discriminator_loss=0.14914824068546295\n",
            "step 2997: generator_loss=2.06288480758667, discriminator_loss=0.14757147431373596\n",
            "step 2998: generator_loss=2.081789016723633, discriminator_loss=0.14624060690402985\n",
            "step 2999: generator_loss=2.0945279598236084, discriminator_loss=0.1452779769897461\n",
            "step 3000: generator_loss=2.087733745574951, discriminator_loss=0.14572805166244507\n",
            "step 3001: generator_loss=2.069744110107422, discriminator_loss=0.14672720432281494\n",
            "step 3002: generator_loss=2.04764986038208, discriminator_loss=0.14812670648097992\n",
            "step 3003: generator_loss=2.0192809104919434, discriminator_loss=0.14997249841690063\n",
            "step 3004: generator_loss=1.9795022010803223, discriminator_loss=0.15299610793590546\n",
            "step 3005: generator_loss=1.9684133529663086, discriminator_loss=0.15388858318328857\n",
            "step 3006: generator_loss=1.968158483505249, discriminator_loss=0.15388181805610657\n",
            "step 3007: generator_loss=1.972113847732544, discriminator_loss=0.15349222719669342\n",
            "step 3008: generator_loss=1.98020339012146, discriminator_loss=0.15275584161281586\n",
            "step 3009: generator_loss=2.0034327507019043, discriminator_loss=0.1510540246963501\n",
            "step 3010: generator_loss=2.055776596069336, discriminator_loss=0.14711619913578033\n",
            "step 3011: generator_loss=2.1180531978607178, discriminator_loss=0.14269563555717468\n",
            "step 3012: generator_loss=2.166205406188965, discriminator_loss=0.13946616649627686\n",
            "step 3013: generator_loss=2.1969399452209473, discriminator_loss=0.1374102085828781\n",
            "step 3014: generator_loss=2.2182908058166504, discriminator_loss=0.13606488704681396\n",
            "step 3015: generator_loss=2.2441115379333496, discriminator_loss=0.13417498767375946\n",
            "step 3016: generator_loss=2.2934601306915283, discriminator_loss=0.1311212033033371\n",
            "step 3017: generator_loss=2.3341619968414307, discriminator_loss=0.1285940557718277\n",
            "step 3018: generator_loss=2.3547685146331787, discriminator_loss=0.1270047277212143\n",
            "step 3019: generator_loss=2.3545851707458496, discriminator_loss=0.12663760781288147\n",
            "step 3020: generator_loss=2.360741376876831, discriminator_loss=0.12576957046985626\n",
            "step 3021: generator_loss=2.348768472671509, discriminator_loss=0.12580657005310059\n",
            "step 3022: generator_loss=2.3550400733947754, discriminator_loss=0.12489110231399536\n",
            "step 3023: generator_loss=2.366468906402588, discriminator_loss=0.12375620752573013\n",
            "step 3024: generator_loss=2.3603081703186035, discriminator_loss=0.12340810894966125\n",
            "step 3025: generator_loss=2.3463759422302246, discriminator_loss=0.12349751591682434\n",
            "step 3026: generator_loss=2.3288393020629883, discriminator_loss=0.1239517405629158\n",
            "step 3027: generator_loss=2.2989084720611572, discriminator_loss=0.1250588297843933\n",
            "step 3028: generator_loss=2.270991325378418, discriminator_loss=0.126126229763031\n",
            "step 3029: generator_loss=2.2333717346191406, discriminator_loss=0.12795321643352509\n",
            "step 3030: generator_loss=2.2065484523773193, discriminator_loss=0.1292305439710617\n",
            "step 3031: generator_loss=2.1867918968200684, discriminator_loss=0.13016119599342346\n",
            "step 3032: generator_loss=2.1614537239074707, discriminator_loss=0.1314694583415985\n",
            "step 3033: generator_loss=2.12626051902771, discriminator_loss=0.13357505202293396\n",
            "step 3034: generator_loss=2.085041046142578, discriminator_loss=0.1362568885087967\n",
            "step 3035: generator_loss=2.041522979736328, discriminator_loss=0.1392851173877716\n",
            "step 3036: generator_loss=2.011573076248169, discriminator_loss=0.14139941334724426\n",
            "step 3037: generator_loss=1.9785995483398438, discriminator_loss=0.14399737119674683\n",
            "step 3038: generator_loss=1.9516575336456299, discriminator_loss=0.14627504348754883\n",
            "step 3039: generator_loss=1.9267361164093018, discriminator_loss=0.1484813094139099\n",
            "step 3040: generator_loss=1.9188652038574219, discriminator_loss=0.14941835403442383\n",
            "step 3041: generator_loss=1.9247221946716309, discriminator_loss=0.149268239736557\n",
            "step 3042: generator_loss=1.9395813941955566, discriminator_loss=0.14847546815872192\n",
            "step 3043: generator_loss=1.9590203762054443, discriminator_loss=0.1474299430847168\n",
            "step 3044: generator_loss=1.9710915088653564, discriminator_loss=0.14692528545856476\n",
            "step 3045: generator_loss=1.9787676334381104, discriminator_loss=0.1470368206501007\n",
            "step 3046: generator_loss=1.9851733446121216, discriminator_loss=0.14720359444618225\n",
            "step 3047: generator_loss=2.025599718093872, discriminator_loss=0.14468087255954742\n",
            "step 3048: generator_loss=2.102052688598633, discriminator_loss=0.13970613479614258\n",
            "step 3049: generator_loss=2.1779026985168457, discriminator_loss=0.13509172201156616\n",
            "step 3050: generator_loss=2.2361416816711426, discriminator_loss=0.13179096579551697\n",
            "step 3051: generator_loss=2.279475450515747, discriminator_loss=0.12940062582492828\n",
            "step 3052: generator_loss=2.303614616394043, discriminator_loss=0.12818071246147156\n",
            "step 3053: generator_loss=2.3063058853149414, discriminator_loss=0.12801812589168549\n",
            "step 3054: generator_loss=2.3084352016448975, discriminator_loss=0.12771964073181152\n",
            "step 3055: generator_loss=2.305039405822754, discriminator_loss=0.12771938741207123\n",
            "step 3056: generator_loss=2.3041176795959473, discriminator_loss=0.12725159525871277\n",
            "step 3057: generator_loss=2.317509174346924, discriminator_loss=0.12615275382995605\n",
            "step 3058: generator_loss=2.3249974250793457, discriminator_loss=0.12528793513774872\n",
            "step 3059: generator_loss=2.3409388065338135, discriminator_loss=0.12400652468204498\n",
            "step 3060: generator_loss=2.3453352451324463, discriminator_loss=0.123445063829422\n",
            "step 3061: generator_loss=2.3432559967041016, discriminator_loss=0.12306123226881027\n",
            "step 3062: generator_loss=2.3238959312438965, discriminator_loss=0.12382388859987259\n",
            "step 3063: generator_loss=2.317781448364258, discriminator_loss=0.12377506494522095\n",
            "step 3064: generator_loss=2.3007235527038574, discriminator_loss=0.1243874579668045\n",
            "step 3065: generator_loss=2.2802860736846924, discriminator_loss=0.125130295753479\n",
            "step 3066: generator_loss=2.299891233444214, discriminator_loss=0.12369529902935028\n",
            "step 3067: generator_loss=2.3349742889404297, discriminator_loss=0.12147478759288788\n",
            "step 3068: generator_loss=2.369879961013794, discriminator_loss=0.11935320496559143\n",
            "step 3069: generator_loss=2.403911828994751, discriminator_loss=0.11740364134311676\n",
            "step 3070: generator_loss=2.423602819442749, discriminator_loss=0.11616809666156769\n",
            "step 3071: generator_loss=2.437324047088623, discriminator_loss=0.11525609344244003\n",
            "step 3072: generator_loss=2.446126937866211, discriminator_loss=0.11466936767101288\n",
            "step 3073: generator_loss=2.4513752460479736, discriminator_loss=0.11422279477119446\n",
            "step 3074: generator_loss=2.4773831367492676, discriminator_loss=0.11279461532831192\n",
            "step 3075: generator_loss=2.5100598335266113, discriminator_loss=0.11100146919488907\n",
            "step 3076: generator_loss=2.5280494689941406, discriminator_loss=0.10994505882263184\n",
            "step 3077: generator_loss=2.5253942012786865, discriminator_loss=0.10981228947639465\n",
            "step 3078: generator_loss=2.522012710571289, discriminator_loss=0.10956158488988876\n",
            "step 3079: generator_loss=2.516655921936035, discriminator_loss=0.10935837775468826\n",
            "step 3080: generator_loss=2.534958600997925, discriminator_loss=0.10811007022857666\n",
            "step 3081: generator_loss=2.555737018585205, discriminator_loss=0.106877900660038\n",
            "step 3082: generator_loss=2.5600576400756836, discriminator_loss=0.10625305771827698\n",
            "step 3083: generator_loss=2.555349111557007, discriminator_loss=0.10606750845909119\n",
            "step 3084: generator_loss=2.538114547729492, discriminator_loss=0.10646864026784897\n",
            "step 3085: generator_loss=2.509035110473633, discriminator_loss=0.10731607675552368\n",
            "step 3086: generator_loss=2.4765663146972656, discriminator_loss=0.10842718929052353\n",
            "step 3087: generator_loss=2.433518409729004, discriminator_loss=0.1101207509636879\n",
            "step 3088: generator_loss=2.383357524871826, discriminator_loss=0.11239594966173172\n",
            "step 3089: generator_loss=2.3352012634277344, discriminator_loss=0.11471632122993469\n",
            "step 3090: generator_loss=2.3059463500976562, discriminator_loss=0.11621053516864777\n",
            "step 3091: generator_loss=2.2954459190368652, discriminator_loss=0.11678136140108109\n",
            "step 3092: generator_loss=2.3291141986846924, discriminator_loss=0.11498357355594635\n",
            "step 3093: generator_loss=2.3781025409698486, discriminator_loss=0.11253973841667175\n",
            "step 3094: generator_loss=2.4399874210357666, discriminator_loss=0.10962021350860596\n",
            "step 3095: generator_loss=2.502056360244751, discriminator_loss=0.10678297281265259\n",
            "step 3096: generator_loss=2.548736095428467, discriminator_loss=0.10485365986824036\n",
            "step 3097: generator_loss=2.593686580657959, discriminator_loss=0.10299713164567947\n",
            "step 3098: generator_loss=2.6383185386657715, discriminator_loss=0.10110202431678772\n",
            "step 3099: generator_loss=2.6835885047912598, discriminator_loss=0.09936945140361786\n",
            "step 3100: generator_loss=2.6985230445861816, discriminator_loss=0.09866209328174591\n",
            "step 3101: generator_loss=2.6867053508758545, discriminator_loss=0.09881439805030823\n",
            "step 3102: generator_loss=2.678104877471924, discriminator_loss=0.09890146553516388\n",
            "step 3103: generator_loss=2.6800758838653564, discriminator_loss=0.09855661541223526\n",
            "step 3104: generator_loss=2.702545404434204, discriminator_loss=0.09737543761730194\n",
            "step 3105: generator_loss=2.7088286876678467, discriminator_loss=0.09690698236227036\n",
            "step 3106: generator_loss=2.716045618057251, discriminator_loss=0.09625788033008575\n",
            "step 3107: generator_loss=2.713775157928467, discriminator_loss=0.09607252478599548\n",
            "step 3108: generator_loss=2.6951141357421875, discriminator_loss=0.09646598994731903\n",
            "step 3109: generator_loss=2.6634843349456787, discriminator_loss=0.09735685586929321\n",
            "step 3110: generator_loss=2.640827178955078, discriminator_loss=0.09792610257863998\n",
            "step 3111: generator_loss=2.6283111572265625, discriminator_loss=0.09816049039363861\n",
            "step 3112: generator_loss=2.623002052307129, discriminator_loss=0.09814095497131348\n",
            "step 3113: generator_loss=2.634127140045166, discriminator_loss=0.09745605289936066\n",
            "step 3114: generator_loss=2.664809226989746, discriminator_loss=0.09603135287761688\n",
            "step 3115: generator_loss=2.691133975982666, discriminator_loss=0.09486795961856842\n",
            "step 3116: generator_loss=2.7026522159576416, discriminator_loss=0.09432616829872131\n",
            "step 3117: generator_loss=2.696981906890869, discriminator_loss=0.09438972920179367\n",
            "step 3118: generator_loss=2.6746926307678223, discriminator_loss=0.09511368721723557\n",
            "step 3119: generator_loss=2.659832239151001, discriminator_loss=0.09552637487649918\n",
            "step 3120: generator_loss=2.651209831237793, discriminator_loss=0.09582532942295074\n",
            "step 3121: generator_loss=2.624837875366211, discriminator_loss=0.0966687723994255\n",
            "step 3122: generator_loss=2.5831797122955322, discriminator_loss=0.09821861237287521\n",
            "step 3123: generator_loss=2.5438718795776367, discriminator_loss=0.09980300068855286\n",
            "step 3124: generator_loss=2.5253348350524902, discriminator_loss=0.10049480944871902\n",
            "step 3125: generator_loss=2.5058350563049316, discriminator_loss=0.10124185681343079\n",
            "step 3126: generator_loss=2.4951729774475098, discriminator_loss=0.10168422758579254\n",
            "step 3127: generator_loss=2.492567777633667, discriminator_loss=0.10168334096670151\n",
            "step 3128: generator_loss=2.497408866882324, discriminator_loss=0.10143312811851501\n",
            "step 3129: generator_loss=2.4998388290405273, discriminator_loss=0.10122086107730865\n",
            "step 3130: generator_loss=2.5099902153015137, discriminator_loss=0.10070423781871796\n",
            "step 3131: generator_loss=2.5331437587738037, discriminator_loss=0.09953445196151733\n",
            "step 3132: generator_loss=2.551058769226074, discriminator_loss=0.09866447746753693\n",
            "step 3133: generator_loss=2.5679516792297363, discriminator_loss=0.09780966490507126\n",
            "step 3134: generator_loss=2.581486701965332, discriminator_loss=0.09710130840539932\n",
            "step 3135: generator_loss=2.5891575813293457, discriminator_loss=0.09651944786310196\n",
            "step 3136: generator_loss=2.581936836242676, discriminator_loss=0.09658773988485336\n",
            "step 3137: generator_loss=2.561134099960327, discriminator_loss=0.09724032133817673\n",
            "step 3138: generator_loss=2.535254955291748, discriminator_loss=0.09810996055603027\n",
            "step 3139: generator_loss=2.5021796226501465, discriminator_loss=0.09941299259662628\n",
            "step 3140: generator_loss=2.4628586769104004, discriminator_loss=0.10100185871124268\n",
            "step 3141: generator_loss=2.432425022125244, discriminator_loss=0.10226427018642426\n",
            "step 3142: generator_loss=2.399867534637451, discriminator_loss=0.10374114662408829\n",
            "step 3143: generator_loss=2.3772315979003906, discriminator_loss=0.10477888584136963\n",
            "step 3144: generator_loss=2.3711581230163574, discriminator_loss=0.10507450252771378\n",
            "step 3145: generator_loss=2.3747975826263428, discriminator_loss=0.10480044782161713\n",
            "step 3146: generator_loss=2.378772258758545, discriminator_loss=0.10459621250629425\n",
            "step 3147: generator_loss=2.3810930252075195, discriminator_loss=0.10449616611003876\n",
            "step 3148: generator_loss=2.3854215145111084, discriminator_loss=0.10446412861347198\n",
            "step 3149: generator_loss=2.394207000732422, discriminator_loss=0.10466539114713669\n",
            "step 3150: generator_loss=2.4046897888183594, discriminator_loss=0.1042003259062767\n",
            "step 3151: generator_loss=2.378655433654785, discriminator_loss=0.10644786059856415\n",
            "step 3152: generator_loss=2.2843198776245117, discriminator_loss=0.11361587047576904\n",
            "step 3153: generator_loss=2.2401318550109863, discriminator_loss=0.11802853643894196\n",
            "step 3154: generator_loss=2.1657826900482178, discriminator_loss=0.12734836339950562\n",
            "step 3155: generator_loss=2.072031021118164, discriminator_loss=0.13874593377113342\n",
            "step 3156: generator_loss=2.056164026260376, discriminator_loss=0.14786039292812347\n",
            "step 3157: generator_loss=1.9001202583312988, discriminator_loss=0.18210861086845398\n",
            "step 3158: generator_loss=1.8975260257720947, discriminator_loss=0.1824427843093872\n",
            "step 3159: generator_loss=1.8079986572265625, discriminator_loss=0.21106195449829102\n",
            "step 3160: generator_loss=2.0750608444213867, discriminator_loss=0.18598875403404236\n",
            "step 3161: generator_loss=1.880730390548706, discriminator_loss=0.21614617109298706\n",
            "step 3162: generator_loss=1.5759954452514648, discriminator_loss=0.27607664465904236\n",
            "step 3163: generator_loss=1.9755750894546509, discriminator_loss=0.21776065230369568\n",
            "step 3164: generator_loss=2.068448305130005, discriminator_loss=0.20299267768859863\n",
            "step 3165: generator_loss=2.030646562576294, discriminator_loss=0.1893339455127716\n",
            "step 3166: generator_loss=2.276658535003662, discriminator_loss=0.17059090733528137\n",
            "step 3167: generator_loss=2.342862129211426, discriminator_loss=0.1587958186864853\n",
            "step 3168: generator_loss=2.187746524810791, discriminator_loss=0.15081748366355896\n",
            "step 3169: generator_loss=2.522578001022339, discriminator_loss=0.13774356245994568\n",
            "step 3170: generator_loss=2.652521848678589, discriminator_loss=0.12940192222595215\n",
            "step 3171: generator_loss=2.9971351623535156, discriminator_loss=0.11940205097198486\n",
            "step 3172: generator_loss=2.794541835784912, discriminator_loss=0.12495572865009308\n",
            "step 3173: generator_loss=2.6223607063293457, discriminator_loss=0.12436458468437195\n",
            "step 3174: generator_loss=3.088531970977783, discriminator_loss=0.11613182723522186\n",
            "step 3175: generator_loss=2.5102078914642334, discriminator_loss=0.12940409779548645\n",
            "step 3176: generator_loss=2.8270974159240723, discriminator_loss=0.11975326389074326\n",
            "step 3177: generator_loss=2.7081809043884277, discriminator_loss=0.12498639523983002\n",
            "step 3178: generator_loss=2.790454864501953, discriminator_loss=0.12201572954654694\n",
            "step 3179: generator_loss=2.6677818298339844, discriminator_loss=0.12065008282661438\n",
            "step 3180: generator_loss=2.65647554397583, discriminator_loss=0.12082824110984802\n",
            "step 3181: generator_loss=2.6670844554901123, discriminator_loss=0.11907891929149628\n",
            "step 3182: generator_loss=2.636448383331299, discriminator_loss=0.12050164490938187\n",
            "step 3183: generator_loss=2.5667757987976074, discriminator_loss=0.11943319439888\n",
            "step 3184: generator_loss=2.535841703414917, discriminator_loss=0.12368692457675934\n",
            "step 3185: generator_loss=2.4492921829223633, discriminator_loss=0.12335805594921112\n",
            "step 3186: generator_loss=2.5171642303466797, discriminator_loss=0.11827602237462997\n",
            "step 3187: generator_loss=2.5261921882629395, discriminator_loss=0.11604668945074081\n",
            "step 3188: generator_loss=2.7571771144866943, discriminator_loss=0.10709277540445328\n",
            "step 3189: generator_loss=2.855409860610962, discriminator_loss=0.10309482365846634\n",
            "step 3190: generator_loss=2.7612123489379883, discriminator_loss=0.1057167574763298\n",
            "step 3191: generator_loss=2.804558277130127, discriminator_loss=0.10204219818115234\n",
            "step 3192: generator_loss=2.768127679824829, discriminator_loss=0.10388556122779846\n",
            "step 3193: generator_loss=2.7566683292388916, discriminator_loss=0.10283659398555756\n",
            "step 3194: generator_loss=2.7850022315979004, discriminator_loss=0.10239376127719879\n",
            "step 3195: generator_loss=2.7001662254333496, discriminator_loss=0.10411449521780014\n",
            "step 3196: generator_loss=2.589972496032715, discriminator_loss=0.10933432728052139\n",
            "step 3197: generator_loss=2.622744083404541, discriminator_loss=0.10777536034584045\n",
            "step 3198: generator_loss=2.452669143676758, discriminator_loss=0.11478453129529953\n",
            "step 3199: generator_loss=2.529989242553711, discriminator_loss=0.11213243007659912\n",
            "step 3200: generator_loss=2.6753158569335938, discriminator_loss=0.1082550585269928\n",
            "step 3201: generator_loss=2.6527538299560547, discriminator_loss=0.1100882738828659\n",
            "step 3202: generator_loss=2.700554609298706, discriminator_loss=0.10624130070209503\n",
            "step 3203: generator_loss=2.704488754272461, discriminator_loss=0.10476943850517273\n",
            "step 3204: generator_loss=2.839231014251709, discriminator_loss=0.10127885639667511\n",
            "step 3205: generator_loss=2.8640894889831543, discriminator_loss=0.09890975058078766\n",
            "step 3206: generator_loss=2.814708709716797, discriminator_loss=0.10229828953742981\n",
            "step 3207: generator_loss=2.7908220291137695, discriminator_loss=0.10146670043468475\n",
            "step 3208: generator_loss=2.7484612464904785, discriminator_loss=0.10478154569864273\n",
            "step 3209: generator_loss=2.7916862964630127, discriminator_loss=0.10437087714672089\n",
            "step 3210: generator_loss=2.729381799697876, discriminator_loss=0.10474439710378647\n",
            "step 3211: generator_loss=2.6144025325775146, discriminator_loss=0.10958831012248993\n",
            "step 3212: generator_loss=2.6184487342834473, discriminator_loss=0.11316395550966263\n",
            "step 3213: generator_loss=2.4805712699890137, discriminator_loss=0.11490648984909058\n",
            "step 3214: generator_loss=2.259087562561035, discriminator_loss=0.12734739482402802\n",
            "step 3215: generator_loss=2.371140480041504, discriminator_loss=0.12354174256324768\n",
            "step 3216: generator_loss=2.2710278034210205, discriminator_loss=0.12872529029846191\n",
            "step 3217: generator_loss=2.189683437347412, discriminator_loss=0.1336694210767746\n",
            "step 3218: generator_loss=2.196481227874756, discriminator_loss=0.1321268528699875\n",
            "step 3219: generator_loss=2.0493264198303223, discriminator_loss=0.13964968919754028\n",
            "step 3220: generator_loss=2.039083242416382, discriminator_loss=0.1390235722064972\n",
            "step 3221: generator_loss=1.9454931020736694, discriminator_loss=0.14268627762794495\n",
            "step 3222: generator_loss=2.0428991317749023, discriminator_loss=0.135477215051651\n",
            "step 3223: generator_loss=2.1983399391174316, discriminator_loss=0.12597395479679108\n",
            "step 3224: generator_loss=2.1959049701690674, discriminator_loss=0.1262633353471756\n",
            "step 3225: generator_loss=2.378938913345337, discriminator_loss=0.11719542741775513\n",
            "step 3226: generator_loss=2.400026798248291, discriminator_loss=0.11587921530008316\n",
            "step 3227: generator_loss=2.3476762771606445, discriminator_loss=0.11763732880353928\n",
            "step 3228: generator_loss=2.3585352897644043, discriminator_loss=0.11630527675151825\n",
            "step 3229: generator_loss=2.3487510681152344, discriminator_loss=0.11755293607711792\n",
            "step 3230: generator_loss=2.2907230854034424, discriminator_loss=0.12011463940143585\n",
            "step 3231: generator_loss=2.2442572116851807, discriminator_loss=0.12291806191205978\n",
            "step 3232: generator_loss=2.18942928314209, discriminator_loss=0.12619802355766296\n",
            "step 3233: generator_loss=2.152693271636963, discriminator_loss=0.12919750809669495\n",
            "step 3234: generator_loss=2.0796382427215576, discriminator_loss=0.13413488864898682\n",
            "step 3235: generator_loss=2.0330469608306885, discriminator_loss=0.13819944858551025\n",
            "step 3236: generator_loss=1.922106385231018, discriminator_loss=0.14605768024921417\n",
            "step 3237: generator_loss=1.890976071357727, discriminator_loss=0.14965932071208954\n",
            "step 3238: generator_loss=1.8520417213439941, discriminator_loss=0.1535874903202057\n",
            "step 3239: generator_loss=1.8252536058425903, discriminator_loss=0.15740840137004852\n",
            "step 3240: generator_loss=1.7919557094573975, discriminator_loss=0.16025826334953308\n",
            "step 3241: generator_loss=1.782907247543335, discriminator_loss=0.16176573932170868\n",
            "step 3242: generator_loss=1.7732930183410645, discriminator_loss=0.1629215031862259\n",
            "step 3243: generator_loss=1.7394287586212158, discriminator_loss=0.16682296991348267\n",
            "step 3244: generator_loss=1.7350589036941528, discriminator_loss=0.1684310883283615\n",
            "step 3245: generator_loss=1.7078676223754883, discriminator_loss=0.1722453236579895\n",
            "step 3246: generator_loss=1.7144383192062378, discriminator_loss=0.17290696501731873\n",
            "step 3247: generator_loss=1.7077875137329102, discriminator_loss=0.17502737045288086\n",
            "step 3248: generator_loss=1.714714765548706, discriminator_loss=0.1758514642715454\n",
            "step 3249: generator_loss=1.7093851566314697, discriminator_loss=0.17781156301498413\n",
            "step 3250: generator_loss=1.6992034912109375, discriminator_loss=0.18018856644630432\n",
            "step 3251: generator_loss=1.7173361778259277, discriminator_loss=0.17979401350021362\n",
            "step 3252: generator_loss=1.7335944175720215, discriminator_loss=0.179457426071167\n",
            "step 3253: generator_loss=1.7483335733413696, discriminator_loss=0.17942401766777039\n",
            "step 3254: generator_loss=1.7544338703155518, discriminator_loss=0.18040812015533447\n",
            "step 3255: generator_loss=1.7835960388183594, discriminator_loss=0.17903873324394226\n",
            "step 3256: generator_loss=1.808870553970337, discriminator_loss=0.17778193950653076\n",
            "step 3257: generator_loss=1.846869945526123, discriminator_loss=0.1755576729774475\n",
            "step 3258: generator_loss=1.9026761054992676, discriminator_loss=0.17143592238426208\n",
            "step 3259: generator_loss=1.964165210723877, discriminator_loss=0.16723574697971344\n",
            "step 3260: generator_loss=2.0198512077331543, discriminator_loss=0.16331592202186584\n",
            "step 3261: generator_loss=2.0624704360961914, discriminator_loss=0.16050580143928528\n",
            "step 3262: generator_loss=2.0812830924987793, discriminator_loss=0.1593017280101776\n",
            "step 3263: generator_loss=2.0595836639404297, discriminator_loss=0.1602083444595337\n",
            "step 3264: generator_loss=2.033491849899292, discriminator_loss=0.16143259406089783\n",
            "step 3265: generator_loss=2.0037856101989746, discriminator_loss=0.16279059648513794\n",
            "step 3266: generator_loss=1.9845894575119019, discriminator_loss=0.1632656455039978\n",
            "step 3267: generator_loss=1.9568486213684082, discriminator_loss=0.16437003016471863\n",
            "step 3268: generator_loss=1.926133155822754, discriminator_loss=0.1656179428100586\n",
            "step 3269: generator_loss=1.888200044631958, discriminator_loss=0.16756924986839294\n",
            "step 3270: generator_loss=1.8447508811950684, discriminator_loss=0.17012205719947815\n",
            "step 3271: generator_loss=1.818110466003418, discriminator_loss=0.1714477241039276\n",
            "step 3272: generator_loss=1.8002042770385742, discriminator_loss=0.17208406329154968\n",
            "step 3273: generator_loss=1.815351963043213, discriminator_loss=0.1695246398448944\n",
            "step 3274: generator_loss=1.8558746576309204, discriminator_loss=0.164573073387146\n",
            "step 3275: generator_loss=1.8906000852584839, discriminator_loss=0.16042178869247437\n",
            "step 3276: generator_loss=1.9178229570388794, discriminator_loss=0.1571453958749771\n",
            "step 3277: generator_loss=1.9480271339416504, discriminator_loss=0.1544160693883896\n",
            "step 3278: generator_loss=1.9453831911087036, discriminator_loss=0.1532343327999115\n",
            "step 3279: generator_loss=1.9298958778381348, discriminator_loss=0.15353891253471375\n",
            "step 3280: generator_loss=1.9270498752593994, discriminator_loss=0.1536620855331421\n",
            "step 3281: generator_loss=1.8931403160095215, discriminator_loss=0.15646132826805115\n",
            "step 3282: generator_loss=1.893126130104065, discriminator_loss=0.15666227042675018\n",
            "step 3283: generator_loss=1.8610073328018188, discriminator_loss=0.1599414050579071\n",
            "step 3284: generator_loss=1.8696978092193604, discriminator_loss=0.16022394597530365\n",
            "step 3285: generator_loss=1.7801848649978638, discriminator_loss=0.17232802510261536\n",
            "step 3286: generator_loss=1.8406801223754883, discriminator_loss=0.16828936338424683\n",
            "step 3287: generator_loss=1.7521848678588867, discriminator_loss=0.18180346488952637\n",
            "step 3288: generator_loss=1.66517972946167, discriminator_loss=0.19807152450084686\n",
            "step 3289: generator_loss=1.6524412631988525, discriminator_loss=0.20549385249614716\n",
            "step 3290: generator_loss=1.7846653461456299, discriminator_loss=0.19170477986335754\n",
            "step 3291: generator_loss=1.6085333824157715, discriminator_loss=0.22165167331695557\n",
            "step 3292: generator_loss=1.6996562480926514, discriminator_loss=0.21330435574054718\n",
            "step 3293: generator_loss=1.6052502393722534, discriminator_loss=0.22154462337493896\n",
            "step 3294: generator_loss=1.7231413125991821, discriminator_loss=0.21637263894081116\n",
            "step 3295: generator_loss=1.7248286008834839, discriminator_loss=0.21815833449363708\n",
            "step 3296: generator_loss=1.9849045276641846, discriminator_loss=0.19810841977596283\n",
            "step 3297: generator_loss=2.105306625366211, discriminator_loss=0.1978369653224945\n",
            "step 3298: generator_loss=2.407719612121582, discriminator_loss=0.17156696319580078\n",
            "step 3299: generator_loss=2.40095591545105, discriminator_loss=0.15980814397335052\n",
            "step 3300: generator_loss=2.509120464324951, discriminator_loss=0.16177354753017426\n",
            "step 3301: generator_loss=2.5332937240600586, discriminator_loss=0.15509150922298431\n",
            "step 3302: generator_loss=2.7169814109802246, discriminator_loss=0.1470859944820404\n",
            "step 3303: generator_loss=2.6145811080932617, discriminator_loss=0.15048936009407043\n",
            "step 3304: generator_loss=2.4869182109832764, discriminator_loss=0.14734667539596558\n",
            "step 3305: generator_loss=2.8193318843841553, discriminator_loss=0.13377127051353455\n",
            "step 3306: generator_loss=2.836050033569336, discriminator_loss=0.13458400964736938\n",
            "step 3307: generator_loss=2.8656368255615234, discriminator_loss=0.1349373459815979\n",
            "step 3308: generator_loss=2.670797348022461, discriminator_loss=0.14110000431537628\n",
            "step 3309: generator_loss=2.6392111778259277, discriminator_loss=0.14465457201004028\n",
            "step 3310: generator_loss=2.5964205265045166, discriminator_loss=0.1432962417602539\n",
            "step 3311: generator_loss=2.610032081604004, discriminator_loss=0.13470245897769928\n",
            "step 3312: generator_loss=2.514781951904297, discriminator_loss=0.1391521692276001\n",
            "step 3313: generator_loss=2.770791530609131, discriminator_loss=0.12609252333641052\n",
            "step 3314: generator_loss=2.649582624435425, discriminator_loss=0.13154473900794983\n",
            "step 3315: generator_loss=2.634521484375, discriminator_loss=0.1295134276151657\n",
            "step 3316: generator_loss=2.60139536857605, discriminator_loss=0.12936216592788696\n",
            "step 3317: generator_loss=2.789348840713501, discriminator_loss=0.12197137624025345\n",
            "step 3318: generator_loss=2.526024103164673, discriminator_loss=0.12823492288589478\n",
            "step 3319: generator_loss=2.7183313369750977, discriminator_loss=0.12034259736537933\n",
            "step 3320: generator_loss=2.6505508422851562, discriminator_loss=0.11972174793481827\n",
            "step 3321: generator_loss=2.860807418823242, discriminator_loss=0.11290740966796875\n",
            "step 3322: generator_loss=2.719029426574707, discriminator_loss=0.11522030830383301\n",
            "step 3323: generator_loss=2.9953970909118652, discriminator_loss=0.11197194457054138\n",
            "step 3324: generator_loss=2.772702693939209, discriminator_loss=0.11605580896139145\n",
            "step 3325: generator_loss=2.6768500804901123, discriminator_loss=0.11853542178869247\n",
            "step 3326: generator_loss=2.621614933013916, discriminator_loss=0.11776066571474075\n",
            "step 3327: generator_loss=2.500666379928589, discriminator_loss=0.12321172654628754\n",
            "step 3328: generator_loss=2.671309471130371, discriminator_loss=0.12136099487543106\n",
            "step 3329: generator_loss=2.4687740802764893, discriminator_loss=0.12635748088359833\n",
            "step 3330: generator_loss=2.352804183959961, discriminator_loss=0.12820877134799957\n",
            "step 3331: generator_loss=2.4849905967712402, discriminator_loss=0.1249222457408905\n",
            "step 3332: generator_loss=2.5917587280273438, discriminator_loss=0.1255493462085724\n",
            "step 3333: generator_loss=2.518134117126465, discriminator_loss=0.12509313225746155\n",
            "step 3334: generator_loss=2.5451509952545166, discriminator_loss=0.12798088788986206\n",
            "step 3335: generator_loss=2.5177650451660156, discriminator_loss=0.13012465834617615\n",
            "step 3336: generator_loss=2.479586601257324, discriminator_loss=0.13286367058753967\n",
            "step 3337: generator_loss=2.3885679244995117, discriminator_loss=0.13191580772399902\n",
            "step 3338: generator_loss=2.523230791091919, discriminator_loss=0.12461210042238235\n",
            "step 3339: generator_loss=2.5789525508880615, discriminator_loss=0.12640732526779175\n",
            "step 3340: generator_loss=2.2658309936523438, discriminator_loss=0.13699540495872498\n",
            "step 3341: generator_loss=2.3549559116363525, discriminator_loss=0.13450022041797638\n",
            "step 3342: generator_loss=2.2999894618988037, discriminator_loss=0.13893479108810425\n",
            "step 3343: generator_loss=2.248016834259033, discriminator_loss=0.1456279158592224\n",
            "step 3344: generator_loss=2.0918185710906982, discriminator_loss=0.1501738727092743\n",
            "step 3345: generator_loss=1.9779798984527588, discriminator_loss=0.15487562119960785\n",
            "step 3346: generator_loss=1.9387931823730469, discriminator_loss=0.15884819626808167\n",
            "step 3347: generator_loss=1.8959639072418213, discriminator_loss=0.15978185832500458\n",
            "step 3348: generator_loss=1.854240894317627, discriminator_loss=0.16600677371025085\n",
            "step 3349: generator_loss=1.8242467641830444, discriminator_loss=0.16611970961093903\n",
            "step 3350: generator_loss=1.882481575012207, discriminator_loss=0.16292807459831238\n",
            "step 3351: generator_loss=1.781043291091919, discriminator_loss=0.17303672432899475\n",
            "step 3352: generator_loss=1.805593490600586, discriminator_loss=0.1722830832004547\n",
            "step 3353: generator_loss=1.7738053798675537, discriminator_loss=0.17605385184288025\n",
            "step 3354: generator_loss=1.6788771152496338, discriminator_loss=0.18742713332176208\n",
            "step 3355: generator_loss=1.672965407371521, discriminator_loss=0.18762463331222534\n",
            "step 3356: generator_loss=1.6831691265106201, discriminator_loss=0.19009239971637726\n",
            "step 3357: generator_loss=1.5914740562438965, discriminator_loss=0.20635220408439636\n",
            "step 3358: generator_loss=1.5485882759094238, discriminator_loss=0.22269514203071594\n",
            "step 3359: generator_loss=1.5234980583190918, discriminator_loss=0.23254238069057465\n",
            "step 3360: generator_loss=1.5784109830856323, discriminator_loss=0.23380409181118011\n",
            "step 3361: generator_loss=1.5008643865585327, discriminator_loss=0.25163164734840393\n",
            "step 3362: generator_loss=1.421633243560791, discriminator_loss=0.28145769238471985\n",
            "step 3363: generator_loss=1.4878292083740234, discriminator_loss=0.27389711141586304\n",
            "step 3364: generator_loss=1.4170081615447998, discriminator_loss=0.2855709195137024\n",
            "step 3365: generator_loss=1.640049695968628, discriminator_loss=0.2517314851284027\n",
            "step 3366: generator_loss=1.482323169708252, discriminator_loss=0.2727731466293335\n",
            "step 3367: generator_loss=1.6919528245925903, discriminator_loss=0.2392634153366089\n",
            "step 3368: generator_loss=1.7115325927734375, discriminator_loss=0.22565039992332458\n",
            "step 3369: generator_loss=1.7618441581726074, discriminator_loss=0.22095859050750732\n",
            "step 3370: generator_loss=1.8409550189971924, discriminator_loss=0.20719236135482788\n",
            "step 3371: generator_loss=1.8654502630233765, discriminator_loss=0.2047319859266281\n",
            "step 3372: generator_loss=1.892953872680664, discriminator_loss=0.20150437951087952\n",
            "step 3373: generator_loss=2.056711435317993, discriminator_loss=0.18797525763511658\n",
            "step 3374: generator_loss=2.056069850921631, discriminator_loss=0.1887488216161728\n",
            "step 3375: generator_loss=2.107569932937622, discriminator_loss=0.18849016726016998\n",
            "step 3376: generator_loss=2.2844746112823486, discriminator_loss=0.17732322216033936\n",
            "step 3377: generator_loss=2.077378034591675, discriminator_loss=0.18886107206344604\n",
            "step 3378: generator_loss=2.1974403858184814, discriminator_loss=0.1852327138185501\n",
            "step 3379: generator_loss=2.328084945678711, discriminator_loss=0.17057770490646362\n",
            "step 3380: generator_loss=2.1126883029937744, discriminator_loss=0.19103378057479858\n",
            "step 3381: generator_loss=2.0610740184783936, discriminator_loss=0.20027396082878113\n",
            "step 3382: generator_loss=2.1718907356262207, discriminator_loss=0.18509671092033386\n",
            "step 3383: generator_loss=2.0950372219085693, discriminator_loss=0.18735839426517487\n",
            "step 3384: generator_loss=2.0704541206359863, discriminator_loss=0.18843892216682434\n",
            "step 3385: generator_loss=2.0360169410705566, discriminator_loss=0.19833745062351227\n",
            "step 3386: generator_loss=2.144329309463501, discriminator_loss=0.18283727765083313\n",
            "step 3387: generator_loss=2.057792901992798, discriminator_loss=0.185380756855011\n",
            "step 3388: generator_loss=2.21866774559021, discriminator_loss=0.17750920355319977\n",
            "step 3389: generator_loss=2.2330245971679688, discriminator_loss=0.16939619183540344\n",
            "step 3390: generator_loss=2.2830650806427, discriminator_loss=0.16206395626068115\n",
            "step 3391: generator_loss=2.352003812789917, discriminator_loss=0.15636232495307922\n",
            "step 3392: generator_loss=2.3468503952026367, discriminator_loss=0.15870848298072815\n",
            "step 3393: generator_loss=2.5133185386657715, discriminator_loss=0.15136456489562988\n",
            "step 3394: generator_loss=2.6252264976501465, discriminator_loss=0.14580026268959045\n",
            "step 3395: generator_loss=2.463758945465088, discriminator_loss=0.14722280204296112\n",
            "step 3396: generator_loss=2.2911975383758545, discriminator_loss=0.15586069226264954\n",
            "step 3397: generator_loss=2.267540454864502, discriminator_loss=0.15476959943771362\n",
            "step 3398: generator_loss=2.2369232177734375, discriminator_loss=0.15782266855239868\n",
            "step 3399: generator_loss=2.2271127700805664, discriminator_loss=0.1618383526802063\n",
            "step 3400: generator_loss=2.020481586456299, discriminator_loss=0.1672464907169342\n",
            "step 3401: generator_loss=2.006080389022827, discriminator_loss=0.1698605716228485\n",
            "step 3402: generator_loss=1.7973670959472656, discriminator_loss=0.18122220039367676\n",
            "step 3403: generator_loss=1.9769365787506104, discriminator_loss=0.16888216137886047\n",
            "step 3404: generator_loss=2.1160945892333984, discriminator_loss=0.15738004446029663\n",
            "step 3405: generator_loss=2.0032920837402344, discriminator_loss=0.16252075135707855\n",
            "step 3406: generator_loss=1.9610086679458618, discriminator_loss=0.16402505338191986\n",
            "step 3407: generator_loss=2.0790483951568604, discriminator_loss=0.1574782431125641\n",
            "step 3408: generator_loss=2.1249513626098633, discriminator_loss=0.15417996048927307\n",
            "step 3409: generator_loss=2.035529136657715, discriminator_loss=0.1596425324678421\n",
            "step 3410: generator_loss=1.9814670085906982, discriminator_loss=0.16373278200626373\n",
            "step 3411: generator_loss=1.9088774919509888, discriminator_loss=0.16683489084243774\n",
            "step 3412: generator_loss=1.8872597217559814, discriminator_loss=0.1683962643146515\n",
            "step 3413: generator_loss=1.8301323652267456, discriminator_loss=0.17431603372097015\n",
            "step 3414: generator_loss=1.7533912658691406, discriminator_loss=0.18026867508888245\n",
            "step 3415: generator_loss=1.6988519430160522, discriminator_loss=0.18668730556964874\n",
            "step 3416: generator_loss=1.6814351081848145, discriminator_loss=0.18829208612442017\n",
            "step 3417: generator_loss=1.6461536884307861, discriminator_loss=0.19277843832969666\n",
            "step 3418: generator_loss=1.654703140258789, discriminator_loss=0.19185557961463928\n",
            "step 3419: generator_loss=1.6392230987548828, discriminator_loss=0.19393476843833923\n",
            "step 3420: generator_loss=1.6250872611999512, discriminator_loss=0.19640833139419556\n",
            "step 3421: generator_loss=1.6234179735183716, discriminator_loss=0.19747775793075562\n",
            "step 3422: generator_loss=1.6649014949798584, discriminator_loss=0.19423484802246094\n",
            "step 3423: generator_loss=1.6466304063796997, discriminator_loss=0.19820736348628998\n",
            "step 3424: generator_loss=1.647807002067566, discriminator_loss=0.1996818333864212\n",
            "step 3425: generator_loss=1.6475155353546143, discriminator_loss=0.20006074011325836\n",
            "step 3426: generator_loss=1.5971691608428955, discriminator_loss=0.20744431018829346\n",
            "step 3427: generator_loss=1.6268277168273926, discriminator_loss=0.20582064986228943\n",
            "step 3428: generator_loss=1.6417016983032227, discriminator_loss=0.20552510023117065\n",
            "step 3429: generator_loss=1.6028317213058472, discriminator_loss=0.21075738966464996\n",
            "step 3430: generator_loss=1.550694465637207, discriminator_loss=0.22044850885868073\n",
            "step 3431: generator_loss=1.559549331665039, discriminator_loss=0.21937918663024902\n",
            "step 3432: generator_loss=1.6278049945831299, discriminator_loss=0.2139209806919098\n",
            "step 3433: generator_loss=1.5917428731918335, discriminator_loss=0.21894367039203644\n",
            "step 3434: generator_loss=1.672656774520874, discriminator_loss=0.21172527968883514\n",
            "step 3435: generator_loss=1.6483182907104492, discriminator_loss=0.21546459197998047\n",
            "step 3436: generator_loss=1.6490488052368164, discriminator_loss=0.2151537835597992\n",
            "step 3437: generator_loss=1.6810336112976074, discriminator_loss=0.2127811461687088\n",
            "step 3438: generator_loss=1.6987882852554321, discriminator_loss=0.211018905043602\n",
            "step 3439: generator_loss=1.744387149810791, discriminator_loss=0.20661503076553345\n",
            "step 3440: generator_loss=1.775700569152832, discriminator_loss=0.203774094581604\n",
            "step 3441: generator_loss=1.8140703439712524, discriminator_loss=0.1998554915189743\n",
            "step 3442: generator_loss=1.8360769748687744, discriminator_loss=0.19736993312835693\n",
            "step 3443: generator_loss=1.858452320098877, discriminator_loss=0.19498905539512634\n",
            "step 3444: generator_loss=1.8904975652694702, discriminator_loss=0.1912587285041809\n",
            "step 3445: generator_loss=1.9024033546447754, discriminator_loss=0.18943700194358826\n",
            "step 3446: generator_loss=1.895763874053955, discriminator_loss=0.18880221247673035\n",
            "step 3447: generator_loss=1.872105598449707, discriminator_loss=0.18989160656929016\n",
            "step 3448: generator_loss=1.8577865362167358, discriminator_loss=0.1908871829509735\n",
            "step 3449: generator_loss=1.7928061485290527, discriminator_loss=0.1951712965965271\n",
            "step 3450: generator_loss=1.742512583732605, discriminator_loss=0.19921636581420898\n",
            "step 3451: generator_loss=1.7044785022735596, discriminator_loss=0.20226237177848816\n",
            "step 3452: generator_loss=1.6664221286773682, discriminator_loss=0.205538809299469\n",
            "step 3453: generator_loss=1.6273369789123535, discriminator_loss=0.20951366424560547\n",
            "step 3454: generator_loss=1.6038812398910522, discriminator_loss=0.2122078239917755\n",
            "step 3455: generator_loss=1.5957444906234741, discriminator_loss=0.21334108710289001\n",
            "step 3456: generator_loss=1.5937029123306274, discriminator_loss=0.2136102318763733\n",
            "step 3457: generator_loss=1.6101692914962769, discriminator_loss=0.2118518352508545\n",
            "step 3458: generator_loss=1.6210750341415405, discriminator_loss=0.21088942885398865\n",
            "step 3459: generator_loss=1.6379456520080566, discriminator_loss=0.20946067571640015\n",
            "step 3460: generator_loss=1.6456289291381836, discriminator_loss=0.20865756273269653\n",
            "step 3461: generator_loss=1.6392114162445068, discriminator_loss=0.20970571041107178\n",
            "step 3462: generator_loss=1.642235517501831, discriminator_loss=0.2097710818052292\n",
            "step 3463: generator_loss=1.6397911310195923, discriminator_loss=0.21070796251296997\n",
            "step 3464: generator_loss=1.6236761808395386, discriminator_loss=0.21347305178642273\n",
            "step 3465: generator_loss=1.5988203287124634, discriminator_loss=0.21758006513118744\n",
            "step 3466: generator_loss=1.5664081573486328, discriminator_loss=0.22312632203102112\n",
            "step 3467: generator_loss=1.529111623764038, discriminator_loss=0.22963351011276245\n",
            "step 3468: generator_loss=1.5060317516326904, discriminator_loss=0.23442545533180237\n",
            "step 3469: generator_loss=1.5001463890075684, discriminator_loss=0.2377576231956482\n",
            "step 3470: generator_loss=1.4736275672912598, discriminator_loss=0.24210159480571747\n",
            "step 3471: generator_loss=1.4703783988952637, discriminator_loss=0.24393227696418762\n",
            "step 3472: generator_loss=1.4756653308868408, discriminator_loss=0.24499326944351196\n",
            "step 3473: generator_loss=1.4918464422225952, discriminator_loss=0.24452657997608185\n",
            "step 3474: generator_loss=1.505725383758545, discriminator_loss=0.24347683787345886\n",
            "step 3475: generator_loss=1.5387358665466309, discriminator_loss=0.2396862804889679\n",
            "step 3476: generator_loss=1.5866601467132568, discriminator_loss=0.23394441604614258\n",
            "step 3477: generator_loss=1.6305410861968994, discriminator_loss=0.22929885983467102\n",
            "step 3478: generator_loss=1.6502931118011475, discriminator_loss=0.2269875407218933\n",
            "step 3479: generator_loss=1.6901695728302002, discriminator_loss=0.2229616343975067\n",
            "step 3480: generator_loss=1.694582462310791, discriminator_loss=0.22197020053863525\n",
            "step 3481: generator_loss=1.7166156768798828, discriminator_loss=0.2188914716243744\n",
            "step 3482: generator_loss=1.7300055027008057, discriminator_loss=0.21700608730316162\n",
            "step 3483: generator_loss=1.7561805248260498, discriminator_loss=0.2136002480983734\n",
            "step 3484: generator_loss=1.7888412475585938, discriminator_loss=0.20960447192192078\n",
            "step 3485: generator_loss=1.8677833080291748, discriminator_loss=0.20108681917190552\n",
            "step 3486: generator_loss=1.948291301727295, discriminator_loss=0.1935933530330658\n",
            "step 3487: generator_loss=2.02066707611084, discriminator_loss=0.18794909119606018\n",
            "step 3488: generator_loss=2.056668281555176, discriminator_loss=0.18588528037071228\n",
            "step 3489: generator_loss=2.110016107559204, discriminator_loss=0.18389157950878143\n",
            "step 3490: generator_loss=2.1842141151428223, discriminator_loss=0.1774037778377533\n",
            "step 3491: generator_loss=2.1427180767059326, discriminator_loss=0.18552368879318237\n",
            "step 3492: generator_loss=2.078251838684082, discriminator_loss=0.1960073858499527\n",
            "step 3493: generator_loss=2.238048791885376, discriminator_loss=0.18293020129203796\n",
            "step 3494: generator_loss=2.3354310989379883, discriminator_loss=0.17643779516220093\n",
            "step 3495: generator_loss=2.2976322174072266, discriminator_loss=0.1794109344482422\n",
            "step 3496: generator_loss=2.397024393081665, discriminator_loss=0.17499662935733795\n",
            "step 3497: generator_loss=2.6533854007720947, discriminator_loss=0.1565142124891281\n",
            "step 3498: generator_loss=2.668954610824585, discriminator_loss=0.15745967626571655\n",
            "step 3499: generator_loss=2.741652488708496, discriminator_loss=0.1522739827632904\n",
            "step 3500: generator_loss=2.672451972961426, discriminator_loss=0.1585652381181717\n",
            "step 3501: generator_loss=2.8107423782348633, discriminator_loss=0.14826825261116028\n",
            "step 3502: generator_loss=2.860776901245117, discriminator_loss=0.1481282263994217\n",
            "step 3503: generator_loss=2.8946921825408936, discriminator_loss=0.14493328332901\n",
            "step 3504: generator_loss=2.9123713970184326, discriminator_loss=0.14261141419410706\n",
            "step 3505: generator_loss=2.8893256187438965, discriminator_loss=0.14015763998031616\n",
            "step 3506: generator_loss=2.911550998687744, discriminator_loss=0.13549673557281494\n",
            "step 3507: generator_loss=3.1029348373413086, discriminator_loss=0.12408384680747986\n",
            "step 3508: generator_loss=2.9534127712249756, discriminator_loss=0.13169890642166138\n",
            "step 3509: generator_loss=3.028777599334717, discriminator_loss=0.12591609358787537\n",
            "step 3510: generator_loss=2.919729232788086, discriminator_loss=0.12471410632133484\n",
            "step 3511: generator_loss=2.9443507194519043, discriminator_loss=0.1227225512266159\n",
            "step 3512: generator_loss=3.049776554107666, discriminator_loss=0.1174565777182579\n",
            "step 3513: generator_loss=3.118053436279297, discriminator_loss=0.11434580385684967\n",
            "step 3514: generator_loss=3.003519058227539, discriminator_loss=0.116979219019413\n",
            "step 3515: generator_loss=2.95395827293396, discriminator_loss=0.11500173062086105\n",
            "step 3516: generator_loss=2.9893529415130615, discriminator_loss=0.11314447969198227\n",
            "step 3517: generator_loss=2.969217538833618, discriminator_loss=0.11279462277889252\n",
            "step 3518: generator_loss=2.9680533409118652, discriminator_loss=0.1111576035618782\n",
            "step 3519: generator_loss=2.7688679695129395, discriminator_loss=0.11839050054550171\n",
            "step 3520: generator_loss=2.5828559398651123, discriminator_loss=0.12379823625087738\n",
            "step 3521: generator_loss=2.762650489807129, discriminator_loss=0.11732123792171478\n",
            "step 3522: generator_loss=2.5572926998138428, discriminator_loss=0.12274837493896484\n",
            "step 3523: generator_loss=2.4589195251464844, discriminator_loss=0.12636736035346985\n",
            "step 3524: generator_loss=2.3861567974090576, discriminator_loss=0.13095203042030334\n",
            "step 3525: generator_loss=2.353534698486328, discriminator_loss=0.13200530409812927\n",
            "step 3526: generator_loss=2.2208056449890137, discriminator_loss=0.139203280210495\n",
            "step 3527: generator_loss=2.093618392944336, discriminator_loss=0.1442302167415619\n",
            "step 3528: generator_loss=2.0480566024780273, discriminator_loss=0.14919471740722656\n",
            "step 3529: generator_loss=2.0770320892333984, discriminator_loss=0.14752818644046783\n",
            "step 3530: generator_loss=2.1965463161468506, discriminator_loss=0.14685383439064026\n",
            "step 3531: generator_loss=2.185297966003418, discriminator_loss=0.14838320016860962\n",
            "step 3532: generator_loss=1.9916064739227295, discriminator_loss=0.15729361772537231\n",
            "step 3533: generator_loss=2.038529396057129, discriminator_loss=0.15634268522262573\n",
            "step 3534: generator_loss=1.9942704439163208, discriminator_loss=0.15828275680541992\n",
            "step 3535: generator_loss=2.04079008102417, discriminator_loss=0.15732696652412415\n",
            "step 3536: generator_loss=1.903267502784729, discriminator_loss=0.16363659501075745\n",
            "step 3537: generator_loss=1.9065966606140137, discriminator_loss=0.16413575410842896\n",
            "step 3538: generator_loss=1.9160741567611694, discriminator_loss=0.16640663146972656\n",
            "step 3539: generator_loss=1.8199008703231812, discriminator_loss=0.17139406502246857\n",
            "step 3540: generator_loss=1.7984585762023926, discriminator_loss=0.17607396841049194\n",
            "step 3541: generator_loss=1.810570240020752, discriminator_loss=0.17485766112804413\n",
            "step 3542: generator_loss=1.8846427202224731, discriminator_loss=0.17109186947345734\n",
            "step 3543: generator_loss=1.825448751449585, discriminator_loss=0.17467881739139557\n",
            "step 3544: generator_loss=1.7689366340637207, discriminator_loss=0.17737284302711487\n",
            "step 3545: generator_loss=1.7833101749420166, discriminator_loss=0.17753589153289795\n",
            "step 3546: generator_loss=1.7524769306182861, discriminator_loss=0.18114274740219116\n",
            "step 3547: generator_loss=1.7427632808685303, discriminator_loss=0.18366549909114838\n",
            "step 3548: generator_loss=1.6830790042877197, discriminator_loss=0.18981227278709412\n",
            "step 3549: generator_loss=1.6388919353485107, discriminator_loss=0.19549520313739777\n",
            "step 3550: generator_loss=1.6115310192108154, discriminator_loss=0.20020660758018494\n",
            "step 3551: generator_loss=1.564103603363037, discriminator_loss=0.20660951733589172\n",
            "step 3552: generator_loss=1.5607093572616577, discriminator_loss=0.20879799127578735\n",
            "step 3553: generator_loss=1.5679137706756592, discriminator_loss=0.2097652554512024\n",
            "step 3554: generator_loss=1.5846145153045654, discriminator_loss=0.20962998270988464\n",
            "step 3555: generator_loss=1.626427173614502, discriminator_loss=0.2068081945180893\n",
            "step 3556: generator_loss=1.6856361627578735, discriminator_loss=0.20195260643959045\n",
            "step 3557: generator_loss=1.7520805597305298, discriminator_loss=0.19763866066932678\n",
            "step 3558: generator_loss=1.7979918718338013, discriminator_loss=0.19365069270133972\n",
            "step 3559: generator_loss=1.8950424194335938, discriminator_loss=0.18655240535736084\n",
            "step 3560: generator_loss=2.000837802886963, discriminator_loss=0.17934992909431458\n",
            "step 3561: generator_loss=2.0489819049835205, discriminator_loss=0.1768445074558258\n",
            "step 3562: generator_loss=2.0525569915771484, discriminator_loss=0.17604395747184753\n",
            "step 3563: generator_loss=2.0650641918182373, discriminator_loss=0.1752026081085205\n",
            "step 3564: generator_loss=2.0711123943328857, discriminator_loss=0.17453497648239136\n",
            "step 3565: generator_loss=2.069070816040039, discriminator_loss=0.17413559556007385\n",
            "step 3566: generator_loss=2.066600799560547, discriminator_loss=0.17349782586097717\n",
            "step 3567: generator_loss=2.055340051651001, discriminator_loss=0.1733531951904297\n",
            "step 3568: generator_loss=2.039569854736328, discriminator_loss=0.17345592379570007\n",
            "step 3569: generator_loss=2.0243990421295166, discriminator_loss=0.17361006140708923\n",
            "step 3570: generator_loss=2.013477087020874, discriminator_loss=0.17335988581180573\n",
            "step 3571: generator_loss=1.9914082288742065, discriminator_loss=0.17375808954238892\n",
            "step 3572: generator_loss=1.9587812423706055, discriminator_loss=0.1750349998474121\n",
            "step 3573: generator_loss=1.9283545017242432, discriminator_loss=0.17647084593772888\n",
            "step 3574: generator_loss=1.9002214670181274, discriminator_loss=0.17760056257247925\n",
            "step 3575: generator_loss=1.8709537982940674, discriminator_loss=0.1793576180934906\n",
            "step 3576: generator_loss=1.8707829713821411, discriminator_loss=0.1785014271736145\n",
            "step 3577: generator_loss=1.8795573711395264, discriminator_loss=0.17681440711021423\n",
            "step 3578: generator_loss=1.8860089778900146, discriminator_loss=0.17557434737682343\n",
            "step 3579: generator_loss=1.8794257640838623, discriminator_loss=0.17593427002429962\n",
            "step 3580: generator_loss=1.8575608730316162, discriminator_loss=0.17762015759944916\n",
            "step 3581: generator_loss=1.8379870653152466, discriminator_loss=0.17924758791923523\n",
            "step 3582: generator_loss=1.8345465660095215, discriminator_loss=0.17942534387111664\n",
            "step 3583: generator_loss=1.8356409072875977, discriminator_loss=0.17905764281749725\n",
            "step 3584: generator_loss=1.8433390855789185, discriminator_loss=0.178096204996109\n",
            "step 3585: generator_loss=1.8422245979309082, discriminator_loss=0.17806144058704376\n",
            "step 3586: generator_loss=1.8504972457885742, discriminator_loss=0.1771569848060608\n",
            "step 3587: generator_loss=1.8744635581970215, discriminator_loss=0.17466548085212708\n",
            "step 3588: generator_loss=1.9135398864746094, discriminator_loss=0.17103484272956848\n",
            "step 3589: generator_loss=1.981523036956787, discriminator_loss=0.16530129313468933\n",
            "step 3590: generator_loss=2.0805177688598633, discriminator_loss=0.15746498107910156\n",
            "step 3591: generator_loss=2.1819634437561035, discriminator_loss=0.15042628347873688\n",
            "step 3592: generator_loss=2.266636371612549, discriminator_loss=0.14490073919296265\n",
            "step 3593: generator_loss=2.319181203842163, discriminator_loss=0.14127367734909058\n",
            "step 3594: generator_loss=2.3252034187316895, discriminator_loss=0.14000320434570312\n",
            "step 3595: generator_loss=2.3177971839904785, discriminator_loss=0.13939644396305084\n",
            "step 3596: generator_loss=2.301492214202881, discriminator_loss=0.1389048993587494\n",
            "step 3597: generator_loss=2.2591071128845215, discriminator_loss=0.14007976651191711\n",
            "step 3598: generator_loss=2.202145576477051, discriminator_loss=0.14246898889541626\n",
            "step 3599: generator_loss=2.1325860023498535, discriminator_loss=0.1455974578857422\n",
            "step 3600: generator_loss=2.0677330493927, discriminator_loss=0.14880385994911194\n",
            "step 3601: generator_loss=2.0196356773376465, discriminator_loss=0.15127524733543396\n",
            "step 3602: generator_loss=1.9984441995620728, discriminator_loss=0.15164753794670105\n",
            "step 3603: generator_loss=1.9820044040679932, discriminator_loss=0.15204598009586334\n",
            "step 3604: generator_loss=1.9606854915618896, discriminator_loss=0.15307927131652832\n",
            "step 3605: generator_loss=1.933840036392212, discriminator_loss=0.1549150049686432\n",
            "step 3606: generator_loss=1.9033875465393066, discriminator_loss=0.15711668133735657\n",
            "step 3607: generator_loss=1.9164035320281982, discriminator_loss=0.15581093728542328\n",
            "step 3608: generator_loss=1.9288378953933716, discriminator_loss=0.15476027131080627\n",
            "step 3609: generator_loss=1.9484612941741943, discriminator_loss=0.15353654325008392\n",
            "step 3610: generator_loss=1.9643458127975464, discriminator_loss=0.1524462103843689\n",
            "step 3611: generator_loss=1.9934017658233643, discriminator_loss=0.1504327654838562\n",
            "step 3612: generator_loss=2.0505518913269043, discriminator_loss=0.14661838114261627\n",
            "step 3613: generator_loss=2.139336585998535, discriminator_loss=0.14057734608650208\n",
            "step 3614: generator_loss=2.225435733795166, discriminator_loss=0.1353401094675064\n",
            "step 3615: generator_loss=2.301334857940674, discriminator_loss=0.13093219697475433\n",
            "step 3616: generator_loss=2.3517191410064697, discriminator_loss=0.12812836468219757\n",
            "step 3617: generator_loss=2.4051008224487305, discriminator_loss=0.12504468858242035\n",
            "step 3618: generator_loss=2.436591148376465, discriminator_loss=0.12299484014511108\n",
            "step 3619: generator_loss=2.44268798828125, discriminator_loss=0.12214332073926926\n",
            "step 3620: generator_loss=2.445462465286255, discriminator_loss=0.12122723460197449\n",
            "step 3621: generator_loss=2.4512972831726074, discriminator_loss=0.1201445460319519\n",
            "step 3622: generator_loss=2.4531519412994385, discriminator_loss=0.11909220367670059\n",
            "step 3623: generator_loss=2.444514751434326, discriminator_loss=0.11836870014667511\n",
            "step 3624: generator_loss=2.410951614379883, discriminator_loss=0.11893198639154434\n",
            "step 3625: generator_loss=2.363537073135376, discriminator_loss=0.12025531381368637\n",
            "step 3626: generator_loss=2.296184539794922, discriminator_loss=0.12270058691501617\n",
            "step 3627: generator_loss=2.2321605682373047, discriminator_loss=0.12543830275535583\n",
            "step 3628: generator_loss=2.183082342147827, discriminator_loss=0.1276901513338089\n",
            "step 3629: generator_loss=2.131436824798584, discriminator_loss=0.13035430014133453\n",
            "step 3630: generator_loss=2.0961153507232666, discriminator_loss=0.13215932250022888\n",
            "step 3631: generator_loss=2.0796871185302734, discriminator_loss=0.13287265598773956\n",
            "step 3632: generator_loss=2.0760245323181152, discriminator_loss=0.1327086091041565\n",
            "step 3633: generator_loss=2.0973610877990723, discriminator_loss=0.13096538186073303\n",
            "step 3634: generator_loss=2.123627185821533, discriminator_loss=0.1292419284582138\n",
            "step 3635: generator_loss=2.134958505630493, discriminator_loss=0.12807774543762207\n",
            "step 3636: generator_loss=2.150843620300293, discriminator_loss=0.1269664764404297\n",
            "step 3637: generator_loss=2.1671204566955566, discriminator_loss=0.12587036192417145\n",
            "step 3638: generator_loss=2.189774990081787, discriminator_loss=0.12445496022701263\n",
            "step 3639: generator_loss=2.219938039779663, discriminator_loss=0.12266547232866287\n",
            "step 3640: generator_loss=2.247913122177124, discriminator_loss=0.12099789083003998\n",
            "step 3641: generator_loss=2.2724227905273438, discriminator_loss=0.11954708397388458\n",
            "step 3642: generator_loss=2.276607036590576, discriminator_loss=0.11942391097545624\n",
            "step 3643: generator_loss=2.261685371398926, discriminator_loss=0.1202925443649292\n",
            "step 3644: generator_loss=2.2405176162719727, discriminator_loss=0.12153726816177368\n",
            "step 3645: generator_loss=2.2052650451660156, discriminator_loss=0.12375390529632568\n",
            "step 3646: generator_loss=2.1586971282958984, discriminator_loss=0.12670886516571045\n",
            "step 3647: generator_loss=2.1041274070739746, discriminator_loss=0.13043688237667084\n",
            "step 3648: generator_loss=2.044121265411377, discriminator_loss=0.13473060727119446\n",
            "step 3649: generator_loss=1.9926207065582275, discriminator_loss=0.13872109353542328\n",
            "step 3650: generator_loss=1.9743022918701172, discriminator_loss=0.14039045572280884\n",
            "step 3651: generator_loss=1.987428903579712, discriminator_loss=0.13952848315238953\n",
            "step 3652: generator_loss=2.0530574321746826, discriminator_loss=0.1347230076789856\n",
            "step 3653: generator_loss=2.1530051231384277, discriminator_loss=0.1280101090669632\n",
            "step 3654: generator_loss=2.2516283988952637, discriminator_loss=0.12219229340553284\n",
            "step 3655: generator_loss=2.3322644233703613, discriminator_loss=0.11772347241640091\n",
            "step 3656: generator_loss=2.3887102603912354, discriminator_loss=0.11499130725860596\n",
            "step 3657: generator_loss=2.4363880157470703, discriminator_loss=0.1124783307313919\n",
            "step 3658: generator_loss=2.503631591796875, discriminator_loss=0.10929994285106659\n",
            "step 3659: generator_loss=2.5646262168884277, discriminator_loss=0.10638082027435303\n",
            "step 3660: generator_loss=2.593273401260376, discriminator_loss=0.10484382510185242\n",
            "step 3661: generator_loss=2.5870800018310547, discriminator_loss=0.1046585962176323\n",
            "step 3662: generator_loss=2.5560338497161865, discriminator_loss=0.10536706447601318\n",
            "step 3663: generator_loss=2.5132408142089844, discriminator_loss=0.10669295489788055\n",
            "step 3664: generator_loss=2.4616811275482178, discriminator_loss=0.10851568728685379\n",
            "step 3665: generator_loss=2.4075145721435547, discriminator_loss=0.11040392518043518\n",
            "step 3666: generator_loss=2.3491878509521484, discriminator_loss=0.11286379396915436\n",
            "step 3667: generator_loss=2.29948091506958, discriminator_loss=0.11510720103979111\n",
            "step 3668: generator_loss=2.263993263244629, discriminator_loss=0.11671080440282822\n",
            "step 3669: generator_loss=2.248455047607422, discriminator_loss=0.11735881865024567\n",
            "step 3670: generator_loss=2.2579336166381836, discriminator_loss=0.11661629378795624\n",
            "step 3671: generator_loss=2.274250030517578, discriminator_loss=0.11558864265680313\n",
            "step 3672: generator_loss=2.293935775756836, discriminator_loss=0.11448036134243011\n",
            "step 3673: generator_loss=2.313260316848755, discriminator_loss=0.11340436339378357\n",
            "step 3674: generator_loss=2.333237648010254, discriminator_loss=0.11240220069885254\n",
            "step 3675: generator_loss=2.3517215251922607, discriminator_loss=0.11147075146436691\n",
            "step 3676: generator_loss=2.370182991027832, discriminator_loss=0.11041200160980225\n",
            "step 3677: generator_loss=2.3755030632019043, discriminator_loss=0.11012448370456696\n",
            "step 3678: generator_loss=2.364823818206787, discriminator_loss=0.11055529117584229\n",
            "step 3679: generator_loss=2.3436851501464844, discriminator_loss=0.11156653612852097\n",
            "step 3680: generator_loss=2.3139991760253906, discriminator_loss=0.11297763139009476\n",
            "step 3681: generator_loss=2.2904107570648193, discriminator_loss=0.11428824812173843\n",
            "step 3682: generator_loss=2.2762036323547363, discriminator_loss=0.11503957211971283\n",
            "step 3683: generator_loss=2.2570338249206543, discriminator_loss=0.1161578893661499\n",
            "step 3684: generator_loss=2.2505836486816406, discriminator_loss=0.116591677069664\n",
            "step 3685: generator_loss=2.2501564025878906, discriminator_loss=0.11659714579582214\n",
            "step 3686: generator_loss=2.251516819000244, discriminator_loss=0.11658633500337601\n",
            "step 3687: generator_loss=2.2434093952178955, discriminator_loss=0.11710654199123383\n",
            "step 3688: generator_loss=2.231069564819336, discriminator_loss=0.11787433922290802\n",
            "step 3689: generator_loss=2.2185769081115723, discriminator_loss=0.11870241165161133\n",
            "step 3690: generator_loss=2.207003116607666, discriminator_loss=0.1194421648979187\n",
            "step 3691: generator_loss=2.2065303325653076, discriminator_loss=0.11949223279953003\n",
            "step 3692: generator_loss=2.2222886085510254, discriminator_loss=0.11848139017820358\n",
            "step 3693: generator_loss=2.247187614440918, discriminator_loss=0.11702042818069458\n",
            "step 3694: generator_loss=2.265915632247925, discriminator_loss=0.11590857058763504\n",
            "step 3695: generator_loss=2.280414581298828, discriminator_loss=0.11500687152147293\n",
            "step 3696: generator_loss=2.277400493621826, discriminator_loss=0.11523469537496567\n",
            "step 3697: generator_loss=2.270188331604004, discriminator_loss=0.11561518162488937\n",
            "step 3698: generator_loss=2.2609002590179443, discriminator_loss=0.1161520779132843\n",
            "step 3699: generator_loss=2.2371253967285156, discriminator_loss=0.11751671135425568\n",
            "step 3700: generator_loss=2.2171237468719482, discriminator_loss=0.1187376081943512\n",
            "step 3701: generator_loss=2.198948383331299, discriminator_loss=0.11980091035366058\n",
            "step 3702: generator_loss=2.187929153442383, discriminator_loss=0.12054562568664551\n",
            "step 3703: generator_loss=2.1870648860931396, discriminator_loss=0.12057127058506012\n",
            "step 3704: generator_loss=2.2034997940063477, discriminator_loss=0.11957511305809021\n",
            "step 3705: generator_loss=2.234372615814209, discriminator_loss=0.11779601871967316\n",
            "step 3706: generator_loss=2.277651309967041, discriminator_loss=0.11537529528141022\n",
            "step 3707: generator_loss=2.3281540870666504, discriminator_loss=0.11265663057565689\n",
            "step 3708: generator_loss=2.3744375705718994, discriminator_loss=0.11028201878070831\n",
            "step 3709: generator_loss=2.402256965637207, discriminator_loss=0.10899944603443146\n",
            "step 3710: generator_loss=2.4185590744018555, discriminator_loss=0.10819709300994873\n",
            "step 3711: generator_loss=2.4235405921936035, discriminator_loss=0.1079854816198349\n",
            "step 3712: generator_loss=2.4225656986236572, discriminator_loss=0.10801488906145096\n",
            "step 3713: generator_loss=2.4047226905822754, discriminator_loss=0.10871593654155731\n",
            "step 3714: generator_loss=2.367001533508301, discriminator_loss=0.11037782579660416\n",
            "step 3715: generator_loss=2.337963819503784, discriminator_loss=0.11177044361829758\n",
            "step 3716: generator_loss=2.303424835205078, discriminator_loss=0.11357125639915466\n",
            "step 3717: generator_loss=2.260976552963257, discriminator_loss=0.11578444391489029\n",
            "step 3718: generator_loss=2.2202281951904297, discriminator_loss=0.11805939674377441\n",
            "step 3719: generator_loss=2.1982333660125732, discriminator_loss=0.11935316771268845\n",
            "step 3720: generator_loss=2.186429023742676, discriminator_loss=0.12014175951480865\n",
            "step 3721: generator_loss=2.1768078804016113, discriminator_loss=0.12090844660997391\n",
            "step 3722: generator_loss=2.164198398590088, discriminator_loss=0.12185928970575333\n",
            "step 3723: generator_loss=2.157700777053833, discriminator_loss=0.12244675308465958\n",
            "step 3724: generator_loss=2.1727919578552246, discriminator_loss=0.12170610576868057\n",
            "step 3725: generator_loss=2.187835693359375, discriminator_loss=0.12080851197242737\n",
            "step 3726: generator_loss=2.200521469116211, discriminator_loss=0.12010654807090759\n",
            "step 3727: generator_loss=2.221637010574341, discriminator_loss=0.11896499991416931\n",
            "step 3728: generator_loss=2.2496914863586426, discriminator_loss=0.11734002083539963\n",
            "step 3729: generator_loss=2.2539405822753906, discriminator_loss=0.11729308217763901\n",
            "step 3730: generator_loss=2.25626802444458, discriminator_loss=0.11732534319162369\n",
            "step 3731: generator_loss=2.2292473316192627, discriminator_loss=0.11922453343868256\n",
            "step 3732: generator_loss=2.201931953430176, discriminator_loss=0.12095290422439575\n",
            "step 3733: generator_loss=2.1743311882019043, discriminator_loss=0.12307056784629822\n",
            "step 3734: generator_loss=2.181225061416626, discriminator_loss=0.12276770174503326\n",
            "step 3735: generator_loss=2.156090497970581, discriminator_loss=0.12501534819602966\n",
            "step 3736: generator_loss=2.178274631500244, discriminator_loss=0.1242513656616211\n",
            "step 3737: generator_loss=2.2095489501953125, discriminator_loss=0.12316104769706726\n",
            "step 3738: generator_loss=2.127241611480713, discriminator_loss=0.1295211911201477\n",
            "step 3739: generator_loss=2.217148780822754, discriminator_loss=0.1229875385761261\n",
            "step 3740: generator_loss=2.1951727867126465, discriminator_loss=0.12772150337696075\n",
            "step 3741: generator_loss=2.203267812728882, discriminator_loss=0.12840953469276428\n",
            "step 3742: generator_loss=2.2796692848205566, discriminator_loss=0.1249922439455986\n",
            "step 3743: generator_loss=2.4620628356933594, discriminator_loss=0.11617739498615265\n",
            "step 3744: generator_loss=2.449277877807617, discriminator_loss=0.12044109404087067\n",
            "step 3745: generator_loss=2.4773173332214355, discriminator_loss=0.12564411759376526\n",
            "step 3746: generator_loss=2.5052852630615234, discriminator_loss=0.12354937195777893\n",
            "step 3747: generator_loss=2.5304534435272217, discriminator_loss=0.12646979093551636\n",
            "step 3748: generator_loss=2.6725568771362305, discriminator_loss=0.11924076825380325\n",
            "step 3749: generator_loss=2.5964126586914062, discriminator_loss=0.12117810547351837\n",
            "step 3750: generator_loss=2.6310009956359863, discriminator_loss=0.11765796691179276\n",
            "step 3751: generator_loss=2.7614734172821045, discriminator_loss=0.11642131209373474\n",
            "step 3752: generator_loss=2.658069133758545, discriminator_loss=0.1201193779706955\n",
            "step 3753: generator_loss=2.621082067489624, discriminator_loss=0.12274451553821564\n",
            "step 3754: generator_loss=2.619370937347412, discriminator_loss=0.12433872371912003\n",
            "step 3755: generator_loss=2.660885810852051, discriminator_loss=0.12329010665416718\n",
            "step 3756: generator_loss=2.7743332386016846, discriminator_loss=0.12199413776397705\n",
            "step 3757: generator_loss=2.7199013233184814, discriminator_loss=0.12935718894004822\n",
            "step 3758: generator_loss=2.6009433269500732, discriminator_loss=0.13189761340618134\n",
            "step 3759: generator_loss=2.583576202392578, discriminator_loss=0.13426300883293152\n",
            "step 3760: generator_loss=2.6643199920654297, discriminator_loss=0.1344393640756607\n",
            "step 3761: generator_loss=2.4522337913513184, discriminator_loss=0.136989027261734\n",
            "step 3762: generator_loss=3.0268394947052, discriminator_loss=0.11009345203638077\n",
            "step 3763: generator_loss=2.8287198543548584, discriminator_loss=0.11535344272851944\n",
            "step 3764: generator_loss=3.021446704864502, discriminator_loss=0.10299316793680191\n",
            "step 3765: generator_loss=3.0561599731445312, discriminator_loss=0.10054431110620499\n",
            "step 3766: generator_loss=3.0203638076782227, discriminator_loss=0.09761925041675568\n",
            "step 3767: generator_loss=3.28295636177063, discriminator_loss=0.09073017537593842\n",
            "step 3768: generator_loss=3.357495069503784, discriminator_loss=0.08701597154140472\n",
            "step 3769: generator_loss=3.6863908767700195, discriminator_loss=0.08372245728969574\n",
            "step 3770: generator_loss=3.454998016357422, discriminator_loss=0.08340218663215637\n",
            "step 3771: generator_loss=3.55310320854187, discriminator_loss=0.08419395983219147\n",
            "step 3772: generator_loss=3.535330295562744, discriminator_loss=0.0831434354186058\n",
            "step 3773: generator_loss=3.2417807579040527, discriminator_loss=0.08480724692344666\n",
            "step 3774: generator_loss=3.3393383026123047, discriminator_loss=0.08477618545293808\n",
            "step 3775: generator_loss=3.3693370819091797, discriminator_loss=0.08452406525611877\n",
            "step 3776: generator_loss=3.056774616241455, discriminator_loss=0.08703821897506714\n",
            "step 3777: generator_loss=3.0509822368621826, discriminator_loss=0.09383273124694824\n",
            "step 3778: generator_loss=2.8892080783843994, discriminator_loss=0.09552126377820969\n",
            "step 3779: generator_loss=2.8335485458374023, discriminator_loss=0.10172224044799805\n",
            "step 3780: generator_loss=2.787938356399536, discriminator_loss=0.10888859629631042\n",
            "step 3781: generator_loss=2.895453453063965, discriminator_loss=0.10981149226427078\n",
            "step 3782: generator_loss=3.0024337768554688, discriminator_loss=0.11205818504095078\n",
            "step 3783: generator_loss=3.013171434402466, discriminator_loss=0.12233923375606537\n",
            "step 3784: generator_loss=2.9762277603149414, discriminator_loss=0.12264548242092133\n",
            "step 3785: generator_loss=3.2706592082977295, discriminator_loss=0.1210995465517044\n",
            "step 3786: generator_loss=3.4142723083496094, discriminator_loss=0.10929735004901886\n",
            "step 3787: generator_loss=3.533379077911377, discriminator_loss=0.12746214866638184\n",
            "step 3788: generator_loss=3.7708473205566406, discriminator_loss=0.11454437673091888\n",
            "step 3789: generator_loss=3.9265055656433105, discriminator_loss=0.08848400413990021\n",
            "step 3790: generator_loss=3.865297794342041, discriminator_loss=0.10417604446411133\n",
            "step 3791: generator_loss=4.014253616333008, discriminator_loss=0.09003062546253204\n",
            "step 3792: generator_loss=4.572897434234619, discriminator_loss=0.07626973092556\n",
            "step 3793: generator_loss=4.282696723937988, discriminator_loss=0.08996756374835968\n",
            "step 3794: generator_loss=4.108819484710693, discriminator_loss=0.08919555693864822\n",
            "step 3795: generator_loss=3.73588490486145, discriminator_loss=0.09571198374032974\n",
            "step 3796: generator_loss=4.2300872802734375, discriminator_loss=0.0884322077035904\n",
            "step 3797: generator_loss=4.268939971923828, discriminator_loss=0.07903134077787399\n",
            "step 3798: generator_loss=4.052422523498535, discriminator_loss=0.07573193311691284\n",
            "step 3799: generator_loss=3.930026054382324, discriminator_loss=0.07877958565950394\n",
            "step 3800: generator_loss=3.5724782943725586, discriminator_loss=0.08317708224058151\n",
            "step 3801: generator_loss=3.6197996139526367, discriminator_loss=0.08091191947460175\n",
            "step 3802: generator_loss=3.5167460441589355, discriminator_loss=0.08419293910264969\n",
            "step 3803: generator_loss=3.1448516845703125, discriminator_loss=0.08920584619045258\n",
            "step 3804: generator_loss=2.853661060333252, discriminator_loss=0.09404172003269196\n",
            "step 3805: generator_loss=2.685680389404297, discriminator_loss=0.0992506891489029\n",
            "step 3806: generator_loss=2.646442413330078, discriminator_loss=0.10380075126886368\n",
            "step 3807: generator_loss=2.5674805641174316, discriminator_loss=0.10124486684799194\n",
            "step 3808: generator_loss=2.462376832962036, discriminator_loss=0.1039985716342926\n",
            "step 3809: generator_loss=2.3990907669067383, discriminator_loss=0.10741917788982391\n",
            "step 3810: generator_loss=2.318708896636963, discriminator_loss=0.11231622099876404\n",
            "step 3811: generator_loss=2.372424840927124, discriminator_loss=0.10910642147064209\n",
            "step 3812: generator_loss=2.3613193035125732, discriminator_loss=0.10909698903560638\n",
            "step 3813: generator_loss=2.582609176635742, discriminator_loss=0.10064657032489777\n",
            "step 3814: generator_loss=2.5257210731506348, discriminator_loss=0.10233235359191895\n",
            "step 3815: generator_loss=2.7220845222473145, discriminator_loss=0.09737862646579742\n",
            "step 3816: generator_loss=2.728949546813965, discriminator_loss=0.0986567884683609\n",
            "step 3817: generator_loss=2.7003073692321777, discriminator_loss=0.09862970560789108\n",
            "step 3818: generator_loss=2.68184757232666, discriminator_loss=0.096553273499012\n",
            "step 3819: generator_loss=2.7997546195983887, discriminator_loss=0.09852384030818939\n",
            "step 3820: generator_loss=2.669999122619629, discriminator_loss=0.10283239185810089\n",
            "step 3821: generator_loss=2.7559385299682617, discriminator_loss=0.10114145278930664\n",
            "step 3822: generator_loss=2.627652168273926, discriminator_loss=0.10364498198032379\n",
            "step 3823: generator_loss=2.5225377082824707, discriminator_loss=0.10535459965467453\n",
            "step 3824: generator_loss=2.6419625282287598, discriminator_loss=0.10559068620204926\n",
            "step 3825: generator_loss=2.571218252182007, discriminator_loss=0.11024122685194016\n",
            "step 3826: generator_loss=2.422657012939453, discriminator_loss=0.11404263973236084\n",
            "step 3827: generator_loss=2.537177562713623, discriminator_loss=0.11620597541332245\n",
            "step 3828: generator_loss=2.1815967559814453, discriminator_loss=0.12468081712722778\n",
            "step 3829: generator_loss=2.2603061199188232, discriminator_loss=0.12473106384277344\n",
            "step 3830: generator_loss=2.2571218013763428, discriminator_loss=0.12880226969718933\n",
            "step 3831: generator_loss=2.0213263034820557, discriminator_loss=0.13743332028388977\n",
            "step 3832: generator_loss=1.9108703136444092, discriminator_loss=0.14407645165920258\n",
            "step 3833: generator_loss=1.9674105644226074, discriminator_loss=0.14633303880691528\n",
            "step 3834: generator_loss=1.8286364078521729, discriminator_loss=0.15158379077911377\n",
            "step 3835: generator_loss=1.7960454225540161, discriminator_loss=0.1548582911491394\n",
            "step 3836: generator_loss=1.822004795074463, discriminator_loss=0.1572631448507309\n",
            "step 3837: generator_loss=1.778419852256775, discriminator_loss=0.15847784280776978\n",
            "step 3838: generator_loss=1.7961713075637817, discriminator_loss=0.15724721550941467\n",
            "step 3839: generator_loss=1.8197362422943115, discriminator_loss=0.1563147008419037\n",
            "step 3840: generator_loss=1.8682687282562256, discriminator_loss=0.15353262424468994\n",
            "step 3841: generator_loss=1.9786007404327393, discriminator_loss=0.14835327863693237\n",
            "step 3842: generator_loss=1.996938705444336, discriminator_loss=0.14686289429664612\n",
            "step 3843: generator_loss=1.9991438388824463, discriminator_loss=0.14698529243469238\n",
            "step 3844: generator_loss=2.025724411010742, discriminator_loss=0.14652693271636963\n",
            "step 3845: generator_loss=2.0190327167510986, discriminator_loss=0.14777959883213043\n",
            "step 3846: generator_loss=2.0366666316986084, discriminator_loss=0.14736482501029968\n",
            "step 3847: generator_loss=2.06199312210083, discriminator_loss=0.14608818292617798\n",
            "step 3848: generator_loss=2.0687758922576904, discriminator_loss=0.1461365967988968\n",
            "step 3849: generator_loss=2.062448024749756, discriminator_loss=0.14698565006256104\n",
            "step 3850: generator_loss=2.0572428703308105, discriminator_loss=0.14774948358535767\n",
            "step 3851: generator_loss=2.035663366317749, discriminator_loss=0.14929839968681335\n",
            "step 3852: generator_loss=2.0360469818115234, discriminator_loss=0.14929968118667603\n",
            "step 3853: generator_loss=2.017502784729004, discriminator_loss=0.15042337775230408\n",
            "step 3854: generator_loss=2.0093743801116943, discriminator_loss=0.15099281072616577\n",
            "step 3855: generator_loss=2.015871047973633, discriminator_loss=0.15008148550987244\n",
            "step 3856: generator_loss=2.022458076477051, discriminator_loss=0.14924213290214539\n",
            "step 3857: generator_loss=2.011570930480957, discriminator_loss=0.1495717167854309\n",
            "step 3858: generator_loss=2.0015010833740234, discriminator_loss=0.14961174130439758\n",
            "step 3859: generator_loss=1.9967031478881836, discriminator_loss=0.14932569861412048\n",
            "step 3860: generator_loss=2.002232551574707, discriminator_loss=0.1482084095478058\n",
            "step 3861: generator_loss=2.0020482540130615, discriminator_loss=0.14736898243427277\n",
            "step 3862: generator_loss=2.003474712371826, discriminator_loss=0.14652128517627716\n",
            "step 3863: generator_loss=2.009939670562744, discriminator_loss=0.14548981189727783\n",
            "step 3864: generator_loss=2.020278215408325, discriminator_loss=0.14414754509925842\n",
            "step 3865: generator_loss=2.050575017929077, discriminator_loss=0.14140944182872772\n",
            "step 3866: generator_loss=2.0824666023254395, discriminator_loss=0.13865603506565094\n",
            "step 3867: generator_loss=2.125014305114746, discriminator_loss=0.13531169295310974\n",
            "step 3868: generator_loss=2.15274715423584, discriminator_loss=0.13284868001937866\n",
            "step 3869: generator_loss=2.1755051612854004, discriminator_loss=0.13083431124687195\n",
            "step 3870: generator_loss=2.1778018474578857, discriminator_loss=0.12999475002288818\n",
            "step 3871: generator_loss=2.173081398010254, discriminator_loss=0.12940296530723572\n",
            "step 3872: generator_loss=2.166361093521118, discriminator_loss=0.12891626358032227\n",
            "step 3873: generator_loss=2.168870210647583, discriminator_loss=0.12794743478298187\n",
            "step 3874: generator_loss=2.160944938659668, discriminator_loss=0.12754705548286438\n",
            "step 3875: generator_loss=2.155784845352173, discriminator_loss=0.12716849148273468\n",
            "step 3876: generator_loss=2.168724536895752, discriminator_loss=0.1258438676595688\n",
            "step 3877: generator_loss=2.1732053756713867, discriminator_loss=0.12506982684135437\n",
            "step 3878: generator_loss=2.1680006980895996, discriminator_loss=0.12497319281101227\n",
            "step 3879: generator_loss=2.1730897426605225, discriminator_loss=0.1243034303188324\n",
            "step 3880: generator_loss=2.1734542846679688, discriminator_loss=0.12394587695598602\n",
            "step 3881: generator_loss=2.1630730628967285, discriminator_loss=0.12429188191890717\n",
            "step 3882: generator_loss=2.157623291015625, discriminator_loss=0.12446887791156769\n",
            "step 3883: generator_loss=2.154755115509033, discriminator_loss=0.12443149089813232\n",
            "step 3884: generator_loss=2.1499476432800293, discriminator_loss=0.1246643215417862\n",
            "step 3885: generator_loss=2.1363863945007324, discriminator_loss=0.1255732625722885\n",
            "step 3886: generator_loss=2.113373279571533, discriminator_loss=0.12708596885204315\n",
            "step 3887: generator_loss=2.0803284645080566, discriminator_loss=0.12935695052146912\n",
            "step 3888: generator_loss=2.0440001487731934, discriminator_loss=0.1322544515132904\n",
            "step 3889: generator_loss=1.9997897148132324, discriminator_loss=0.13568615913391113\n",
            "step 3890: generator_loss=1.962907314300537, discriminator_loss=0.1391811966896057\n",
            "step 3891: generator_loss=1.9388091564178467, discriminator_loss=0.14163771271705627\n",
            "step 3892: generator_loss=1.9268629550933838, discriminator_loss=0.14317119121551514\n",
            "step 3893: generator_loss=1.9309625625610352, discriminator_loss=0.14339789748191833\n",
            "step 3894: generator_loss=1.938807487487793, discriminator_loss=0.14325177669525146\n",
            "step 3895: generator_loss=1.9420387744903564, discriminator_loss=0.14361479878425598\n",
            "step 3896: generator_loss=1.9431413412094116, discriminator_loss=0.14393773674964905\n",
            "step 3897: generator_loss=1.9446693658828735, discriminator_loss=0.1445300579071045\n",
            "step 3898: generator_loss=1.954056978225708, discriminator_loss=0.14435788989067078\n",
            "step 3899: generator_loss=1.9829373359680176, discriminator_loss=0.14268311858177185\n",
            "step 3900: generator_loss=2.0146143436431885, discriminator_loss=0.1408727467060089\n",
            "step 3901: generator_loss=2.0325279235839844, discriminator_loss=0.1401534080505371\n",
            "step 3902: generator_loss=2.03741192817688, discriminator_loss=0.1406077742576599\n",
            "step 3903: generator_loss=2.028653144836426, discriminator_loss=0.14178819954395294\n",
            "step 3904: generator_loss=2.010140895843506, discriminator_loss=0.14406605064868927\n",
            "step 3905: generator_loss=2.007707118988037, discriminator_loss=0.14502805471420288\n",
            "step 3906: generator_loss=2.051273822784424, discriminator_loss=0.142449289560318\n",
            "step 3907: generator_loss=2.1555256843566895, discriminator_loss=0.13562992215156555\n",
            "step 3908: generator_loss=2.2810702323913574, discriminator_loss=0.12838347256183624\n",
            "step 3909: generator_loss=2.3622374534606934, discriminator_loss=0.1241123229265213\n",
            "step 3910: generator_loss=2.4350905418395996, discriminator_loss=0.12024123221635818\n",
            "step 3911: generator_loss=2.451575517654419, discriminator_loss=0.11935442686080933\n",
            "step 3912: generator_loss=2.472504138946533, discriminator_loss=0.11748316138982773\n",
            "step 3913: generator_loss=2.421088695526123, discriminator_loss=0.1196984276175499\n",
            "step 3914: generator_loss=2.424043893814087, discriminator_loss=0.1183253675699234\n",
            "step 3915: generator_loss=2.3477988243103027, discriminator_loss=0.1224023848772049\n",
            "step 3916: generator_loss=2.3657774925231934, discriminator_loss=0.11930302530527115\n",
            "step 3917: generator_loss=2.292264461517334, discriminator_loss=0.12444881349802017\n",
            "step 3918: generator_loss=2.173048257827759, discriminator_loss=0.1353999674320221\n",
            "step 3919: generator_loss=2.084822177886963, discriminator_loss=0.1480419635772705\n",
            "step 3920: generator_loss=2.0477895736694336, discriminator_loss=0.16007167100906372\n",
            "step 3921: generator_loss=2.1257286071777344, discriminator_loss=0.15896417200565338\n",
            "step 3922: generator_loss=2.075045347213745, discriminator_loss=0.17927882075309753\n",
            "step 3923: generator_loss=2.1775736808776855, discriminator_loss=0.1768677681684494\n",
            "step 3924: generator_loss=2.2223544120788574, discriminator_loss=0.19473186135292053\n",
            "step 3925: generator_loss=2.3079938888549805, discriminator_loss=0.18599560856819153\n",
            "step 3926: generator_loss=2.4007067680358887, discriminator_loss=0.18640904128551483\n",
            "step 3927: generator_loss=2.697545289993286, discriminator_loss=0.1563412994146347\n",
            "step 3928: generator_loss=2.5739192962646484, discriminator_loss=0.16838717460632324\n",
            "step 3929: generator_loss=2.6266329288482666, discriminator_loss=0.17639294266700745\n",
            "step 3930: generator_loss=2.6937179565429688, discriminator_loss=0.1688128411769867\n",
            "step 3931: generator_loss=2.824793815612793, discriminator_loss=0.14758998155593872\n",
            "step 3932: generator_loss=2.933703899383545, discriminator_loss=0.1354776918888092\n",
            "step 3933: generator_loss=2.7294199466705322, discriminator_loss=0.15126129984855652\n",
            "step 3934: generator_loss=2.9082953929901123, discriminator_loss=0.1323722004890442\n",
            "step 3935: generator_loss=3.0050911903381348, discriminator_loss=0.11631462723016739\n",
            "step 3936: generator_loss=3.1094157695770264, discriminator_loss=0.10882346332073212\n",
            "step 3937: generator_loss=3.3634204864501953, discriminator_loss=0.09748659282922745\n",
            "step 3938: generator_loss=3.195660352706909, discriminator_loss=0.09909211099147797\n",
            "step 3939: generator_loss=3.2395739555358887, discriminator_loss=0.09632159769535065\n",
            "step 3940: generator_loss=3.297153949737549, discriminator_loss=0.09192994982004166\n",
            "step 3941: generator_loss=3.2297396659851074, discriminator_loss=0.09276578575372696\n",
            "step 3942: generator_loss=3.214989423751831, discriminator_loss=0.0916765108704567\n",
            "step 3943: generator_loss=3.135633707046509, discriminator_loss=0.09237947314977646\n",
            "step 3944: generator_loss=2.997626781463623, discriminator_loss=0.09655722230672836\n",
            "step 3945: generator_loss=3.1121983528137207, discriminator_loss=0.09285728633403778\n",
            "step 3946: generator_loss=2.8463528156280518, discriminator_loss=0.10322951525449753\n",
            "step 3947: generator_loss=2.881516695022583, discriminator_loss=0.1000688225030899\n",
            "step 3948: generator_loss=2.7016148567199707, discriminator_loss=0.10788309574127197\n",
            "step 3949: generator_loss=2.6924362182617188, discriminator_loss=0.10634525120258331\n",
            "step 3950: generator_loss=2.6667537689208984, discriminator_loss=0.10713519155979156\n",
            "step 3951: generator_loss=2.5515594482421875, discriminator_loss=0.11187741160392761\n",
            "step 3952: generator_loss=2.562232732772827, discriminator_loss=0.11143700778484344\n",
            "step 3953: generator_loss=2.6236071586608887, discriminator_loss=0.10574504733085632\n",
            "step 3954: generator_loss=2.4203948974609375, discriminator_loss=0.11288751661777496\n",
            "step 3955: generator_loss=2.4405412673950195, discriminator_loss=0.1124778538942337\n",
            "step 3956: generator_loss=2.402317523956299, discriminator_loss=0.11311571300029755\n",
            "step 3957: generator_loss=2.3279707431793213, discriminator_loss=0.11636847257614136\n",
            "step 3958: generator_loss=2.2604310512542725, discriminator_loss=0.11973536014556885\n",
            "step 3959: generator_loss=2.314295530319214, discriminator_loss=0.1160128116607666\n",
            "step 3960: generator_loss=2.277034044265747, discriminator_loss=0.11794444173574448\n",
            "step 3961: generator_loss=2.3732128143310547, discriminator_loss=0.11205583810806274\n",
            "step 3962: generator_loss=2.1965675354003906, discriminator_loss=0.12152431905269623\n",
            "step 3963: generator_loss=2.2205405235290527, discriminator_loss=0.1193450316786766\n",
            "step 3964: generator_loss=2.3718390464782715, discriminator_loss=0.11646457761526108\n",
            "step 3965: generator_loss=2.264744281768799, discriminator_loss=0.11531754583120346\n",
            "step 3966: generator_loss=2.3523683547973633, discriminator_loss=0.11299379169940948\n",
            "step 3967: generator_loss=2.3165066242218018, discriminator_loss=0.11540703475475311\n",
            "step 3968: generator_loss=2.242839813232422, discriminator_loss=0.11667881906032562\n",
            "step 3969: generator_loss=2.2611494064331055, discriminator_loss=0.11635160446166992\n",
            "step 3970: generator_loss=2.258820056915283, discriminator_loss=0.11676165461540222\n",
            "step 3971: generator_loss=2.345778226852417, discriminator_loss=0.11541885137557983\n",
            "step 3972: generator_loss=2.2669782638549805, discriminator_loss=0.11741544306278229\n",
            "step 3973: generator_loss=2.18761944770813, discriminator_loss=0.12068019062280655\n",
            "step 3974: generator_loss=2.2081005573272705, discriminator_loss=0.12028852850198746\n",
            "step 3975: generator_loss=2.1872920989990234, discriminator_loss=0.1216123104095459\n",
            "step 3976: generator_loss=2.167072296142578, discriminator_loss=0.123999685049057\n",
            "step 3977: generator_loss=2.1174254417419434, discriminator_loss=0.1264411062002182\n",
            "step 3978: generator_loss=2.0426230430603027, discriminator_loss=0.13069231808185577\n",
            "step 3979: generator_loss=2.067589282989502, discriminator_loss=0.13066691160202026\n",
            "step 3980: generator_loss=1.9906996488571167, discriminator_loss=0.13472217321395874\n",
            "step 3981: generator_loss=1.9436169862747192, discriminator_loss=0.13788729906082153\n",
            "step 3982: generator_loss=1.941352367401123, discriminator_loss=0.13887393474578857\n",
            "step 3983: generator_loss=1.9528236389160156, discriminator_loss=0.1392498016357422\n",
            "step 3984: generator_loss=1.9287469387054443, discriminator_loss=0.14141002297401428\n",
            "step 3985: generator_loss=1.915146827697754, discriminator_loss=0.14312130212783813\n",
            "step 3986: generator_loss=1.9792548418045044, discriminator_loss=0.14141955971717834\n",
            "step 3987: generator_loss=1.8959083557128906, discriminator_loss=0.1465606987476349\n",
            "step 3988: generator_loss=1.9090386629104614, discriminator_loss=0.14667809009552002\n",
            "step 3989: generator_loss=1.8452726602554321, discriminator_loss=0.15203046798706055\n",
            "step 3990: generator_loss=1.8652012348175049, discriminator_loss=0.1524587869644165\n",
            "step 3991: generator_loss=1.8633058071136475, discriminator_loss=0.15332752466201782\n",
            "step 3992: generator_loss=1.8366626501083374, discriminator_loss=0.1563398540019989\n",
            "step 3993: generator_loss=1.8507386445999146, discriminator_loss=0.1575627326965332\n",
            "step 3994: generator_loss=1.796521782875061, discriminator_loss=0.16270792484283447\n",
            "step 3995: generator_loss=1.7739170789718628, discriminator_loss=0.16654840111732483\n",
            "step 3996: generator_loss=1.7537798881530762, discriminator_loss=0.1699429452419281\n",
            "step 3997: generator_loss=1.7754229307174683, discriminator_loss=0.1697959005832672\n",
            "step 3998: generator_loss=1.7695319652557373, discriminator_loss=0.17136302590370178\n",
            "step 3999: generator_loss=1.8018137216567993, discriminator_loss=0.16983124613761902\n",
            "step 4000: generator_loss=1.8653064966201782, discriminator_loss=0.16546911001205444\n",
            "step 4001: generator_loss=1.9387229681015015, discriminator_loss=0.16062785685062408\n",
            "step 4002: generator_loss=2.052110195159912, discriminator_loss=0.15312540531158447\n",
            "step 4003: generator_loss=2.213043212890625, discriminator_loss=0.1435665786266327\n",
            "step 4004: generator_loss=2.3586978912353516, discriminator_loss=0.1358972191810608\n",
            "step 4005: generator_loss=2.5314390659332275, discriminator_loss=0.12814182043075562\n",
            "step 4006: generator_loss=2.6605491638183594, discriminator_loss=0.1224629133939743\n",
            "step 4007: generator_loss=2.737945556640625, discriminator_loss=0.11910289525985718\n",
            "step 4008: generator_loss=2.7702231407165527, discriminator_loss=0.11644704639911652\n",
            "step 4009: generator_loss=2.781938314437866, discriminator_loss=0.11456470191478729\n",
            "step 4010: generator_loss=2.7896018028259277, discriminator_loss=0.11220524460077286\n",
            "step 4011: generator_loss=2.807858467102051, discriminator_loss=0.10960071533918381\n",
            "step 4012: generator_loss=2.7966206073760986, discriminator_loss=0.10767411440610886\n",
            "step 4013: generator_loss=2.752228260040283, discriminator_loss=0.106813944876194\n",
            "step 4014: generator_loss=2.702709436416626, discriminator_loss=0.10633479803800583\n",
            "step 4015: generator_loss=2.646135091781616, discriminator_loss=0.10635553300380707\n",
            "step 4016: generator_loss=2.5627102851867676, discriminator_loss=0.10755503177642822\n",
            "step 4017: generator_loss=2.4839887619018555, discriminator_loss=0.10906124860048294\n",
            "step 4018: generator_loss=2.39333438873291, discriminator_loss=0.11174501478672028\n",
            "step 4019: generator_loss=2.2961206436157227, discriminator_loss=0.11540952324867249\n",
            "step 4020: generator_loss=2.205432891845703, discriminator_loss=0.11954578012228012\n",
            "step 4021: generator_loss=2.1392898559570312, discriminator_loss=0.12292258441448212\n",
            "step 4022: generator_loss=2.1103248596191406, discriminator_loss=0.12419228255748749\n",
            "step 4023: generator_loss=2.096712350845337, discriminator_loss=0.12476258724927902\n",
            "step 4024: generator_loss=2.092891216278076, discriminator_loss=0.12491729855537415\n",
            "step 4025: generator_loss=2.095881938934326, discriminator_loss=0.12482647597789764\n",
            "step 4026: generator_loss=2.09366774559021, discriminator_loss=0.12511885166168213\n",
            "step 4027: generator_loss=2.0910444259643555, discriminator_loss=0.12551480531692505\n",
            "step 4028: generator_loss=2.0942859649658203, discriminator_loss=0.1257593333721161\n",
            "step 4029: generator_loss=2.11474347114563, discriminator_loss=0.12476050853729248\n",
            "step 4030: generator_loss=2.131685733795166, discriminator_loss=0.12425456196069717\n",
            "step 4031: generator_loss=2.1556918621063232, discriminator_loss=0.12330188602209091\n",
            "step 4032: generator_loss=2.1538467407226562, discriminator_loss=0.12397105991840363\n",
            "step 4033: generator_loss=2.147660255432129, discriminator_loss=0.1249898299574852\n",
            "step 4034: generator_loss=2.1375885009765625, discriminator_loss=0.12640246748924255\n",
            "step 4035: generator_loss=2.1184945106506348, discriminator_loss=0.12839283049106598\n",
            "step 4036: generator_loss=2.0853030681610107, discriminator_loss=0.13136757910251617\n",
            "step 4037: generator_loss=2.0592033863067627, discriminator_loss=0.13380348682403564\n",
            "step 4038: generator_loss=2.04899525642395, discriminator_loss=0.13515588641166687\n",
            "step 4039: generator_loss=2.072654962539673, discriminator_loss=0.1339138001203537\n",
            "step 4040: generator_loss=2.1008753776550293, discriminator_loss=0.13255661725997925\n",
            "step 4041: generator_loss=2.1577401161193848, discriminator_loss=0.12939786911010742\n",
            "step 4042: generator_loss=2.2248072624206543, discriminator_loss=0.12561199069023132\n",
            "step 4043: generator_loss=2.2668285369873047, discriminator_loss=0.12369448691606522\n",
            "step 4044: generator_loss=2.2989020347595215, discriminator_loss=0.12215854972600937\n",
            "step 4045: generator_loss=2.315138816833496, discriminator_loss=0.12163028120994568\n",
            "step 4046: generator_loss=2.3117241859436035, discriminator_loss=0.12203231453895569\n",
            "step 4047: generator_loss=2.2849442958831787, discriminator_loss=0.12367768585681915\n",
            "step 4048: generator_loss=2.232607364654541, discriminator_loss=0.12674033641815186\n",
            "step 4049: generator_loss=2.1662473678588867, discriminator_loss=0.13080158829689026\n",
            "step 4050: generator_loss=2.1034388542175293, discriminator_loss=0.13478101789951324\n",
            "step 4051: generator_loss=2.091437578201294, discriminator_loss=0.1355561912059784\n",
            "step 4052: generator_loss=2.1110165119171143, discriminator_loss=0.1342751681804657\n",
            "step 4053: generator_loss=2.1739611625671387, discriminator_loss=0.13001617789268494\n",
            "step 4054: generator_loss=2.2693750858306885, discriminator_loss=0.1240023821592331\n",
            "step 4055: generator_loss=2.3520846366882324, discriminator_loss=0.11914296448230743\n",
            "step 4056: generator_loss=2.4069247245788574, discriminator_loss=0.11611749976873398\n",
            "step 4057: generator_loss=2.4425134658813477, discriminator_loss=0.11390596628189087\n",
            "step 4058: generator_loss=2.46543025970459, discriminator_loss=0.11222000420093536\n",
            "step 4059: generator_loss=2.4853386878967285, discriminator_loss=0.11064957082271576\n",
            "step 4060: generator_loss=2.5085368156433105, discriminator_loss=0.10876668989658356\n",
            "step 4061: generator_loss=2.5198135375976562, discriminator_loss=0.10742668807506561\n",
            "step 4062: generator_loss=2.5268187522888184, discriminator_loss=0.10616748780012131\n",
            "step 4063: generator_loss=2.5253193378448486, discriminator_loss=0.10519522428512573\n",
            "step 4064: generator_loss=2.5115034580230713, discriminator_loss=0.10506100952625275\n",
            "step 4065: generator_loss=2.4785642623901367, discriminator_loss=0.10570384562015533\n",
            "step 4066: generator_loss=2.4198503494262695, discriminator_loss=0.10779879242181778\n",
            "step 4067: generator_loss=2.382567882537842, discriminator_loss=0.1090853214263916\n",
            "step 4068: generator_loss=2.350032329559326, discriminator_loss=0.11018656194210052\n",
            "step 4069: generator_loss=2.340440273284912, discriminator_loss=0.11021122336387634\n",
            "step 4070: generator_loss=2.348703384399414, discriminator_loss=0.10930763185024261\n",
            "step 4071: generator_loss=2.3380894660949707, discriminator_loss=0.1099872887134552\n",
            "step 4072: generator_loss=2.366136074066162, discriminator_loss=0.10815692692995071\n",
            "step 4073: generator_loss=2.347738742828369, discriminator_loss=0.1096634641289711\n",
            "step 4074: generator_loss=2.3552608489990234, discriminator_loss=0.10974081605672836\n",
            "step 4075: generator_loss=2.302490711212158, discriminator_loss=0.11654184758663177\n",
            "step 4076: generator_loss=2.3317058086395264, discriminator_loss=0.11729378998279572\n",
            "step 4077: generator_loss=2.3838629722595215, discriminator_loss=0.11942669004201889\n",
            "step 4078: generator_loss=2.4219281673431396, discriminator_loss=0.12297782301902771\n",
            "step 4079: generator_loss=2.383544921875, discriminator_loss=0.14063102006912231\n",
            "step 4080: generator_loss=2.415015697479248, discriminator_loss=0.16010743379592896\n",
            "step 4081: generator_loss=2.2738137245178223, discriminator_loss=0.19396555423736572\n",
            "step 4082: generator_loss=2.5166707038879395, discriminator_loss=0.1666431725025177\n",
            "step 4083: generator_loss=2.3999102115631104, discriminator_loss=0.20512446761131287\n",
            "step 4084: generator_loss=2.686405897140503, discriminator_loss=0.17094244062900543\n",
            "step 4085: generator_loss=2.7780826091766357, discriminator_loss=0.16590271890163422\n",
            "step 4086: generator_loss=2.812439441680908, discriminator_loss=0.16813533008098602\n",
            "step 4087: generator_loss=2.690343141555786, discriminator_loss=0.16984868049621582\n",
            "step 4088: generator_loss=3.0048182010650635, discriminator_loss=0.14163243770599365\n",
            "step 4089: generator_loss=3.4779233932495117, discriminator_loss=0.10930975526571274\n",
            "step 4090: generator_loss=3.540217876434326, discriminator_loss=0.09585969150066376\n",
            "step 4091: generator_loss=3.73919415473938, discriminator_loss=0.08983805775642395\n",
            "step 4092: generator_loss=3.910447120666504, discriminator_loss=0.08372209221124649\n",
            "step 4093: generator_loss=3.971954584121704, discriminator_loss=0.0811455175280571\n",
            "step 4094: generator_loss=4.172607898712158, discriminator_loss=0.07702349126338959\n",
            "step 4095: generator_loss=4.050167083740234, discriminator_loss=0.07922592759132385\n",
            "step 4096: generator_loss=4.111318588256836, discriminator_loss=0.07727114856243134\n",
            "step 4097: generator_loss=3.781372308731079, discriminator_loss=0.08021435141563416\n",
            "step 4098: generator_loss=3.774721145629883, discriminator_loss=0.08094090968370438\n",
            "step 4099: generator_loss=3.552473545074463, discriminator_loss=0.08426539599895477\n",
            "step 4100: generator_loss=3.4869775772094727, discriminator_loss=0.0834280401468277\n",
            "step 4101: generator_loss=3.1982154846191406, discriminator_loss=0.09368366003036499\n",
            "step 4102: generator_loss=3.1938576698303223, discriminator_loss=0.0913035124540329\n",
            "step 4103: generator_loss=2.95701265335083, discriminator_loss=0.09829595685005188\n",
            "step 4104: generator_loss=2.793952703475952, discriminator_loss=0.1069832593202591\n",
            "step 4105: generator_loss=2.758552074432373, discriminator_loss=0.10374210774898529\n",
            "step 4106: generator_loss=2.637850522994995, discriminator_loss=0.10313795506954193\n",
            "step 4107: generator_loss=2.7600901126861572, discriminator_loss=0.10168416053056717\n",
            "step 4108: generator_loss=2.626783847808838, discriminator_loss=0.10332980006933212\n",
            "step 4109: generator_loss=2.455254554748535, discriminator_loss=0.10658562183380127\n",
            "step 4110: generator_loss=2.412155866622925, discriminator_loss=0.10441799461841583\n",
            "step 4111: generator_loss=2.4211277961730957, discriminator_loss=0.10586726665496826\n",
            "step 4112: generator_loss=2.4342384338378906, discriminator_loss=0.10422112792730331\n",
            "step 4113: generator_loss=2.3700056076049805, discriminator_loss=0.10909278690814972\n",
            "step 4114: generator_loss=2.212461471557617, discriminator_loss=0.11482210457324982\n",
            "step 4115: generator_loss=2.18668270111084, discriminator_loss=0.11606001853942871\n",
            "step 4116: generator_loss=2.183607816696167, discriminator_loss=0.11575638502836227\n",
            "step 4117: generator_loss=2.1720523834228516, discriminator_loss=0.11736181378364563\n",
            "step 4118: generator_loss=2.2537689208984375, discriminator_loss=0.11537298560142517\n",
            "step 4119: generator_loss=2.2274246215820312, discriminator_loss=0.1161736398935318\n",
            "step 4120: generator_loss=2.3748574256896973, discriminator_loss=0.1123526319861412\n",
            "step 4121: generator_loss=2.2451906204223633, discriminator_loss=0.11643701046705246\n",
            "step 4122: generator_loss=2.286151647567749, discriminator_loss=0.11587648093700409\n",
            "step 4123: generator_loss=2.245541572570801, discriminator_loss=0.11787615716457367\n",
            "step 4124: generator_loss=2.1906063556671143, discriminator_loss=0.12042940407991409\n",
            "step 4125: generator_loss=2.1804962158203125, discriminator_loss=0.12147868424654007\n",
            "step 4126: generator_loss=2.142559289932251, discriminator_loss=0.12334524095058441\n",
            "step 4127: generator_loss=2.1195969581604004, discriminator_loss=0.12462083995342255\n",
            "step 4128: generator_loss=2.040175437927246, discriminator_loss=0.12951546907424927\n",
            "step 4129: generator_loss=2.0319037437438965, discriminator_loss=0.13036563992500305\n",
            "step 4130: generator_loss=2.015589714050293, discriminator_loss=0.13261494040489197\n",
            "step 4131: generator_loss=1.9854683876037598, discriminator_loss=0.13409483432769775\n",
            "step 4132: generator_loss=1.9869773387908936, discriminator_loss=0.13479045033454895\n",
            "step 4133: generator_loss=1.9857795238494873, discriminator_loss=0.13438208401203156\n",
            "step 4134: generator_loss=1.9615530967712402, discriminator_loss=0.13624553382396698\n",
            "step 4135: generator_loss=1.9819562435150146, discriminator_loss=0.13527947664260864\n",
            "step 4136: generator_loss=1.988917589187622, discriminator_loss=0.13512378931045532\n",
            "step 4137: generator_loss=1.996179223060608, discriminator_loss=0.13499803841114044\n",
            "step 4138: generator_loss=1.9774808883666992, discriminator_loss=0.13703006505966187\n",
            "step 4139: generator_loss=1.986835241317749, discriminator_loss=0.13710972666740417\n",
            "step 4140: generator_loss=2.041459560394287, discriminator_loss=0.13347157835960388\n",
            "step 4141: generator_loss=2.120694160461426, discriminator_loss=0.12853918969631195\n",
            "step 4142: generator_loss=2.212890148162842, discriminator_loss=0.12331528961658478\n",
            "step 4143: generator_loss=2.313823699951172, discriminator_loss=0.11839012801647186\n",
            "step 4144: generator_loss=2.3537917137145996, discriminator_loss=0.11627375334501266\n",
            "step 4145: generator_loss=2.3799777030944824, discriminator_loss=0.1152307540178299\n",
            "step 4146: generator_loss=2.396130323410034, discriminator_loss=0.11433159559965134\n",
            "step 4147: generator_loss=2.41632080078125, discriminator_loss=0.11383510380983353\n",
            "step 4148: generator_loss=2.401294469833374, discriminator_loss=0.113745778799057\n",
            "step 4149: generator_loss=2.3968558311462402, discriminator_loss=0.11370065808296204\n",
            "step 4150: generator_loss=2.3895070552825928, discriminator_loss=0.11343534290790558\n",
            "step 4151: generator_loss=2.366992950439453, discriminator_loss=0.1140715479850769\n",
            "step 4152: generator_loss=2.370728015899658, discriminator_loss=0.11337241530418396\n",
            "step 4153: generator_loss=2.3731579780578613, discriminator_loss=0.1127934679389\n",
            "step 4154: generator_loss=2.358306884765625, discriminator_loss=0.11294261366128922\n",
            "step 4155: generator_loss=2.3618383407592773, discriminator_loss=0.11219558864831924\n",
            "step 4156: generator_loss=2.361621379852295, discriminator_loss=0.11174209415912628\n",
            "step 4157: generator_loss=2.3426718711853027, discriminator_loss=0.11238057911396027\n",
            "step 4158: generator_loss=2.3227179050445557, discriminator_loss=0.1129620224237442\n",
            "step 4159: generator_loss=2.30766224861145, discriminator_loss=0.11340685933828354\n",
            "step 4160: generator_loss=2.2780232429504395, discriminator_loss=0.11450451612472534\n",
            "step 4161: generator_loss=2.2667078971862793, discriminator_loss=0.11482037603855133\n",
            "step 4162: generator_loss=2.291977882385254, discriminator_loss=0.11309303343296051\n",
            "step 4163: generator_loss=2.292649984359741, discriminator_loss=0.11280485987663269\n",
            "step 4164: generator_loss=2.276857852935791, discriminator_loss=0.11342541873455048\n",
            "step 4165: generator_loss=2.245506525039673, discriminator_loss=0.11526671051979065\n",
            "step 4166: generator_loss=2.2076897621154785, discriminator_loss=0.11758163571357727\n",
            "step 4167: generator_loss=2.180349588394165, discriminator_loss=0.11955808848142624\n",
            "step 4168: generator_loss=2.184277296066284, discriminator_loss=0.11949902772903442\n",
            "step 4169: generator_loss=2.20175838470459, discriminator_loss=0.11899194121360779\n",
            "step 4170: generator_loss=2.2258362770080566, discriminator_loss=0.11786897480487823\n",
            "step 4171: generator_loss=2.2399959564208984, discriminator_loss=0.1174144372344017\n",
            "step 4172: generator_loss=2.2539520263671875, discriminator_loss=0.11715728044509888\n",
            "step 4173: generator_loss=2.2515251636505127, discriminator_loss=0.11760741472244263\n",
            "step 4174: generator_loss=2.2427120208740234, discriminator_loss=0.11855797469615936\n",
            "step 4175: generator_loss=2.2322630882263184, discriminator_loss=0.11966830492019653\n",
            "step 4176: generator_loss=2.208137035369873, discriminator_loss=0.12150764465332031\n",
            "step 4177: generator_loss=2.175522804260254, discriminator_loss=0.12379536032676697\n",
            "step 4178: generator_loss=2.1382570266723633, discriminator_loss=0.12633636593818665\n",
            "step 4179: generator_loss=2.098177433013916, discriminator_loss=0.12911224365234375\n",
            "step 4180: generator_loss=2.0608067512512207, discriminator_loss=0.13180437684059143\n",
            "step 4181: generator_loss=2.0584185123443604, discriminator_loss=0.13210821151733398\n",
            "step 4182: generator_loss=2.091603994369507, discriminator_loss=0.12968966364860535\n",
            "step 4183: generator_loss=2.1332192420959473, discriminator_loss=0.1270211935043335\n",
            "step 4184: generator_loss=2.183346748352051, discriminator_loss=0.12364286184310913\n",
            "step 4185: generator_loss=2.2249584197998047, discriminator_loss=0.12111279368400574\n",
            "step 4186: generator_loss=2.2630043029785156, discriminator_loss=0.11868122220039368\n",
            "step 4187: generator_loss=2.2906155586242676, discriminator_loss=0.11703554540872574\n",
            "step 4188: generator_loss=2.322638988494873, discriminator_loss=0.11502307653427124\n",
            "step 4189: generator_loss=2.3404650688171387, discriminator_loss=0.11379780620336533\n",
            "step 4190: generator_loss=2.3424668312072754, discriminator_loss=0.1136242002248764\n",
            "step 4191: generator_loss=2.359010696411133, discriminator_loss=0.11250288784503937\n",
            "step 4192: generator_loss=2.3771371841430664, discriminator_loss=0.11139091849327087\n",
            "step 4193: generator_loss=2.381120443344116, discriminator_loss=0.11083093285560608\n",
            "step 4194: generator_loss=2.3682618141174316, discriminator_loss=0.11129781603813171\n",
            "step 4195: generator_loss=2.3488762378692627, discriminator_loss=0.1119346097111702\n",
            "step 4196: generator_loss=2.3501899242401123, discriminator_loss=0.11145289987325668\n",
            "step 4197: generator_loss=2.371070384979248, discriminator_loss=0.11002091318368912\n",
            "step 4198: generator_loss=2.4022796154022217, discriminator_loss=0.10803765058517456\n",
            "step 4199: generator_loss=2.4298601150512695, discriminator_loss=0.10634031891822815\n",
            "step 4200: generator_loss=2.463751792907715, discriminator_loss=0.10429637134075165\n",
            "step 4201: generator_loss=2.4916210174560547, discriminator_loss=0.10250533372163773\n",
            "step 4202: generator_loss=2.5159506797790527, discriminator_loss=0.1009950190782547\n",
            "step 4203: generator_loss=2.52571964263916, discriminator_loss=0.10011927783489227\n",
            "step 4204: generator_loss=2.5450448989868164, discriminator_loss=0.09865880012512207\n",
            "step 4205: generator_loss=2.581540107727051, discriminator_loss=0.09660886228084564\n",
            "step 4206: generator_loss=2.625049352645874, discriminator_loss=0.09437521547079086\n",
            "step 4207: generator_loss=2.6801600456237793, discriminator_loss=0.09164425730705261\n",
            "step 4208: generator_loss=2.7313809394836426, discriminator_loss=0.08914855122566223\n",
            "step 4209: generator_loss=2.7631752490997314, discriminator_loss=0.0874786376953125\n",
            "step 4210: generator_loss=2.7827162742614746, discriminator_loss=0.0860370546579361\n",
            "step 4211: generator_loss=2.775630474090576, discriminator_loss=0.08574976772069931\n",
            "step 4212: generator_loss=2.7468323707580566, discriminator_loss=0.08604299277067184\n",
            "step 4213: generator_loss=2.709433078765869, discriminator_loss=0.08668109774589539\n",
            "step 4214: generator_loss=2.684556007385254, discriminator_loss=0.08688879758119583\n",
            "step 4215: generator_loss=2.654995918273926, discriminator_loss=0.08737485110759735\n",
            "step 4216: generator_loss=2.6317853927612305, discriminator_loss=0.08780762553215027\n",
            "step 4217: generator_loss=2.622159004211426, discriminator_loss=0.08769246935844421\n",
            "step 4218: generator_loss=2.6146440505981445, discriminator_loss=0.0875401645898819\n",
            "step 4219: generator_loss=2.6065025329589844, discriminator_loss=0.0874759778380394\n",
            "step 4220: generator_loss=2.5993146896362305, discriminator_loss=0.08737653493881226\n",
            "step 4221: generator_loss=2.584463119506836, discriminator_loss=0.0877581536769867\n",
            "step 4222: generator_loss=2.570312738418579, discriminator_loss=0.08800004422664642\n",
            "step 4223: generator_loss=2.5508809089660645, discriminator_loss=0.08860699832439423\n",
            "step 4224: generator_loss=2.533116340637207, discriminator_loss=0.089229054749012\n",
            "step 4225: generator_loss=2.536630630493164, discriminator_loss=0.08897767961025238\n",
            "step 4226: generator_loss=2.563136577606201, discriminator_loss=0.08780967444181442\n",
            "step 4227: generator_loss=2.5994577407836914, discriminator_loss=0.08637697249650955\n",
            "step 4228: generator_loss=2.6322743892669678, discriminator_loss=0.08510904014110565\n",
            "step 4229: generator_loss=2.665245532989502, discriminator_loss=0.08391577750444412\n",
            "step 4230: generator_loss=2.6895623207092285, discriminator_loss=0.08300640434026718\n",
            "step 4231: generator_loss=2.7154457569122314, discriminator_loss=0.08204320073127747\n",
            "step 4232: generator_loss=2.7296175956726074, discriminator_loss=0.08151863515377045\n",
            "step 4233: generator_loss=2.735521078109741, discriminator_loss=0.08123394846916199\n",
            "step 4234: generator_loss=2.7513790130615234, discriminator_loss=0.0806371420621872\n",
            "step 4235: generator_loss=2.757347583770752, discriminator_loss=0.08035551011562347\n",
            "step 4236: generator_loss=2.7728638648986816, discriminator_loss=0.07978422939777374\n",
            "step 4237: generator_loss=2.771697759628296, discriminator_loss=0.07977299392223358\n",
            "step 4238: generator_loss=2.7861578464508057, discriminator_loss=0.07916420698165894\n",
            "step 4239: generator_loss=2.795065402984619, discriminator_loss=0.0787355825304985\n",
            "step 4240: generator_loss=2.790426254272461, discriminator_loss=0.07876282185316086\n",
            "step 4241: generator_loss=2.7831902503967285, discriminator_loss=0.07886868715286255\n",
            "step 4242: generator_loss=2.788698196411133, discriminator_loss=0.07848532497882843\n",
            "step 4243: generator_loss=2.8054661750793457, discriminator_loss=0.07780168205499649\n",
            "step 4244: generator_loss=2.8097479343414307, discriminator_loss=0.07744661718606949\n",
            "step 4245: generator_loss=2.8037424087524414, discriminator_loss=0.07746585458517075\n",
            "step 4246: generator_loss=2.7821245193481445, discriminator_loss=0.07799462229013443\n",
            "step 4247: generator_loss=2.7600631713867188, discriminator_loss=0.07851086556911469\n",
            "step 4248: generator_loss=2.766522169113159, discriminator_loss=0.07819079607725143\n",
            "step 4249: generator_loss=2.752324342727661, discriminator_loss=0.07848771661520004\n",
            "step 4250: generator_loss=2.7327003479003906, discriminator_loss=0.0790313184261322\n",
            "step 4251: generator_loss=2.723745107650757, discriminator_loss=0.07917293161153793\n",
            "step 4252: generator_loss=2.7099685668945312, discriminator_loss=0.07952126860618591\n",
            "step 4253: generator_loss=2.7106735706329346, discriminator_loss=0.0794433057308197\n",
            "step 4254: generator_loss=2.7079663276672363, discriminator_loss=0.07938829064369202\n",
            "step 4255: generator_loss=2.702359914779663, discriminator_loss=0.07950666546821594\n",
            "step 4256: generator_loss=2.724346399307251, discriminator_loss=0.07862695306539536\n",
            "step 4257: generator_loss=2.7418038845062256, discriminator_loss=0.07789098471403122\n",
            "step 4258: generator_loss=2.757389545440674, discriminator_loss=0.07728536427021027\n",
            "step 4259: generator_loss=2.786041736602783, discriminator_loss=0.07622594386339188\n",
            "step 4260: generator_loss=2.8460166454315186, discriminator_loss=0.07416532933712006\n",
            "step 4261: generator_loss=2.900564670562744, discriminator_loss=0.07241977751255035\n",
            "step 4262: generator_loss=2.982879877090454, discriminator_loss=0.07000607997179031\n",
            "step 4263: generator_loss=3.0535595417022705, discriminator_loss=0.06802279502153397\n",
            "step 4264: generator_loss=3.085585117340088, discriminator_loss=0.06716037541627884\n",
            "step 4265: generator_loss=3.0828168392181396, discriminator_loss=0.06704257428646088\n",
            "step 4266: generator_loss=3.0458672046661377, discriminator_loss=0.06770443916320801\n",
            "step 4267: generator_loss=3.017446517944336, discriminator_loss=0.06824667751789093\n",
            "step 4268: generator_loss=2.978961944580078, discriminator_loss=0.06903713196516037\n",
            "step 4269: generator_loss=2.9302966594696045, discriminator_loss=0.07015226781368256\n",
            "step 4270: generator_loss=2.912301540374756, discriminator_loss=0.07047303020954132\n",
            "step 4271: generator_loss=2.90323805809021, discriminator_loss=0.0704849511384964\n",
            "step 4272: generator_loss=2.8960390090942383, discriminator_loss=0.07051465660333633\n",
            "step 4273: generator_loss=2.8735764026641846, discriminator_loss=0.07097189128398895\n",
            "step 4274: generator_loss=2.8512048721313477, discriminator_loss=0.0715358704328537\n",
            "step 4275: generator_loss=2.816513776779175, discriminator_loss=0.07241073250770569\n",
            "step 4276: generator_loss=2.783290147781372, discriminator_loss=0.07339571416378021\n",
            "step 4277: generator_loss=2.78426194190979, discriminator_loss=0.07325087487697601\n",
            "step 4278: generator_loss=2.789105176925659, discriminator_loss=0.07307262718677521\n",
            "step 4279: generator_loss=2.78298020362854, discriminator_loss=0.0731213390827179\n",
            "step 4280: generator_loss=2.780839204788208, discriminator_loss=0.07307900488376617\n",
            "step 4281: generator_loss=2.7856884002685547, discriminator_loss=0.07284233719110489\n",
            "step 4282: generator_loss=2.8002865314483643, discriminator_loss=0.07228045165538788\n",
            "step 4283: generator_loss=2.8599042892456055, discriminator_loss=0.07032590359449387\n",
            "step 4284: generator_loss=2.917266368865967, discriminator_loss=0.06854735314846039\n",
            "step 4285: generator_loss=2.9552295207977295, discriminator_loss=0.06740057468414307\n",
            "step 4286: generator_loss=2.985731840133667, discriminator_loss=0.06647894531488419\n",
            "step 4287: generator_loss=2.990618944168091, discriminator_loss=0.06626991927623749\n",
            "step 4288: generator_loss=2.97192645072937, discriminator_loss=0.06669024378061295\n",
            "step 4289: generator_loss=2.9457790851593018, discriminator_loss=0.06724948436021805\n",
            "step 4290: generator_loss=2.907945156097412, discriminator_loss=0.06817828863859177\n",
            "step 4291: generator_loss=2.8547682762145996, discriminator_loss=0.0696321427822113\n",
            "step 4292: generator_loss=2.8022289276123047, discriminator_loss=0.07109680026769638\n",
            "step 4293: generator_loss=2.7490134239196777, discriminator_loss=0.07275721430778503\n",
            "step 4294: generator_loss=2.707887649536133, discriminator_loss=0.0740753561258316\n",
            "step 4295: generator_loss=2.66916561126709, discriminator_loss=0.07543345540761948\n",
            "step 4296: generator_loss=2.6458308696746826, discriminator_loss=0.0762416273355484\n",
            "step 4297: generator_loss=2.661651134490967, discriminator_loss=0.07559479027986526\n",
            "step 4298: generator_loss=2.7001001834869385, discriminator_loss=0.07415211200714111\n",
            "step 4299: generator_loss=2.7236075401306152, discriminator_loss=0.07325795292854309\n",
            "step 4300: generator_loss=2.730083465576172, discriminator_loss=0.07304873317480087\n",
            "step 4301: generator_loss=2.717421531677246, discriminator_loss=0.07349439710378647\n",
            "step 4302: generator_loss=2.688059091567993, discriminator_loss=0.07461749017238617\n",
            "step 4303: generator_loss=2.6795074939727783, discriminator_loss=0.07495541870594025\n",
            "step 4304: generator_loss=2.6771154403686523, discriminator_loss=0.07509838044643402\n",
            "step 4305: generator_loss=2.6680054664611816, discriminator_loss=0.07553128898143768\n",
            "step 4306: generator_loss=2.657496452331543, discriminator_loss=0.07597919553518295\n",
            "step 4307: generator_loss=2.665945291519165, discriminator_loss=0.07582160830497742\n",
            "step 4308: generator_loss=2.665304183959961, discriminator_loss=0.0759502649307251\n",
            "step 4309: generator_loss=2.657106876373291, discriminator_loss=0.07635192573070526\n",
            "step 4310: generator_loss=2.6366982460021973, discriminator_loss=0.07713820040225983\n",
            "step 4311: generator_loss=2.619030714035034, discriminator_loss=0.07784251868724823\n",
            "step 4312: generator_loss=2.602935314178467, discriminator_loss=0.07839837670326233\n",
            "step 4313: generator_loss=2.570166826248169, discriminator_loss=0.07979264855384827\n",
            "step 4314: generator_loss=2.547003746032715, discriminator_loss=0.0807207003235817\n",
            "step 4315: generator_loss=2.535491466522217, discriminator_loss=0.08112543821334839\n",
            "step 4316: generator_loss=2.547837018966675, discriminator_loss=0.08055785298347473\n",
            "step 4317: generator_loss=2.5728604793548584, discriminator_loss=0.07952127605676651\n",
            "step 4318: generator_loss=2.598148822784424, discriminator_loss=0.0785338431596756\n",
            "step 4319: generator_loss=2.6078171730041504, discriminator_loss=0.07811659574508667\n",
            "step 4320: generator_loss=2.6306257247924805, discriminator_loss=0.07720968127250671\n",
            "step 4321: generator_loss=2.6598315238952637, discriminator_loss=0.07605317234992981\n",
            "step 4322: generator_loss=2.674082040786743, discriminator_loss=0.07559078931808472\n",
            "step 4323: generator_loss=2.673078775405884, discriminator_loss=0.07570146024227142\n",
            "step 4324: generator_loss=2.6859781742095947, discriminator_loss=0.0752091184258461\n",
            "step 4325: generator_loss=2.6918697357177734, discriminator_loss=0.07498881220817566\n",
            "step 4326: generator_loss=2.6845712661743164, discriminator_loss=0.0752796158194542\n",
            "step 4327: generator_loss=2.674039363861084, discriminator_loss=0.07574018836021423\n",
            "step 4328: generator_loss=2.6611201763153076, discriminator_loss=0.0762186348438263\n",
            "step 4329: generator_loss=2.6351187229156494, discriminator_loss=0.07728744298219681\n",
            "step 4330: generator_loss=2.5905842781066895, discriminator_loss=0.07900966703891754\n",
            "step 4331: generator_loss=2.558586597442627, discriminator_loss=0.08043905347585678\n",
            "step 4332: generator_loss=2.5282981395721436, discriminator_loss=0.08178199827671051\n",
            "step 4333: generator_loss=2.4899635314941406, discriminator_loss=0.08353909105062485\n",
            "step 4334: generator_loss=2.4479198455810547, discriminator_loss=0.08555205166339874\n",
            "step 4335: generator_loss=2.424002170562744, discriminator_loss=0.08673462271690369\n",
            "step 4336: generator_loss=2.4413976669311523, discriminator_loss=0.08601678907871246\n",
            "step 4337: generator_loss=2.4805829524993896, discriminator_loss=0.08429531753063202\n",
            "step 4338: generator_loss=2.5111083984375, discriminator_loss=0.08308064192533493\n",
            "step 4339: generator_loss=2.5435290336608887, discriminator_loss=0.08179739117622375\n",
            "step 4340: generator_loss=2.5779671669006348, discriminator_loss=0.08055635541677475\n",
            "step 4341: generator_loss=2.6000466346740723, discriminator_loss=0.07993318140506744\n",
            "step 4342: generator_loss=2.6166458129882812, discriminator_loss=0.07944485545158386\n",
            "step 4343: generator_loss=2.6115899085998535, discriminator_loss=0.07984428852796555\n",
            "step 4344: generator_loss=2.5858259201049805, discriminator_loss=0.08094356954097748\n",
            "step 4345: generator_loss=2.5498719215393066, discriminator_loss=0.08259332925081253\n",
            "step 4346: generator_loss=2.5075106620788574, discriminator_loss=0.08459217846393585\n",
            "step 4347: generator_loss=2.4802074432373047, discriminator_loss=0.08583928644657135\n",
            "step 4348: generator_loss=2.457434892654419, discriminator_loss=0.08696393668651581\n",
            "step 4349: generator_loss=2.4486775398254395, discriminator_loss=0.08750664442777634\n",
            "step 4350: generator_loss=2.4277420043945312, discriminator_loss=0.08870431780815125\n",
            "step 4351: generator_loss=2.3992247581481934, discriminator_loss=0.090200275182724\n",
            "step 4352: generator_loss=2.38728666305542, discriminator_loss=0.09093154966831207\n",
            "step 4353: generator_loss=2.369474411010742, discriminator_loss=0.09208008646965027\n",
            "step 4354: generator_loss=2.3638756275177, discriminator_loss=0.09253201633691788\n",
            "step 4355: generator_loss=2.3535730838775635, discriminator_loss=0.09331370890140533\n",
            "step 4356: generator_loss=2.336280584335327, discriminator_loss=0.09449975192546844\n",
            "step 4357: generator_loss=2.357473850250244, discriminator_loss=0.09363946318626404\n",
            "step 4358: generator_loss=2.4607174396514893, discriminator_loss=0.08872410655021667\n",
            "step 4359: generator_loss=2.5822224617004395, discriminator_loss=0.08348135650157928\n",
            "step 4360: generator_loss=2.7076685428619385, discriminator_loss=0.07897280156612396\n",
            "step 4361: generator_loss=2.7974281311035156, discriminator_loss=0.07591596245765686\n",
            "step 4362: generator_loss=2.874001979827881, discriminator_loss=0.07349704205989838\n",
            "step 4363: generator_loss=2.932175636291504, discriminator_loss=0.07176201045513153\n",
            "step 4364: generator_loss=2.966662645339966, discriminator_loss=0.07067688554525375\n",
            "step 4365: generator_loss=2.9608213901519775, discriminator_loss=0.07061867415904999\n",
            "step 4366: generator_loss=2.9334969520568848, discriminator_loss=0.07107929140329361\n",
            "step 4367: generator_loss=2.927286148071289, discriminator_loss=0.07090619206428528\n",
            "step 4368: generator_loss=2.895583152770996, discriminator_loss=0.07149018347263336\n",
            "step 4369: generator_loss=2.8374974727630615, discriminator_loss=0.07281474024057388\n",
            "step 4370: generator_loss=2.7589211463928223, discriminator_loss=0.07493605464696884\n",
            "step 4371: generator_loss=2.6790738105773926, discriminator_loss=0.07747168838977814\n",
            "step 4372: generator_loss=2.6093077659606934, discriminator_loss=0.07979331910610199\n",
            "step 4373: generator_loss=2.5746657848358154, discriminator_loss=0.0808752253651619\n",
            "step 4374: generator_loss=2.573315382003784, discriminator_loss=0.08072777092456818\n",
            "step 4375: generator_loss=2.5943551063537598, discriminator_loss=0.07977136224508286\n",
            "step 4376: generator_loss=2.629136085510254, discriminator_loss=0.07826772332191467\n",
            "step 4377: generator_loss=2.649552583694458, discriminator_loss=0.07744327187538147\n",
            "step 4378: generator_loss=2.6649539470672607, discriminator_loss=0.07671504467725754\n",
            "step 4379: generator_loss=2.705219268798828, discriminator_loss=0.0751376673579216\n",
            "step 4380: generator_loss=2.7738332748413086, discriminator_loss=0.07276006788015366\n",
            "step 4381: generator_loss=2.8285961151123047, discriminator_loss=0.07084397971630096\n",
            "step 4382: generator_loss=2.8638827800750732, discriminator_loss=0.06960787624120712\n",
            "step 4383: generator_loss=2.8944835662841797, discriminator_loss=0.06858759373426437\n",
            "step 4384: generator_loss=2.8802690505981445, discriminator_loss=0.06880607455968857\n",
            "step 4385: generator_loss=2.8672428131103516, discriminator_loss=0.06915108859539032\n",
            "step 4386: generator_loss=2.833167552947998, discriminator_loss=0.07001309841871262\n",
            "step 4387: generator_loss=2.788478374481201, discriminator_loss=0.07139184325933456\n",
            "step 4388: generator_loss=2.777975082397461, discriminator_loss=0.07165402173995972\n",
            "step 4389: generator_loss=2.758634567260742, discriminator_loss=0.07250680774450302\n",
            "step 4390: generator_loss=2.7288174629211426, discriminator_loss=0.07427182793617249\n",
            "step 4391: generator_loss=2.740234375, discriminator_loss=0.07529771327972412\n",
            "step 4392: generator_loss=2.6438848972320557, discriminator_loss=0.08237363398075104\n",
            "step 4393: generator_loss=2.622293710708618, discriminator_loss=0.08753490447998047\n",
            "step 4394: generator_loss=2.578909397125244, discriminator_loss=0.0991906225681305\n",
            "step 4395: generator_loss=2.5584750175476074, discriminator_loss=0.11481505632400513\n",
            "step 4396: generator_loss=2.5115487575531006, discriminator_loss=0.13518530130386353\n",
            "step 4397: generator_loss=2.554161310195923, discriminator_loss=0.15828225016593933\n",
            "step 4398: generator_loss=2.7406697273254395, discriminator_loss=0.15440452098846436\n",
            "step 4399: generator_loss=2.5333199501037598, discriminator_loss=0.1935996413230896\n",
            "step 4400: generator_loss=2.733778953552246, discriminator_loss=0.18086180090904236\n",
            "step 4401: generator_loss=3.357520580291748, discriminator_loss=0.11705251038074493\n",
            "step 4402: generator_loss=3.2345871925354004, discriminator_loss=0.13460350036621094\n",
            "step 4403: generator_loss=3.1438088417053223, discriminator_loss=0.12722977995872498\n",
            "step 4404: generator_loss=3.3959343433380127, discriminator_loss=0.11093329638242722\n",
            "step 4405: generator_loss=3.6422183513641357, discriminator_loss=0.09762541204690933\n",
            "step 4406: generator_loss=3.9147751331329346, discriminator_loss=0.08424930274486542\n",
            "step 4407: generator_loss=3.988577365875244, discriminator_loss=0.07598497718572617\n",
            "step 4408: generator_loss=4.143856048583984, discriminator_loss=0.07009220123291016\n",
            "step 4409: generator_loss=4.433774471282959, discriminator_loss=0.06384864449501038\n",
            "step 4410: generator_loss=4.396613121032715, discriminator_loss=0.06155335158109665\n",
            "step 4411: generator_loss=4.554781436920166, discriminator_loss=0.05986706539988518\n",
            "step 4412: generator_loss=4.557697296142578, discriminator_loss=0.05843684822320938\n",
            "step 4413: generator_loss=4.350696086883545, discriminator_loss=0.06122449412941933\n",
            "step 4414: generator_loss=4.134925365447998, discriminator_loss=0.06257089972496033\n",
            "step 4415: generator_loss=3.8218934535980225, discriminator_loss=0.06720392405986786\n",
            "step 4416: generator_loss=3.872753381729126, discriminator_loss=0.06668879091739655\n",
            "step 4417: generator_loss=3.8512117862701416, discriminator_loss=0.06575925648212433\n",
            "step 4418: generator_loss=3.456723213195801, discriminator_loss=0.07545410096645355\n",
            "step 4419: generator_loss=3.5874013900756836, discriminator_loss=0.07329057902097702\n",
            "step 4420: generator_loss=3.4092514514923096, discriminator_loss=0.0806930810213089\n",
            "step 4421: generator_loss=3.1084065437316895, discriminator_loss=0.09056507050991058\n",
            "step 4422: generator_loss=3.030411720275879, discriminator_loss=0.08958891779184341\n",
            "step 4423: generator_loss=3.0935006141662598, discriminator_loss=0.08563908189535141\n",
            "step 4424: generator_loss=2.952324390411377, discriminator_loss=0.08667105436325073\n",
            "step 4425: generator_loss=3.040334701538086, discriminator_loss=0.08012273162603378\n",
            "step 4426: generator_loss=2.9119749069213867, discriminator_loss=0.08182591944932938\n",
            "step 4427: generator_loss=3.056464672088623, discriminator_loss=0.07471472024917603\n",
            "step 4428: generator_loss=3.1681313514709473, discriminator_loss=0.07060955464839935\n",
            "step 4429: generator_loss=3.070889949798584, discriminator_loss=0.07242915779352188\n",
            "step 4430: generator_loss=3.2133166790008545, discriminator_loss=0.06877048313617706\n",
            "step 4431: generator_loss=3.0383400917053223, discriminator_loss=0.07167623192071915\n",
            "step 4432: generator_loss=3.090144157409668, discriminator_loss=0.06875090301036835\n",
            "step 4433: generator_loss=3.01737380027771, discriminator_loss=0.06922642886638641\n",
            "step 4434: generator_loss=2.999112606048584, discriminator_loss=0.06892527639865875\n",
            "step 4435: generator_loss=2.864628791809082, discriminator_loss=0.07256346940994263\n",
            "step 4436: generator_loss=2.9178977012634277, discriminator_loss=0.07219818234443665\n",
            "step 4437: generator_loss=2.755319118499756, discriminator_loss=0.07519517838954926\n",
            "step 4438: generator_loss=2.647484540939331, discriminator_loss=0.07890814542770386\n",
            "step 4439: generator_loss=2.5670409202575684, discriminator_loss=0.08155696094036102\n",
            "step 4440: generator_loss=2.507589817047119, discriminator_loss=0.08382520079612732\n",
            "step 4441: generator_loss=2.4236927032470703, discriminator_loss=0.0870257169008255\n",
            "step 4442: generator_loss=2.3523502349853516, discriminator_loss=0.09050498902797699\n",
            "step 4443: generator_loss=2.2785351276397705, discriminator_loss=0.09391424059867859\n",
            "step 4444: generator_loss=2.2923827171325684, discriminator_loss=0.0931316688656807\n",
            "step 4445: generator_loss=2.2687718868255615, discriminator_loss=0.0944068431854248\n",
            "step 4446: generator_loss=2.241328716278076, discriminator_loss=0.0957726314663887\n",
            "step 4447: generator_loss=2.246408700942993, discriminator_loss=0.09581099450588226\n",
            "step 4448: generator_loss=2.278109550476074, discriminator_loss=0.09425605833530426\n",
            "step 4449: generator_loss=2.2861135005950928, discriminator_loss=0.09438782930374146\n",
            "step 4450: generator_loss=2.2843077182769775, discriminator_loss=0.09503807127475739\n",
            "step 4451: generator_loss=2.3856632709503174, discriminator_loss=0.09085345268249512\n",
            "step 4452: generator_loss=2.3910794258117676, discriminator_loss=0.09136362373828888\n",
            "step 4453: generator_loss=2.422715187072754, discriminator_loss=0.09112977236509323\n",
            "step 4454: generator_loss=2.4660587310791016, discriminator_loss=0.09038213640451431\n",
            "step 4455: generator_loss=2.456343173980713, discriminator_loss=0.09175987541675568\n",
            "step 4456: generator_loss=2.463240385055542, discriminator_loss=0.09149342030286789\n",
            "step 4457: generator_loss=2.3494162559509277, discriminator_loss=0.0967738926410675\n",
            "step 4458: generator_loss=2.279594898223877, discriminator_loss=0.10002745687961578\n",
            "step 4459: generator_loss=2.215653419494629, discriminator_loss=0.10449367761611938\n",
            "step 4460: generator_loss=2.1667261123657227, discriminator_loss=0.10739010572433472\n",
            "step 4461: generator_loss=2.165205955505371, discriminator_loss=0.10816985368728638\n",
            "step 4462: generator_loss=2.1218156814575195, discriminator_loss=0.1108865737915039\n",
            "step 4463: generator_loss=2.0901248455047607, discriminator_loss=0.11324279755353928\n",
            "step 4464: generator_loss=2.0912837982177734, discriminator_loss=0.11451417207717896\n",
            "step 4465: generator_loss=2.042353630065918, discriminator_loss=0.11842872947454453\n",
            "step 4466: generator_loss=2.0057373046875, discriminator_loss=0.12257800996303558\n",
            "step 4467: generator_loss=1.9662811756134033, discriminator_loss=0.12664657831192017\n",
            "step 4468: generator_loss=1.9359924793243408, discriminator_loss=0.1298852562904358\n",
            "step 4469: generator_loss=1.979345679283142, discriminator_loss=0.1283721923828125\n",
            "step 4470: generator_loss=1.9722256660461426, discriminator_loss=0.12849846482276917\n",
            "step 4471: generator_loss=2.048048257827759, discriminator_loss=0.12475539743900299\n",
            "step 4472: generator_loss=2.080270290374756, discriminator_loss=0.12285961955785751\n",
            "step 4473: generator_loss=2.1406970024108887, discriminator_loss=0.12000636756420135\n",
            "step 4474: generator_loss=2.1778149604797363, discriminator_loss=0.11842106282711029\n",
            "step 4475: generator_loss=2.1931824684143066, discriminator_loss=0.11859451234340668\n",
            "step 4476: generator_loss=2.2292773723602295, discriminator_loss=0.11727986484766006\n",
            "step 4477: generator_loss=2.2420787811279297, discriminator_loss=0.1170613169670105\n",
            "step 4478: generator_loss=2.239917755126953, discriminator_loss=0.11728940159082413\n",
            "step 4479: generator_loss=2.264457941055298, discriminator_loss=0.1161661148071289\n",
            "step 4480: generator_loss=2.3048293590545654, discriminator_loss=0.11429141461849213\n",
            "step 4481: generator_loss=2.3424172401428223, discriminator_loss=0.11273963004350662\n",
            "step 4482: generator_loss=2.384675979614258, discriminator_loss=0.11103637516498566\n",
            "step 4483: generator_loss=2.403573513031006, discriminator_loss=0.11011803150177002\n",
            "step 4484: generator_loss=2.4128713607788086, discriminator_loss=0.10943762212991714\n",
            "step 4485: generator_loss=2.439380168914795, discriminator_loss=0.10824034363031387\n",
            "step 4486: generator_loss=2.4574098587036133, discriminator_loss=0.10724550485610962\n",
            "step 4487: generator_loss=2.4609484672546387, discriminator_loss=0.10674456506967545\n",
            "step 4488: generator_loss=2.4562528133392334, discriminator_loss=0.10667281597852707\n",
            "step 4489: generator_loss=2.469686269760132, discriminator_loss=0.10574884712696075\n",
            "step 4490: generator_loss=2.483802318572998, discriminator_loss=0.10445569455623627\n",
            "step 4491: generator_loss=2.4803733825683594, discriminator_loss=0.10404372215270996\n",
            "step 4492: generator_loss=2.5002307891845703, discriminator_loss=0.1025388091802597\n",
            "step 4493: generator_loss=2.5133843421936035, discriminator_loss=0.1008385568857193\n",
            "step 4494: generator_loss=2.519040584564209, discriminator_loss=0.09965887665748596\n",
            "step 4495: generator_loss=2.513334274291992, discriminator_loss=0.09913793206214905\n",
            "step 4496: generator_loss=2.5027289390563965, discriminator_loss=0.09865669161081314\n",
            "step 4497: generator_loss=2.482395648956299, discriminator_loss=0.09866171330213547\n",
            "step 4498: generator_loss=2.4664463996887207, discriminator_loss=0.0985473245382309\n",
            "step 4499: generator_loss=2.4622907638549805, discriminator_loss=0.09772057831287384\n",
            "step 4500: generator_loss=2.4474265575408936, discriminator_loss=0.09772462397813797\n",
            "step 4501: generator_loss=2.425950527191162, discriminator_loss=0.098238505423069\n",
            "step 4502: generator_loss=2.4127326011657715, discriminator_loss=0.09826596081256866\n",
            "step 4503: generator_loss=2.398606538772583, discriminator_loss=0.09873349964618683\n",
            "step 4504: generator_loss=2.3769588470458984, discriminator_loss=0.1001211628317833\n",
            "step 4505: generator_loss=2.3300135135650635, discriminator_loss=0.10528381168842316\n",
            "step 4506: generator_loss=2.282689094543457, discriminator_loss=0.1123514473438263\n",
            "step 4507: generator_loss=2.2459354400634766, discriminator_loss=0.1243705227971077\n",
            "step 4508: generator_loss=2.1648707389831543, discriminator_loss=0.14413855969905853\n",
            "step 4509: generator_loss=2.2424564361572266, discriminator_loss=0.14509433507919312\n",
            "step 4510: generator_loss=2.2097206115722656, discriminator_loss=0.15687331557273865\n",
            "step 4511: generator_loss=2.1351823806762695, discriminator_loss=0.17439152300357819\n",
            "step 4512: generator_loss=2.180525302886963, discriminator_loss=0.178645059466362\n",
            "step 4513: generator_loss=2.257014274597168, discriminator_loss=0.17772352695465088\n",
            "step 4514: generator_loss=2.6843016147613525, discriminator_loss=0.1256309151649475\n",
            "step 4515: generator_loss=2.411386489868164, discriminator_loss=0.18804572522640228\n",
            "step 4516: generator_loss=2.7749269008636475, discriminator_loss=0.13321250677108765\n",
            "step 4517: generator_loss=2.8641982078552246, discriminator_loss=0.12211743742227554\n",
            "step 4518: generator_loss=2.900221347808838, discriminator_loss=0.12222635746002197\n",
            "step 4519: generator_loss=3.056434154510498, discriminator_loss=0.10259729623794556\n",
            "step 4520: generator_loss=2.864098072052002, discriminator_loss=0.11006414145231247\n",
            "step 4521: generator_loss=3.3929965496063232, discriminator_loss=0.08321511745452881\n",
            "step 4522: generator_loss=3.4551517963409424, discriminator_loss=0.08014288544654846\n",
            "step 4523: generator_loss=3.6728224754333496, discriminator_loss=0.07316307723522186\n",
            "step 4524: generator_loss=3.673600196838379, discriminator_loss=0.0738154724240303\n",
            "step 4525: generator_loss=3.9438934326171875, discriminator_loss=0.06815837323665619\n",
            "step 4526: generator_loss=3.9499337673187256, discriminator_loss=0.06757023930549622\n",
            "step 4527: generator_loss=3.905257225036621, discriminator_loss=0.06773513555526733\n",
            "step 4528: generator_loss=3.8633201122283936, discriminator_loss=0.06761960685253143\n",
            "step 4529: generator_loss=3.7161760330200195, discriminator_loss=0.0701918751001358\n",
            "step 4530: generator_loss=3.6053831577301025, discriminator_loss=0.0725148543715477\n",
            "step 4531: generator_loss=3.4647693634033203, discriminator_loss=0.07810671627521515\n",
            "step 4532: generator_loss=3.3476734161376953, discriminator_loss=0.07894421368837357\n",
            "step 4533: generator_loss=3.299471855163574, discriminator_loss=0.07997894287109375\n",
            "step 4534: generator_loss=3.2119381427764893, discriminator_loss=0.0798305794596672\n",
            "step 4535: generator_loss=2.992201805114746, discriminator_loss=0.08574624359607697\n",
            "step 4536: generator_loss=2.9860358238220215, discriminator_loss=0.08427092432975769\n",
            "step 4537: generator_loss=2.9320645332336426, discriminator_loss=0.08380253612995148\n",
            "step 4538: generator_loss=2.9361305236816406, discriminator_loss=0.0817260891199112\n",
            "step 4539: generator_loss=2.8974080085754395, discriminator_loss=0.08222740888595581\n",
            "step 4540: generator_loss=2.8968234062194824, discriminator_loss=0.0777004286646843\n",
            "step 4541: generator_loss=2.9235310554504395, discriminator_loss=0.07649916410446167\n",
            "step 4542: generator_loss=2.91066312789917, discriminator_loss=0.07704266160726547\n",
            "step 4543: generator_loss=2.784090518951416, discriminator_loss=0.08049634099006653\n",
            "step 4544: generator_loss=2.9312148094177246, discriminator_loss=0.07921063899993896\n",
            "step 4545: generator_loss=2.8498969078063965, discriminator_loss=0.0811431035399437\n",
            "step 4546: generator_loss=2.8532614707946777, discriminator_loss=0.08157780021429062\n",
            "step 4547: generator_loss=2.7636499404907227, discriminator_loss=0.084678053855896\n",
            "step 4548: generator_loss=2.701667070388794, discriminator_loss=0.0866994559764862\n",
            "step 4549: generator_loss=2.5108983516693115, discriminator_loss=0.08997073024511337\n",
            "step 4550: generator_loss=2.519839286804199, discriminator_loss=0.09114304929971695\n",
            "step 4551: generator_loss=2.4126524925231934, discriminator_loss=0.09381040185689926\n",
            "step 4552: generator_loss=2.4072232246398926, discriminator_loss=0.09561098366975784\n",
            "step 4553: generator_loss=2.381185531616211, discriminator_loss=0.09676216542720795\n",
            "step 4554: generator_loss=2.3969216346740723, discriminator_loss=0.09787966310977936\n",
            "step 4555: generator_loss=2.331007480621338, discriminator_loss=0.09914013743400574\n",
            "step 4556: generator_loss=2.2932565212249756, discriminator_loss=0.1018887534737587\n",
            "step 4557: generator_loss=2.2208540439605713, discriminator_loss=0.10516966879367828\n",
            "step 4558: generator_loss=2.231520175933838, discriminator_loss=0.10810123383998871\n",
            "step 4559: generator_loss=2.2329325675964355, discriminator_loss=0.10974182933568954\n",
            "step 4560: generator_loss=2.086148262023926, discriminator_loss=0.11575973033905029\n",
            "step 4561: generator_loss=2.06962251663208, discriminator_loss=0.11688977479934692\n",
            "step 4562: generator_loss=2.0577025413513184, discriminator_loss=0.11908436566591263\n",
            "step 4563: generator_loss=2.145782470703125, discriminator_loss=0.11769919097423553\n",
            "step 4564: generator_loss=2.1043012142181396, discriminator_loss=0.11818723380565643\n",
            "step 4565: generator_loss=2.114227294921875, discriminator_loss=0.11890007555484772\n",
            "step 4566: generator_loss=2.1985602378845215, discriminator_loss=0.11585689336061478\n",
            "step 4567: generator_loss=2.2828125953674316, discriminator_loss=0.11291462182998657\n",
            "step 4568: generator_loss=2.23427152633667, discriminator_loss=0.11365047097206116\n",
            "step 4569: generator_loss=2.266167640686035, discriminator_loss=0.1128067821264267\n",
            "step 4570: generator_loss=2.2971043586730957, discriminator_loss=0.11076265573501587\n",
            "step 4571: generator_loss=2.3406829833984375, discriminator_loss=0.10927101969718933\n",
            "step 4572: generator_loss=2.4165070056915283, discriminator_loss=0.10724680125713348\n",
            "step 4573: generator_loss=2.3593249320983887, discriminator_loss=0.10903133451938629\n",
            "step 4574: generator_loss=2.404597759246826, discriminator_loss=0.10683465749025345\n",
            "step 4575: generator_loss=2.4399781227111816, discriminator_loss=0.1053866371512413\n",
            "step 4576: generator_loss=2.4355087280273438, discriminator_loss=0.1056106686592102\n",
            "step 4577: generator_loss=2.4281506538391113, discriminator_loss=0.10596329718828201\n",
            "step 4578: generator_loss=2.4044833183288574, discriminator_loss=0.10689568519592285\n",
            "step 4579: generator_loss=2.4052252769470215, discriminator_loss=0.10670031607151031\n",
            "step 4580: generator_loss=2.4052953720092773, discriminator_loss=0.1062304824590683\n",
            "step 4581: generator_loss=2.3908159732818604, discriminator_loss=0.10650938749313354\n",
            "step 4582: generator_loss=2.360729694366455, discriminator_loss=0.10749013721942902\n",
            "step 4583: generator_loss=2.321929693222046, discriminator_loss=0.10901152342557907\n",
            "step 4584: generator_loss=2.276643753051758, discriminator_loss=0.11091120541095734\n",
            "step 4585: generator_loss=2.2206177711486816, discriminator_loss=0.11347578465938568\n",
            "step 4586: generator_loss=2.156132936477661, discriminator_loss=0.11721818149089813\n",
            "step 4587: generator_loss=2.102261543273926, discriminator_loss=0.12047506123781204\n",
            "step 4588: generator_loss=2.0591859817504883, discriminator_loss=0.12332058697938919\n",
            "step 4589: generator_loss=2.0384929180145264, discriminator_loss=0.12491666525602341\n",
            "step 4590: generator_loss=2.039442300796509, discriminator_loss=0.12485577166080475\n",
            "step 4591: generator_loss=2.075852155685425, discriminator_loss=0.12235172837972641\n",
            "step 4592: generator_loss=2.13132381439209, discriminator_loss=0.11853966116905212\n",
            "step 4593: generator_loss=2.1553995609283447, discriminator_loss=0.11700347065925598\n",
            "step 4594: generator_loss=2.15017032623291, discriminator_loss=0.11742866039276123\n",
            "step 4595: generator_loss=2.1244750022888184, discriminator_loss=0.11924822628498077\n",
            "step 4596: generator_loss=2.086069107055664, discriminator_loss=0.12198445200920105\n",
            "step 4597: generator_loss=2.0523900985717773, discriminator_loss=0.12476103007793427\n",
            "step 4598: generator_loss=2.0373618602752686, discriminator_loss=0.12614229321479797\n",
            "step 4599: generator_loss=2.029273748397827, discriminator_loss=0.1273512840270996\n",
            "step 4600: generator_loss=2.0051565170288086, discriminator_loss=0.12982338666915894\n",
            "step 4601: generator_loss=1.988671898841858, discriminator_loss=0.13178235292434692\n",
            "step 4602: generator_loss=1.9789150953292847, discriminator_loss=0.13361269235610962\n",
            "step 4603: generator_loss=1.9714524745941162, discriminator_loss=0.1353008896112442\n",
            "step 4604: generator_loss=1.9621070623397827, discriminator_loss=0.13701176643371582\n",
            "step 4605: generator_loss=1.9440288543701172, discriminator_loss=0.13962817192077637\n",
            "step 4606: generator_loss=1.9177573919296265, discriminator_loss=0.1429636925458908\n",
            "step 4607: generator_loss=1.8909722566604614, discriminator_loss=0.146466463804245\n",
            "step 4608: generator_loss=1.8717807531356812, discriminator_loss=0.1496766060590744\n",
            "step 4609: generator_loss=1.887793779373169, discriminator_loss=0.14952805638313293\n",
            "step 4610: generator_loss=1.931975245475769, discriminator_loss=0.14719292521476746\n",
            "step 4611: generator_loss=1.9911301136016846, discriminator_loss=0.14369337260723114\n",
            "step 4612: generator_loss=2.0559604167938232, discriminator_loss=0.1401044726371765\n",
            "step 4613: generator_loss=2.109922409057617, discriminator_loss=0.13728582859039307\n",
            "step 4614: generator_loss=2.147954225540161, discriminator_loss=0.13563993573188782\n",
            "step 4615: generator_loss=2.195676565170288, discriminator_loss=0.13313570618629456\n",
            "step 4616: generator_loss=2.254077672958374, discriminator_loss=0.12985333800315857\n",
            "step 4617: generator_loss=2.3053133487701416, discriminator_loss=0.12699870765209198\n",
            "step 4618: generator_loss=2.363826036453247, discriminator_loss=0.12358525395393372\n",
            "step 4619: generator_loss=2.39005970954895, discriminator_loss=0.12135544419288635\n",
            "step 4620: generator_loss=2.395397901535034, discriminator_loss=0.12009299546480179\n",
            "step 4621: generator_loss=2.359684944152832, discriminator_loss=0.12057190388441086\n",
            "step 4622: generator_loss=2.286932945251465, discriminator_loss=0.12316948175430298\n",
            "step 4623: generator_loss=2.2009530067443848, discriminator_loss=0.1265687793493271\n",
            "step 4624: generator_loss=2.1219663619995117, discriminator_loss=0.13043341040611267\n",
            "step 4625: generator_loss=2.0498692989349365, discriminator_loss=0.13430821895599365\n",
            "step 4626: generator_loss=2.010382890701294, discriminator_loss=0.13619443774223328\n",
            "step 4627: generator_loss=2.0047879219055176, discriminator_loss=0.13583239912986755\n",
            "step 4628: generator_loss=2.0069146156311035, discriminator_loss=0.13501903414726257\n",
            "step 4629: generator_loss=2.009228467941284, discriminator_loss=0.1344071626663208\n",
            "step 4630: generator_loss=2.015230894088745, discriminator_loss=0.13369855284690857\n",
            "step 4631: generator_loss=2.028318166732788, discriminator_loss=0.1323453187942505\n",
            "step 4632: generator_loss=2.037050724029541, discriminator_loss=0.1313280463218689\n",
            "step 4633: generator_loss=2.0460586547851562, discriminator_loss=0.13043195009231567\n",
            "step 4634: generator_loss=2.082336187362671, discriminator_loss=0.12760087847709656\n",
            "step 4635: generator_loss=2.1296846866607666, discriminator_loss=0.12436749041080475\n",
            "step 4636: generator_loss=2.2201004028320312, discriminator_loss=0.11851698905229568\n",
            "step 4637: generator_loss=2.3414602279663086, discriminator_loss=0.11168406903743744\n",
            "step 4638: generator_loss=2.456073760986328, discriminator_loss=0.10614101588726044\n",
            "step 4639: generator_loss=2.5496222972869873, discriminator_loss=0.10187945514917374\n",
            "step 4640: generator_loss=2.6306490898132324, discriminator_loss=0.09845450520515442\n",
            "step 4641: generator_loss=2.687030553817749, discriminator_loss=0.09599827229976654\n",
            "step 4642: generator_loss=2.727442741394043, discriminator_loss=0.09395956248044968\n",
            "step 4643: generator_loss=2.7482850551605225, discriminator_loss=0.09274928271770477\n",
            "step 4644: generator_loss=2.7489302158355713, discriminator_loss=0.0919201523065567\n",
            "step 4645: generator_loss=2.741748332977295, discriminator_loss=0.0912541076540947\n",
            "step 4646: generator_loss=2.7506091594696045, discriminator_loss=0.09004880487918854\n",
            "step 4647: generator_loss=2.746613025665283, discriminator_loss=0.0891941636800766\n",
            "step 4648: generator_loss=2.747204065322876, discriminator_loss=0.08803369849920273\n",
            "step 4649: generator_loss=2.7402515411376953, discriminator_loss=0.08721810579299927\n",
            "step 4650: generator_loss=2.7156054973602295, discriminator_loss=0.08688099682331085\n",
            "step 4651: generator_loss=2.6969289779663086, discriminator_loss=0.08645009994506836\n",
            "step 4652: generator_loss=2.672971725463867, discriminator_loss=0.08635014295578003\n",
            "step 4653: generator_loss=2.655046224594116, discriminator_loss=0.08599047362804413\n",
            "step 4654: generator_loss=2.653670310974121, discriminator_loss=0.08525930345058441\n",
            "step 4655: generator_loss=2.6586265563964844, discriminator_loss=0.08427805453538895\n",
            "step 4656: generator_loss=2.6584153175354004, discriminator_loss=0.08363203704357147\n",
            "step 4657: generator_loss=2.6587038040161133, discriminator_loss=0.0829673632979393\n",
            "step 4658: generator_loss=2.689152956008911, discriminator_loss=0.08125097304582596\n",
            "step 4659: generator_loss=2.721916913986206, discriminator_loss=0.07965035736560822\n",
            "step 4660: generator_loss=2.7436017990112305, discriminator_loss=0.07854387164115906\n",
            "step 4661: generator_loss=2.7615463733673096, discriminator_loss=0.07761141657829285\n",
            "step 4662: generator_loss=2.7839767932891846, discriminator_loss=0.07663965225219727\n",
            "step 4663: generator_loss=2.795257091522217, discriminator_loss=0.07601262629032135\n",
            "step 4664: generator_loss=2.796189308166504, discriminator_loss=0.07581203430891037\n",
            "step 4665: generator_loss=2.799680233001709, discriminator_loss=0.07547858357429504\n",
            "step 4666: generator_loss=2.796900987625122, discriminator_loss=0.07542312145233154\n",
            "step 4667: generator_loss=2.798909902572632, discriminator_loss=0.07515187561511993\n",
            "step 4668: generator_loss=2.796483278274536, discriminator_loss=0.074960857629776\n",
            "step 4669: generator_loss=2.776219367980957, discriminator_loss=0.07544493675231934\n",
            "step 4670: generator_loss=2.738070011138916, discriminator_loss=0.07661767303943634\n",
            "step 4671: generator_loss=2.697723865509033, discriminator_loss=0.0778651237487793\n",
            "step 4672: generator_loss=2.6742260456085205, discriminator_loss=0.07859007269144058\n",
            "step 4673: generator_loss=2.6391513347625732, discriminator_loss=0.07976247370243073\n",
            "step 4674: generator_loss=2.5992281436920166, discriminator_loss=0.08123551309108734\n",
            "step 4675: generator_loss=2.572476625442505, discriminator_loss=0.08225855231285095\n",
            "step 4676: generator_loss=2.5569732189178467, discriminator_loss=0.08284255862236023\n",
            "step 4677: generator_loss=2.539294481277466, discriminator_loss=0.0836232602596283\n",
            "step 4678: generator_loss=2.5359463691711426, discriminator_loss=0.0837041586637497\n",
            "step 4679: generator_loss=2.542410135269165, discriminator_loss=0.08343888819217682\n",
            "step 4680: generator_loss=2.5499794483184814, discriminator_loss=0.08319353312253952\n",
            "step 4681: generator_loss=2.5516934394836426, discriminator_loss=0.0832185298204422\n",
            "step 4682: generator_loss=2.582557201385498, discriminator_loss=0.08217974007129669\n",
            "step 4683: generator_loss=2.613403797149658, discriminator_loss=0.08097641170024872\n",
            "step 4684: generator_loss=2.6379666328430176, discriminator_loss=0.08022935688495636\n",
            "step 4685: generator_loss=2.6689014434814453, discriminator_loss=0.07915903627872467\n",
            "step 4686: generator_loss=2.6721644401550293, discriminator_loss=0.07912410795688629\n",
            "step 4687: generator_loss=2.653088092803955, discriminator_loss=0.07993528991937637\n",
            "step 4688: generator_loss=2.623810291290283, discriminator_loss=0.0811353474855423\n",
            "step 4689: generator_loss=2.5730433464050293, discriminator_loss=0.0832749605178833\n",
            "step 4690: generator_loss=2.534893274307251, discriminator_loss=0.08494573831558228\n",
            "step 4691: generator_loss=2.5078887939453125, discriminator_loss=0.08610976487398148\n",
            "step 4692: generator_loss=2.481370449066162, discriminator_loss=0.08743306249380112\n",
            "step 4693: generator_loss=2.4479832649230957, discriminator_loss=0.08896283805370331\n",
            "step 4694: generator_loss=2.437135696411133, discriminator_loss=0.08954966068267822\n",
            "step 4695: generator_loss=2.4249377250671387, discriminator_loss=0.09015995264053345\n",
            "step 4696: generator_loss=2.429116725921631, discriminator_loss=0.0901014506816864\n",
            "step 4697: generator_loss=2.442993640899658, discriminator_loss=0.0896926075220108\n",
            "step 4698: generator_loss=2.4629971981048584, discriminator_loss=0.08895586431026459\n",
            "step 4699: generator_loss=2.4727370738983154, discriminator_loss=0.08873537182807922\n",
            "step 4700: generator_loss=2.4957289695739746, discriminator_loss=0.0879206508398056\n",
            "step 4701: generator_loss=2.5222558975219727, discriminator_loss=0.08697313815355301\n",
            "step 4702: generator_loss=2.5394718647003174, discriminator_loss=0.08640697598457336\n",
            "step 4703: generator_loss=2.5415070056915283, discriminator_loss=0.0864226222038269\n",
            "step 4704: generator_loss=2.5417351722717285, discriminator_loss=0.08651408553123474\n",
            "step 4705: generator_loss=2.5343222618103027, discriminator_loss=0.08695260435342789\n",
            "step 4706: generator_loss=2.541407346725464, discriminator_loss=0.08663363754749298\n",
            "step 4707: generator_loss=2.5675110816955566, discriminator_loss=0.08562297374010086\n",
            "step 4708: generator_loss=2.610750675201416, discriminator_loss=0.08381021022796631\n",
            "step 4709: generator_loss=2.706190586090088, discriminator_loss=0.08021117746829987\n",
            "step 4710: generator_loss=2.818955898284912, discriminator_loss=0.07625582814216614\n",
            "step 4711: generator_loss=2.92525315284729, discriminator_loss=0.07293419539928436\n",
            "step 4712: generator_loss=2.9948651790618896, discriminator_loss=0.07066668570041656\n",
            "step 4713: generator_loss=3.049757719039917, discriminator_loss=0.06886343657970428\n",
            "step 4714: generator_loss=3.141857862472534, discriminator_loss=0.06621184200048447\n",
            "step 4715: generator_loss=3.1857810020446777, discriminator_loss=0.06473186612129211\n",
            "step 4716: generator_loss=3.1922543048858643, discriminator_loss=0.06405265629291534\n",
            "step 4717: generator_loss=3.1897497177124023, discriminator_loss=0.06354320049285889\n",
            "step 4718: generator_loss=3.1965529918670654, discriminator_loss=0.06285738945007324\n",
            "step 4719: generator_loss=3.1920793056488037, discriminator_loss=0.06245330721139908\n",
            "step 4720: generator_loss=3.171640396118164, discriminator_loss=0.06244022399187088\n",
            "step 4721: generator_loss=3.12318754196167, discriminator_loss=0.06298986077308655\n",
            "step 4722: generator_loss=3.0514838695526123, discriminator_loss=0.06436699628829956\n",
            "step 4723: generator_loss=2.972005844116211, discriminator_loss=0.06596685945987701\n",
            "step 4724: generator_loss=2.8840394020080566, discriminator_loss=0.06801113486289978\n",
            "step 4725: generator_loss=2.8037118911743164, discriminator_loss=0.070121631026268\n",
            "step 4726: generator_loss=2.7327866554260254, discriminator_loss=0.07219719886779785\n",
            "step 4727: generator_loss=2.665468215942383, discriminator_loss=0.07438139617443085\n",
            "step 4728: generator_loss=2.6055240631103516, discriminator_loss=0.07648853957653046\n",
            "step 4729: generator_loss=2.570608615875244, discriminator_loss=0.07779422402381897\n",
            "step 4730: generator_loss=2.595979690551758, discriminator_loss=0.07667945325374603\n",
            "step 4731: generator_loss=2.6590170860290527, discriminator_loss=0.074216827750206\n",
            "step 4732: generator_loss=2.714367151260376, discriminator_loss=0.07212816178798676\n",
            "step 4733: generator_loss=2.794564723968506, discriminator_loss=0.06943687796592712\n",
            "step 4734: generator_loss=2.9083521366119385, discriminator_loss=0.06585108488798141\n",
            "step 4735: generator_loss=3.0054287910461426, discriminator_loss=0.06320004165172577\n",
            "step 4736: generator_loss=3.077785015106201, discriminator_loss=0.061425402760505676\n",
            "step 4737: generator_loss=3.142915725708008, discriminator_loss=0.05979783833026886\n",
            "step 4738: generator_loss=3.2105560302734375, discriminator_loss=0.05824057757854462\n",
            "step 4739: generator_loss=3.2607712745666504, discriminator_loss=0.057091109454631805\n",
            "step 4740: generator_loss=3.2603402137756348, discriminator_loss=0.05687163397669792\n",
            "step 4741: generator_loss=3.2204127311706543, discriminator_loss=0.05747917294502258\n",
            "step 4742: generator_loss=3.160421848297119, discriminator_loss=0.05849672853946686\n",
            "step 4743: generator_loss=3.0942044258117676, discriminator_loss=0.05974766984581947\n",
            "step 4744: generator_loss=3.04807186126709, discriminator_loss=0.06065496802330017\n",
            "step 4745: generator_loss=2.9938669204711914, discriminator_loss=0.0618164986371994\n",
            "step 4746: generator_loss=2.9077820777893066, discriminator_loss=0.06395573914051056\n",
            "step 4747: generator_loss=2.816366195678711, discriminator_loss=0.06662680208683014\n",
            "step 4748: generator_loss=2.73726487159729, discriminator_loss=0.0690804272890091\n",
            "step 4749: generator_loss=2.6732707023620605, discriminator_loss=0.07130253314971924\n",
            "step 4750: generator_loss=2.627286672592163, discriminator_loss=0.07298019528388977\n",
            "step 4751: generator_loss=2.5995945930480957, discriminator_loss=0.07406497001647949\n",
            "step 4752: generator_loss=2.578477382659912, discriminator_loss=0.07494783401489258\n",
            "step 4753: generator_loss=2.582000494003296, discriminator_loss=0.07485558092594147\n",
            "step 4754: generator_loss=2.6025891304016113, discriminator_loss=0.07417847216129303\n",
            "step 4755: generator_loss=2.6209330558776855, discriminator_loss=0.07364660501480103\n",
            "step 4756: generator_loss=2.622541904449463, discriminator_loss=0.07371951639652252\n",
            "step 4757: generator_loss=2.6154286861419678, discriminator_loss=0.07418909668922424\n",
            "step 4758: generator_loss=2.59670352935791, discriminator_loss=0.07528659701347351\n",
            "step 4759: generator_loss=2.592761754989624, discriminator_loss=0.07577274739742279\n",
            "step 4760: generator_loss=2.5818872451782227, discriminator_loss=0.07659461349248886\n",
            "step 4761: generator_loss=2.577911853790283, discriminator_loss=0.07713457196950912\n",
            "step 4762: generator_loss=2.563643455505371, discriminator_loss=0.07820028811693192\n",
            "step 4763: generator_loss=2.5658164024353027, discriminator_loss=0.07859573513269424\n",
            "step 4764: generator_loss=2.5899524688720703, discriminator_loss=0.07799134403467178\n",
            "step 4765: generator_loss=2.6158699989318848, discriminator_loss=0.0773104876279831\n",
            "step 4766: generator_loss=2.6521992683410645, discriminator_loss=0.07626350224018097\n",
            "step 4767: generator_loss=2.7141470909118652, discriminator_loss=0.0742666944861412\n",
            "step 4768: generator_loss=2.7479255199432373, discriminator_loss=0.07326148450374603\n",
            "step 4769: generator_loss=2.739452362060547, discriminator_loss=0.07368087768554688\n",
            "step 4770: generator_loss=2.6932783126831055, discriminator_loss=0.07535979151725769\n",
            "step 4771: generator_loss=2.6575565338134766, discriminator_loss=0.0767570436000824\n",
            "step 4772: generator_loss=2.602445602416992, discriminator_loss=0.07890501618385315\n",
            "step 4773: generator_loss=2.5363030433654785, discriminator_loss=0.08166415989398956\n",
            "step 4774: generator_loss=2.4762914180755615, discriminator_loss=0.08428771048784256\n",
            "step 4775: generator_loss=2.438690185546875, discriminator_loss=0.08607123792171478\n",
            "step 4776: generator_loss=2.4447591304779053, discriminator_loss=0.08580382168292999\n",
            "step 4777: generator_loss=2.470869779586792, discriminator_loss=0.08472396433353424\n",
            "step 4778: generator_loss=2.4962894916534424, discriminator_loss=0.08372603356838226\n",
            "step 4779: generator_loss=2.5469274520874023, discriminator_loss=0.08185739815235138\n",
            "step 4780: generator_loss=2.589195728302002, discriminator_loss=0.08027532696723938\n",
            "step 4781: generator_loss=2.6412172317504883, discriminator_loss=0.07830338925123215\n",
            "step 4782: generator_loss=2.6654820442199707, discriminator_loss=0.07758134603500366\n",
            "step 4783: generator_loss=2.6756277084350586, discriminator_loss=0.07743868231773376\n",
            "step 4784: generator_loss=2.6964423656463623, discriminator_loss=0.07669630646705627\n",
            "step 4785: generator_loss=2.696849822998047, discriminator_loss=0.0769185796380043\n",
            "step 4786: generator_loss=2.7020511627197266, discriminator_loss=0.07678663730621338\n",
            "step 4787: generator_loss=2.719085216522217, discriminator_loss=0.07629866898059845\n",
            "step 4788: generator_loss=2.730837821960449, discriminator_loss=0.07596780359745026\n",
            "step 4789: generator_loss=2.764584541320801, discriminator_loss=0.07469727098941803\n",
            "step 4790: generator_loss=2.7853164672851562, discriminator_loss=0.07390618324279785\n",
            "step 4791: generator_loss=2.819592237472534, discriminator_loss=0.0726027712225914\n",
            "step 4792: generator_loss=2.8609063625335693, discriminator_loss=0.07092927396297455\n",
            "step 4793: generator_loss=2.881502866744995, discriminator_loss=0.070088692009449\n",
            "step 4794: generator_loss=2.8754444122314453, discriminator_loss=0.06974274665117264\n",
            "step 4795: generator_loss=2.8411834239959717, discriminator_loss=0.07041973620653152\n",
            "step 4796: generator_loss=2.78676700592041, discriminator_loss=0.07178466767072678\n",
            "step 4797: generator_loss=2.723930597305298, discriminator_loss=0.07362006604671478\n",
            "step 4798: generator_loss=2.676560640335083, discriminator_loss=0.07502518594264984\n",
            "step 4799: generator_loss=2.632028579711914, discriminator_loss=0.07639073580503464\n",
            "step 4800: generator_loss=2.608585834503174, discriminator_loss=0.07721184194087982\n",
            "step 4801: generator_loss=2.618018388748169, discriminator_loss=0.07667979598045349\n",
            "step 4802: generator_loss=2.6681549549102783, discriminator_loss=0.0746910497546196\n",
            "step 4803: generator_loss=2.712584972381592, discriminator_loss=0.07304634898900986\n",
            "step 4804: generator_loss=2.7633533477783203, discriminator_loss=0.07127469033002853\n",
            "step 4805: generator_loss=2.775176525115967, discriminator_loss=0.07120062410831451\n",
            "step 4806: generator_loss=2.8146181106567383, discriminator_loss=0.07022114098072052\n",
            "step 4807: generator_loss=2.7808480262756348, discriminator_loss=0.07283467799425125\n",
            "step 4808: generator_loss=2.8086884021759033, discriminator_loss=0.07319772243499756\n",
            "step 4809: generator_loss=2.802917957305908, discriminator_loss=0.07639291882514954\n",
            "step 4810: generator_loss=2.7964377403259277, discriminator_loss=0.07880279421806335\n",
            "step 4811: generator_loss=2.732668876647949, discriminator_loss=0.09129707515239716\n",
            "step 4812: generator_loss=2.8406238555908203, discriminator_loss=0.08528485894203186\n",
            "step 4813: generator_loss=2.674494504928589, discriminator_loss=0.11284497380256653\n",
            "step 4814: generator_loss=2.8035173416137695, discriminator_loss=0.10729114711284637\n",
            "step 4815: generator_loss=2.8018977642059326, discriminator_loss=0.1202358677983284\n",
            "step 4816: generator_loss=2.811720132827759, discriminator_loss=0.13202795386314392\n",
            "step 4817: generator_loss=2.754852294921875, discriminator_loss=0.14804613590240479\n",
            "step 4818: generator_loss=2.984666347503662, discriminator_loss=0.14863422513008118\n",
            "step 4819: generator_loss=3.1338157653808594, discriminator_loss=0.1388932168483734\n",
            "step 4820: generator_loss=3.1757001876831055, discriminator_loss=0.14583230018615723\n",
            "step 4821: generator_loss=3.4397125244140625, discriminator_loss=0.128767192363739\n",
            "step 4822: generator_loss=3.5679261684417725, discriminator_loss=0.12464926391839981\n",
            "step 4823: generator_loss=3.8883864879608154, discriminator_loss=0.09304507076740265\n",
            "step 4824: generator_loss=4.321341037750244, discriminator_loss=0.0753670334815979\n",
            "step 4825: generator_loss=4.481842041015625, discriminator_loss=0.0643310546875\n",
            "step 4826: generator_loss=4.5096635818481445, discriminator_loss=0.06212206557393074\n",
            "step 4827: generator_loss=4.796173095703125, discriminator_loss=0.056051623076200485\n",
            "step 4828: generator_loss=4.6004791259765625, discriminator_loss=0.057887002825737\n",
            "step 4829: generator_loss=4.717073440551758, discriminator_loss=0.056003957986831665\n",
            "step 4830: generator_loss=4.992791652679443, discriminator_loss=0.05407567322254181\n",
            "step 4831: generator_loss=4.804469108581543, discriminator_loss=0.05602865666151047\n",
            "step 4832: generator_loss=4.590009689331055, discriminator_loss=0.059241145849227905\n",
            "step 4833: generator_loss=4.708575248718262, discriminator_loss=0.05816378444433212\n",
            "step 4834: generator_loss=4.536535263061523, discriminator_loss=0.06078597530722618\n",
            "step 4835: generator_loss=4.490503311157227, discriminator_loss=0.06181573122739792\n",
            "step 4836: generator_loss=4.247899532318115, discriminator_loss=0.06347312033176422\n",
            "step 4837: generator_loss=4.012809753417969, discriminator_loss=0.06820860505104065\n",
            "step 4838: generator_loss=3.927215576171875, discriminator_loss=0.06598176807165146\n",
            "step 4839: generator_loss=3.7915499210357666, discriminator_loss=0.06712344288825989\n",
            "step 4840: generator_loss=3.452955961227417, discriminator_loss=0.07647547125816345\n",
            "step 4841: generator_loss=3.457958221435547, discriminator_loss=0.07510428875684738\n",
            "step 4842: generator_loss=3.636146068572998, discriminator_loss=0.06447599083185196\n",
            "step 4843: generator_loss=3.4044432640075684, discriminator_loss=0.06874485313892365\n",
            "step 4844: generator_loss=3.5242738723754883, discriminator_loss=0.06488581001758575\n",
            "step 4845: generator_loss=3.5140457153320312, discriminator_loss=0.05982740968465805\n",
            "step 4846: generator_loss=3.4820199012756348, discriminator_loss=0.06269418448209763\n",
            "step 4847: generator_loss=3.3033246994018555, discriminator_loss=0.06326981633901596\n",
            "step 4848: generator_loss=3.4901962280273438, discriminator_loss=0.06210801377892494\n",
            "step 4849: generator_loss=3.1510045528411865, discriminator_loss=0.06401920318603516\n",
            "step 4850: generator_loss=3.2971444129943848, discriminator_loss=0.062049299478530884\n",
            "step 4851: generator_loss=3.1872189044952393, discriminator_loss=0.06598159670829773\n",
            "step 4852: generator_loss=2.793374538421631, discriminator_loss=0.071708545088768\n",
            "step 4853: generator_loss=2.963085889816284, discriminator_loss=0.0705561637878418\n",
            "step 4854: generator_loss=2.8185572624206543, discriminator_loss=0.07329240441322327\n",
            "step 4855: generator_loss=2.8146047592163086, discriminator_loss=0.07457149028778076\n",
            "step 4856: generator_loss=2.7592477798461914, discriminator_loss=0.07673947513103485\n",
            "step 4857: generator_loss=2.557758331298828, discriminator_loss=0.08120011538267136\n",
            "step 4858: generator_loss=2.5607199668884277, discriminator_loss=0.08330366760492325\n",
            "step 4859: generator_loss=2.5846986770629883, discriminator_loss=0.08322636038064957\n",
            "step 4860: generator_loss=2.4111993312835693, discriminator_loss=0.08821268379688263\n",
            "step 4861: generator_loss=2.4197850227355957, discriminator_loss=0.08953025937080383\n",
            "step 4862: generator_loss=2.3897721767425537, discriminator_loss=0.09027890861034393\n",
            "step 4863: generator_loss=2.3881006240844727, discriminator_loss=0.0920381098985672\n",
            "step 4864: generator_loss=2.4575493335723877, discriminator_loss=0.09092573821544647\n",
            "step 4865: generator_loss=2.3576302528381348, discriminator_loss=0.09223593026399612\n",
            "step 4866: generator_loss=2.362039089202881, discriminator_loss=0.09186769276857376\n",
            "step 4867: generator_loss=2.4780983924865723, discriminator_loss=0.08936600387096405\n",
            "step 4868: generator_loss=2.45320987701416, discriminator_loss=0.08832484483718872\n",
            "step 4869: generator_loss=2.4912524223327637, discriminator_loss=0.0873817503452301\n",
            "step 4870: generator_loss=2.508284330368042, discriminator_loss=0.08687561750411987\n",
            "step 4871: generator_loss=2.4726810455322266, discriminator_loss=0.08757703006267548\n",
            "step 4872: generator_loss=2.487821102142334, discriminator_loss=0.08940549939870834\n",
            "step 4873: generator_loss=2.5021820068359375, discriminator_loss=0.09155100584030151\n",
            "step 4874: generator_loss=2.379876136779785, discriminator_loss=0.09824229776859283\n",
            "step 4875: generator_loss=2.343513011932373, discriminator_loss=0.10328827053308487\n",
            "step 4876: generator_loss=2.372854709625244, discriminator_loss=0.1062827929854393\n",
            "step 4877: generator_loss=2.231177806854248, discriminator_loss=0.12883713841438293\n",
            "step 4878: generator_loss=2.273529291152954, discriminator_loss=0.13796693086624146\n",
            "step 4879: generator_loss=2.4479575157165527, discriminator_loss=0.12336356937885284\n",
            "step 4880: generator_loss=2.2824316024780273, discriminator_loss=0.16157816350460052\n",
            "step 4881: generator_loss=2.3447327613830566, discriminator_loss=0.1671203225851059\n",
            "step 4882: generator_loss=2.4339680671691895, discriminator_loss=0.15910017490386963\n",
            "step 4883: generator_loss=2.4162449836730957, discriminator_loss=0.18342630565166473\n",
            "step 4884: generator_loss=2.5643959045410156, discriminator_loss=0.1586291790008545\n",
            "step 4885: generator_loss=2.7516961097717285, discriminator_loss=0.14440613985061646\n",
            "step 4886: generator_loss=2.992225170135498, discriminator_loss=0.13278891146183014\n",
            "step 4887: generator_loss=2.7643630504608154, discriminator_loss=0.14543378353118896\n",
            "step 4888: generator_loss=3.0463719367980957, discriminator_loss=0.11610971391201019\n",
            "step 4889: generator_loss=3.1716203689575195, discriminator_loss=0.10737249255180359\n",
            "step 4890: generator_loss=3.66155743598938, discriminator_loss=0.08311405777931213\n",
            "step 4891: generator_loss=3.6350064277648926, discriminator_loss=0.08245088160037994\n",
            "step 4892: generator_loss=3.8419036865234375, discriminator_loss=0.07619722932577133\n",
            "step 4893: generator_loss=3.857381820678711, discriminator_loss=0.07473431527614594\n",
            "step 4894: generator_loss=3.6650590896606445, discriminator_loss=0.07743621617555618\n",
            "step 4895: generator_loss=3.7779464721679688, discriminator_loss=0.0743178129196167\n",
            "step 4896: generator_loss=3.6771788597106934, discriminator_loss=0.07455739378929138\n",
            "step 4897: generator_loss=3.4435360431671143, discriminator_loss=0.07871781289577484\n",
            "step 4898: generator_loss=3.4762074947357178, discriminator_loss=0.07730495929718018\n",
            "step 4899: generator_loss=3.3133039474487305, discriminator_loss=0.07860231399536133\n",
            "step 4900: generator_loss=3.126809597015381, discriminator_loss=0.07919098436832428\n",
            "step 4901: generator_loss=3.1053500175476074, discriminator_loss=0.08046622574329376\n",
            "step 4902: generator_loss=2.903188705444336, discriminator_loss=0.08551890403032303\n",
            "step 4903: generator_loss=2.926142692565918, discriminator_loss=0.08547773212194443\n",
            "step 4904: generator_loss=2.812375545501709, discriminator_loss=0.08859986811876297\n",
            "step 4905: generator_loss=2.50752592086792, discriminator_loss=0.09913943707942963\n",
            "step 4906: generator_loss=2.451725959777832, discriminator_loss=0.10431823134422302\n",
            "step 4907: generator_loss=2.3729734420776367, discriminator_loss=0.10730509459972382\n",
            "step 4908: generator_loss=2.4101996421813965, discriminator_loss=0.10712830722332001\n",
            "step 4909: generator_loss=2.410200834274292, discriminator_loss=0.10738087445497513\n",
            "step 4910: generator_loss=2.1736812591552734, discriminator_loss=0.1391930878162384\n",
            "step 4911: generator_loss=2.39176082611084, discriminator_loss=0.11755235493183136\n",
            "step 4912: generator_loss=2.440861701965332, discriminator_loss=0.13484209775924683\n",
            "step 4913: generator_loss=2.3975563049316406, discriminator_loss=0.14419394731521606\n",
            "step 4914: generator_loss=2.3725852966308594, discriminator_loss=0.15862546861171722\n",
            "step 4915: generator_loss=2.2674262523651123, discriminator_loss=0.1809520125389099\n",
            "step 4916: generator_loss=2.7036709785461426, discriminator_loss=0.1175416111946106\n",
            "step 4917: generator_loss=2.498387575149536, discriminator_loss=0.18155497312545776\n",
            "step 4918: generator_loss=2.495244026184082, discriminator_loss=0.1929532289505005\n",
            "step 4919: generator_loss=2.708345890045166, discriminator_loss=0.16889643669128418\n",
            "step 4920: generator_loss=2.838888645172119, discriminator_loss=0.16187676787376404\n",
            "step 4921: generator_loss=2.765421152114868, discriminator_loss=0.17572197318077087\n",
            "step 4922: generator_loss=3.0596532821655273, discriminator_loss=0.1355091631412506\n",
            "step 4923: generator_loss=3.0820677280426025, discriminator_loss=0.12231017649173737\n",
            "step 4924: generator_loss=3.042160987854004, discriminator_loss=0.12419858574867249\n",
            "step 4925: generator_loss=3.2981958389282227, discriminator_loss=0.09912917017936707\n",
            "step 4926: generator_loss=3.472287654876709, discriminator_loss=0.08673121780157089\n",
            "step 4927: generator_loss=3.4846017360687256, discriminator_loss=0.08221492171287537\n",
            "step 4928: generator_loss=3.3661835193634033, discriminator_loss=0.08455678820610046\n",
            "step 4929: generator_loss=3.4787163734436035, discriminator_loss=0.07546494901180267\n",
            "step 4930: generator_loss=3.42651104927063, discriminator_loss=0.07490897178649902\n",
            "step 4931: generator_loss=3.462506055831909, discriminator_loss=0.072716124355793\n",
            "step 4932: generator_loss=3.3105270862579346, discriminator_loss=0.07483722269535065\n",
            "step 4933: generator_loss=3.1506476402282715, discriminator_loss=0.07875628769397736\n",
            "step 4934: generator_loss=3.041184186935425, discriminator_loss=0.08015282452106476\n",
            "step 4935: generator_loss=3.0253562927246094, discriminator_loss=0.0810316801071167\n",
            "step 4936: generator_loss=2.8598461151123047, discriminator_loss=0.08708713948726654\n",
            "step 4937: generator_loss=2.7396223545074463, discriminator_loss=0.09135004878044128\n",
            "step 4938: generator_loss=2.7360739707946777, discriminator_loss=0.09159614145755768\n",
            "step 4939: generator_loss=2.6141128540039062, discriminator_loss=0.09314046055078506\n",
            "step 4940: generator_loss=2.4228813648223877, discriminator_loss=0.10360915958881378\n",
            "step 4941: generator_loss=2.4408135414123535, discriminator_loss=0.10051939636468887\n",
            "step 4942: generator_loss=2.2853875160217285, discriminator_loss=0.10943131148815155\n",
            "step 4943: generator_loss=2.370582103729248, discriminator_loss=0.10367777943611145\n",
            "step 4944: generator_loss=2.2755377292633057, discriminator_loss=0.10605110228061676\n",
            "step 4945: generator_loss=2.358549118041992, discriminator_loss=0.10428053140640259\n",
            "step 4946: generator_loss=2.2712109088897705, discriminator_loss=0.10605418682098389\n",
            "step 4947: generator_loss=2.376542091369629, discriminator_loss=0.1027889996767044\n",
            "step 4948: generator_loss=2.4859378337860107, discriminator_loss=0.09997972846031189\n",
            "step 4949: generator_loss=2.516704559326172, discriminator_loss=0.0960346907377243\n",
            "step 4950: generator_loss=2.5298972129821777, discriminator_loss=0.09512907266616821\n",
            "step 4951: generator_loss=2.6469154357910156, discriminator_loss=0.09170229732990265\n",
            "step 4952: generator_loss=2.642550230026245, discriminator_loss=0.09400326013565063\n",
            "step 4953: generator_loss=2.54750919342041, discriminator_loss=0.09629277884960175\n",
            "step 4954: generator_loss=2.330451011657715, discriminator_loss=0.1009739562869072\n",
            "step 4955: generator_loss=2.487182378768921, discriminator_loss=0.10064753890037537\n",
            "step 4956: generator_loss=2.318159341812134, discriminator_loss=0.10749562084674835\n",
            "step 4957: generator_loss=2.394425392150879, discriminator_loss=0.10938439518213272\n",
            "step 4958: generator_loss=2.1394081115722656, discriminator_loss=0.11871494352817535\n",
            "step 4959: generator_loss=2.0816807746887207, discriminator_loss=0.12181073427200317\n",
            "step 4960: generator_loss=2.137725830078125, discriminator_loss=0.12421312928199768\n",
            "step 4961: generator_loss=2.0951948165893555, discriminator_loss=0.12470574676990509\n",
            "step 4962: generator_loss=1.9933868646621704, discriminator_loss=0.12824317812919617\n",
            "step 4963: generator_loss=2.1068577766418457, discriminator_loss=0.12583422660827637\n",
            "step 4964: generator_loss=1.9762659072875977, discriminator_loss=0.1318536102771759\n",
            "step 4965: generator_loss=2.0323486328125, discriminator_loss=0.13050314784049988\n",
            "step 4966: generator_loss=2.003925323486328, discriminator_loss=0.1332826018333435\n",
            "step 4967: generator_loss=1.9671289920806885, discriminator_loss=0.1356583684682846\n",
            "step 4968: generator_loss=1.986738681793213, discriminator_loss=0.13539758324623108\n",
            "step 4969: generator_loss=2.0061473846435547, discriminator_loss=0.1353641003370285\n",
            "step 4970: generator_loss=2.05466365814209, discriminator_loss=0.1345367729663849\n",
            "step 4971: generator_loss=2.0767064094543457, discriminator_loss=0.13355344533920288\n",
            "step 4972: generator_loss=2.1427979469299316, discriminator_loss=0.13158860802650452\n",
            "step 4973: generator_loss=2.1309690475463867, discriminator_loss=0.1321549415588379\n",
            "step 4974: generator_loss=2.154068946838379, discriminator_loss=0.13152524828910828\n",
            "step 4975: generator_loss=2.1745710372924805, discriminator_loss=0.13104216754436493\n",
            "step 4976: generator_loss=2.195410966873169, discriminator_loss=0.13120536506175995\n",
            "step 4977: generator_loss=2.1859281063079834, discriminator_loss=0.13201463222503662\n",
            "step 4978: generator_loss=2.1514170169830322, discriminator_loss=0.13398800790309906\n",
            "step 4979: generator_loss=2.143766403198242, discriminator_loss=0.13438645005226135\n",
            "step 4980: generator_loss=2.1653518676757812, discriminator_loss=0.13283342123031616\n",
            "step 4981: generator_loss=2.152465581893921, discriminator_loss=0.13336455821990967\n",
            "step 4982: generator_loss=2.1339168548583984, discriminator_loss=0.13399258255958557\n",
            "step 4983: generator_loss=2.1236720085144043, discriminator_loss=0.134246826171875\n",
            "step 4984: generator_loss=2.1023638248443604, discriminator_loss=0.13521072268486023\n",
            "step 4985: generator_loss=2.0906291007995605, discriminator_loss=0.13538174331188202\n",
            "step 4986: generator_loss=2.128920793533325, discriminator_loss=0.13215261697769165\n",
            "step 4987: generator_loss=2.218891143798828, discriminator_loss=0.12584087252616882\n",
            "step 4988: generator_loss=2.316953659057617, discriminator_loss=0.11928167939186096\n",
            "step 4989: generator_loss=2.3867735862731934, discriminator_loss=0.11482350528240204\n",
            "step 4990: generator_loss=2.4279699325561523, discriminator_loss=0.11175166815519333\n",
            "step 4991: generator_loss=2.4805688858032227, discriminator_loss=0.10830501466989517\n",
            "step 4992: generator_loss=2.509681224822998, discriminator_loss=0.10581642389297485\n",
            "step 4993: generator_loss=2.5182723999023438, discriminator_loss=0.1043505072593689\n",
            "step 4994: generator_loss=2.5292696952819824, discriminator_loss=0.1027773767709732\n",
            "step 4995: generator_loss=2.573120594024658, discriminator_loss=0.09999820590019226\n",
            "step 4996: generator_loss=2.6112890243530273, discriminator_loss=0.09735031425952911\n",
            "step 4997: generator_loss=2.621886968612671, discriminator_loss=0.09612363576889038\n",
            "step 4998: generator_loss=2.6170573234558105, discriminator_loss=0.09535644948482513\n",
            "step 4999: generator_loss=2.58255934715271, discriminator_loss=0.09579361230134964\n",
            "step 5000: generator_loss=2.5311715602874756, discriminator_loss=0.09715184569358826\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optax.adam(learning_rate)\n",
        "n_steps = 5_000\n",
        "steps_per_save = 250\n",
        "seed = 0\n",
        "key = jax.random.key(seed)\n",
        "batch_size = 128\n",
        "latent_dim = 64\n",
        "loss_type = \"nonsaturating\"\n",
        "\n",
        "# Train GAN and save checkpoints\n",
        "generator_training_state, discriminator_training_state, key = train_gan(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    n_steps=n_steps,\n",
        "    steps_per_save=steps_per_save,\n",
        "    key=key,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "184e87a8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot one real histogram and one generated histogram\n",
        "# Re-running this cell will draw new samples each time.\n",
        "\n",
        "y_max = 0.02\n",
        "\n",
        "# Restore generator from the last checkpoint for this constraints experiment\n",
        "experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "steps = sorted(ckpt_manager.all_steps())\n",
        "if not steps:\n",
        "    raise ValueError(f\"No checkpoints found for experiment_name={experiment_name!r}.\")\n",
        "\n",
        "# Template states for restore (we only actually use the generator here)\n",
        "gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "    optimizer=optimizer,\n",
        "    key=jax.random.key(0),\n",
        "    latent_dim=latent_dim,\n",
        ")\n",
        "restored = ckpt_manager.restore(\n",
        "    steps[-1],\n",
        "    args=ocp.args.StandardRestore(\n",
        "        item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "    ),\n",
        ")\n",
        "generator_for_plot = restored[\"generator\"]\n",
        "\n",
        "# Use and update the global PRNG key defined above\n",
        "key, z_key, real_key = jax.random.split(key, 3)\n",
        "\n",
        "# Sample a latent vector and generate one fake histogram\n",
        "z = jax.random.normal(z_key, (1, latent_dim))\n",
        "fake_hist = generator_for_plot.apply_fn(\n",
        "    {\"params\": generator_for_plot.params},\n",
        "    z,\n",
        ")[0]\n",
        "\n",
        "# Sample one real histogram from the training set\n",
        "real_idx = jax.random.randint(real_key, shape=(), minval=0, maxval=train_data.shape[0])\n",
        "real_hist = train_data[real_idx]\n",
        "\n",
        "# Ensure both are non-negative and normalized to sum to 1\n",
        "fake_hist = jnp.maximum(fake_hist, 0.0)\n",
        "fake_hist = fake_hist / fake_hist.sum()\n",
        "real_hist = real_hist / real_hist.sum()\n",
        "\n",
        "# Convert to NumPy for plotting\n",
        "fake_hist_np = np.array(fake_hist)\n",
        "real_hist_np = np.array(real_hist)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(real_hist_np, label=\"Real histogram\")\n",
        "plt.plot(fake_hist_np, label=\"Generated histogram\")\n",
        "plt.xlabel(\"Bin index\")\n",
        "plt.ylabel(\"Probability (normalized)\")\n",
        "plt.ylim(top=y_max)\n",
        "plt.title(\"Real vs Generated Histogram\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb76bbaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_experiment_for_loss_type(\n",
        "#     loss_type: str,\n",
        "#     train_data,\n",
        "#     learning_rate: float = 1e-3,\n",
        "#     batch_size: int = 128,\n",
        "#     latent_dim: int = 64,\n",
        "#     n_steps: int = 5_000,\n",
        "#     steps_per_save: int = 250,\n",
        "#     seed: int = 0,\n",
        "# ):\n",
        "#     \"\"\"Train GAN and save checkpoints for a given loss_type.\n",
        "\n",
        "#     This function returns training results; plotting is handled separately.\n",
        "\n",
        "#     loss_type: \"nonsaturating\" or \"saturating\".\n",
        "#     \"\"\"\n",
        "#     print(f\"=== Running experiment with loss_type={loss_type} ===\")\n",
        "\n",
        "#     # Optimizer and PRNG key\n",
        "#     optimizer = optax.adam(learning_rate)\n",
        "#     key = jax.random.key(seed)\n",
        "\n",
        "#     # Train GAN and save checkpoints\n",
        "#     generator_training_state, discriminator_training_state, key = train_gan(\n",
        "#         train_data=train_data,\n",
        "#         optimizer=optimizer,\n",
        "#         n_steps=n_steps,\n",
        "#         steps_per_save=steps_per_save,\n",
        "#         key=key,\n",
        "#         batch_size=batch_size,\n",
        "#         latent_dim=latent_dim,\n",
        "#         loss_type=loss_type,\n",
        "#     )\n",
        "\n",
        "#     # --- Loss curves over checkpoints for this loss_type ---\n",
        "#     gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "#         optimizer,\n",
        "#         key=jax.random.key(0),\n",
        "#         latent_dim=latent_dim,\n",
        "#     )\n",
        "#     experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "#     ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "#     steps = sorted(ckpt_manager.all_steps())\n",
        "\n",
        "#     g_losses, d_losses = [], []\n",
        "#     real_images = train_data[\"image\"]\n",
        "\n",
        "#     for step in steps:\n",
        "#         restored = ckpt_manager.restore(\n",
        "#             step,\n",
        "#             args=ocp.args.StandardRestore(\n",
        "#                 item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "#             ),\n",
        "#         )\n",
        "#         gen_state = restored[\"generator\"]\n",
        "#         disc_state = restored[\"discriminator\"]\n",
        "\n",
        "#         key = jax.random.key(0)\n",
        "#         key, z_key = jax.random.split(key)\n",
        "#         z_vectors = jax.random.normal(\n",
        "#             z_key,\n",
        "#             (real_images.shape[0], latent_dim),\n",
        "#         )\n",
        "\n",
        "#         g_loss = calculate_generator_loss(\n",
        "#             gen_state.params,\n",
        "#             disc_state.params,\n",
        "#             gen_state.apply_fn,\n",
        "#             disc_state.apply_fn,\n",
        "#             z_vectors,\n",
        "#             loss_type=loss_type,\n",
        "#         )\n",
        "#         d_loss = calculate_discriminator_loss(\n",
        "#             disc_state.params,\n",
        "#             gen_state.params,\n",
        "#             gen_state.apply_fn,\n",
        "#             disc_state.apply_fn,\n",
        "#             z_vectors,\n",
        "#             real_images,\n",
        "#         )\n",
        "\n",
        "#         g_losses.append(float(g_loss))\n",
        "#         d_losses.append(float(d_loss))\n",
        "\n",
        "#     # Collect results (plotting handled separately)\n",
        "#     return {\n",
        "#         \"generator_state\": generator_training_state,\n",
        "#         \"discriminator_state\": discriminator_training_state,\n",
        "#         \"steps\": steps,\n",
        "#         \"g_losses\": g_losses,\n",
        "#         \"d_losses\": d_losses,\n",
        "#         \"loss_type\": loss_type,\n",
        "#         \"latent_dim\": latent_dim,\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda4c1e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_experiment_for_loss_type(results, train_data):\n",
        "#     \"\"\"Plot sample images and loss dynamics for a single loss_type run.\n",
        "\n",
        "#     Expects the dict returned by `run_experiment_for_loss_type` and the\n",
        "#     corresponding `train_data` used for training.\n",
        "#     \"\"\"\n",
        "#     loss_type = results[\"loss_type\"]\n",
        "#     generator_training_state = results[\"generator_state\"]\n",
        "#     steps = results[\"steps\"]\n",
        "#     g_losses = results[\"g_losses\"]\n",
        "#     d_losses = results[\"d_losses\"]\n",
        "#     latent_dim = results.get(\"latent_dim\", 64)\n",
        "\n",
        "#     # --- Plot a sample real vs fake image ---\n",
        "#     key = jax.random.key(0)\n",
        "#     key, z_key, real_image_key = jax.random.split(key, 3)\n",
        "#     z_vector = jax.random.normal(z_key, (1, latent_dim))\n",
        "#     real_idx = jax.random.randint(\n",
        "#         real_image_key, shape=(), minval=0, maxval=train_data.shape[0]\n",
        "#     )\n",
        "\n",
        "#     fake_image_flat = generator_training_state.apply_fn(\n",
        "#         {\"params\": generator_training_state.params},\n",
        "#         z_vector,\n",
        "#     )\n",
        "#     fake_image = fake_image_flat[0].reshape(28, 28)\n",
        "\n",
        "#     real_image_flat = train_data[real_idx]\n",
        "#     real_image = real_image_flat.reshape(28, 28)\n",
        "\n",
        "#     fig, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
        "#     axes[0].imshow(real_image, cmap=\"gray\"); axes[0].set_title(f\"Real ({loss_type})\"); axes[0].axis(\"off\")\n",
        "#     axes[1].imshow(fake_image, cmap=\"gray\"); axes[1].set_title(f\"Fake ({loss_type})\"); axes[1].axis(\"off\")\n",
        "#     plt.tight_layout(); plt.show()\n",
        "\n",
        "#     # --- Plot loss curves over checkpoints for this loss_type ---\n",
        "#     plt.figure()\n",
        "#     plt.plot(steps, g_losses, label=f\"generator ({loss_type})\")\n",
        "#     plt.plot(steps, d_losses, label=f\"discriminator ({loss_type})\")\n",
        "#     plt.xlabel(\"training step\")\n",
        "#     plt.ylabel(\"loss\")\n",
        "#     plt.title(f\"GAN losses ({loss_type})\")\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n",
        "\n",
        "#     # --- Plot training dynamics in (g, d) loss space ---\n",
        "#     g = np.array(g_losses)\n",
        "#     d = np.array(d_losses)\n",
        "\n",
        "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
        "#     points = np.column_stack([g, d])\n",
        "#     segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "#     t = np.linspace(0.0, 1.0, len(points) - 1)\n",
        "#     colors = cm.coolwarm(t)\n",
        "\n",
        "#     lc = LineCollection(segments, colors=colors, linewidths=1.5)\n",
        "#     ax.add_collection(lc)\n",
        "\n",
        "#     ax.scatter(g, d, c=t.tolist() + [1.0], cmap=\"coolwarm\", s=8, alpha=0.7)\n",
        "#     ax.scatter(g[0], d[0], color=\"blue\", s=50, label=\"start\")\n",
        "#     ax.scatter(g[-1], d[-1], color=\"red\", s=50, label=\"end\")\n",
        "\n",
        "#     ax.set_xlabel(\"Generator loss\")\n",
        "#     ax.set_ylabel(\"Discriminator loss\")\n",
        "#     ax.set_title(f\"GAN training dynamics in loss space ({loss_type})\")\n",
        "#     ax.grid(True)\n",
        "#     ax.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6610b8cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Convenience: run the full experiment for both loss types\n",
        "\n",
        "# results_by_loss_type = {}\n",
        "# for lt in [\"saturating\", \"nonsaturating\"]:\n",
        "#     results_by_loss_type[lt] = run_experiment_for_loss_type(\n",
        "#         loss_type=lt,\n",
        "#         train_data=train_data,\n",
        "#         learning_rate=1e-3,\n",
        "#         batch_size=128,\n",
        "#         latent_dim=64,\n",
        "#         n_steps=5_000,\n",
        "#         steps_per_save=250,\n",
        "#         seed=0,\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a819429",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot_experiment_for_loss_type(results_by_loss_type[\"saturating\"], train_data)\n",
        "# plot_experiment_for_loss_type(results_by_loss_type[\"nonsaturating\"], train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c19808",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# out_dir = Path(\"../../lab-notes/2025-12-17_gan-nonsaturating_loss\")\n",
        "# out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# for i, num in enumerate(sorted(plt.get_fignums()), start=1):\n",
        "#     fig = plt.figure(num)\n",
        "#     fig.savefig(\n",
        "#         out_dir / f\"2025-12-17_gan-nonsaturating_loss_{i}.png\",\n",
        "#         dpi=200,\n",
        "#         bbox_inches=\"tight\",\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "b932c49e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data,\n",
        "    optimizer,\n",
        "    batch_size: int,\n",
        "    latent_dim: int,\n",
        "    loss_type: str = \"nonsaturating\",\n",
        "):\n",
        "    \"\"\"Reload checkpoints for this notebook's run and plot D vs G loss.\n",
        "\n",
        "    Uses the same naming convention as `train_gan` and evaluates losses\n",
        "    at each saved checkpoint on a fresh mini-batch from `train_data`.\n",
        "    \"\"\"\n",
        "    experiment_name = make_constraints_experiment_name(optimizer, loss_type)\n",
        "    ckpt_manager = initialise_checkpoint_manager(experiment_name)\n",
        "    steps = sorted(ckpt_manager.all_steps())\n",
        "\n",
        "    if not steps:\n",
        "        print(f\"No checkpoints found for experiment_name={experiment_name!r}.\")\n",
        "        return\n",
        "\n",
        "    # Template states for restoring checkpoints\n",
        "    gen_tmpl, disc_tmpl, _ = setup_gan_training(\n",
        "        optimizer=optimizer,\n",
        "        key=jax.random.key(0),\n",
        "        latent_dim=latent_dim,\n",
        "    )\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    for step in steps:\n",
        "        restored = ckpt_manager.restore(\n",
        "            step,\n",
        "            args=ocp.args.StandardRestore(\n",
        "                item={\"generator\": gen_tmpl, \"discriminator\": disc_tmpl}\n",
        "            ),\n",
        "        )\n",
        "        gen_state = restored[\"generator\"]\n",
        "        disc_state = restored[\"discriminator\"]\n",
        "\n",
        "        # Use a fresh key for evaluation at each checkpoint\n",
        "        key_eval = jax.random.key(0)\n",
        "        key_eval, key_z, key_real = jax.random.split(key_eval, 3)\n",
        "\n",
        "        z_vectors = jax.random.normal(key_z, (batch_size, latent_dim))\n",
        "        real_images_batch = subsample_images_for_batch(key_real, train_data, batch_size)\n",
        "\n",
        "        g_loss = calculate_generator_loss(\n",
        "            gen_state.params,\n",
        "            disc_state.params,\n",
        "            gen_state.apply_fn,\n",
        "            disc_state.apply_fn,\n",
        "            z_vectors,\n",
        "            loss_type=loss_type,\n",
        "        )\n",
        "        d_loss = calculate_discriminator_loss(\n",
        "            disc_state.params,\n",
        "            gen_state.params,\n",
        "            gen_state.apply_fn,\n",
        "            disc_state.apply_fn,\n",
        "            z_vectors,\n",
        "            real_images_batch,\n",
        "        )\n",
        "\n",
        "        g_losses.append(float(g_loss))\n",
        "        d_losses.append(float(d_loss))\n",
        "\n",
        "    # Convert to NumPy arrays for plotting\n",
        "    g = np.array(g_losses)\n",
        "    d = np.array(d_losses)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    points = np.column_stack([d, g])  # x: discriminator, y: generator\n",
        "\n",
        "    if len(points) > 1:\n",
        "        segments = np.stack([points[:-1], points[1:]], axis=1)\n",
        "        t = np.linspace(0.0, 1.0, len(points) - 1)\n",
        "        colors = cm.coolwarm(t)\n",
        "\n",
        "        lc = LineCollection(segments, colors=colors, linewidths=1.5)\n",
        "        ax.add_collection(lc)\n",
        "\n",
        "    # Scatter the points (including first/last)\n",
        "    ax.scatter(points[:, 0], points[:, 1], c=np.linspace(0.0, 1.0, len(points)), cmap=\"coolwarm\", s=12, alpha=0.8)\n",
        "\n",
        "    if len(points) > 0:\n",
        "        ax.scatter(points[0, 0], points[0, 1], color=\"blue\", s=60, label=\"start\")\n",
        "        ax.scatter(points[-1, 0], points[-1, 1], color=\"red\", s=60, label=\"end\")\n",
        "\n",
        "    ax.set_xlabel(\"Discriminator loss\")\n",
        "    ax.set_ylabel(\"Generator loss\")\n",
        "    ax.set_title(f\"GAN training dynamics in loss space ({loss_type})\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "e52d1762",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_gan_loss_trajectory_from_checkpoints(\n",
        "    train_data=train_data,\n",
        "    optimizer=optimizer,\n",
        "    batch_size=batch_size,\n",
        "    latent_dim=latent_dim,\n",
        "    loss_type=loss_type,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "egt-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
